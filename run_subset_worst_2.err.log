nohup: ignoring input
2025-08-20 10:49:06.487733: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-20 10:49:06.489693: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 10:49:06.521060: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-20 10:49:06.521111: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-20 10:49:06.522381: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-20 10:49:06.528098: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 10:49:06.528281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-20 10:49:07.107967: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/avoid/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/avoid/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"penalty"}min=? [F ((x = (5 - 1)) & (y = (5 - 1)))] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
DEBUG:paynt.parser.jani:keeping 65660/171860 choices with non-conflicting hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 17567 states and 61860 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"rewardmodel_penalty"}min=? [F ((x = (5 - 1)) & (y = (5 - 1)))] 
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -687.3681030273438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -387.5767517089844
INFO:tools.evaluation_results_class:Average Discounted Reward = -162.04666137695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7555886736214605
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 331911.03125
INFO:tools.evaluation_results_class:Current Best Return = -687.3681030273438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7555886736214605
INFO:tools.evaluation_results_class:Average Episode Length = 345.6542473919523
INFO:tools.evaluation_results_class:Counted Episodes = 671
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 822.4153442382812
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 5.4571309089660645
INFO:agents.father_agent:Step: 10, Training loss: 5.5928521156311035
INFO:agents.father_agent:Step: 15, Training loss: 5.4207587242126465
INFO:agents.father_agent:Step: 20, Training loss: 7.339557647705078
INFO:agents.father_agent:Step: 25, Training loss: 5.713373184204102
INFO:agents.father_agent:Step: 30, Training loss: 7.341328144073486
INFO:agents.father_agent:Step: 35, Training loss: 5.096835613250732
INFO:agents.father_agent:Step: 40, Training loss: 6.390040874481201
INFO:agents.father_agent:Step: 45, Training loss: 7.001483917236328
INFO:agents.father_agent:Step: 50, Training loss: 5.080015659332275
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 55, Training loss: 5.308711528778076
INFO:agents.father_agent:Step: 60, Training loss: 6.830934047698975
INFO:agents.father_agent:Step: 65, Training loss: 7.5284342765808105
INFO:agents.father_agent:Step: 70, Training loss: 6.3043341636657715
INFO:agents.father_agent:Step: 75, Training loss: 5.1982102394104
INFO:agents.father_agent:Step: 80, Training loss: 7.534063816070557
INFO:agents.father_agent:Step: 85, Training loss: 6.074037075042725
INFO:agents.father_agent:Step: 90, Training loss: 7.558059215545654
INFO:agents.father_agent:Step: 95, Training loss: 9.403200149536133
INFO:agents.father_agent:Step: 100, Training loss: 9.929793357849121
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -175.3417205810547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 224.6582794189453
INFO:tools.evaluation_results_class:Average Discounted Reward = 140.80532836914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34099.85546875
INFO:tools.evaluation_results_class:Current Best Return = -175.3417205810547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.36336185203825
INFO:tools.evaluation_results_class:Counted Episodes = 3974
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 105, Training loss: 9.798786163330078
INFO:agents.father_agent:Step: 110, Training loss: 8.686873435974121
INFO:agents.father_agent:Step: 115, Training loss: 9.486442565917969
INFO:agents.father_agent:Step: 120, Training loss: 10.509774208068848
INFO:agents.father_agent:Step: 125, Training loss: 12.167176246643066
INFO:agents.father_agent:Step: 130, Training loss: 13.149860382080078
INFO:agents.father_agent:Step: 135, Training loss: 13.33604621887207
INFO:agents.father_agent:Step: 140, Training loss: 14.054290771484375
INFO:agents.father_agent:Step: 145, Training loss: 12.955365180969238
INFO:agents.father_agent:Step: 150, Training loss: 12.60655689239502
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 155, Training loss: 14.900924682617188
INFO:agents.father_agent:Step: 160, Training loss: 14.318451881408691
INFO:agents.father_agent:Step: 165, Training loss: 14.961260795593262
INFO:agents.father_agent:Step: 170, Training loss: 14.444662094116211
INFO:agents.father_agent:Step: 175, Training loss: 13.749639511108398
INFO:agents.father_agent:Step: 180, Training loss: 13.29824161529541
INFO:agents.father_agent:Step: 185, Training loss: 13.566843032836914
INFO:agents.father_agent:Step: 190, Training loss: 12.851439476013184
INFO:agents.father_agent:Step: 195, Training loss: 11.809492111206055
INFO:agents.father_agent:Step: 200, Training loss: 12.966689109802246
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -129.34043884277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.6595764160156
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.79913330078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20131.22265625
INFO:tools.evaluation_results_class:Current Best Return = -129.34043884277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.66551997883038
INFO:tools.evaluation_results_class:Counted Episodes = 7558
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -125.56991577148438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.4300842285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.76666259765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19026.873046875
INFO:tools.evaluation_results_class:Current Best Return = -125.56991577148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.25781556572923
INFO:tools.evaluation_results_class:Counted Episodes = 7645
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -130.10919189453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.89080810546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.81187438964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20555.52734375
INFO:tools.evaluation_results_class:Current Best Return = -130.10919189453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.72844946295786
INFO:tools.evaluation_results_class:Counted Episodes = 7262
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -126.24482727050781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.7551574707031
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.41387939453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18907.287109375
INFO:tools.evaluation_results_class:Current Best Return = -126.24482727050781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.68316313170987
INFO:tools.evaluation_results_class:Counted Episodes = 7638
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 201.33648128251616
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.93367004394531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 317.0663146972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 253.1450653076172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12852.2607421875
INFO:tools.evaluation_results_class:Current Best Return = -82.93367004394531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.748173130972454
INFO:tools.evaluation_results_class:Counted Episodes = 3558
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.58409118652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.4158935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.80313110351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17317.306640625
INFO:tools.evaluation_results_class:Current Best Return = -141.58409118652344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.38504309146511
INFO:tools.evaluation_results_class:Counted Episodes = 3597
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.00338745117188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 315.9966125488281
INFO:tools.evaluation_results_class:Average Discounted Reward = 251.66615295410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7981.0546875
INFO:tools.evaluation_results_class:Current Best Return = -84.00338745117188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.994348686069515
INFO:tools.evaluation_results_class:Counted Episodes = 3539
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -149.12127685546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.87872314453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.37152099609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45628.90234375
INFO:tools.evaluation_results_class:Current Best Return = -149.12127685546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.80235822571589
INFO:tools.evaluation_results_class:Counted Episodes = 3562
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -107.79444122314453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.20556640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.73812866210938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10409.8017578125
INFO:tools.evaluation_results_class:Current Best Return = -107.79444122314453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.32888888888889
INFO:tools.evaluation_results_class:Counted Episodes = 3600
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -204.86053466796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 195.13946533203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 131.9199676513672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23176.22265625
INFO:tools.evaluation_results_class:Current Best Return = -204.86053466796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.62996087199553
INFO:tools.evaluation_results_class:Counted Episodes = 3578
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.30469512939453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 323.6953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 261.64788818359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11782.3310546875
INFO:tools.evaluation_results_class:Current Best Return = -76.30469512939453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.454256165473346
INFO:tools.evaluation_results_class:Counted Episodes = 3771
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.97109985351562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.02890014648438
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.2743377685547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17125.767578125
INFO:tools.evaluation_results_class:Current Best Return = -145.97109985351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.46698488464598
INFO:tools.evaluation_results_class:Counted Episodes = 3771
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.91465759277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 318.0853576660156
INFO:tools.evaluation_results_class:Average Discounted Reward = 256.8304138183594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7507.85498046875
INFO:tools.evaluation_results_class:Current Best Return = -81.91465759277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.452425125894514
INFO:tools.evaluation_results_class:Counted Episodes = 3773
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -151.14044189453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.85955810546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.55435180664062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 46075.6796875
INFO:tools.evaluation_results_class:Current Best Return = -151.14044189453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.34741288278775
INFO:tools.evaluation_results_class:Counted Episodes = 3788
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -106.31253051757812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.6874694824219
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.89967346191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10114.173828125
INFO:tools.evaluation_results_class:Current Best Return = -106.31253051757812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.608533333333334
INFO:tools.evaluation_results_class:Counted Episodes = 3750
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -201.12835693359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 198.87164306640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 137.34283447265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22420.611328125
INFO:tools.evaluation_results_class:Current Best Return = -201.12835693359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.62237017310253
INFO:tools.evaluation_results_class:Counted Episodes = 3755
INFO:robust_rl.robust_rl_trainer:Iteration 2 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -140.50759887695312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.4924011230469
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.29579162597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21216.9765625
INFO:tools.evaluation_results_class:Current Best Return = -125.56991577148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.58774593952199
INFO:tools.evaluation_results_class:Counted Episodes = 7573
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.647720336914062
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 12.424437522888184
INFO:agents.father_agent:Step: 10, Training loss: 11.789220809936523
INFO:agents.father_agent:Step: 15, Training loss: 12.265420913696289
INFO:agents.father_agent:Step: 20, Training loss: 10.132111549377441
INFO:agents.father_agent:Step: 25, Training loss: 9.90392780303955
INFO:agents.father_agent:Step: 30, Training loss: 10.731410026550293
INFO:agents.father_agent:Step: 35, Training loss: 9.8353910446167
INFO:agents.father_agent:Step: 40, Training loss: 9.201559066772461
INFO:agents.father_agent:Step: 45, Training loss: 9.114295959472656
INFO:agents.father_agent:Step: 50, Training loss: 8.728938102722168
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -136.70611572265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.29388427734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.18917846679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20394.548828125
INFO:tools.evaluation_results_class:Current Best Return = -125.56991577148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.339795368874526
INFO:tools.evaluation_results_class:Counted Episodes = 7428
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.62489318847656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.3751220703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.65586853027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22823.376953125
INFO:tools.evaluation_results_class:Current Best Return = -141.62489318847656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.76787264833575
INFO:tools.evaluation_results_class:Counted Episodes = 6910
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -139.4208221435547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.5791931152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.99761962890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21818.40625
INFO:tools.evaluation_results_class:Current Best Return = -139.4208221435547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.755184486937786
INFO:tools.evaluation_results_class:Counted Episodes = 7426
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 197.71314735130093
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 3 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -147.66806030273438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.33193969726562
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.8829803466797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22471.98828125
INFO:tools.evaluation_results_class:Current Best Return = -125.56991577148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.48095109429884
INFO:tools.evaluation_results_class:Counted Episodes = 7402
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.910075187683105
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 8.697128295898438
INFO:agents.father_agent:Step: 10, Training loss: 7.52566385269165
INFO:agents.father_agent:Step: 15, Training loss: 7.602410793304443
INFO:agents.father_agent:Step: 20, Training loss: 7.724061012268066
INFO:agents.father_agent:Step: 25, Training loss: 7.724673748016357
INFO:agents.father_agent:Step: 30, Training loss: 7.044968128204346
INFO:agents.father_agent:Step: 35, Training loss: 6.486347198486328
INFO:agents.father_agent:Step: 40, Training loss: 6.054247856140137
INFO:agents.father_agent:Step: 45, Training loss: 5.634399890899658
INFO:agents.father_agent:Step: 50, Training loss: 5.1412434577941895
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -141.1471710205078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.8528137207031
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.49789428710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20967.8359375
INFO:tools.evaluation_results_class:Current Best Return = -125.56991577148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.70898954703833
INFO:tools.evaluation_results_class:Counted Episodes = 7175
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -148.19715881347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.80284118652344
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.21939086914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22194.712890625
INFO:tools.evaluation_results_class:Current Best Return = -148.19715881347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.896899569796766
INFO:tools.evaluation_results_class:Counted Episodes = 6741
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -144.10499572753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.89500427246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.50979614257812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20953.095703125
INFO:tools.evaluation_results_class:Current Best Return = -144.10499572753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.82395143487859
INFO:tools.evaluation_results_class:Counted Episodes = 7248
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 191.28517779562623
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 4 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -146.4576873779297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.5423126220703
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.29934692382812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21187.173828125
INFO:tools.evaluation_results_class:Current Best Return = -125.56991577148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.452707381249134
INFO:tools.evaluation_results_class:Counted Episodes = 7221
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.739163875579834
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 5.149921417236328
INFO:agents.father_agent:Step: 10, Training loss: 4.537121772766113
INFO:agents.father_agent:Step: 15, Training loss: 3.9869978427886963
INFO:agents.father_agent:Step: 20, Training loss: 3.7810072898864746
INFO:agents.father_agent:Step: 25, Training loss: 3.2493419647216797
INFO:agents.father_agent:Step: 30, Training loss: 3.108502149581909
INFO:agents.father_agent:Step: 35, Training loss: 2.5474212169647217
INFO:agents.father_agent:Step: 40, Training loss: 2.1112172603607178
INFO:agents.father_agent:Step: 45, Training loss: 2.006261110305786
INFO:agents.father_agent:Step: 50, Training loss: 2.242753505706787
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -131.58837890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.41162109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.58309936523438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18396.5703125
INFO:tools.evaluation_results_class:Current Best Return = -125.56991577148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.240202988440934
INFO:tools.evaluation_results_class:Counted Episodes = 7094
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.2666473388672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.7333679199219
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.50799560546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20799.396484375
INFO:tools.evaluation_results_class:Current Best Return = -141.2666473388672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.95481569560047
INFO:tools.evaluation_results_class:Counted Episodes = 6728
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.29515075683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.704833984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.5911102294922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21047.701171875
INFO:tools.evaluation_results_class:Current Best Return = -141.29515075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.028876856865196
INFO:tools.evaluation_results_class:Counted Episodes = 7203
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 173.89309925917746
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 5 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.84451293945312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.1554870605469
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.2889404296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17844.34765625
INFO:tools.evaluation_results_class:Current Best Return = -125.56991577148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.69041974619997
INFO:tools.evaluation_results_class:Counted Episodes = 7171
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.8902102708816528
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 2.0098798274993896
INFO:agents.father_agent:Step: 10, Training loss: 1.7372043132781982
INFO:agents.father_agent:Step: 15, Training loss: 1.9425971508026123
INFO:agents.father_agent:Step: 20, Training loss: 1.4921051263809204
INFO:agents.father_agent:Step: 25, Training loss: 2.0325911045074463
INFO:agents.father_agent:Step: 30, Training loss: 1.9395842552185059
INFO:agents.father_agent:Step: 35, Training loss: 1.949607491493225
INFO:agents.father_agent:Step: 40, Training loss: 2.0103633403778076
INFO:agents.father_agent:Step: 45, Training loss: 1.681742548942566
INFO:agents.father_agent:Step: 50, Training loss: 1.8925772905349731
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -118.57335662841797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.4266357421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.56504821777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14206.767578125
INFO:tools.evaluation_results_class:Current Best Return = -118.57335662841797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.63001531393568
INFO:tools.evaluation_results_class:Counted Episodes = 6530
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -129.87863159179688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.1213684082031
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.55567932128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14859.9619140625
INFO:tools.evaluation_results_class:Current Best Return = -129.87863159179688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.20939554272888
INFO:tools.evaluation_results_class:Counted Episodes = 6237
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -133.86318969726562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.1368103027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.70419311523438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17569.03125
INFO:tools.evaluation_results_class:Current Best Return = -133.86318969726562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.648145475537596
INFO:tools.evaluation_results_class:Counted Episodes = 6929
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 164.64195155927086
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 6 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -122.41411590576172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.58587646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.67616271972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14867.025390625
INFO:tools.evaluation_results_class:Current Best Return = -118.57335662841797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.345200060855014
INFO:tools.evaluation_results_class:Counted Episodes = 6573
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.8479621410369873
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 1.8202922344207764
INFO:agents.father_agent:Step: 10, Training loss: 2.1217827796936035
INFO:agents.father_agent:Step: 15, Training loss: 2.213282346725464
INFO:agents.father_agent:Step: 20, Training loss: 1.914052128791809
INFO:agents.father_agent:Step: 25, Training loss: 2.0148613452911377
INFO:agents.father_agent:Step: 30, Training loss: 2.2905771732330322
INFO:agents.father_agent:Step: 35, Training loss: 1.9593554735183716
INFO:agents.father_agent:Step: 40, Training loss: 2.0009067058563232
INFO:agents.father_agent:Step: 45, Training loss: 1.6543278694152832
INFO:agents.father_agent:Step: 50, Training loss: 1.6745256185531616
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -96.48362731933594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 303.516357421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.1160125732422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11469.6396484375
INFO:tools.evaluation_results_class:Current Best Return = -96.48362731933594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.66676410406314
INFO:tools.evaluation_results_class:Counted Episodes = 6842
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -111.59351348876953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 288.406494140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 218.51724243164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14854.9296875
INFO:tools.evaluation_results_class:Current Best Return = -111.59351348876953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.93111144174974
INFO:tools.evaluation_results_class:Counted Episodes = 6721
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -125.2837905883789
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.7162170410156
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.959228515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16274.6005859375
INFO:tools.evaluation_results_class:Current Best Return = -125.2837905883789
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.45406012924979
INFO:tools.evaluation_results_class:Counted Episodes = 7118
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 144.08723707106893
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.608978271484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 336.3910217285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 250.98851013183594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8453.466796875
INFO:tools.evaluation_results_class:Current Best Return = -63.608978271484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.48196469685342
INFO:tools.evaluation_results_class:Counted Episodes = 2606
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -103.01900482177734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 296.9809875488281
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.0384979248047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10011.884765625
INFO:tools.evaluation_results_class:Current Best Return = -103.01900482177734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.029860875466575
INFO:tools.evaluation_results_class:Counted Episodes = 2947
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -47.05897521972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 352.9410400390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 266.4584655761719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4584.98681640625
INFO:tools.evaluation_results_class:Current Best Return = -47.05897521972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.343283582089555
INFO:tools.evaluation_results_class:Counted Episodes = 2747
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -38.070926666259766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 361.9290771484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 269.09527587890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10916.990234375
INFO:tools.evaluation_results_class:Current Best Return = -38.070926666259766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.120810600155885
INFO:tools.evaluation_results_class:Counted Episodes = 2566
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -89.5583724975586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 310.4416198730469
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.60874938964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8270.919921875
INFO:tools.evaluation_results_class:Current Best Return = -89.5583724975586
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.95397631133672
INFO:tools.evaluation_results_class:Counted Episodes = 2955
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -130.66432189941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.335693359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.2526092529297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14726.775390625
INFO:tools.evaluation_results_class:Current Best Return = -130.66432189941406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.39284746681116
INFO:tools.evaluation_results_class:Counted Episodes = 3691
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -132.41522216796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.58477783203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.69996643066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15181.001953125
INFO:tools.evaluation_results_class:Current Best Return = -132.41522216796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.417391304347824
INFO:tools.evaluation_results_class:Counted Episodes = 3680
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -131.73654174804688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.2634582519531
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.6032257080078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15239.5498046875
INFO:tools.evaluation_results_class:Current Best Return = -131.73654174804688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.267784690289425
INFO:tools.evaluation_results_class:Counted Episodes = 3697
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -132.2333221435547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.76666259765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.3430633544922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15441.0283203125
INFO:tools.evaluation_results_class:Current Best Return = -132.2333221435547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.123683499864974
INFO:tools.evaluation_results_class:Counted Episodes = 3703
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -127.79253387451172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.20745849609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.5023651123047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18898.240234375
INFO:tools.evaluation_results_class:Current Best Return = -127.79253387451172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.654622101776575
INFO:tools.evaluation_results_class:Counted Episodes = 3321
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -129.77691650390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.22308349609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.5846405029297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14927.830078125
INFO:tools.evaluation_results_class:Current Best Return = -129.77691650390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.07427341227126
INFO:tools.evaluation_results_class:Counted Episodes = 3716
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.73316955566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 332.266845703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 260.4059753417969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10776.6982421875
INFO:tools.evaluation_results_class:Current Best Return = -67.73316955566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.365800187911056
INFO:tools.evaluation_results_class:Counted Episodes = 3193
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -126.40137481689453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.5986328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.26756286621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13482.099609375
INFO:tools.evaluation_results_class:Current Best Return = -126.40137481689453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.637614678899084
INFO:tools.evaluation_results_class:Counted Episodes = 3488
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.1501693725586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 328.8498229980469
INFO:tools.evaluation_results_class:Average Discounted Reward = 258.156005859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7676.1416015625
INFO:tools.evaluation_results_class:Current Best Return = -71.1501693725586
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.67877686951257
INFO:tools.evaluation_results_class:Counted Episodes = 3303
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.79804992675781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 319.2019348144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.99330139160156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25398.859375
INFO:tools.evaluation_results_class:Current Best Return = -80.79804992675781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.48869346733668
INFO:tools.evaluation_results_class:Counted Episodes = 3184
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -98.81372833251953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 301.186279296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.93450927734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8821.951171875
INFO:tools.evaluation_results_class:Current Best Return = -98.81372833251953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.89273356401384
INFO:tools.evaluation_results_class:Counted Episodes = 3468
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.1334228515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.8665771484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.61534118652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16467.16015625
INFO:tools.evaluation_results_class:Current Best Return = -141.1334228515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.81972602739726
INFO:tools.evaluation_results_class:Counted Episodes = 3650
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.16241455078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.83758544921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.2597198486328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17095.8828125
INFO:tools.evaluation_results_class:Current Best Return = -143.16241455078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.98323715306403
INFO:tools.evaluation_results_class:Counted Episodes = 3639
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.96527099609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.03472900390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.92822265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15639.8662109375
INFO:tools.evaluation_results_class:Current Best Return = -142.96527099609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.72354388843314
INFO:tools.evaluation_results_class:Counted Episodes = 3657
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.79180908203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.20819091796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.6402130126953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17212.970703125
INFO:tools.evaluation_results_class:Current Best Return = -145.79180908203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.22923588039867
INFO:tools.evaluation_results_class:Counted Episodes = 3612
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -140.84226989746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.15771484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.571533203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21191.751953125
INFO:tools.evaluation_results_class:Current Best Return = -140.84226989746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.56897046171494
INFO:tools.evaluation_results_class:Counted Episodes = 3487
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.02297973632812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.9770202636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.76373291015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16155.4619140625
INFO:tools.evaluation_results_class:Current Best Return = -141.02297973632812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.76333789329686
INFO:tools.evaluation_results_class:Counted Episodes = 3655
INFO:robust_rl.robust_rl_trainer:Iteration 7 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -102.5500717163086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 297.4499206542969
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.47702026367188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12341.775390625
INFO:tools.evaluation_results_class:Current Best Return = -96.48362731933594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.90444763271162
INFO:tools.evaluation_results_class:Counted Episodes = 6970
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.7148094177246094
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 1.4523981809616089
INFO:agents.father_agent:Step: 10, Training loss: 1.523729681968689
INFO:agents.father_agent:Step: 15, Training loss: 1.299924373626709
INFO:agents.father_agent:Step: 20, Training loss: 1.4653233289718628
INFO:agents.father_agent:Step: 25, Training loss: 1.3754709959030151
INFO:agents.father_agent:Step: 30, Training loss: 1.119699478149414
INFO:agents.father_agent:Step: 35, Training loss: 1.386449933052063
INFO:agents.father_agent:Step: 40, Training loss: 1.2165783643722534
INFO:agents.father_agent:Step: 45, Training loss: 1.3299189805984497
INFO:agents.father_agent:Step: 50, Training loss: 1.170699954032898
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -93.39095306396484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 305.79541015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.72683715820312
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9980154792617583
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9524.048828125
INFO:tools.evaluation_results_class:Current Best Return = -93.39095306396484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.214923595951575
INFO:tools.evaluation_results_class:Counted Episodes = 5039
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -105.98182678222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.54254150390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.3894500732422
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988399071925754
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11736.142578125
INFO:tools.evaluation_results_class:Current Best Return = -105.98182678222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988399071925754
INFO:tools.evaluation_results_class:Average Episode Length = 57.14810518174787
INFO:tools.evaluation_results_class:Counted Episodes = 5172
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.7285614013672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.2714385986328
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.491455078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21808.76171875
INFO:tools.evaluation_results_class:Current Best Return = -155.7285614013672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.35433702100369
INFO:tools.evaluation_results_class:Counted Episodes = 6237
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 183.03003677110257
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 8 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -116.93172454833984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.0682678222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.43357849121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16039.5478515625
INFO:tools.evaluation_results_class:Current Best Return = -93.39095306396484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.88786279683377
INFO:tools.evaluation_results_class:Counted Episodes = 6064
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.6736048460006714
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 1.6825991868972778
INFO:agents.father_agent:Step: 10, Training loss: 1.498110294342041
INFO:agents.father_agent:Step: 15, Training loss: 1.6279748678207397
INFO:agents.father_agent:Step: 20, Training loss: 1.4591858386993408
INFO:agents.father_agent:Step: 25, Training loss: 1.4780945777893066
INFO:agents.father_agent:Step: 30, Training loss: 1.261664867401123
INFO:agents.father_agent:Step: 35, Training loss: 1.3868318796157837
INFO:agents.father_agent:Step: 40, Training loss: 1.1971155405044556
INFO:agents.father_agent:Step: 45, Training loss: 1.0999726057052612
INFO:agents.father_agent:Step: 50, Training loss: 1.1238516569137573
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 55, Training loss: 1.2238802909851074
INFO:agents.father_agent:Step: 60, Training loss: 1.0347808599472046
INFO:agents.father_agent:Step: 65, Training loss: 1.0681369304656982
INFO:agents.father_agent:Step: 70, Training loss: 0.9702684879302979
INFO:agents.father_agent:Step: 75, Training loss: 1.0743025541305542
INFO:agents.father_agent:Step: 80, Training loss: 1.0364433526992798
INFO:agents.father_agent:Step: 85, Training loss: 1.0488531589508057
INFO:agents.father_agent:Step: 90, Training loss: 0.9279797673225403
INFO:agents.father_agent:Step: 95, Training loss: 0.8544421792030334
INFO:agents.father_agent:Step: 100, Training loss: 0.7784105539321899
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.90192413330078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 323.09808349609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 250.71902465820312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8049.451171875
INFO:tools.evaluation_results_class:Current Best Return = -76.90192413330078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.18621973929236
INFO:tools.evaluation_results_class:Counted Episodes = 6444
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -74.60479736328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 325.39520263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 251.90679931640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7181.3349609375
INFO:tools.evaluation_results_class:Current Best Return = -74.60479736328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.77002664994513
INFO:tools.evaluation_results_class:Counted Episodes = 6379
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -78.74476623535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 321.2552185058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.07113647460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7943.115234375
INFO:tools.evaluation_results_class:Current Best Return = -78.74476623535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.83062141639547
INFO:tools.evaluation_results_class:Counted Episodes = 6453
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.3665771484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.6334228515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.8345489501953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26186.103515625
INFO:tools.evaluation_results_class:Current Best Return = -166.3665771484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.65854396851824
INFO:tools.evaluation_results_class:Counted Episodes = 6607
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 198.5704561093684
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 9 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -102.99382019042969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 297.0061950683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.43917846679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16852.20703125
INFO:tools.evaluation_results_class:Current Best Return = -74.60479736328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.97138749101366
INFO:tools.evaluation_results_class:Counted Episodes = 6955
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.5816181898117065
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 1.4647183418273926
INFO:agents.father_agent:Step: 10, Training loss: 1.437322974205017
INFO:agents.father_agent:Step: 15, Training loss: 1.4011294841766357
INFO:agents.father_agent:Step: 20, Training loss: 1.4506640434265137
INFO:agents.father_agent:Step: 25, Training loss: 1.4671399593353271
INFO:agents.father_agent:Step: 30, Training loss: 1.0390442609786987
INFO:agents.father_agent:Step: 35, Training loss: 1.2223750352859497
INFO:agents.father_agent:Step: 40, Training loss: 0.9992073178291321
INFO:agents.father_agent:Step: 45, Training loss: 1.0513336658477783
INFO:agents.father_agent:Step: 50, Training loss: 0.9475524425506592
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 55, Training loss: 0.9330921173095703
INFO:agents.father_agent:Step: 60, Training loss: 0.9147344827651978
INFO:agents.father_agent:Step: 65, Training loss: 0.8782901167869568
INFO:agents.father_agent:Step: 70, Training loss: 0.7562635540962219
INFO:agents.father_agent:Step: 75, Training loss: 0.8546719551086426
INFO:agents.father_agent:Step: 80, Training loss: 0.8125759959220886
INFO:agents.father_agent:Step: 85, Training loss: 0.7243844270706177
INFO:agents.father_agent:Step: 90, Training loss: 0.7894080877304077
INFO:agents.father_agent:Step: 95, Training loss: 0.65395587682724
INFO:agents.father_agent:Step: 100, Training loss: 0.676719069480896
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -100.96106719970703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 299.0389404296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 219.9688720703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12542.4521484375
INFO:tools.evaluation_results_class:Current Best Return = -74.60479736328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.738745387453875
INFO:tools.evaluation_results_class:Counted Episodes = 5420
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -99.2821044921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.7178955078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.0217742919922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12081.447265625
INFO:tools.evaluation_results_class:Current Best Return = -74.60479736328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.3128778164499
INFO:tools.evaluation_results_class:Counted Episodes = 5459
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -102.13751220703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 297.86248779296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 218.65904235839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13914.96484375
INFO:tools.evaluation_results_class:Current Best Return = -102.13751220703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.94400875433157
INFO:tools.evaluation_results_class:Counted Episodes = 5483
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -192.80470275878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 207.19529724121094
INFO:tools.evaluation_results_class:Average Discounted Reward = 132.08154296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35900.53125
INFO:tools.evaluation_results_class:Current Best Return = -192.80470275878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.838416366736126
INFO:tools.evaluation_results_class:Counted Episodes = 5279
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 222.8352318803323
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 10 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -111.26161193847656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 288.7384033203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.61070251464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18029.556640625
INFO:tools.evaluation_results_class:Current Best Return = -74.60479736328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.75706594885599
INFO:tools.evaluation_results_class:Counted Episodes = 5944
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.4468872547149658
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 1.4512391090393066
INFO:agents.father_agent:Step: 10, Training loss: 1.3261460065841675
INFO:agents.father_agent:Step: 15, Training loss: 1.2874642610549927
INFO:agents.father_agent:Step: 20, Training loss: 1.2675697803497314
INFO:agents.father_agent:Step: 25, Training loss: 1.0628355741500854
INFO:agents.father_agent:Step: 30, Training loss: 1.409671425819397
INFO:agents.father_agent:Step: 35, Training loss: 1.291064739227295
INFO:agents.father_agent:Step: 40, Training loss: 0.9950326681137085
INFO:agents.father_agent:Step: 45, Training loss: 1.1454130411148071
INFO:agents.father_agent:Step: 50, Training loss: 0.9807918667793274
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 55, Training loss: 0.6314178705215454
INFO:agents.father_agent:Step: 60, Training loss: 0.7362412810325623
INFO:agents.father_agent:Step: 65, Training loss: 0.671994149684906
INFO:agents.father_agent:Step: 70, Training loss: 0.6916661858558655
INFO:agents.father_agent:Step: 75, Training loss: 0.6118281483650208
INFO:agents.father_agent:Step: 80, Training loss: 0.5492561459541321
INFO:agents.father_agent:Step: 85, Training loss: 0.7093717455863953
INFO:agents.father_agent:Step: 90, Training loss: 0.8317016363143921
INFO:agents.father_agent:Step: 95, Training loss: 0.5950323939323425
INFO:agents.father_agent:Step: 100, Training loss: 0.4431802034378052
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -78.99105072021484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 321.0089416503906
INFO:tools.evaluation_results_class:Average Discounted Reward = 252.9261474609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7681.837890625
INFO:tools.evaluation_results_class:Current Best Return = -74.60479736328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.99618712421176
INFO:tools.evaluation_results_class:Counted Episodes = 6819
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -81.31515502929688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 318.6848449707031
INFO:tools.evaluation_results_class:Average Discounted Reward = 250.9651641845703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9656.884765625
INFO:tools.evaluation_results_class:Current Best Return = -74.60479736328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.073202998677054
INFO:tools.evaluation_results_class:Counted Episodes = 6803
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.88346862792969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 315.11651611328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.96548461914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10394.94140625
INFO:tools.evaluation_results_class:Current Best Return = -84.88346862792969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.5674720094284
INFO:tools.evaluation_results_class:Counted Episodes = 6788
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.7992401123047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.2007598876953
INFO:tools.evaluation_results_class:Average Discounted Reward = 160.85791015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28336.416015625
INFO:tools.evaluation_results_class:Current Best Return = -170.7992401123047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.21706398996236
INFO:tools.evaluation_results_class:Counted Episodes = 6376
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 203.36799864825946
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 11 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -100.13883972167969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 299.8611755371094
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.38438415527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12833.734375
INFO:tools.evaluation_results_class:Current Best Return = -74.60479736328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.65353014900432
INFO:tools.evaluation_results_class:Counted Episodes = 7181
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.2713308334350586
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 2.3270514011383057
INFO:agents.father_agent:Step: 10, Training loss: 2.0041284561157227
INFO:agents.father_agent:Step: 15, Training loss: 1.7665706872940063
INFO:agents.father_agent:Step: 20, Training loss: 1.5393482446670532
INFO:agents.father_agent:Step: 25, Training loss: 1.380423903465271
INFO:agents.father_agent:Step: 30, Training loss: 1.0792438983917236
INFO:agents.father_agent:Step: 35, Training loss: 0.9215567708015442
INFO:agents.father_agent:Step: 40, Training loss: 0.7322413921356201
INFO:agents.father_agent:Step: 45, Training loss: 0.7223635911941528
INFO:agents.father_agent:Step: 50, Training loss: 0.8252215385437012
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 55, Training loss: 0.6652360558509827
INFO:agents.father_agent:Step: 60, Training loss: 0.631783664226532
INFO:agents.father_agent:Step: 65, Training loss: 0.6089236736297607
INFO:agents.father_agent:Step: 70, Training loss: 0.6021750569343567
INFO:agents.father_agent:Step: 75, Training loss: 0.5831199288368225
INFO:agents.father_agent:Step: 80, Training loss: 0.5581710338592529
INFO:agents.father_agent:Step: 85, Training loss: 0.8520468473434448
INFO:agents.father_agent:Step: 90, Training loss: 0.6657866835594177
INFO:agents.father_agent:Step: 95, Training loss: 0.7782962322235107
INFO:agents.father_agent:Step: 100, Training loss: 0.6326902508735657
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.19602966308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 349.8039855957031
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.5342102050781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13112.0732421875
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.15477258167841
INFO:tools.evaluation_results_class:Counted Episodes = 7805
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -50.54643249511719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 349.4535827636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.3859558105469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18538.66015625
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.53412770366533
INFO:tools.evaluation_results_class:Counted Episodes = 7721
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.93053436279297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 347.0694580078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 283.82757568359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15442.36328125
INFO:tools.evaluation_results_class:Current Best Return = -52.93053436279297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.32516616707937
INFO:tools.evaluation_results_class:Counted Episodes = 7673
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.14169311523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 221.85830688476562
INFO:tools.evaluation_results_class:Average Discounted Reward = 160.39566040039062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31017.099609375
INFO:tools.evaluation_results_class:Current Best Return = -178.14169311523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.839248722552135
INFO:tools.evaluation_results_class:Counted Episodes = 7241
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 195.32666475220577
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.949501037597656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 340.0505065917969
INFO:tools.evaluation_results_class:Average Discounted Reward = 284.38330078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 47042.328125
INFO:tools.evaluation_results_class:Current Best Return = -59.949501037597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.952618453865334
INFO:tools.evaluation_results_class:Counted Episodes = 3208
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -88.11572265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 311.88427734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 250.4946746826172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5216.740234375
INFO:tools.evaluation_results_class:Current Best Return = -88.11572265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.4998588766582
INFO:tools.evaluation_results_class:Counted Episodes = 3543
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.17264938354492
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 347.8273620605469
INFO:tools.evaluation_results_class:Average Discounted Reward = 286.4124755859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20508.01953125
INFO:tools.evaluation_results_class:Current Best Return = -52.17264938354492
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.437928199506715
INFO:tools.evaluation_results_class:Counted Episodes = 3649
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -113.43114471435547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 286.4416809082031
INFO:tools.evaluation_results_class:Average Discounted Reward = 251.93849182128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9996898263027295
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 199108.890625
INFO:tools.evaluation_results_class:Current Best Return = -113.43114471435547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996898263027295
INFO:tools.evaluation_results_class:Average Episode Length = 43.58157568238214
INFO:tools.evaluation_results_class:Counted Episodes = 3224
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -45.34731674194336
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 354.6526794433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 288.3913879394531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2722.71923828125
INFO:tools.evaluation_results_class:Current Best Return = -45.34731674194336
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.95695995641515
INFO:tools.evaluation_results_class:Counted Episodes = 3671
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -42.25654220581055
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 357.74346923828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 292.7767028808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3951.057373046875
INFO:tools.evaluation_results_class:Current Best Return = -42.25654220581055
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.01451153148484
INFO:tools.evaluation_results_class:Counted Episodes = 3859
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.79021453857422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 356.20977783203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 290.6585693359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4347.49169921875
INFO:tools.evaluation_results_class:Current Best Return = -43.79021453857422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.47683498178032
INFO:tools.evaluation_results_class:Counted Episodes = 3842
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -41.98523712158203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 358.0147705078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 292.24267578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4286.74560546875
INFO:tools.evaluation_results_class:Current Best Return = -41.98523712158203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.55011655011655
INFO:tools.evaluation_results_class:Counted Episodes = 3861
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -42.57069778442383
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 357.4292907714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 292.9268798828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3877.692626953125
INFO:tools.evaluation_results_class:Current Best Return = -42.57069778442383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.742725880551305
INFO:tools.evaluation_results_class:Counted Episodes = 3918
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.15463256835938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 320.8453674316406
INFO:tools.evaluation_results_class:Average Discounted Reward = 264.1449279785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31892.896484375
INFO:tools.evaluation_results_class:Current Best Return = -79.15463256835938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.7922967189729
INFO:tools.evaluation_results_class:Counted Episodes = 3505
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -41.17466354370117
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 358.8253479003906
INFO:tools.evaluation_results_class:Average Discounted Reward = 293.3682556152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3898.60595703125
INFO:tools.evaluation_results_class:Current Best Return = -41.17466354370117
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.27760577915377
INFO:tools.evaluation_results_class:Counted Episodes = 3876
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -41.08440399169922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 358.91558837890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 293.53271484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3783.549072265625
INFO:tools.evaluation_results_class:Current Best Return = -41.08440399169922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.13381369016984
INFO:tools.evaluation_results_class:Counted Episodes = 3886
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -42.259098052978516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 357.74090576171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 292.694580078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4095.64892578125
INFO:tools.evaluation_results_class:Current Best Return = -42.259098052978516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.9087647360328
INFO:tools.evaluation_results_class:Counted Episodes = 3902
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.81354522705078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 344.18646240234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 280.5545349121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4756.86328125
INFO:tools.evaluation_results_class:Current Best Return = -55.81354522705078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.2369302086016
INFO:tools.evaluation_results_class:Counted Episodes = 3883
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -41.62099075317383
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 358.3789978027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 293.23394775390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3807.87744140625
INFO:tools.evaluation_results_class:Current Best Return = -41.62099075317383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.06466512702079
INFO:tools.evaluation_results_class:Counted Episodes = 3897
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.1502799987793
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 343.8497314453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 280.69696044921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5135.63330078125
INFO:tools.evaluation_results_class:Current Best Return = -56.1502799987793
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.691756272401435
INFO:tools.evaluation_results_class:Counted Episodes = 3906
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.61784362792969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 325.38214111328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 263.4476318359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8924.7451171875
INFO:tools.evaluation_results_class:Current Best Return = -74.61784362792969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.75650364203954
INFO:tools.evaluation_results_class:Counted Episodes = 3844
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -161.0302276611328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.9697723388672
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.67140197753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20614.158203125
INFO:tools.evaluation_results_class:Current Best Return = -161.0302276611328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.2093023255814
INFO:tools.evaluation_results_class:Counted Episodes = 3440
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -108.82723236083984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 291.1727600097656
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.83700561523438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16521.958984375
INFO:tools.evaluation_results_class:Current Best Return = -108.82723236083984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.38992201559688
INFO:tools.evaluation_results_class:Counted Episodes = 3334
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -146.3478240966797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.6521759033203
INFO:tools.evaluation_results_class:Average Discounted Reward = 206.42591857910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40227.7734375
INFO:tools.evaluation_results_class:Current Best Return = -146.3478240966797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.30034982508746
INFO:tools.evaluation_results_class:Counted Episodes = 4002
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -114.31682586669922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.68316650390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.8656768798828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14265.0849609375
INFO:tools.evaluation_results_class:Current Best Return = -114.31682586669922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.486548368631944
INFO:tools.evaluation_results_class:Counted Episodes = 3494
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -195.98585510253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 204.01414489746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 141.59902954101562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31691.12109375
INFO:tools.evaluation_results_class:Current Best Return = -195.98585510253906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.20155253673413
INFO:tools.evaluation_results_class:Counted Episodes = 3607
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -193.6878662109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 206.3121337890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 143.3243408203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31252.06640625
INFO:tools.evaluation_results_class:Current Best Return = -193.6878662109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.410878661087864
INFO:tools.evaluation_results_class:Counted Episodes = 3585
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -197.36526489257812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 202.63473510742188
INFO:tools.evaluation_results_class:Average Discounted Reward = 140.31332397460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34652.74609375
INFO:tools.evaluation_results_class:Current Best Return = -197.36526489257812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.452666852834405
INFO:tools.evaluation_results_class:Counted Episodes = 3581
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.44515991210938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 209.55484008789062
INFO:tools.evaluation_results_class:Average Discounted Reward = 146.47903442382812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30951.697265625
INFO:tools.evaluation_results_class:Current Best Return = -190.44515991210938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.22383073496659
INFO:tools.evaluation_results_class:Counted Episodes = 3592
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.0002899169922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 226.9997100830078
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.32565307617188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31192.568359375
INFO:tools.evaluation_results_class:Current Best Return = -173.0002899169922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.658999424956875
INFO:tools.evaluation_results_class:Counted Episodes = 3478
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -195.2509002685547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 204.7490997314453
INFO:tools.evaluation_results_class:Average Discounted Reward = 142.3482208251953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33226.51171875
INFO:tools.evaluation_results_class:Current Best Return = -195.2509002685547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.22876901798064
INFO:tools.evaluation_results_class:Counted Episodes = 3615
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -187.1881103515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 212.8118896484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 149.45716857910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30245.396484375
INFO:tools.evaluation_results_class:Current Best Return = -187.1881103515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.374270630730756
INFO:tools.evaluation_results_class:Counted Episodes = 3599
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.12530517578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 208.87469482421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 146.0160675048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29993.505859375
INFO:tools.evaluation_results_class:Current Best Return = -191.12530517578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.27696146382035
INFO:tools.evaluation_results_class:Counted Episodes = 3607
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.0636749267578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 208.9363250732422
INFO:tools.evaluation_results_class:Average Discounted Reward = 146.61489868164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29248.845703125
INFO:tools.evaluation_results_class:Current Best Return = -191.0636749267578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.867004790081715
INFO:tools.evaluation_results_class:Counted Episodes = 3549
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -194.49388122558594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 205.50611877441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 142.67440795898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31594.390625
INFO:tools.evaluation_results_class:Current Best Return = -194.49388122558594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.49638688160089
INFO:tools.evaluation_results_class:Counted Episodes = 3598
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -189.30850219726562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 210.69149780273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 148.05674743652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30330.2421875
INFO:tools.evaluation_results_class:Current Best Return = -189.30850219726562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.9808072255151
INFO:tools.evaluation_results_class:Counted Episodes = 3543
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 12 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -305.7569580078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.73098373413086
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.218475341796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9109461966604824
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 576055.0
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 202.26716141001856
INFO:tools.evaluation_results_class:Counted Episodes = 1078
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.4293562173843384
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 1.5606509447097778
INFO:agents.father_agent:Step: 10, Training loss: 1.967985987663269
INFO:agents.father_agent:Step: 15, Training loss: 2.8009707927703857
INFO:agents.father_agent:Step: 20, Training loss: 3.251885175704956
INFO:agents.father_agent:Step: 25, Training loss: 3.6269774436950684
INFO:agents.father_agent:Step: 30, Training loss: 3.11970591545105
INFO:agents.father_agent:Step: 35, Training loss: 2.4167160987854004
INFO:agents.father_agent:Step: 40, Training loss: 1.9977600574493408
INFO:agents.father_agent:Step: 45, Training loss: 1.708481788635254
INFO:agents.father_agent:Step: 50, Training loss: 1.3781052827835083
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 55, Training loss: 1.0038963556289673
INFO:agents.father_agent:Step: 60, Training loss: 0.8731141090393066
INFO:agents.father_agent:Step: 65, Training loss: 0.7502021789550781
INFO:agents.father_agent:Step: 70, Training loss: 0.6140508651733398
INFO:agents.father_agent:Step: 75, Training loss: 0.6276983022689819
INFO:agents.father_agent:Step: 80, Training loss: 0.6576697826385498
INFO:agents.father_agent:Step: 85, Training loss: 0.7702600955963135
INFO:agents.father_agent:Step: 90, Training loss: 0.7090001702308655
INFO:agents.father_agent:Step: 95, Training loss: 0.48341551423072815
INFO:agents.father_agent:Step: 100, Training loss: 0.6414197087287903
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.973854064941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 347.0261535644531
INFO:tools.evaluation_results_class:Average Discounted Reward = 283.21331787109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5713.67236328125
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.65223917938454
INFO:tools.evaluation_results_class:Counted Episodes = 7994
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -55.11295700073242
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 344.8870544433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 281.5066833496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6280.361328125
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.55916125811283
INFO:tools.evaluation_results_class:Counted Episodes = 8012
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.955780029296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 340.0442199707031
INFO:tools.evaluation_results_class:Average Discounted Reward = 275.5462951660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7058.43701171875
INFO:tools.evaluation_results_class:Current Best Return = -59.955780029296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.552986022871664
INFO:tools.evaluation_results_class:Counted Episodes = 7870
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -179.50262451171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 220.49737548828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 159.5360565185547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27626.09375
INFO:tools.evaluation_results_class:Current Best Return = -179.50262451171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.8714073674268
INFO:tools.evaluation_results_class:Counted Episodes = 7411
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 197.49224333286114
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 13 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -383.3116760253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 15.039544105529785
INFO:tools.evaluation_results_class:Average Discounted Reward = -72.6826171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9959785522788204
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 125697.6484375
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 165.2319034852547
INFO:tools.evaluation_results_class:Counted Episodes = 1492
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.0219900608062744
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 2.38022518157959
INFO:agents.father_agent:Step: 10, Training loss: 2.8873448371887207
INFO:agents.father_agent:Step: 15, Training loss: 2.8784050941467285
INFO:agents.father_agent:Step: 20, Training loss: 2.6351494789123535
INFO:agents.father_agent:Step: 25, Training loss: 2.0975542068481445
INFO:agents.father_agent:Step: 30, Training loss: 1.8970118761062622
INFO:agents.father_agent:Step: 35, Training loss: 1.5027616024017334
INFO:agents.father_agent:Step: 40, Training loss: 1.6254661083221436
INFO:agents.father_agent:Step: 45, Training loss: 1.4709073305130005
INFO:agents.father_agent:Step: 50, Training loss: 1.166748046875
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 55, Training loss: 1.1457592248916626
INFO:agents.father_agent:Step: 60, Training loss: 0.8369643092155457
INFO:agents.father_agent:Step: 65, Training loss: 0.9954957962036133
INFO:agents.father_agent:Step: 70, Training loss: 0.8936324119567871
INFO:agents.father_agent:Step: 75, Training loss: 0.7144972085952759
INFO:agents.father_agent:Step: 80, Training loss: 0.7104464769363403
INFO:agents.father_agent:Step: 85, Training loss: 0.7658663392066956
INFO:agents.father_agent:Step: 90, Training loss: 0.696327805519104
INFO:agents.father_agent:Step: 95, Training loss: 0.6326948404312134
INFO:agents.father_agent:Step: 100, Training loss: 0.7328024506568909
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.51436614990234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 333.4856262207031
INFO:tools.evaluation_results_class:Average Discounted Reward = 267.0904541015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6500.20654296875
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.38122217725617
INFO:tools.evaluation_results_class:Counted Episodes = 7413
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -67.02715301513672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 332.97283935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 266.4576416015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6513.58349609375
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.56328515466703
INFO:tools.evaluation_results_class:Counted Episodes = 7403
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.16703033447266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 328.8329772949219
INFO:tools.evaluation_results_class:Average Discounted Reward = 261.65399169921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7873.85595703125
INFO:tools.evaluation_results_class:Current Best Return = -71.16703033447266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.11370420624152
INFO:tools.evaluation_results_class:Counted Episodes = 7370
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -183.1056671142578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 216.8943328857422
INFO:tools.evaluation_results_class:Average Discounted Reward = 153.92384338378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29087.46875
INFO:tools.evaluation_results_class:Current Best Return = -183.1056671142578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.953758865248226
INFO:tools.evaluation_results_class:Counted Episodes = 7050
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 201.3646619761283
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 14 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -140.33316040039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.6668395996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.20115661621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27013.60546875
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.54597393322621
INFO:tools.evaluation_results_class:Counted Episodes = 5601
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.3148070573806763
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 1.4300365447998047
INFO:agents.father_agent:Step: 10, Training loss: 1.3744795322418213
INFO:agents.father_agent:Step: 15, Training loss: 1.6636962890625
INFO:agents.father_agent:Step: 20, Training loss: 1.8068079948425293
INFO:agents.father_agent:Step: 25, Training loss: 1.548142671585083
INFO:agents.father_agent:Step: 30, Training loss: 1.5044424533843994
INFO:agents.father_agent:Step: 35, Training loss: 1.4316762685775757
INFO:agents.father_agent:Step: 40, Training loss: 1.3556849956512451
INFO:agents.father_agent:Step: 45, Training loss: 1.5478099584579468
INFO:agents.father_agent:Step: 50, Training loss: 0.9542419910430908
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 55, Training loss: 1.1222349405288696
INFO:agents.father_agent:Step: 60, Training loss: 0.9122360944747925
INFO:agents.father_agent:Step: 65, Training loss: 0.7338025569915771
INFO:agents.father_agent:Step: 70, Training loss: 0.9257851839065552
INFO:agents.father_agent:Step: 75, Training loss: 0.8248211741447449
INFO:agents.father_agent:Step: 80, Training loss: 0.680425226688385
INFO:agents.father_agent:Step: 85, Training loss: 0.7043942213058472
INFO:agents.father_agent:Step: 90, Training loss: 0.5970943570137024
INFO:agents.father_agent:Step: 95, Training loss: 0.5648679733276367
INFO:agents.father_agent:Step: 100, Training loss: 0.7480590343475342
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.973541259765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 341.0264587402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 275.94140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6501.12353515625
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.835828600929275
INFO:tools.evaluation_results_class:Counted Episodes = 7748
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -60.14738464355469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 339.85260009765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 274.79315185546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6817.43994140625
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.93737060041408
INFO:tools.evaluation_results_class:Counted Episodes = 7728
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.97832489013672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 328.02166748046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 262.54833984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9920.595703125
INFO:tools.evaluation_results_class:Current Best Return = -71.97832489013672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.847083552285866
INFO:tools.evaluation_results_class:Counted Episodes = 7612
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -185.50860595703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 214.49139404296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 152.9132537841797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30869.72265625
INFO:tools.evaluation_results_class:Current Best Return = -185.50860595703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.30597119284016
INFO:tools.evaluation_results_class:Counted Episodes = 7151
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 204.75057768188935
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 15 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -108.38245391845703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 291.6175537109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.57473754882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29133.90234375
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.154798761609904
INFO:tools.evaluation_results_class:Counted Episodes = 4845
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.7214974761009216
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 0.7021499872207642
INFO:agents.father_agent:Step: 10, Training loss: 0.8087725639343262
INFO:agents.father_agent:Step: 15, Training loss: 1.2550580501556396
INFO:agents.father_agent:Step: 20, Training loss: 1.2412827014923096
INFO:agents.father_agent:Step: 25, Training loss: 1.6600091457366943
INFO:agents.father_agent:Step: 30, Training loss: 1.3851103782653809
INFO:agents.father_agent:Step: 35, Training loss: 1.5549958944320679
INFO:agents.father_agent:Step: 40, Training loss: 1.614501714706421
INFO:agents.father_agent:Step: 45, Training loss: 1.399269700050354
INFO:agents.father_agent:Step: 50, Training loss: 1.2840840816497803
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 55, Training loss: 1.0905250310897827
INFO:agents.father_agent:Step: 60, Training loss: 0.941838800907135
INFO:agents.father_agent:Step: 65, Training loss: 0.5816922187805176
INFO:agents.father_agent:Step: 70, Training loss: 0.6729143857955933
INFO:agents.father_agent:Step: 75, Training loss: 0.5901694297790527
INFO:agents.father_agent:Step: 80, Training loss: 0.5718463659286499
INFO:agents.father_agent:Step: 85, Training loss: 0.5605475902557373
INFO:agents.father_agent:Step: 90, Training loss: 0.8576458096504211
INFO:agents.father_agent:Step: 95, Training loss: 0.6701818704605103
INFO:agents.father_agent:Step: 100, Training loss: 0.6494607329368591
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.49183654785156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 327.5081481933594
INFO:tools.evaluation_results_class:Average Discounted Reward = 260.940185546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6081.34130859375
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.47883817427386
INFO:tools.evaluation_results_class:Counted Episodes = 7230
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -75.31874084472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 324.6812744140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 258.7337646484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6148.37451171875
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.33931777378815
INFO:tools.evaluation_results_class:Counted Episodes = 7241
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -78.62022399902344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 321.3797912597656
INFO:tools.evaluation_results_class:Average Discounted Reward = 255.03675842285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6730.9140625
INFO:tools.evaluation_results_class:Current Best Return = -78.62022399902344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.78755690440061
INFO:tools.evaluation_results_class:Counted Episodes = 7249
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -183.29827880859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 216.70172119140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 152.7544403076172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27599.322265625
INFO:tools.evaluation_results_class:Current Best Return = -183.29827880859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.7447486600029
INFO:tools.evaluation_results_class:Counted Episodes = 6903
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 197.27709983559285
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 16 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -224.84231567382812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 175.15768432617188
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.29328155517578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 46591.43359375
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.75459529184134
INFO:tools.evaluation_results_class:Counted Episodes = 3101
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.6524677276611328
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 1.8336490392684937
INFO:agents.father_agent:Step: 10, Training loss: 1.4939931631088257
INFO:agents.father_agent:Step: 15, Training loss: 2.0006227493286133
INFO:agents.father_agent:Step: 20, Training loss: 2.3424320220947266
INFO:agents.father_agent:Step: 25, Training loss: 1.9882036447525024
INFO:agents.father_agent:Step: 30, Training loss: 1.798128604888916
INFO:agents.father_agent:Step: 35, Training loss: 1.5221967697143555
INFO:agents.father_agent:Step: 40, Training loss: 1.3932570219039917
INFO:agents.father_agent:Step: 45, Training loss: 1.3387370109558105
INFO:agents.father_agent:Step: 50, Training loss: 1.4400266408920288
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 55, Training loss: 1.1089603900909424
INFO:agents.father_agent:Step: 60, Training loss: 1.0564802885055542
INFO:agents.father_agent:Step: 65, Training loss: 0.9729033708572388
INFO:agents.father_agent:Step: 70, Training loss: 0.7311263084411621
INFO:agents.father_agent:Step: 75, Training loss: 0.6605061292648315
INFO:agents.father_agent:Step: 80, Training loss: 0.5933016538619995
INFO:agents.father_agent:Step: 85, Training loss: 0.7728123068809509
INFO:agents.father_agent:Step: 90, Training loss: 0.6355141401290894
INFO:agents.father_agent:Step: 95, Training loss: 0.6533854603767395
INFO:agents.father_agent:Step: 100, Training loss: 0.55898118019104
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.36679077148438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 332.6332092285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 267.2345886230469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5139.45849609375
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.06300053390283
INFO:tools.evaluation_results_class:Counted Episodes = 7492
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -65.30703735351562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 334.6929626464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 268.4189147949219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4928.28125
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.53910690121786
INFO:tools.evaluation_results_class:Counted Episodes = 7390
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.78382873535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 328.2161865234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 261.9946594238281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6253.5859375
INFO:tools.evaluation_results_class:Current Best Return = -71.78382873535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.026412027631046
INFO:tools.evaluation_results_class:Counted Episodes = 7383
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -175.78878784179688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 224.21121215820312
INFO:tools.evaluation_results_class:Average Discounted Reward = 161.8552703857422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24924.01171875
INFO:tools.evaluation_results_class:Current Best Return = -175.78878784179688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.43458646616541
INFO:tools.evaluation_results_class:Counted Episodes = 7315
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 189.70847213474488
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.45237350463867
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 349.5476379394531
INFO:tools.evaluation_results_class:Average Discounted Reward = 259.6900329589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4653.3310546875
INFO:tools.evaluation_results_class:Current Best Return = -50.45237350463867
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.64719717757742
INFO:tools.evaluation_results_class:Counted Episodes = 2551
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.16328430175781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 330.8367004394531
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.3206024169922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4766.111328125
INFO:tools.evaluation_results_class:Current Best Return = -69.16328430175781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.41636495293266
INFO:tools.evaluation_results_class:Counted Episodes = 2762
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -23.480318069458008
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 376.5196838378906
INFO:tools.evaluation_results_class:Average Discounted Reward = 278.5162048339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1492.8499755859375
INFO:tools.evaluation_results_class:Current Best Return = -23.480318069458008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.26083499005964
INFO:tools.evaluation_results_class:Counted Episodes = 2515
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -20.116539001464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 379.8834533691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 281.1415100097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3373.908203125
INFO:tools.evaluation_results_class:Current Best Return = -20.116539001464844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.49979975971166
INFO:tools.evaluation_results_class:Counted Episodes = 2497
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.30226135253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 333.69775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 251.99435424804688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4052.27685546875
INFO:tools.evaluation_results_class:Current Best Return = -66.30226135253906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.356154406409324
INFO:tools.evaluation_results_class:Counted Episodes = 2746
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.66736602783203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 330.3326416015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 266.3431701660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5902.1728515625
INFO:tools.evaluation_results_class:Current Best Return = -69.66736602783203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.05959569440798
INFO:tools.evaluation_results_class:Counted Episodes = 3809
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.49645233154297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 331.5035400390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 267.2232971191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5888.8916015625
INFO:tools.evaluation_results_class:Current Best Return = -68.49645233154297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.172141918528254
INFO:tools.evaluation_results_class:Counted Episodes = 3805
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.50902557373047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 330.490966796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 266.68798828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5317.751953125
INFO:tools.evaluation_results_class:Current Best Return = -69.50902557373047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.96309866527087
INFO:tools.evaluation_results_class:Counted Episodes = 3821
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.2207260131836
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 329.7792663574219
INFO:tools.evaluation_results_class:Average Discounted Reward = 266.1678771972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5737.650390625
INFO:tools.evaluation_results_class:Current Best Return = -70.2207260131836
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.81658031088083
INFO:tools.evaluation_results_class:Counted Episodes = 3860
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.71751403808594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 325.282470703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 254.4168701171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6808.0419921875
INFO:tools.evaluation_results_class:Current Best Return = -74.71751403808594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.370223173341486
INFO:tools.evaluation_results_class:Counted Episodes = 3271
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.27140045166016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 330.7286071777344
INFO:tools.evaluation_results_class:Average Discounted Reward = 267.00537109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5923.02197265625
INFO:tools.evaluation_results_class:Current Best Return = -69.27140045166016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.90187891440501
INFO:tools.evaluation_results_class:Counted Episodes = 3832
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.17916107177734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 329.8208312988281
INFO:tools.evaluation_results_class:Average Discounted Reward = 266.1803283691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5975.14306640625
INFO:tools.evaluation_results_class:Current Best Return = -70.17916107177734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.90049621311047
INFO:tools.evaluation_results_class:Counted Episodes = 3829
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.51004791259766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 328.4899597167969
INFO:tools.evaluation_results_class:Average Discounted Reward = 265.0250244140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6185.19677734375
INFO:tools.evaluation_results_class:Current Best Return = -71.51004791259766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.83902948082442
INFO:tools.evaluation_results_class:Counted Episodes = 3833
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.01951599121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 323.9804992675781
INFO:tools.evaluation_results_class:Average Discounted Reward = 262.2205810546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5799.10400390625
INFO:tools.evaluation_results_class:Current Best Return = -76.01951599121094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.84693360364926
INFO:tools.evaluation_results_class:Counted Episodes = 3946
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.82221984863281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 332.17779541015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 268.4181823730469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5824.08837890625
INFO:tools.evaluation_results_class:Current Best Return = -67.82221984863281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.75266026472878
INFO:tools.evaluation_results_class:Counted Episodes = 3853
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.89336395263672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 325.10662841796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 263.06195068359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5881.3642578125
INFO:tools.evaluation_results_class:Current Best Return = -74.89336395263672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.93414387031408
INFO:tools.evaluation_results_class:Counted Episodes = 3948
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.2297134399414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 330.7702941894531
INFO:tools.evaluation_results_class:Average Discounted Reward = 267.349365234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5550.58447265625
INFO:tools.evaluation_results_class:Current Best Return = -69.2297134399414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.67700258397933
INFO:tools.evaluation_results_class:Counted Episodes = 3870
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.48060607910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 329.5193786621094
INFO:tools.evaluation_results_class:Average Discounted Reward = 265.7397766113281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5837.7841796875
INFO:tools.evaluation_results_class:Current Best Return = -70.48060607910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.9748427672956
INFO:tools.evaluation_results_class:Counted Episodes = 3816
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.99398040771484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 332.0060119628906
INFO:tools.evaluation_results_class:Average Discounted Reward = 267.8999328613281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5829.908203125
INFO:tools.evaluation_results_class:Current Best Return = -67.99398040771484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.93040293040293
INFO:tools.evaluation_results_class:Counted Episodes = 3822
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.91337585449219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 329.0866394042969
INFO:tools.evaluation_results_class:Average Discounted Reward = 265.4512023925781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5533.021484375
INFO:tools.evaluation_results_class:Current Best Return = -70.91337585449219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.92750588851086
INFO:tools.evaluation_results_class:Counted Episodes = 3821
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.77595520019531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 329.22406005859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 265.6343994140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5823.021484375
INFO:tools.evaluation_results_class:Current Best Return = -70.77595520019531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.887845592070946
INFO:tools.evaluation_results_class:Counted Episodes = 3834
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -61.16093444824219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 338.83905029296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 271.4957275390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7703.9267578125
INFO:tools.evaluation_results_class:Current Best Return = -61.16093444824219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.00789177001128
INFO:tools.evaluation_results_class:Counted Episodes = 3548
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -149.9795684814453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.0204315185547
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.23330688476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17825.830078125
INFO:tools.evaluation_results_class:Current Best Return = -149.9795684814453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.74956822107081
INFO:tools.evaluation_results_class:Counted Episodes = 3474
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.2453384399414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 332.7546691894531
INFO:tools.evaluation_results_class:Average Discounted Reward = 262.6527099609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6886.04931640625
INFO:tools.evaluation_results_class:Current Best Return = -67.2453384399414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.881029890500145
INFO:tools.evaluation_results_class:Counted Episodes = 3379
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -78.48509216308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 321.5149230957031
INFO:tools.evaluation_results_class:Average Discounted Reward = 256.2198181152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24837.138671875
INFO:tools.evaluation_results_class:Current Best Return = -78.48509216308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.17154217551832
INFO:tools.evaluation_results_class:Counted Episodes = 3521
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -118.6574935913086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.3424987792969
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.4497528076172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14898.294921875
INFO:tools.evaluation_results_class:Current Best Return = -118.6574935913086
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.315285671552495
INFO:tools.evaluation_results_class:Counted Episodes = 3343
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -187.97874450683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 212.02125549316406
INFO:tools.evaluation_results_class:Average Discounted Reward = 150.2743682861328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26881.94921875
INFO:tools.evaluation_results_class:Current Best Return = -187.97874450683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.63542234332425
INFO:tools.evaluation_results_class:Counted Episodes = 3670
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -193.37091064453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 206.62908935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.01605224609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24232.798828125
INFO:tools.evaluation_results_class:Current Best Return = -193.37091064453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.811108056090184
INFO:tools.evaluation_results_class:Counted Episodes = 3637
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -192.85462951660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 207.14537048339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.87498474121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27598.876953125
INFO:tools.evaluation_results_class:Current Best Return = -192.85462951660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.76906198573779
INFO:tools.evaluation_results_class:Counted Episodes = 3646
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -192.11083984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 207.88916015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.85655212402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27936.123046875
INFO:tools.evaluation_results_class:Current Best Return = -192.11083984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.22056384742952
INFO:tools.evaluation_results_class:Counted Episodes = 3618
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -151.9674835205078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.0325164794922
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.0489501953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23366.306640625
INFO:tools.evaluation_results_class:Current Best Return = -151.9674835205078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.5161096055405
INFO:tools.evaluation_results_class:Counted Episodes = 3321
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -185.46836853027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 214.53163146972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 152.24513244628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25535.3671875
INFO:tools.evaluation_results_class:Current Best Return = -185.46836853027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.793179317931795
INFO:tools.evaluation_results_class:Counted Episodes = 3636
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -186.2354278564453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 213.7645721435547
INFO:tools.evaluation_results_class:Average Discounted Reward = 151.2998046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26291.544921875
INFO:tools.evaluation_results_class:Current Best Return = -186.2354278564453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.13014644929539
INFO:tools.evaluation_results_class:Counted Episodes = 3619
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -196.62864685058594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 203.37135314941406
INFO:tools.evaluation_results_class:Average Discounted Reward = 142.02894592285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28676.6328125
INFO:tools.evaluation_results_class:Current Best Return = -196.62864685058594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.0831955922865
INFO:tools.evaluation_results_class:Counted Episodes = 3630
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -185.36044311523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 214.63955688476562
INFO:tools.evaluation_results_class:Average Discounted Reward = 152.9046630859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24715.34375
INFO:tools.evaluation_results_class:Current Best Return = -185.36044311523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.16901798063624
INFO:tools.evaluation_results_class:Counted Episodes = 3615
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.618896484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 211.381103515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 149.25082397460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25912.06640625
INFO:tools.evaluation_results_class:Current Best Return = -188.618896484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.74856674856675
INFO:tools.evaluation_results_class:Counted Episodes = 3663
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -186.51454162597656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 213.48545837402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 152.02769470214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27541.693359375
INFO:tools.evaluation_results_class:Current Best Return = -186.51454162597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.188694929343306
INFO:tools.evaluation_results_class:Counted Episodes = 3609
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -184.89981079101562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 215.10018920898438
INFO:tools.evaluation_results_class:Average Discounted Reward = 153.0541229248047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25786.298828125
INFO:tools.evaluation_results_class:Current Best Return = -184.89981079101562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.614250614250615
INFO:tools.evaluation_results_class:Counted Episodes = 3663
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.255126953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 209.744873046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 147.28378295898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25777.1640625
INFO:tools.evaluation_results_class:Current Best Return = -190.255126953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.256794231835826
INFO:tools.evaluation_results_class:Counted Episodes = 3606
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -189.6468505859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 210.3531494140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 148.587890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26168.419921875
INFO:tools.evaluation_results_class:Current Best Return = -189.6468505859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.79123287671233
INFO:tools.evaluation_results_class:Counted Episodes = 3650
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -189.0590057373047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 210.9409942626953
INFO:tools.evaluation_results_class:Average Discounted Reward = 149.11880493164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26422.955078125
INFO:tools.evaluation_results_class:Current Best Return = -189.0590057373047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.99341383095499
INFO:tools.evaluation_results_class:Counted Episodes = 3644
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.42897033691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 211.57102966308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 149.66275024414062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27639.29296875
INFO:tools.evaluation_results_class:Current Best Return = -188.42897033691406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.79087661445452
INFO:tools.evaluation_results_class:Counted Episodes = 3639
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 17 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -234.23387145996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 165.76612854003906
INFO:tools.evaluation_results_class:Average Discounted Reward = 73.08850860595703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 46604.6796875
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.77756593581188
INFO:tools.evaluation_results_class:Counted Episodes = 3147
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.8145524263381958
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 1.824046015739441
INFO:agents.father_agent:Step: 10, Training loss: 2.043991804122925
INFO:agents.father_agent:Step: 15, Training loss: 2.282374143600464
INFO:agents.father_agent:Step: 20, Training loss: 2.4446871280670166
INFO:agents.father_agent:Step: 25, Training loss: 2.765225887298584
INFO:agents.father_agent:Step: 30, Training loss: 2.679929256439209
INFO:agents.father_agent:Step: 35, Training loss: 2.4789228439331055
INFO:agents.father_agent:Step: 40, Training loss: 2.464689254760742
INFO:agents.father_agent:Step: 45, Training loss: 2.4656975269317627
INFO:agents.father_agent:Step: 50, Training loss: 2.039374589920044
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 55, Training loss: 2.3526339530944824
INFO:agents.father_agent:Step: 60, Training loss: 1.8569637537002563
INFO:agents.father_agent:Step: 65, Training loss: 1.7105605602264404
INFO:agents.father_agent:Step: 70, Training loss: 1.3926703929901123
INFO:agents.father_agent:Step: 75, Training loss: 1.1085448265075684
INFO:agents.father_agent:Step: 80, Training loss: 0.930012583732605
INFO:agents.father_agent:Step: 85, Training loss: 0.7752764225006104
INFO:agents.father_agent:Step: 90, Training loss: 0.7878876328468323
INFO:agents.father_agent:Step: 95, Training loss: 0.6485519409179688
INFO:agents.father_agent:Step: 100, Training loss: 0.7472383379936218
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.6165771484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 346.3834228515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 281.8553771972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4818.9091796875
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.23391812865497
INFO:tools.evaluation_results_class:Counted Episodes = 7866
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -52.3984260559082
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 347.6015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 282.94622802734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5133.42138671875
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.216079127567845
INFO:tools.evaluation_results_class:Counted Episodes = 7886
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.26202392578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 340.73797607421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 275.8282165527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6391.97607421875
INFO:tools.evaluation_results_class:Current Best Return = -59.26202392578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.888546877003975
INFO:tools.evaluation_results_class:Counted Episodes = 7797
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -184.718994140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 215.281005859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 154.83169555664062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30859.392578125
INFO:tools.evaluation_results_class:Current Best Return = -184.718994140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.84044701763835
INFO:tools.evaluation_results_class:Counted Episodes = 7427
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 196.0944102582063
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 18 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -94.12220764160156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 305.8777770996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.98800659179688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13783.662109375
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.59069568928724
INFO:tools.evaluation_results_class:Counted Episodes = 7029
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.1390650272369385
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 2.175471067428589
INFO:agents.father_agent:Step: 10, Training loss: 1.2648921012878418
INFO:agents.father_agent:Step: 15, Training loss: 1.0774368047714233
INFO:agents.father_agent:Step: 20, Training loss: 0.8057266473770142
INFO:agents.father_agent:Step: 25, Training loss: 0.791973888874054
INFO:agents.father_agent:Step: 30, Training loss: 0.6186750531196594
INFO:agents.father_agent:Step: 35, Training loss: 0.664088249206543
INFO:agents.father_agent:Step: 40, Training loss: 0.6886892914772034
INFO:agents.father_agent:Step: 45, Training loss: 0.6545582413673401
INFO:agents.father_agent:Step: 50, Training loss: 0.7478469014167786
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 55, Training loss: 0.5969191193580627
INFO:agents.father_agent:Step: 60, Training loss: 0.7199921011924744
INFO:agents.father_agent:Step: 65, Training loss: 0.6688017249107361
INFO:agents.father_agent:Step: 70, Training loss: 0.7602031826972961
INFO:agents.father_agent:Step: 75, Training loss: 0.674869179725647
INFO:agents.father_agent:Step: 80, Training loss: 0.6900800466537476
INFO:agents.father_agent:Step: 85, Training loss: 0.6854627132415771
INFO:agents.father_agent:Step: 90, Training loss: 0.6133692264556885
INFO:agents.father_agent:Step: 95, Training loss: 0.7275064587593079
INFO:agents.father_agent:Step: 100, Training loss: 0.7926723957061768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.346431732177734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 348.653564453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.07183837890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5429.458984375
INFO:tools.evaluation_results_class:Current Best Return = -50.19602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.14662194537614
INFO:tools.evaluation_results_class:Counted Episodes = 8348
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -48.066619873046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 351.9333801269531
INFO:tools.evaluation_results_class:Average Discounted Reward = 289.9785461425781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4432.02685546875
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.060997488338714
INFO:tools.evaluation_results_class:Counted Episodes = 8361
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.057559967041016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 345.94244384765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 283.62799072265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5881.58251953125
INFO:tools.evaluation_results_class:Current Best Return = -54.057559967041016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.83616093068347
INFO:tools.evaluation_results_class:Counted Episodes = 8252
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -183.6179962158203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 216.3820037841797
INFO:tools.evaluation_results_class:Average Discounted Reward = 155.48023986816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31126.787109375
INFO:tools.evaluation_results_class:Current Best Return = -183.6179962158203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.275500476644424
INFO:tools.evaluation_results_class:Counted Episodes = 7343
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 196.11104423543793
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 19 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.7454071044922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.4554443359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.73699951171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.98829475924448
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16682.23046875
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 71.89624900239426
INFO:tools.evaluation_results_class:Counted Episodes = 3759
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.0712822675704956
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 1.1059938669204712
INFO:agents.father_agent:Step: 10, Training loss: 1.3636599779129028
INFO:agents.father_agent:Step: 15, Training loss: 1.331534743309021
INFO:agents.father_agent:Step: 20, Training loss: 1.4366626739501953
INFO:agents.father_agent:Step: 25, Training loss: 1.4259523153305054
INFO:agents.father_agent:Step: 30, Training loss: 1.0557786226272583
INFO:agents.father_agent:Step: 35, Training loss: 0.989066481590271
INFO:agents.father_agent:Step: 40, Training loss: 0.9642989635467529
INFO:agents.father_agent:Step: 45, Training loss: 0.8500921726226807
INFO:agents.father_agent:Step: 50, Training loss: 0.7348640561103821
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 55, Training loss: 0.6753649711608887
INFO:agents.father_agent:Step: 60, Training loss: 0.66905277967453
INFO:agents.father_agent:Step: 65, Training loss: 0.6973859071731567
INFO:agents.father_agent:Step: 70, Training loss: 0.7811292409896851
INFO:agents.father_agent:Step: 75, Training loss: 0.7265821695327759
INFO:agents.father_agent:Step: 80, Training loss: 0.7479393482208252
INFO:agents.father_agent:Step: 85, Training loss: 0.8179003000259399
INFO:agents.father_agent:Step: 90, Training loss: 0.6634916663169861
INFO:agents.father_agent:Step: 95, Training loss: 0.5146451592445374
INFO:agents.father_agent:Step: 100, Training loss: 0.5086169242858887
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.49505615234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 346.50494384765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 283.75390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6296.99755859375
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.1764705882353
INFO:tools.evaluation_results_class:Counted Episodes = 8092
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -53.411094665527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 346.5888977050781
INFO:tools.evaluation_results_class:Average Discounted Reward = 283.3135070800781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5268.962890625
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.286775631500745
INFO:tools.evaluation_results_class:Counted Episodes = 8076
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.62568664550781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 337.37432861328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 273.06170654296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7679.75927734375
INFO:tools.evaluation_results_class:Current Best Return = -62.62568664550781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.69092301800077
INFO:tools.evaluation_results_class:Counted Episodes = 7833
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -185.21925354003906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 214.78074645996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 153.90164184570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29987.041015625
INFO:tools.evaluation_results_class:Current Best Return = -185.21925354003906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.079593220338985
INFO:tools.evaluation_results_class:Counted Episodes = 7375
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 194.5839705797176
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 20 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -114.3831787109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.6168212890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.9439697265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14134.0478515625
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.850054525627044
INFO:tools.evaluation_results_class:Counted Episodes = 7336
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.5627992153167725
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 1.4231512546539307
INFO:agents.father_agent:Step: 10, Training loss: 1.3311201333999634
INFO:agents.father_agent:Step: 15, Training loss: 1.139892816543579
INFO:agents.father_agent:Step: 20, Training loss: 0.7814235687255859
INFO:agents.father_agent:Step: 25, Training loss: 0.8208847045898438
INFO:agents.father_agent:Step: 30, Training loss: 0.7825167179107666
INFO:agents.father_agent:Step: 35, Training loss: 0.8332119584083557
INFO:agents.father_agent:Step: 40, Training loss: 0.6885592341423035
INFO:agents.father_agent:Step: 45, Training loss: 0.5914437770843506
INFO:agents.father_agent:Step: 50, Training loss: 0.5815470218658447
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 55, Training loss: 0.5328631401062012
INFO:agents.father_agent:Step: 60, Training loss: 0.6708584427833557
INFO:agents.father_agent:Step: 65, Training loss: 0.498685747385025
INFO:agents.father_agent:Step: 70, Training loss: 0.6318480968475342
INFO:agents.father_agent:Step: 75, Training loss: 0.6958488821983337
INFO:agents.father_agent:Step: 80, Training loss: 0.5640595555305481
INFO:agents.father_agent:Step: 85, Training loss: 0.5806534886360168
INFO:agents.father_agent:Step: 90, Training loss: 0.48102596402168274
INFO:agents.father_agent:Step: 95, Training loss: 0.6283241510391235
INFO:agents.father_agent:Step: 100, Training loss: 0.3879772126674652
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.30710220336914
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 350.6929016113281
INFO:tools.evaluation_results_class:Average Discounted Reward = 280.7427062988281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5654.28515625
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.67626502655857
INFO:tools.evaluation_results_class:Counted Episodes = 7154
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -50.51108169555664
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 349.4889221191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 279.9387512207031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6137.68701171875
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.42243902439024
INFO:tools.evaluation_results_class:Counted Episodes = 7175
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.480464935302734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 342.51953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 270.56671142578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13361.9228515625
INFO:tools.evaluation_results_class:Current Best Return = -57.480464935302734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.42547545333923
INFO:tools.evaluation_results_class:Counted Episodes = 6783
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -186.51390075683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 213.48609924316406
INFO:tools.evaluation_results_class:Average Discounted Reward = 148.2078094482422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29687.826171875
INFO:tools.evaluation_results_class:Current Best Return = -186.51390075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.84472769409038
INFO:tools.evaluation_results_class:Counted Episodes = 6904
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=2, o2x_init=2, goright1_init=1, goright2_init=0, o1y=1, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 195.12981747960964
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=2, o2x_init=2, goright1_init=1, goright2_init=0, o1y=1, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 21 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.29179382324219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 315.7082214355469
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.8724365234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8844.8046875
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.37190900098912
INFO:tools.evaluation_results_class:Counted Episodes = 7077
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.3909910917282104
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 1.3788646459579468
INFO:agents.father_agent:Step: 10, Training loss: 1.1022965908050537
INFO:agents.father_agent:Step: 15, Training loss: 0.6201736927032471
INFO:agents.father_agent:Step: 20, Training loss: 0.659756064414978
INFO:agents.father_agent:Step: 25, Training loss: 0.5182607769966125
INFO:agents.father_agent:Step: 30, Training loss: 0.6717966794967651
INFO:agents.father_agent:Step: 35, Training loss: 0.48847225308418274
INFO:agents.father_agent:Step: 40, Training loss: 0.5556687116622925
INFO:agents.father_agent:Step: 45, Training loss: 0.5442574620246887
INFO:agents.father_agent:Step: 50, Training loss: 0.557379961013794
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 55, Training loss: 0.668574333190918
INFO:agents.father_agent:Step: 60, Training loss: 0.6153392195701599
INFO:agents.father_agent:Step: 65, Training loss: 0.5519055724143982
INFO:agents.father_agent:Step: 70, Training loss: 0.503440797328949
INFO:agents.father_agent:Step: 75, Training loss: 0.36013296246528625
INFO:agents.father_agent:Step: 80, Training loss: 0.7997964024543762
INFO:agents.father_agent:Step: 85, Training loss: 0.4271036982536316
INFO:agents.father_agent:Step: 90, Training loss: 0.30178821086883545
INFO:agents.father_agent:Step: 95, Training loss: 0.3745475113391876
INFO:agents.father_agent:Step: 100, Training loss: 0.4203920364379883
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.63020706176758
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 347.3697814941406
INFO:tools.evaluation_results_class:Average Discounted Reward = 285.1147155761719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12912.638671875
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.582846762683275
INFO:tools.evaluation_results_class:Counted Episodes = 7707
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -52.06284713745117
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 347.9371643066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 285.3549499511719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12840.01953125
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.658805235195025
INFO:tools.evaluation_results_class:Counted Episodes = 7717
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.909114837646484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 339.09088134765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 273.19061279296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15726.90234375
INFO:tools.evaluation_results_class:Current Best Return = -60.909114837646484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.302851524090464
INFO:tools.evaluation_results_class:Counted Episodes = 7119
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -187.217529296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 212.782470703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 149.3916778564453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32607.328125
INFO:tools.evaluation_results_class:Current Best Return = -187.217529296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.59988452655889
INFO:tools.evaluation_results_class:Counted Episodes = 6928
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 201.65945847222648
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.01676177978516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 327.9832458496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 276.3797302246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 59905.75
INFO:tools.evaluation_results_class:Current Best Return = -72.01676177978516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.1258697027198
INFO:tools.evaluation_results_class:Counted Episodes = 3162
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -90.00310516357422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 309.99688720703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.033935546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5216.13818359375
INFO:tools.evaluation_results_class:Current Best Return = -90.00310516357422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.808563450201675
INFO:tools.evaluation_results_class:Counted Episodes = 3223
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.36024475097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 318.6397399902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 262.6064147949219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35939.71484375
INFO:tools.evaluation_results_class:Current Best Return = -81.36024475097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.09316770186335
INFO:tools.evaluation_results_class:Counted Episodes = 3381
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -185.09835815429688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 214.76400756835938
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.25135803222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9996643168848607
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 323230.53125
INFO:tools.evaluation_results_class:Current Best Return = -185.09835815429688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996643168848607
INFO:tools.evaluation_results_class:Average Episode Length = 47.74118831822759
INFO:tools.evaluation_results_class:Counted Episodes = 2979
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.88640213012695
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 356.11358642578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.49066162109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2882.482666015625
INFO:tools.evaluation_results_class:Current Best Return = -43.88640213012695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.268448576409064
INFO:tools.evaluation_results_class:Counted Episodes = 3442
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -45.11796569824219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 354.88201904296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 286.8155517578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4585.7802734375
INFO:tools.evaluation_results_class:Current Best Return = -45.11796569824219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.21255776026094
INFO:tools.evaluation_results_class:Counted Episodes = 3679
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -44.42616653442383
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 355.5738220214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.160888671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4282.86865234375
INFO:tools.evaluation_results_class:Current Best Return = -44.42616653442383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.23887079261672
INFO:tools.evaluation_results_class:Counted Episodes = 3684
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.5908088684082
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 356.4091796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.43634033203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4054.75341796875
INFO:tools.evaluation_results_class:Current Best Return = -43.5908088684082
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.75729223995597
INFO:tools.evaluation_results_class:Counted Episodes = 3634
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.3711051940918
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 356.62890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.5748291015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3838.105224609375
INFO:tools.evaluation_results_class:Current Best Return = -43.3711051940918
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.727322856355116
INFO:tools.evaluation_results_class:Counted Episodes = 3627
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.94195556640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.05804443359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.91258239746094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 80363.9375
INFO:tools.evaluation_results_class:Current Best Return = -136.94195556640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.582451348583135
INFO:tools.evaluation_results_class:Counted Episodes = 2929
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.75960922241211
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 356.2403869628906
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.78759765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3983.2646484375
INFO:tools.evaluation_results_class:Current Best Return = -43.75960922241211
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.289452166802945
INFO:tools.evaluation_results_class:Counted Episodes = 3669
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -42.90861511230469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 357.09136962890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.99176025390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3955.931640625
INFO:tools.evaluation_results_class:Current Best Return = -42.90861511230469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.694840834248076
INFO:tools.evaluation_results_class:Counted Episodes = 3644
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.27267837524414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 356.7273254394531
INFO:tools.evaluation_results_class:Average Discounted Reward = 288.5326843261719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4246.74072265625
INFO:tools.evaluation_results_class:Current Best Return = -43.27267837524414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.07094503113999
INFO:tools.evaluation_results_class:Counted Episodes = 3693
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.4422607421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 346.5577392578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 279.1168212890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4439.69482421875
INFO:tools.evaluation_results_class:Current Best Return = -53.4422607421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.36418236418236
INFO:tools.evaluation_results_class:Counted Episodes = 3663
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.96503829956055
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 356.03497314453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.85162353515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4152.12744140625
INFO:tools.evaluation_results_class:Current Best Return = -43.96503829956055
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.16552854411363
INFO:tools.evaluation_results_class:Counted Episodes = 3661
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.58949661254883
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 346.4104919433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 278.4182434082031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4377.58740234375
INFO:tools.evaluation_results_class:Current Best Return = -53.58949661254883
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.71762441572725
INFO:tools.evaluation_results_class:Counted Episodes = 3637
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.32168960571289
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 356.6783142089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 288.90679931640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4278.62158203125
INFO:tools.evaluation_results_class:Current Best Return = -43.32168960571289
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.747917226552005
INFO:tools.evaluation_results_class:Counted Episodes = 3721
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.895416259765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 356.1045837402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.22418212890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4299.978515625
INFO:tools.evaluation_results_class:Current Best Return = -43.895416259765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.65385671150151
INFO:tools.evaluation_results_class:Counted Episodes = 3643
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.950801849365234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 356.0491943359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.05841064453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4045.556884765625
INFO:tools.evaluation_results_class:Current Best Return = -43.950801849365234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.62023217247098
INFO:tools.evaluation_results_class:Counted Episodes = 3618
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.9631233215332
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 356.036865234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.92462158203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4545.22412109375
INFO:tools.evaluation_results_class:Current Best Return = -43.9631233215332
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.052060737527114
INFO:tools.evaluation_results_class:Counted Episodes = 3688
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -45.06304168701172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 354.93695068359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 286.7943115234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4204.2041015625
INFO:tools.evaluation_results_class:Current Best Return = -45.06304168701172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.08766233766234
INFO:tools.evaluation_results_class:Counted Episodes = 3696
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.508522033691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 343.4914855957031
INFO:tools.evaluation_results_class:Average Discounted Reward = 276.40692138671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5003.51513671875
INFO:tools.evaluation_results_class:Current Best Return = -56.508522033691406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.24830944008656
INFO:tools.evaluation_results_class:Counted Episodes = 3697
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.75822830200195
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 344.24176025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 277.3298645019531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5062.822265625
INFO:tools.evaluation_results_class:Current Best Return = -55.75822830200195
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.163720424258905
INFO:tools.evaluation_results_class:Counted Episodes = 3677
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.830299377441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 344.1697082519531
INFO:tools.evaluation_results_class:Average Discounted Reward = 276.14727783203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5026.50146484375
INFO:tools.evaluation_results_class:Current Best Return = -55.830299377441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.897350993377486
INFO:tools.evaluation_results_class:Counted Episodes = 3624
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.9986572265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.0013427734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.94754028320312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 65893.171875
INFO:tools.evaluation_results_class:Current Best Return = -155.9986572265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.47791164658634
INFO:tools.evaluation_results_class:Counted Episodes = 2988
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.25407791137695
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 345.74591064453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 277.33404541015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4619.48828125
INFO:tools.evaluation_results_class:Current Best Return = -54.25407791137695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.98921758363284
INFO:tools.evaluation_results_class:Counted Episodes = 3617
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -85.8977279663086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 314.1022644042969
INFO:tools.evaluation_results_class:Average Discounted Reward = 252.56729125976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9985.1005859375
INFO:tools.evaluation_results_class:Current Best Return = -85.8977279663086
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.68010680907877
INFO:tools.evaluation_results_class:Counted Episodes = 3745
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -168.1642608642578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 231.8357391357422
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.82305908203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23958.12890625
INFO:tools.evaluation_results_class:Current Best Return = -168.1642608642578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.60564208051719
INFO:tools.evaluation_results_class:Counted Episodes = 3403
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -130.5059051513672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.49407958984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.0915069580078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22883.619140625
INFO:tools.evaluation_results_class:Current Best Return = -130.5059051513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.501641497045306
INFO:tools.evaluation_results_class:Counted Episodes = 3046
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.22918701171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.77081298828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.88937377929688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38342.23828125
INFO:tools.evaluation_results_class:Current Best Return = -155.22918701171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.08919722497522
INFO:tools.evaluation_results_class:Counted Episodes = 4036
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.13491821289062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.8650817871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.45237731933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15385.1376953125
INFO:tools.evaluation_results_class:Current Best Return = -119.13491821289062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.49842452019479
INFO:tools.evaluation_results_class:Counted Episodes = 3491
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -197.9394073486328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 202.0605926513672
INFO:tools.evaluation_results_class:Average Discounted Reward = 137.26443481445312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32394.251953125
INFO:tools.evaluation_results_class:Current Best Return = -197.9394073486328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.163996504515005
INFO:tools.evaluation_results_class:Counted Episodes = 3433
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -196.40318298339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 203.59681701660156
INFO:tools.evaluation_results_class:Average Discounted Reward = 138.576904296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34155.3359375
INFO:tools.evaluation_results_class:Current Best Return = -196.40318298339844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.0465991316932
INFO:tools.evaluation_results_class:Counted Episodes = 3455
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.98056030273438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 209.01943969726562
INFO:tools.evaluation_results_class:Average Discounted Reward = 143.8874053955078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33291.9140625
INFO:tools.evaluation_results_class:Current Best Return = -190.98056030273438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.07803887438352
INFO:tools.evaluation_results_class:Counted Episodes = 3447
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -200.20948791503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 199.79051208496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 135.87051391601562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34633.32421875
INFO:tools.evaluation_results_class:Current Best Return = -200.20948791503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.548728208059444
INFO:tools.evaluation_results_class:Counted Episodes = 3499
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -183.8423614501953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 216.1576385498047
INFO:tools.evaluation_results_class:Average Discounted Reward = 160.21722412109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33887.546875
INFO:tools.evaluation_results_class:Current Best Return = -183.8423614501953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.94465372355839
INFO:tools.evaluation_results_class:Counted Episodes = 3451
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -192.2451629638672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 207.7548370361328
INFO:tools.evaluation_results_class:Average Discounted Reward = 142.8815155029297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30268.634765625
INFO:tools.evaluation_results_class:Current Best Return = -192.2451629638672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.91019347386659
INFO:tools.evaluation_results_class:Counted Episodes = 3463
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -197.92172241210938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 202.07827758789062
INFO:tools.evaluation_results_class:Average Discounted Reward = 137.64222717285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36090.44140625
INFO:tools.evaluation_results_class:Current Best Return = -197.92172241210938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.96143809799942
INFO:tools.evaluation_results_class:Counted Episodes = 3449
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -198.43881225585938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 201.56118774414062
INFO:tools.evaluation_results_class:Average Discounted Reward = 136.7936553955078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33906.56640625
INFO:tools.evaluation_results_class:Current Best Return = -198.43881225585938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.95707656612529
INFO:tools.evaluation_results_class:Counted Episodes = 3448
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -201.04246520996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 198.95753479003906
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.69004821777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35457.00390625
INFO:tools.evaluation_results_class:Current Best Return = -201.04246520996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.017814726840854
INFO:tools.evaluation_results_class:Counted Episodes = 3368
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.81361389160156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 208.18638610839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 143.44850158691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32526.95703125
INFO:tools.evaluation_results_class:Current Best Return = -191.81361389160156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.61858049624928
INFO:tools.evaluation_results_class:Counted Episodes = 3466
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -202.3071746826172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 197.6928253173828
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.23008728027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35189.15234375
INFO:tools.evaluation_results_class:Current Best Return = -202.3071746826172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.21003500583431
INFO:tools.evaluation_results_class:Counted Episodes = 3428
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -199.4007568359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 200.5992431640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 136.00253295898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35951.046875
INFO:tools.evaluation_results_class:Current Best Return = -199.4007568359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.28079392877992
INFO:tools.evaluation_results_class:Counted Episodes = 3426
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -193.0167694091797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 206.9832305908203
INFO:tools.evaluation_results_class:Average Discounted Reward = 141.92152404785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32692.021484375
INFO:tools.evaluation_results_class:Current Best Return = -193.0167694091797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.94104046242774
INFO:tools.evaluation_results_class:Counted Episodes = 3460
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -201.7880096435547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 198.2119903564453
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.4164581298828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34156.36328125
INFO:tools.evaluation_results_class:Current Best Return = -201.7880096435547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.60011474469306
INFO:tools.evaluation_results_class:Counted Episodes = 3486
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -201.10951232910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 198.89048767089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.8220977783203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35684.03125
INFO:tools.evaluation_results_class:Current Best Return = -201.10951232910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.710086455331414
INFO:tools.evaluation_results_class:Counted Episodes = 3470
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -195.59747314453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 204.40252685546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 139.79234313964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32549.169921875
INFO:tools.evaluation_results_class:Current Best Return = -195.59747314453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.62539591131586
INFO:tools.evaluation_results_class:Counted Episodes = 3473
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -199.49867248535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 200.50132751464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 136.30352783203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33357.12890625
INFO:tools.evaluation_results_class:Current Best Return = -199.49867248535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.75434462444772
INFO:tools.evaluation_results_class:Counted Episodes = 3395
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -201.28878784179688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 198.71121215820312
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.77267456054688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34882.7109375
INFO:tools.evaluation_results_class:Current Best Return = -201.28878784179688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.78761061946903
INFO:tools.evaluation_results_class:Counted Episodes = 3390
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -197.1303253173828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 202.8696746826172
INFO:tools.evaluation_results_class:Average Discounted Reward = 138.1578826904297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34453.015625
INFO:tools.evaluation_results_class:Current Best Return = -197.1303253173828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.18431255592007
INFO:tools.evaluation_results_class:Counted Episodes = 3353
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.60678100585938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 211.39321899414062
INFO:tools.evaluation_results_class:Average Discounted Reward = 156.3202667236328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 39215.296875
INFO:tools.evaluation_results_class:Current Best Return = -188.60678100585938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.58764271323036
INFO:tools.evaluation_results_class:Counted Episodes = 2978
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -203.44683837890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 196.55316162109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 132.3178253173828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35774.53515625
INFO:tools.evaluation_results_class:Current Best Return = -203.44683837890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.84406379208506
INFO:tools.evaluation_results_class:Counted Episodes = 3386
INFO:robust_rl.robust_rl_trainer:Iteration 22 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.315650939941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 348.6843566894531
INFO:tools.evaluation_results_class:Average Discounted Reward = 285.8410949707031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14567.99609375
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.86947477899116
INFO:tools.evaluation_results_class:Counted Episodes = 7692
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.3612810969352722
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Step: 5, Training loss: 0.5163800716400146
INFO:agents.father_agent:Step: 10, Training loss: 0.29836684465408325
INFO:agents.father_agent:Step: 15, Training loss: 0.4587623178958893
INFO:agents.father_agent:Step: 20, Training loss: 0.4274452328681946
INFO:agents.father_agent:Step: 25, Training loss: 0.4280904233455658
INFO:agents.father_agent:Step: 30, Training loss: 0.5907578468322754
INFO:agents.father_agent:Step: 35, Training loss: 0.41156166791915894
INFO:agents.father_agent:Step: 40, Training loss: 0.6168356537818909
INFO:agents.father_agent:Step: 45, Training loss: 0.3732006549835205
INFO:agents.father_agent:Step: 50, Training loss: 0.4022837281227112
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -48.832462310791016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 351.16754150390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 290.44091796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6312.5185546875
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 35.62153189471188
INFO:tools.evaluation_results_class:Counted Episodes = 8434
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.626197814941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 345.3738098144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 283.8609313964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8618.4111328125
INFO:tools.evaluation_results_class:Current Best Return = -54.626197814941406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.82546998180715
INFO:tools.evaluation_results_class:Counted Episodes = 8245
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -185.37171936035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 214.62828063964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 153.1011199951172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31172.404296875
INFO:tools.evaluation_results_class:Current Best Return = -185.37171936035156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.79641379310345
INFO:tools.evaluation_results_class:Counted Episodes = 7250
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 198.13486802551677
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 23 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -101.5587387084961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 298.4412536621094
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.54861450195312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14247.7275390625
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.03617115130128
INFO:tools.evaluation_results_class:Counted Episodes = 6801
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.1799839735031128
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 1.1075053215026855
INFO:agents.father_agent:Step: 10, Training loss: 0.8438680171966553
INFO:agents.father_agent:Step: 15, Training loss: 0.7798663377761841
INFO:agents.father_agent:Step: 20, Training loss: 0.8652233481407166
INFO:agents.father_agent:Step: 25, Training loss: 0.8778380155563354
INFO:agents.father_agent:Step: 30, Training loss: 1.0657320022583008
INFO:agents.father_agent:Step: 35, Training loss: 1.062796711921692
INFO:agents.father_agent:Step: 40, Training loss: 0.9377554655075073
INFO:agents.father_agent:Step: 45, Training loss: 0.9311704039573669
INFO:agents.father_agent:Step: 50, Training loss: 0.7973044514656067
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 55, Training loss: 0.7947137355804443
INFO:agents.father_agent:Step: 60, Training loss: 0.6937143206596375
INFO:agents.father_agent:Step: 65, Training loss: 0.4440068006515503
INFO:agents.father_agent:Step: 70, Training loss: 0.4007740616798401
INFO:agents.father_agent:Step: 75, Training loss: 0.39981532096862793
INFO:agents.father_agent:Step: 80, Training loss: 0.3513491451740265
INFO:agents.father_agent:Step: 85, Training loss: 0.4360663890838623
INFO:agents.father_agent:Step: 90, Training loss: 0.26881054043769836
INFO:agents.father_agent:Step: 95, Training loss: 0.342428982257843
INFO:agents.father_agent:Step: 100, Training loss: 0.38832613825798035
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.81974411010742
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 351.1802673339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.5489501953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3747.429931640625
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.4898369443824
INFO:tools.evaluation_results_class:Counted Episodes = 4477
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -49.28878402709961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 350.7112121582031
INFO:tools.evaluation_results_class:Average Discounted Reward = 247.223876953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4451.8798828125
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.31931252826776
INFO:tools.evaluation_results_class:Counted Episodes = 4422
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.49699783325195
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 342.50299072265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.9136199951172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9973.875
INFO:tools.evaluation_results_class:Current Best Return = -57.49699783325195
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.59621802002225
INFO:tools.evaluation_results_class:Counted Episodes = 4495
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -267.1878356933594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 132.81214904785156
INFO:tools.evaluation_results_class:Average Discounted Reward = 57.326839447021484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 84822.6796875
INFO:tools.evaluation_results_class:Current Best Return = -267.1878356933594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.7182320441989
INFO:tools.evaluation_results_class:Counted Episodes = 4344
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=1, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 309.5991812897386
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=1, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 24 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.80516052246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.1948547363281
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.33787536621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22611.26171875
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.124773960217
INFO:tools.evaluation_results_class:Counted Episodes = 4424
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.4533149003982544
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 1.8352197408676147
INFO:agents.father_agent:Step: 10, Training loss: 1.9250494241714478
INFO:agents.father_agent:Step: 15, Training loss: 1.6736149787902832
INFO:agents.father_agent:Step: 20, Training loss: 1.75169038772583
INFO:agents.father_agent:Step: 25, Training loss: 1.8081282377243042
INFO:agents.father_agent:Step: 30, Training loss: 1.4626423120498657
INFO:agents.father_agent:Step: 35, Training loss: 1.4764610528945923
INFO:agents.father_agent:Step: 40, Training loss: 1.4152722358703613
INFO:agents.father_agent:Step: 45, Training loss: 1.5553617477416992
INFO:agents.father_agent:Step: 50, Training loss: 1.2896348237991333
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 55, Training loss: 1.2988450527191162
INFO:agents.father_agent:Step: 60, Training loss: 1.2485195398330688
INFO:agents.father_agent:Step: 65, Training loss: 1.2199453115463257
INFO:agents.father_agent:Step: 70, Training loss: 0.758249044418335
INFO:agents.father_agent:Step: 75, Training loss: 0.6280034184455872
INFO:agents.father_agent:Step: 80, Training loss: 0.6228944063186646
INFO:agents.father_agent:Step: 85, Training loss: 0.5044851303100586
INFO:agents.father_agent:Step: 90, Training loss: 0.5693815350532532
INFO:agents.father_agent:Step: 95, Training loss: 0.47135910391807556
INFO:agents.father_agent:Step: 100, Training loss: 0.42387914657592773
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.60216522216797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 320.3978271484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.68443298339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7544.73046875
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.17200129324281
INFO:tools.evaluation_results_class:Counted Episodes = 6186
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -79.31935119628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 320.6806335449219
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.4878692626953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7582.1748046875
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.48368146214099
INFO:tools.evaluation_results_class:Counted Episodes = 6128
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -89.55309295654297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 310.4468994140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.45263671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11448.052734375
INFO:tools.evaluation_results_class:Current Best Return = -89.55309295654297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.816874090835626
INFO:tools.evaluation_results_class:Counted Episodes = 6187
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -198.9544677734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 201.0455322265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.1701202392578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40743.46875
INFO:tools.evaluation_results_class:Current Best Return = -198.9544677734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.47197106690778
INFO:tools.evaluation_results_class:Counted Episodes = 6083
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=1, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 217.53617379964825
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=1, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 25 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1114.24609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1112.234375
INFO:tools.evaluation_results_class:Average Discounted Reward = -262.697998046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.029296875
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1018978.3125
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 594.396484375
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.3410050868988037
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 2.359290361404419
INFO:agents.father_agent:Step: 10, Training loss: 1.4555087089538574
INFO:agents.father_agent:Step: 15, Training loss: 2.7122316360473633
INFO:agents.father_agent:Step: 20, Training loss: 1.484619140625
INFO:agents.father_agent:Step: 25, Training loss: 1.9483813047409058
INFO:agents.father_agent:Step: 30, Training loss: 1.2758125066757202
INFO:agents.father_agent:Step: 35, Training loss: 2.73555850982666
INFO:agents.father_agent:Step: 40, Training loss: 1.954997181892395
INFO:agents.father_agent:Step: 45, Training loss: 1.791502833366394
INFO:agents.father_agent:Step: 50, Training loss: 2.4226651191711426
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 55, Training loss: 2.594432830810547
INFO:agents.father_agent:Step: 60, Training loss: 2.4793128967285156
INFO:agents.father_agent:Step: 65, Training loss: 2.2451515197753906
INFO:agents.father_agent:Step: 70, Training loss: 2.224052667617798
INFO:agents.father_agent:Step: 75, Training loss: 2.511068105697632
INFO:agents.father_agent:Step: 80, Training loss: 3.185626268386841
INFO:agents.father_agent:Step: 85, Training loss: 2.8815464973449707
INFO:agents.father_agent:Step: 90, Training loss: 3.4621152877807617
INFO:agents.father_agent:Step: 95, Training loss: 3.532745599746704
INFO:agents.father_agent:Step: 100, Training loss: 3.369647741317749
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -87.90705108642578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 312.09295654296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.39317321777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9936.8876953125
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.45228215767635
INFO:tools.evaluation_results_class:Counted Episodes = 7230
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -90.64171600341797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 309.3582763671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 244.22853088378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9910.6083984375
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.27929418251999
INFO:tools.evaluation_results_class:Counted Episodes = 7254
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -100.39093780517578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 299.60906982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.5665283203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12261.8212890625
INFO:tools.evaluation_results_class:Current Best Return = -100.39093780517578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.66993652625505
INFO:tools.evaluation_results_class:Counted Episodes = 6932
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -183.8898468017578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 216.1101531982422
INFO:tools.evaluation_results_class:Average Discounted Reward = 152.0409393310547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29801.537109375
INFO:tools.evaluation_results_class:Current Best Return = -183.8898468017578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.20131482834186
INFO:tools.evaluation_results_class:Counted Episodes = 6845
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 202.37943157851362
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 26 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -436.9559326171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -44.05271530151367
INFO:tools.evaluation_results_class:Average Discounted Reward = -82.83172607421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.982690794649882
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 186832.96875
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 186.3855232100708
INFO:tools.evaluation_results_class:Counted Episodes = 1271
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.0975632667541504
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Step: 5, Training loss: 1.7326072454452515
INFO:agents.father_agent:Step: 10, Training loss: 2.6526026725769043
INFO:agents.father_agent:Step: 15, Training loss: 2.413867473602295
INFO:agents.father_agent:Step: 20, Training loss: 2.7295989990234375
INFO:agents.father_agent:Step: 25, Training loss: 2.8577098846435547
INFO:agents.father_agent:Step: 30, Training loss: 3.1704142093658447
INFO:agents.father_agent:Step: 35, Training loss: 3.8424086570739746
INFO:agents.father_agent:Step: 40, Training loss: 3.452850103378296
INFO:agents.father_agent:Step: 45, Training loss: 3.531709909439087
INFO:agents.father_agent:Step: 50, Training loss: 3.304166793823242
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 55, Training loss: 2.946262836456299
INFO:agents.father_agent:Step: 60, Training loss: 2.829911947250366
INFO:agents.father_agent:Step: 65, Training loss: 2.702824115753174
INFO:agents.father_agent:Step: 70, Training loss: 2.7702736854553223
INFO:agents.father_agent:Step: 75, Training loss: 2.838013172149658
INFO:agents.father_agent:Step: 80, Training loss: 2.625304698944092
INFO:agents.father_agent:Step: 85, Training loss: 2.2067642211914062
INFO:agents.father_agent:Step: 90, Training loss: 1.835016131401062
INFO:agents.father_agent:Step: 95, Training loss: 1.6709916591644287
INFO:agents.father_agent:Step: 100, Training loss: 1.4034146070480347
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -97.05634307861328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 302.94366455078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.50454711914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11337.994140625
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.42842132913055
INFO:tools.evaluation_results_class:Counted Episodes = 7614
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -99.67688751220703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.3231201171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.33673095703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11405.7119140625
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.31970114038537
INFO:tools.evaluation_results_class:Counted Episodes = 7629
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -101.97774505615234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 298.0222473144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.0167236328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11579.2353515625
INFO:tools.evaluation_results_class:Current Best Return = -101.97774505615234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.577634754625905
INFO:tools.evaluation_results_class:Counted Episodes = 7458
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -133.53587341308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.464111328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.90969848632812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17862.24609375
INFO:tools.evaluation_results_class:Current Best Return = -133.53587341308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.26787137921871
INFO:tools.evaluation_results_class:Counted Episodes = 7526
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=1, o1y=3, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 150.14330313422337
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=1, o1y=3, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.84807205200195
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 342.15191650390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 269.33148193359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12149.8017578125
INFO:tools.evaluation_results_class:Current Best Return = -57.84807205200195
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.746683512318384
INFO:tools.evaluation_results_class:Counted Episodes = 3166
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -110.27550506591797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.7244873046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.38400268554688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9930.794921875
INFO:tools.evaluation_results_class:Current Best Return = -110.27550506591797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.00830367734282
INFO:tools.evaluation_results_class:Counted Episodes = 3372
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.61001586914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 333.3899841308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 261.32025146484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9495.6728515625
INFO:tools.evaluation_results_class:Current Best Return = -66.61001586914062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.55377996312231
INFO:tools.evaluation_results_class:Counted Episodes = 3254
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.640869140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 328.359130859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 257.70269775390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29624.353515625
INFO:tools.evaluation_results_class:Current Best Return = -71.640869140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.706253948199624
INFO:tools.evaluation_results_class:Counted Episodes = 3166
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -87.40593719482422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 312.59405517578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.61289978027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8663.216796875
INFO:tools.evaluation_results_class:Current Best Return = -87.40593719482422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.31232792903028
INFO:tools.evaluation_results_class:Counted Episodes = 3269
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -102.24414825439453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 297.755859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.93251037597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12621.91796875
INFO:tools.evaluation_results_class:Current Best Return = -102.24414825439453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.07052489905787
INFO:tools.evaluation_results_class:Counted Episodes = 3715
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -102.40155792236328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 297.59844970703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.01351928710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12330.638671875
INFO:tools.evaluation_results_class:Current Best Return = -102.40155792236328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.87698092935805
INFO:tools.evaluation_results_class:Counted Episodes = 3723
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -100.04511260986328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 299.95489501953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.09547424316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12420.5078125
INFO:tools.evaluation_results_class:Current Best Return = -100.04511260986328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.00214822771214
INFO:tools.evaluation_results_class:Counted Episodes = 3724
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -103.71336364746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 296.28662109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.58148193359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12872.2119140625
INFO:tools.evaluation_results_class:Current Best Return = -103.71336364746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.05260332796565
INFO:tools.evaluation_results_class:Counted Episodes = 3726
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -109.2425537109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 290.7574462890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.44815063476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19443.115234375
INFO:tools.evaluation_results_class:Current Best Return = -109.2425537109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.30989272943981
INFO:tools.evaluation_results_class:Counted Episodes = 3356
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -101.85346221923828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 298.14654541015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.57789611816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12164.787109375
INFO:tools.evaluation_results_class:Current Best Return = -101.85346221923828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.91304347826087
INFO:tools.evaluation_results_class:Counted Episodes = 3726
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -99.61473083496094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.3852844238281
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.66162109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11855.095703125
INFO:tools.evaluation_results_class:Current Best Return = -99.61473083496094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.82710779082177
INFO:tools.evaluation_results_class:Counted Episodes = 3748
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -100.66783142089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 299.3321838378906
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.82200622558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11634.640625
INFO:tools.evaluation_results_class:Current Best Return = -100.66783142089844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.85201072386059
INFO:tools.evaluation_results_class:Counted Episodes = 3730
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -108.7978515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 291.2021484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.1941375732422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11263.5537109375
INFO:tools.evaluation_results_class:Current Best Return = -108.7978515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.98605898123324
INFO:tools.evaluation_results_class:Counted Episodes = 3730
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -101.30097198486328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 298.69903564453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.8955535888672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12212.1103515625
INFO:tools.evaluation_results_class:Current Best Return = -101.30097198486328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.036578805809576
INFO:tools.evaluation_results_class:Counted Episodes = 3718
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -108.77570343017578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 291.22430419921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.67623901367188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12013.1337890625
INFO:tools.evaluation_results_class:Current Best Return = -108.77570343017578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.74746936600959
INFO:tools.evaluation_results_class:Counted Episodes = 3754
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -102.9206771850586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 297.0793151855469
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.5260772705078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12354.69140625
INFO:tools.evaluation_results_class:Current Best Return = -102.9206771850586
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.05216456036569
INFO:tools.evaluation_results_class:Counted Episodes = 3719
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -98.35382843017578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 301.64617919921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.18846130371094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12216.0732421875
INFO:tools.evaluation_results_class:Current Best Return = -98.35382843017578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.213592233009706
INFO:tools.evaluation_results_class:Counted Episodes = 3708
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -100.6421890258789
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 299.3578186035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.216064453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12016.1630859375
INFO:tools.evaluation_results_class:Current Best Return = -100.6421890258789
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.060573572768696
INFO:tools.evaluation_results_class:Counted Episodes = 3731
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -101.20689392089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 298.7930908203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.16786193847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12210.8291015625
INFO:tools.evaluation_results_class:Current Best Return = -101.20689392089844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.86153434910452
INFO:tools.evaluation_results_class:Counted Episodes = 3741
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -99.89592742919922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.10406494140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.90162658691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11947.935546875
INFO:tools.evaluation_results_class:Current Best Return = -99.89592742919922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.1137772984632
INFO:tools.evaluation_results_class:Counted Episodes = 3709
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -108.76453399658203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 291.2354736328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.41702270507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12276.7001953125
INFO:tools.evaluation_results_class:Current Best Return = -108.76453399658203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.81653333333333
INFO:tools.evaluation_results_class:Counted Episodes = 3750
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -109.50922393798828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 290.49078369140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.78648376464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11269.314453125
INFO:tools.evaluation_results_class:Current Best Return = -109.50922393798828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.81769580326116
INFO:tools.evaluation_results_class:Counted Episodes = 3741
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -110.77268981933594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.227294921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.89041137695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11975.5146484375
INFO:tools.evaluation_results_class:Current Best Return = -110.77268981933594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.74500931594357
INFO:tools.evaluation_results_class:Counted Episodes = 3757
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -92.25505828857422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 307.74493408203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.6206817626953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12544.478515625
INFO:tools.evaluation_results_class:Current Best Return = -92.25505828857422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.08690476190476
INFO:tools.evaluation_results_class:Counted Episodes = 3360
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -110.22639465332031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.7735900878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.0506134033203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11722.2939453125
INFO:tools.evaluation_results_class:Current Best Return = -110.22639465332031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.90987124463519
INFO:tools.evaluation_results_class:Counted Episodes = 3728
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -106.01905822753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.98095703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.9325714111328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10773.880859375
INFO:tools.evaluation_results_class:Current Best Return = -106.01905822753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.93558776167472
INFO:tools.evaluation_results_class:Counted Episodes = 3726
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -112.24795532226562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.7520446777344
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.20831298828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11595.7236328125
INFO:tools.evaluation_results_class:Current Best Return = -112.24795532226562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.2300395256917
INFO:tools.evaluation_results_class:Counted Episodes = 3795
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -110.05420684814453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.94580078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.1699981689453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10603.1875
INFO:tools.evaluation_results_class:Current Best Return = -110.05420684814453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.2021052631579
INFO:tools.evaluation_results_class:Counted Episodes = 3800
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -102.44319152832031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 297.5567932128906
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.88809204101562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11962.98828125
INFO:tools.evaluation_results_class:Current Best Return = -102.44319152832031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.965702036441584
INFO:tools.evaluation_results_class:Counted Episodes = 3732
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.03326416015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.96673583984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.97756958007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11009.8056640625
INFO:tools.evaluation_results_class:Current Best Return = -119.03326416015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.836909871244636
INFO:tools.evaluation_results_class:Counted Episodes = 3728
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.24793243408203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 339.7520751953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 273.9656982421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10459.50390625
INFO:tools.evaluation_results_class:Current Best Return = -60.24793243408203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.01487603305785
INFO:tools.evaluation_results_class:Counted Episodes = 3630
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.0535125732422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.9465026855469
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.520751953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14590.7431640625
INFO:tools.evaluation_results_class:Current Best Return = -136.0535125732422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.6972426972427
INFO:tools.evaluation_results_class:Counted Episodes = 3663
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.63443756103516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 328.3655700683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 263.27874755859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7075.62646484375
INFO:tools.evaluation_results_class:Current Best Return = -71.63443756103516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.798132381213954
INFO:tools.evaluation_results_class:Counted Episodes = 3641
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -91.89739990234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 308.10260009765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 247.9543914794922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28360.939453125
INFO:tools.evaluation_results_class:Current Best Return = -91.89739990234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.81367989056088
INFO:tools.evaluation_results_class:Counted Episodes = 3655
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -100.01022338867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 299.9897766113281
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.09864807128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10885.517578125
INFO:tools.evaluation_results_class:Current Best Return = -100.01022338867188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.174585635359115
INFO:tools.evaluation_results_class:Counted Episodes = 3620
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -127.91360473632812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.0863952636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.72235107421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17065.935546875
INFO:tools.evaluation_results_class:Current Best Return = -127.91360473632812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.93184867185404
INFO:tools.evaluation_results_class:Counted Episodes = 3727
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -135.50772094726562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.4922790527344
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.0462188720703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18136.4140625
INFO:tools.evaluation_results_class:Current Best Return = -135.50772094726562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.69632392115077
INFO:tools.evaluation_results_class:Counted Episodes = 3754
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -137.8871307373047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.11285400390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.6846923828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19149.70703125
INFO:tools.evaluation_results_class:Current Best Return = -137.8871307373047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.88203753351206
INFO:tools.evaluation_results_class:Counted Episodes = 3730
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -133.2815399169922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.7184753417969
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.92127990722656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18336.33984375
INFO:tools.evaluation_results_class:Current Best Return = -133.2815399169922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.91411701556629
INFO:tools.evaluation_results_class:Counted Episodes = 3726
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -116.55094909667969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.4490661621094
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.5853271484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18175.1953125
INFO:tools.evaluation_results_class:Current Best Return = -116.55094909667969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.30400890868597
INFO:tools.evaluation_results_class:Counted Episodes = 3592
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.0830841064453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.9169006347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.4738311767578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19250.646484375
INFO:tools.evaluation_results_class:Current Best Return = -134.0830841064453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.81184668989547
INFO:tools.evaluation_results_class:Counted Episodes = 3731
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -129.72897338867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.2710266113281
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.2489471435547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16508.455078125
INFO:tools.evaluation_results_class:Current Best Return = -129.72897338867188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.7444089456869
INFO:tools.evaluation_results_class:Counted Episodes = 3756
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -133.19418334960938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.8058166503906
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.90931701660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17328.0546875
INFO:tools.evaluation_results_class:Current Best Return = -133.19418334960938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.789529914529915
INFO:tools.evaluation_results_class:Counted Episodes = 3744
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.61094665527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.3890686035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.16127014160156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18444.669921875
INFO:tools.evaluation_results_class:Current Best Return = -141.61094665527344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.136424912375304
INFO:tools.evaluation_results_class:Counted Episodes = 3709
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -133.40867614746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.59130859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.05169677734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16619.140625
INFO:tools.evaluation_results_class:Current Best Return = -133.40867614746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.660276890308836
INFO:tools.evaluation_results_class:Counted Episodes = 3756
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.39100646972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.6090087890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.9544677734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19560.96484375
INFO:tools.evaluation_results_class:Current Best Return = -142.39100646972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.911087305838244
INFO:tools.evaluation_results_class:Counted Episodes = 3734
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -131.52767944335938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.4723205566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.45713806152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17394.515625
INFO:tools.evaluation_results_class:Current Best Return = -131.52767944335938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.848622626370684
INFO:tools.evaluation_results_class:Counted Episodes = 3739
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -133.12388610839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.8760986328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.6475067138672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17356.7109375
INFO:tools.evaluation_results_class:Current Best Return = -133.12388610839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.17004048582996
INFO:tools.evaluation_results_class:Counted Episodes = 3705
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -131.1778564453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.8221435546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.8665313720703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18260.9765625
INFO:tools.evaluation_results_class:Current Best Return = -131.1778564453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.84006418828564
INFO:tools.evaluation_results_class:Counted Episodes = 3739
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -131.47097778320312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.5290222167969
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.2550048828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17462.953125
INFO:tools.evaluation_results_class:Current Best Return = -131.47097778320312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.0967221923697
INFO:tools.evaluation_results_class:Counted Episodes = 3722
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -128.18495178222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.8150634765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.63284301757812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16502.111328125
INFO:tools.evaluation_results_class:Current Best Return = -128.18495178222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.78809714438217
INFO:tools.evaluation_results_class:Counted Episodes = 3747
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.17481994628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.8251647949219
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.79376220703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17144.041015625
INFO:tools.evaluation_results_class:Current Best Return = -143.17481994628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.02259279182356
INFO:tools.evaluation_results_class:Counted Episodes = 3718
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.36691284179688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.6330871582031
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.74185180664062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17720.095703125
INFO:tools.evaluation_results_class:Current Best Return = -142.36691284179688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.873864243719936
INFO:tools.evaluation_results_class:Counted Episodes = 3742
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.32691955566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.673095703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.62059020996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18828.44140625
INFO:tools.evaluation_results_class:Current Best Return = -141.32691955566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.849772788024595
INFO:tools.evaluation_results_class:Counted Episodes = 3741
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -87.06901550292969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 312.9309997558594
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.4242706298828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12254.54296875
INFO:tools.evaluation_results_class:Current Best Return = -87.06901550292969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.89909265878471
INFO:tools.evaluation_results_class:Counted Episodes = 3637
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.44979858398438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.55020141601562
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.67031860351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18148.6484375
INFO:tools.evaluation_results_class:Current Best Return = -145.44979858398438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.972543741588154
INFO:tools.evaluation_results_class:Counted Episodes = 3715
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.86837768554688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.1316223144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.22409057617188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17621.0234375
INFO:tools.evaluation_results_class:Current Best Return = -138.86837768554688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.32324324324324
INFO:tools.evaluation_results_class:Counted Episodes = 3700
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -149.0034637451172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.9965362548828
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.02407836914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19388.4296875
INFO:tools.evaluation_results_class:Current Best Return = -149.0034637451172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.729989327641405
INFO:tools.evaluation_results_class:Counted Episodes = 3748
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -152.2425079345703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.7574920654297
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.2731170654297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18269.8515625
INFO:tools.evaluation_results_class:Current Best Return = -152.2425079345703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.430618201114356
INFO:tools.evaluation_results_class:Counted Episodes = 3769
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -131.9017333984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.0982666015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.44422912597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17559.029296875
INFO:tools.evaluation_results_class:Current Best Return = -131.9017333984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.58937583001328
INFO:tools.evaluation_results_class:Counted Episodes = 3765
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.8403778076172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.1596221923828
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.74757385253906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15639.1708984375
INFO:tools.evaluation_results_class:Current Best Return = -145.8403778076172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.56706507304117
INFO:tools.evaluation_results_class:Counted Episodes = 3765
INFO:robust_rl.robust_rl_trainer:Iteration 27 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -96.58343505859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 303.41656494140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.85000610351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10850.7333984375
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.52772290267352
INFO:tools.evaluation_results_class:Counted Episodes = 7593
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.5318078994750977
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 1.203798770904541
INFO:agents.father_agent:Step: 10, Training loss: 0.9760342240333557
INFO:agents.father_agent:Step: 15, Training loss: 0.707390546798706
INFO:agents.father_agent:Step: 20, Training loss: 0.868134617805481
INFO:agents.father_agent:Step: 25, Training loss: 0.882068395614624
INFO:agents.father_agent:Step: 30, Training loss: 0.731939971446991
INFO:agents.father_agent:Step: 35, Training loss: 0.8296213746070862
INFO:agents.father_agent:Step: 40, Training loss: 0.8221533894538879
INFO:agents.father_agent:Step: 45, Training loss: 0.8238678574562073
INFO:agents.father_agent:Step: 50, Training loss: 1.0062947273254395
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -101.23770904541016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 298.7622985839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.63058471679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10197.5478515625
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.98823364616893
INFO:tools.evaluation_results_class:Counted Episodes = 7139
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -104.41419219970703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 295.5858154296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.8205108642578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10798.6904296875
INFO:tools.evaluation_results_class:Current Best Return = -104.41419219970703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.92155196828094
INFO:tools.evaluation_results_class:Counted Episodes = 7062
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -182.20672607421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 217.79327392578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 154.2860870361328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29685.599609375
INFO:tools.evaluation_results_class:Current Best Return = -182.20672607421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.27485380116959
INFO:tools.evaluation_results_class:Counted Episodes = 6840
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=1, o1y=3, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 199.95565267936868
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=1, o1y=3, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 28 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -381.9171142578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -8.816298484802246
INFO:tools.evaluation_results_class:Average Discounted Reward = -17.66948127746582
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9343922651933702
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 200866.078125
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 168.0531767955801
INFO:tools.evaluation_results_class:Counted Episodes = 1448
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.480897307395935
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 1.7189353704452515
INFO:agents.father_agent:Step: 10, Training loss: 1.6650217771530151
INFO:agents.father_agent:Step: 15, Training loss: 1.033545970916748
INFO:agents.father_agent:Step: 20, Training loss: 1.0197327136993408
INFO:agents.father_agent:Step: 25, Training loss: 1.140230417251587
INFO:agents.father_agent:Step: 30, Training loss: 1.0210323333740234
INFO:agents.father_agent:Step: 35, Training loss: 1.15964937210083
INFO:agents.father_agent:Step: 40, Training loss: 1.0398688316345215
INFO:agents.father_agent:Step: 45, Training loss: 0.993904709815979
INFO:agents.father_agent:Step: 50, Training loss: 1.2289735078811646
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 55, Training loss: 1.195337176322937
INFO:agents.father_agent:Step: 60, Training loss: 1.0289841890335083
INFO:agents.father_agent:Step: 65, Training loss: 1.1623483896255493
INFO:agents.father_agent:Step: 70, Training loss: 1.0717154741287231
INFO:agents.father_agent:Step: 75, Training loss: 0.8864443898200989
INFO:agents.father_agent:Step: 80, Training loss: 0.8666013479232788
INFO:agents.father_agent:Step: 85, Training loss: 0.7642080187797546
INFO:agents.father_agent:Step: 90, Training loss: 0.7308334112167358
INFO:agents.father_agent:Step: 95, Training loss: 0.6479013562202454
INFO:agents.father_agent:Step: 100, Training loss: 0.5498440265655518
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.1725845336914
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 327.8274230957031
INFO:tools.evaluation_results_class:Average Discounted Reward = 264.6919250488281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7160.4873046875
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.99947780678851
INFO:tools.evaluation_results_class:Counted Episodes = 7660
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -73.87627410888672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 326.12371826171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 262.904296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7711.4736328125
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.28777166797591
INFO:tools.evaluation_results_class:Counted Episodes = 7638
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.32059478759766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 316.6794128417969
INFO:tools.evaluation_results_class:Average Discounted Reward = 250.4881591796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11074.6181640625
INFO:tools.evaluation_results_class:Current Best Return = -83.32059478759766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.14174541411896
INFO:tools.evaluation_results_class:Counted Episodes = 7196
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -183.36285400390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 216.63714599609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 155.2893829345703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27526.81640625
INFO:tools.evaluation_results_class:Current Best Return = -183.36285400390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.078965661059364
INFO:tools.evaluation_results_class:Counted Episodes = 7193
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 200.4565806285441
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 29 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -175.49276733398438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 220.94520568847656
INFO:tools.evaluation_results_class:Average Discounted Reward = 151.7986602783203
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9913120962352417
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29489.939453125
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.37402539541101
INFO:tools.evaluation_results_class:Counted Episodes = 4489
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.9767591953277588
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 1.2494392395019531
INFO:agents.father_agent:Step: 10, Training loss: 0.9780796766281128
INFO:agents.father_agent:Step: 15, Training loss: 0.9981979131698608
INFO:agents.father_agent:Step: 20, Training loss: 0.7753078937530518
INFO:agents.father_agent:Step: 25, Training loss: 0.9266684651374817
INFO:agents.father_agent:Step: 30, Training loss: 0.9228315949440002
INFO:agents.father_agent:Step: 35, Training loss: 0.895184338092804
INFO:agents.father_agent:Step: 40, Training loss: 0.9840940237045288
INFO:agents.father_agent:Step: 45, Training loss: 0.8583725690841675
INFO:agents.father_agent:Step: 50, Training loss: 0.8275651931762695
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 55, Training loss: 0.6981421709060669
INFO:agents.father_agent:Step: 60, Training loss: 0.8032023906707764
INFO:agents.father_agent:Step: 65, Training loss: 0.7859696745872498
INFO:agents.father_agent:Step: 70, Training loss: 0.5812509059906006
INFO:agents.father_agent:Step: 75, Training loss: 0.5872711539268494
INFO:agents.father_agent:Step: 80, Training loss: 0.39284396171569824
INFO:agents.father_agent:Step: 85, Training loss: 0.5222288370132446
INFO:agents.father_agent:Step: 90, Training loss: 0.43580350279808044
INFO:agents.father_agent:Step: 95, Training loss: 0.42482030391693115
INFO:agents.father_agent:Step: 100, Training loss: 0.6911883354187012
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -761.216796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -765.611328125
INFO:tools.evaluation_results_class:Average Discounted Reward = -172.57321166992188
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.013671875
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1425784.375
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 592.482421875
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -647.021484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -646.611328125
INFO:tools.evaluation_results_class:Average Discounted Reward = -132.69857788085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.025390625
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1134807.75
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 586.447265625
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1033.7042236328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1026.9532470703125
INFO:tools.evaluation_results_class:Average Discounted Reward = -242.21627807617188
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.04085603112840467
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1305026.875
INFO:tools.evaluation_results_class:Current Best Return = -1033.7042236328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.04085603112840467
INFO:tools.evaluation_results_class:Average Episode Length = 585.0019455252918
INFO:tools.evaluation_results_class:Counted Episodes = 514
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1004.57421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1013.7734375
INFO:tools.evaluation_results_class:Average Discounted Reward = -215.65304565429688
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.001953125
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1693965.0
INFO:tools.evaluation_results_class:Current Best Return = -1004.57421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.001953125
INFO:tools.evaluation_results_class:Average Episode Length = 600.041015625
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=1, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 28292.845832869512
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=1, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 30 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -139.40777587890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.59222412109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 188.3694305419922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17579.765625
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.525008868393044
INFO:tools.evaluation_results_class:Counted Episodes = 5638
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.2197246551513672
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 1.2506190538406372
INFO:agents.father_agent:Step: 10, Training loss: 0.974763810634613
INFO:agents.father_agent:Step: 15, Training loss: 0.8616572618484497
INFO:agents.father_agent:Step: 20, Training loss: 0.9102120995521545
INFO:agents.father_agent:Step: 25, Training loss: 0.8010303974151611
INFO:agents.father_agent:Step: 30, Training loss: 0.9961329102516174
INFO:agents.father_agent:Step: 35, Training loss: 0.9437063932418823
INFO:agents.father_agent:Step: 40, Training loss: 1.007274866104126
INFO:agents.father_agent:Step: 45, Training loss: 1.2600723505020142
INFO:agents.father_agent:Step: 50, Training loss: 1.0673155784606934
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 55, Training loss: 1.035881757736206
INFO:agents.father_agent:Step: 60, Training loss: 1.1505701541900635
INFO:agents.father_agent:Step: 65, Training loss: 0.7883312106132507
INFO:agents.father_agent:Step: 70, Training loss: 0.7868226170539856
INFO:agents.father_agent:Step: 75, Training loss: 0.6952623128890991
INFO:agents.father_agent:Step: 80, Training loss: 0.613037109375
INFO:agents.father_agent:Step: 85, Training loss: 0.5426677465438843
INFO:agents.father_agent:Step: 90, Training loss: 0.41976869106292725
INFO:agents.father_agent:Step: 95, Training loss: 0.5129774808883667
INFO:agents.father_agent:Step: 100, Training loss: 0.30756109952926636
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -98.49026489257812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 301.3434753417969
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.648681640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9995944849959448
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20809.0
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.0831305758313
INFO:tools.evaluation_results_class:Counted Episodes = 2466
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -97.85002136230469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 302.1499938964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.4735565185547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26137.888671875
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.58208350222942
INFO:tools.evaluation_results_class:Counted Episodes = 2467
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.31851196289062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.68148803710938
INFO:tools.evaluation_results_class:Average Discounted Reward = 144.2047119140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 89030.1640625
INFO:tools.evaluation_results_class:Current Best Return = -154.31851196289062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 121.98475222363405
INFO:tools.evaluation_results_class:Counted Episodes = 2361
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -339.8827819824219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 60.117210388183594
INFO:tools.evaluation_results_class:Average Discounted Reward = -31.942331314086914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 94605.859375
INFO:tools.evaluation_results_class:Current Best Return = -339.8827819824219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.88317757009345
INFO:tools.evaluation_results_class:Counted Episodes = 2568
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=1, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 376.6746242113611
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=1, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 31 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -101.55931091308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 298.4407043457031
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.44664001464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13069.525390625
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.58371781515106
INFO:tools.evaluation_results_class:Counted Episodes = 6719
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.209092140197754
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 1.1278412342071533
INFO:agents.father_agent:Step: 10, Training loss: 1.0577021837234497
INFO:agents.father_agent:Step: 15, Training loss: 0.7238051891326904
INFO:agents.father_agent:Step: 20, Training loss: 0.8305778503417969
INFO:agents.father_agent:Step: 25, Training loss: 0.5829258561134338
INFO:agents.father_agent:Step: 30, Training loss: 0.6285169720649719
INFO:agents.father_agent:Step: 35, Training loss: 0.5923324227333069
INFO:agents.father_agent:Step: 40, Training loss: 0.39949485659599304
INFO:agents.father_agent:Step: 45, Training loss: 0.5296128392219543
INFO:agents.father_agent:Step: 50, Training loss: 0.3366830348968506
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 55, Training loss: 0.7324230074882507
INFO:agents.father_agent:Step: 60, Training loss: 0.407356321811676
INFO:agents.father_agent:Step: 65, Training loss: 0.489141047000885
INFO:agents.father_agent:Step: 70, Training loss: 0.426323264837265
INFO:agents.father_agent:Step: 75, Training loss: 0.3840353786945343
INFO:agents.father_agent:Step: 80, Training loss: 0.33307331800460815
INFO:agents.father_agent:Step: 85, Training loss: 0.32326459884643555
INFO:agents.father_agent:Step: 90, Training loss: 0.3268320560455322
INFO:agents.father_agent:Step: 95, Training loss: 0.3321225941181183
INFO:agents.father_agent:Step: 100, Training loss: 0.394395649433136
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.36848449707031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 331.6315002441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.3562774658203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4853.31201171875
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 71.39688812052358
INFO:tools.evaluation_results_class:Counted Episodes = 4049
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -63.946937561035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 336.0530700683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.50404357910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5007.62451171875
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.19531639262581
INFO:tools.evaluation_results_class:Counted Episodes = 4014
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.56201934814453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 328.43798828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.927490234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10262.15234375
INFO:tools.evaluation_results_class:Current Best Return = -71.56201934814453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.38347718865597
INFO:tools.evaluation_results_class:Counted Episodes = 4055
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -274.33935546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 125.66062927246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.96538543701172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 70431.390625
INFO:tools.evaluation_results_class:Current Best Return = -274.33935546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.41354502604813
INFO:tools.evaluation_results_class:Counted Episodes = 4031
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=1, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 306.91266792030166
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=1, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -22.974655151367188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 377.0253601074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 302.193359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1703.0823974609375
INFO:tools.evaluation_results_class:Current Best Return = -22.974655151367188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.82027649769585
INFO:tools.evaluation_results_class:Counted Episodes = 3472
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -85.88572692871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 314.1142578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 252.94635009765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5066.49658203125
INFO:tools.evaluation_results_class:Current Best Return = -85.88572692871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.76216511755057
INFO:tools.evaluation_results_class:Counted Episodes = 3658
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -19.70652198791504
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 380.2934875488281
INFO:tools.evaluation_results_class:Average Discounted Reward = 304.9066467285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2241.0888671875
INFO:tools.evaluation_results_class:Current Best Return = -19.70652198791504
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.65675057208238
INFO:tools.evaluation_results_class:Counted Episodes = 3496
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -15.501145362854004
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 384.49884033203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 308.76495361328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4775.4892578125
INFO:tools.evaluation_results_class:Current Best Return = -15.501145362854004
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.63459335624284
INFO:tools.evaluation_results_class:Counted Episodes = 3492
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.8079719543457
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 345.1920166015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 277.4433898925781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3128.00830078125
INFO:tools.evaluation_results_class:Current Best Return = -54.8079719543457
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.46376811594203
INFO:tools.evaluation_results_class:Counted Episodes = 3588
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.21221923828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 316.78778076171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.87872314453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9184.3046875
INFO:tools.evaluation_results_class:Current Best Return = -83.21221923828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 76.47802786709539
INFO:tools.evaluation_results_class:Counted Episodes = 1866
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.28907775878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 322.7109069824219
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.99139404296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6434.33935546875
INFO:tools.evaluation_results_class:Current Best Return = -77.28907775878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.04068522483941
INFO:tools.evaluation_results_class:Counted Episodes = 1868
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.36537170410156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 320.6346130371094
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.61529541015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11167.6240234375
INFO:tools.evaluation_results_class:Current Best Return = -79.36537170410156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 75.92033988316516
INFO:tools.evaluation_results_class:Counted Episodes = 1883
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.87947845458984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 320.1205139160156
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.03701782226562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12021.1044921875
INFO:tools.evaluation_results_class:Current Best Return = -79.87947845458984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.42236699239956
INFO:tools.evaluation_results_class:Counted Episodes = 1842
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.075599670410156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 349.9244079589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 273.8006286621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4384.57421875
INFO:tools.evaluation_results_class:Current Best Return = -50.075599670410156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.26991565135895
INFO:tools.evaluation_results_class:Counted Episodes = 3201
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.09100341796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 322.90899658203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.80068969726562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8684.072265625
INFO:tools.evaluation_results_class:Current Best Return = -77.09100341796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 76.31825439063331
INFO:tools.evaluation_results_class:Counted Episodes = 1879
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.59479522705078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 320.40521240234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.65963745117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9550.154296875
INFO:tools.evaluation_results_class:Current Best Return = -79.59479522705078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 76.20074349442379
INFO:tools.evaluation_results_class:Counted Episodes = 1883
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.2232894897461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 324.7767028808594
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.8846435546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6815.9775390625
INFO:tools.evaluation_results_class:Current Best Return = -75.2232894897461
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 76.53418803418803
INFO:tools.evaluation_results_class:Counted Episodes = 1872
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.35417938232422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 319.64581298828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 206.97328186035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12679.142578125
INFO:tools.evaluation_results_class:Current Best Return = -80.35417938232422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 80.95395110858442
INFO:tools.evaluation_results_class:Counted Episodes = 1759
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.8550033569336
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 322.1449890136719
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.57357788085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7045.1884765625
INFO:tools.evaluation_results_class:Current Best Return = -77.8550033569336
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 76.85928303905833
INFO:tools.evaluation_results_class:Counted Episodes = 1869
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.68704223632812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 324.3129577636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.5027618408203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8173.076171875
INFO:tools.evaluation_results_class:Current Best Return = -75.68704223632812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 80.79569892473118
INFO:tools.evaluation_results_class:Counted Episodes = 1767
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.02318572998047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 317.976806640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.09437561035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10670.166015625
INFO:tools.evaluation_results_class:Current Best Return = -82.02318572998047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 76.01053740779768
INFO:tools.evaluation_results_class:Counted Episodes = 1898
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.57142639160156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 317.4285583496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.8973388671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10408.8466796875
INFO:tools.evaluation_results_class:Current Best Return = -82.57142639160156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 75.78651088688264
INFO:tools.evaluation_results_class:Counted Episodes = 1883
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.42520141601562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 319.5747985839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.44175720214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9841.0146484375
INFO:tools.evaluation_results_class:Current Best Return = -80.42520141601562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 76.77533512064343
INFO:tools.evaluation_results_class:Counted Episodes = 1865
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.94313049316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 319.0568542480469
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.11196899414062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11713.1982421875
INFO:tools.evaluation_results_class:Current Best Return = -80.94313049316406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.0
INFO:tools.evaluation_results_class:Counted Episodes = 1864
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.19999694824219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 316.79998779296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.86489868164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10904.24609375
INFO:tools.evaluation_results_class:Current Best Return = -83.19999694824219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 75.5475065616798
INFO:tools.evaluation_results_class:Counted Episodes = 1905
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.85682678222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 320.1431884765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.0164031982422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12424.14453125
INFO:tools.evaluation_results_class:Current Best Return = -79.85682678222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.54938956714761
INFO:tools.evaluation_results_class:Counted Episodes = 1802
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.47018432617188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 322.5298156738281
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.54441833496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8682.2412109375
INFO:tools.evaluation_results_class:Current Best Return = -77.47018432617188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.04599659284497
INFO:tools.evaluation_results_class:Counted Episodes = 1761
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.44782257080078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 324.55218505859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.51856994628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9587.6298828125
INFO:tools.evaluation_results_class:Current Best Return = -75.44782257080078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.7545871559633
INFO:tools.evaluation_results_class:Counted Episodes = 1744
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -31.28795051574707
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 368.7120361328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 283.4292907714844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3027.328125
INFO:tools.evaluation_results_class:Current Best Return = -31.28795051574707
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.226004084411166
INFO:tools.evaluation_results_class:Counted Episodes = 2938
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -78.65521240234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 321.34478759765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.06451416015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11190.8525390625
INFO:tools.evaluation_results_class:Current Best Return = -78.65521240234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 80.81464788732394
INFO:tools.evaluation_results_class:Counted Episodes = 1775
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.2197036743164
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 320.7803039550781
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.42820739746094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12644.5927734375
INFO:tools.evaluation_results_class:Current Best Return = -79.2197036743164
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 80.75877689694224
INFO:tools.evaluation_results_class:Counted Episodes = 1766
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.96073913574219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 329.0392761230469
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.74034118652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6833.77001953125
INFO:tools.evaluation_results_class:Current Best Return = -70.96073913574219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.57546012269938
INFO:tools.evaluation_results_class:Counted Episodes = 1630
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.36396789550781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 328.6360168457031
INFO:tools.evaluation_results_class:Average Discounted Reward = 206.61981201171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7817.388671875
INFO:tools.evaluation_results_class:Current Best Return = -71.36396789550781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.73039215686275
INFO:tools.evaluation_results_class:Counted Episodes = 1632
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.78709411621094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 320.212890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.87644958496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8072.572265625
INFO:tools.evaluation_results_class:Current Best Return = -79.78709411621094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.01075268817205
INFO:tools.evaluation_results_class:Counted Episodes = 1860
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.87187957763672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 329.12811279296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.45069885253906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5770.55615234375
INFO:tools.evaluation_results_class:Current Best Return = -70.87187957763672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 69.46449136276391
INFO:tools.evaluation_results_class:Counted Episodes = 2084
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.3629379272461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 331.6370544433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.63087463378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5056.04150390625
INFO:tools.evaluation_results_class:Current Best Return = -68.3629379272461
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 69.50456072971676
INFO:tools.evaluation_results_class:Counted Episodes = 2083
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.49393463134766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 319.5060729980469
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.8325958251953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9603.017578125
INFO:tools.evaluation_results_class:Current Best Return = -80.49393463134766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 75.6657880864523
INFO:tools.evaluation_results_class:Counted Episodes = 1897
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -78.66358184814453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 321.33642578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.8030548095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12666.62109375
INFO:tools.evaluation_results_class:Current Best Return = -78.66358184814453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.55802469135803
INFO:tools.evaluation_results_class:Counted Episodes = 1620
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.41194152832031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 324.5880432128906
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.47796630859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10436.365234375
INFO:tools.evaluation_results_class:Current Best Return = -75.41194152832031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.18220597196832
INFO:tools.evaluation_results_class:Counted Episodes = 1641
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.76290130615234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 328.2370910644531
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.7270965576172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7958.19140625
INFO:tools.evaluation_results_class:Current Best Return = -71.76290130615234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.35135135135135
INFO:tools.evaluation_results_class:Counted Episodes = 1628
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -96.48242950439453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 303.517578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.4071044921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15991.4306640625
INFO:tools.evaluation_results_class:Current Best Return = -96.48242950439453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 71.75301204819277
INFO:tools.evaluation_results_class:Counted Episodes = 1992
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -216.94261169433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 183.05738830566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 112.54178619384766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38131.4296875
INFO:tools.evaluation_results_class:Current Best Return = -216.94261169433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 71.83033932135729
INFO:tools.evaluation_results_class:Counted Episodes = 2004
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.61564636230469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 326.3843688964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.4722442626953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10543.67578125
INFO:tools.evaluation_results_class:Current Best Return = -73.61564636230469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 70.1807580174927
INFO:tools.evaluation_results_class:Counted Episodes = 2058
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -23.575458526611328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 376.4245300292969
INFO:tools.evaluation_results_class:Average Discounted Reward = 263.42425537109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1224.1214599609375
INFO:tools.evaluation_results_class:Current Best Return = -23.575458526611328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 70.60168233547749
INFO:tools.evaluation_results_class:Counted Episodes = 2021
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -197.8446044921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 202.1553955078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 133.1200408935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36709.53515625
INFO:tools.evaluation_results_class:Current Best Return = -197.8446044921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 70.35717809570794
INFO:tools.evaluation_results_class:Counted Episodes = 2027
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -290.7293701171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.27062225341797
INFO:tools.evaluation_results_class:Average Discounted Reward = 28.548208236694336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 74857.7578125
INFO:tools.evaluation_results_class:Current Best Return = -290.7293701171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.77525773195876
INFO:tools.evaluation_results_class:Counted Episodes = 1940
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -288.93701171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 111.06298065185547
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.205507278442383
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 69937.4921875
INFO:tools.evaluation_results_class:Current Best Return = -288.93701171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.2278545826933
INFO:tools.evaluation_results_class:Counted Episodes = 1953
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -290.74359130859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.25642395019531
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.02700424194336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 73717.3984375
INFO:tools.evaluation_results_class:Current Best Return = -290.74359130859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.89516957862281
INFO:tools.evaluation_results_class:Counted Episodes = 1946
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -284.43743896484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 115.56256103515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.239044189453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 70218.625
INFO:tools.evaluation_results_class:Current Best Return = -284.43743896484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.56980351602895
INFO:tools.evaluation_results_class:Counted Episodes = 1934
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -152.3269500732422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.6730499267578
INFO:tools.evaluation_results_class:Average Discounted Reward = 151.8086395263672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22740.9140625
INFO:tools.evaluation_results_class:Current Best Return = -152.3269500732422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 69.9214531173294
INFO:tools.evaluation_results_class:Counted Episodes = 2037
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -295.265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 104.73436737060547
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.765653610229492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 73330.8671875
INFO:tools.evaluation_results_class:Current Best Return = -295.265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.81395348837209
INFO:tools.evaluation_results_class:Counted Episodes = 1935
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -290.5588684082031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.44114685058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 28.81552505493164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 70450.40625
INFO:tools.evaluation_results_class:Current Best Return = -290.5588684082031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.3080859774821
INFO:tools.evaluation_results_class:Counted Episodes = 1954
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -303.92547607421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 96.07450866699219
INFO:tools.evaluation_results_class:Average Discounted Reward = 17.74615478515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 75034.328125
INFO:tools.evaluation_results_class:Current Best Return = -303.92547607421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.59095580678314
INFO:tools.evaluation_results_class:Counted Episodes = 1946
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -293.8609313964844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 106.1390609741211
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.196247100830078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 75662.59375
INFO:tools.evaluation_results_class:Current Best Return = -293.8609313964844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 74.14375
INFO:tools.evaluation_results_class:Counted Episodes = 1920
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -300.5062561035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 99.4937515258789
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.787282943725586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 75926.109375
INFO:tools.evaluation_results_class:Current Best Return = -300.5062561035156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.64166666666667
INFO:tools.evaluation_results_class:Counted Episodes = 1920
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -301.6900329589844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 98.30997467041016
INFO:tools.evaluation_results_class:Average Discounted Reward = 20.08571434020996
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 73903.6171875
INFO:tools.evaluation_results_class:Current Best Return = -301.6900329589844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.7621483375959
INFO:tools.evaluation_results_class:Counted Episodes = 1955
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -285.4909362792969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 114.50906372070312
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.12358856201172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 67518.4296875
INFO:tools.evaluation_results_class:Current Best Return = -285.4909362792969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.36253776435045
INFO:tools.evaluation_results_class:Counted Episodes = 1986
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -293.6370544433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 106.36295318603516
INFO:tools.evaluation_results_class:Average Discounted Reward = 27.49102210998535
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 77087.0078125
INFO:tools.evaluation_results_class:Current Best Return = -293.6370544433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.01807228915662
INFO:tools.evaluation_results_class:Counted Episodes = 1992
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -300.2701721191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 99.72984313964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 20.046630859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 76523.96875
INFO:tools.evaluation_results_class:Current Best Return = -300.2701721191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.73086800205444
INFO:tools.evaluation_results_class:Counted Episodes = 1947
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -295.7516784667969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 104.24830627441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.948060989379883
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 72036.890625
INFO:tools.evaluation_results_class:Current Best Return = -295.7516784667969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 74.57470067673087
INFO:tools.evaluation_results_class:Counted Episodes = 1921
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -286.4814147949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 113.5185775756836
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.21929168701172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 75617.796875
INFO:tools.evaluation_results_class:Current Best Return = -286.4814147949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.85394402035624
INFO:tools.evaluation_results_class:Counted Episodes = 1965
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -298.5105895996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 101.4894027709961
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.737667083740234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 75132.984375
INFO:tools.evaluation_results_class:Current Best Return = -298.5105895996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.63975782038345
INFO:tools.evaluation_results_class:Counted Episodes = 1982
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -300.6359558105469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 99.36405181884766
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.42259979248047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 73863.234375
INFO:tools.evaluation_results_class:Current Best Return = -300.6359558105469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.12135176651306
INFO:tools.evaluation_results_class:Counted Episodes = 1953
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -287.77874755859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 112.22125244140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.59309196472168
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 68882.6484375
INFO:tools.evaluation_results_class:Current Best Return = -287.77874755859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.21355236139631
INFO:tools.evaluation_results_class:Counted Episodes = 1948
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -108.10967254638672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 291.89031982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.44757080078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17344.861328125
INFO:tools.evaluation_results_class:Current Best Return = -108.10967254638672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 71.18843469591226
INFO:tools.evaluation_results_class:Counted Episodes = 2006
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -294.3911437988281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 105.60884857177734
INFO:tools.evaluation_results_class:Average Discounted Reward = 24.900672912597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 73468.6875
INFO:tools.evaluation_results_class:Current Best Return = -294.3911437988281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 74.04323211528565
INFO:tools.evaluation_results_class:Counted Episodes = 1943
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -291.6776428222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 108.3223648071289
INFO:tools.evaluation_results_class:Average Discounted Reward = 27.781166076660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 70918.59375
INFO:tools.evaluation_results_class:Current Best Return = -291.6776428222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.57634961439588
INFO:tools.evaluation_results_class:Counted Episodes = 1945
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -300.39990234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 99.60009765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.13173484802246
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 77593.3046875
INFO:tools.evaluation_results_class:Current Best Return = -300.39990234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.45514445007602
INFO:tools.evaluation_results_class:Counted Episodes = 1973
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -316.8521728515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 83.14781951904297
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.723708152770996
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 79691.578125
INFO:tools.evaluation_results_class:Current Best Return = -316.8521728515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 74.74276696475539
INFO:tools.evaluation_results_class:Counted Episodes = 1901
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -304.58636474609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 95.41363525390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 16.886470794677734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 76966.7265625
INFO:tools.evaluation_results_class:Current Best Return = -304.58636474609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.00563813429011
INFO:tools.evaluation_results_class:Counted Episodes = 1951
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -245.53652954101562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 154.46347045898438
INFO:tools.evaluation_results_class:Average Discounted Reward = 79.46546173095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36723.90625
INFO:tools.evaluation_results_class:Current Best Return = -245.53652954101562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.11701584057231
INFO:tools.evaluation_results_class:Counted Episodes = 1957
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -246.7708740234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 153.2291259765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.5694351196289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41505.296875
INFO:tools.evaluation_results_class:Current Best Return = -246.7708740234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.73319755600815
INFO:tools.evaluation_results_class:Counted Episodes = 1964
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -289.5020751953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 110.49793243408203
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.634130477905273
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 72308.46875
INFO:tools.evaluation_results_class:Current Best Return = -289.5020751953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.88819875776397
INFO:tools.evaluation_results_class:Counted Episodes = 1932
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -298.9417724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 101.05821990966797
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.945741653442383
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 71687.0546875
INFO:tools.evaluation_results_class:Current Best Return = -298.9417724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.59402369912416
INFO:tools.evaluation_results_class:Counted Episodes = 1941
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -307.1753845214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 92.82461547851562
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.71734619140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 78000.359375
INFO:tools.evaluation_results_class:Current Best Return = -307.1753845214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.33948717948718
INFO:tools.evaluation_results_class:Counted Episodes = 1950
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -305.7952575683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 94.2047348022461
INFO:tools.evaluation_results_class:Average Discounted Reward = 15.378957748413086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 76781.9453125
INFO:tools.evaluation_results_class:Current Best Return = -305.7952575683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.51851851851852
INFO:tools.evaluation_results_class:Counted Episodes = 1944
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 32 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -88.82768249511719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 311.17230224609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.482177734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8523.97265625
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.49533068952284
INFO:tools.evaluation_results_class:Counted Episodes = 7817
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.8770317435264587
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 0.8488659858703613
INFO:agents.father_agent:Step: 10, Training loss: 0.7422643899917603
INFO:agents.father_agent:Step: 15, Training loss: 0.801142156124115
INFO:agents.father_agent:Step: 20, Training loss: 0.6468077898025513
INFO:agents.father_agent:Step: 25, Training loss: 0.81463623046875
INFO:agents.father_agent:Step: 30, Training loss: 0.8441023230552673
INFO:agents.father_agent:Step: 35, Training loss: 0.8233814239501953
INFO:agents.father_agent:Step: 40, Training loss: 0.9703472852706909
INFO:agents.father_agent:Step: 45, Training loss: 0.5633774995803833
INFO:agents.father_agent:Step: 50, Training loss: 0.5861519575119019
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 55, Training loss: 0.46980196237564087
INFO:agents.father_agent:Step: 60, Training loss: 0.5279951095581055
INFO:agents.father_agent:Step: 65, Training loss: 0.48284292221069336
INFO:agents.father_agent:Step: 70, Training loss: 0.7480201125144958
INFO:agents.father_agent:Step: 75, Training loss: 0.4167277216911316
INFO:agents.father_agent:Step: 80, Training loss: 0.41182875633239746
INFO:agents.father_agent:Step: 85, Training loss: 0.44624167680740356
INFO:agents.father_agent:Step: 90, Training loss: 0.46089065074920654
INFO:agents.father_agent:Step: 95, Training loss: 0.37505316734313965
INFO:agents.father_agent:Step: 100, Training loss: 0.2960317134857178
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.321563720703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 342.6784362792969
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.02520751953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5176.85400390625
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.56656741811995
INFO:tools.evaluation_results_class:Counted Episodes = 4702
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -56.94873046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 343.05126953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.95941162109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4528.5185546875
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.64345225379193
INFO:tools.evaluation_results_class:Counted Episodes = 4681
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -61.183143615722656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 338.8168640136719
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.9053497314453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6299.47607421875
INFO:tools.evaluation_results_class:Current Best Return = -61.183143615722656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.25176924726571
INFO:tools.evaluation_results_class:Counted Episodes = 4663
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -271.0519104003906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 128.94810485839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 58.01267623901367
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 79888.4296875
INFO:tools.evaluation_results_class:Current Best Return = -271.0519104003906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.8701409653994
INFO:tools.evaluation_results_class:Counted Episodes = 4682
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=1, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 301.36323374582116
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=1, goright2_init=1, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 33 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -99.9501953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.0498046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.15963745117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28736.802734375
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.42916430862101
INFO:tools.evaluation_results_class:Counted Episodes = 5301
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.9895225167274475
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 1.4558082818984985
INFO:agents.father_agent:Step: 10, Training loss: 1.3265963792800903
INFO:agents.father_agent:Step: 15, Training loss: 1.187036395072937
INFO:agents.father_agent:Step: 20, Training loss: 1.125041127204895
INFO:agents.father_agent:Step: 25, Training loss: 1.364446997642517
INFO:agents.father_agent:Step: 30, Training loss: 0.987995982170105
INFO:agents.father_agent:Step: 35, Training loss: 0.8216624855995178
INFO:agents.father_agent:Step: 40, Training loss: 0.7218248248100281
INFO:agents.father_agent:Step: 45, Training loss: 0.5696362257003784
INFO:agents.father_agent:Step: 50, Training loss: 0.751102864742279
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 55, Training loss: 0.5640284419059753
INFO:agents.father_agent:Step: 60, Training loss: 0.4948742985725403
INFO:agents.father_agent:Step: 65, Training loss: 0.5697873830795288
INFO:agents.father_agent:Step: 70, Training loss: 0.4765911400318146
INFO:agents.father_agent:Step: 75, Training loss: 0.44697335362434387
INFO:agents.father_agent:Step: 80, Training loss: 0.4509150981903076
INFO:agents.father_agent:Step: 85, Training loss: 0.5501683950424194
INFO:agents.father_agent:Step: 90, Training loss: 0.45383086800575256
INFO:agents.father_agent:Step: 95, Training loss: 0.40189042687416077
INFO:agents.father_agent:Step: 100, Training loss: 0.3739587068557739
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.15055084228516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 324.8494567871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.72561645507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9387.916015625
INFO:tools.evaluation_results_class:Current Best Return = -48.066619873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.79754996776273
INFO:tools.evaluation_results_class:Counted Episodes = 3102
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
2025-08-20 13:49:06.808210: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-20 13:49:06.810106: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 13:49:06.840309: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-20 13:49:06.840349: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-20 13:49:06.841598: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-20 13:49:06.847209: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 13:49:06.847428: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-20 13:49:07.378521: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/network/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/network/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"packets_sent"}max=? [F "done"] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 5480 states and 71240 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"packets_sent"}max=? [F "label_done"] 
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.203400611877441
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.03400421142578
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.497554779052734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9.87109661102295
INFO:tools.evaluation_results_class:Current Best Return = 5.203400611877441
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.18891687657431
INFO:tools.evaluation_results_class:Counted Episodes = 3176
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 56.318023681640625
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 11.767834663391113
INFO:agents.father_agent:Step: 10, Training loss: 10.039379119873047
INFO:agents.father_agent:Step: 15, Training loss: 14.715780258178711
INFO:agents.father_agent:Step: 20, Training loss: 8.113865852355957
INFO:agents.father_agent:Step: 25, Training loss: 9.847865104675293
INFO:agents.father_agent:Step: 30, Training loss: 10.72488021850586
INFO:agents.father_agent:Step: 35, Training loss: 10.96140193939209
INFO:agents.father_agent:Step: 40, Training loss: 7.360009670257568
INFO:agents.father_agent:Step: 45, Training loss: 13.529546737670898
INFO:agents.father_agent:Step: 50, Training loss: 10.872797012329102
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Step: 55, Training loss: 9.54607105255127
INFO:agents.father_agent:Step: 60, Training loss: 12.556471824645996
INFO:agents.father_agent:Step: 65, Training loss: 14.151487350463867
INFO:agents.father_agent:Step: 70, Training loss: 8.45274543762207
INFO:agents.father_agent:Step: 75, Training loss: 15.5670747756958
INFO:agents.father_agent:Step: 80, Training loss: 9.617650032043457
INFO:agents.father_agent:Step: 85, Training loss: 12.50661563873291
INFO:agents.father_agent:Step: 90, Training loss: 12.718794822692871
INFO:agents.father_agent:Step: 95, Training loss: 13.675116539001465
INFO:agents.father_agent:Step: 100, Training loss: 10.197232246398926
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.896725654602051
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 70.96725463867188
INFO:tools.evaluation_results_class:Average Discounted Reward = 55.15611267089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12.11150074005127
INFO:tools.evaluation_results_class:Current Best Return = 6.896725654602051
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.18891687657431
INFO:tools.evaluation_results_class:Counted Episodes = 3176
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Step: 105, Training loss: 15.424294471740723
INFO:agents.father_agent:Step: 110, Training loss: 12.860420227050781
INFO:agents.father_agent:Step: 115, Training loss: 10.228102684020996
INFO:agents.father_agent:Step: 120, Training loss: 14.241908073425293
INFO:agents.father_agent:Step: 125, Training loss: 12.573640823364258
INFO:agents.father_agent:Step: 130, Training loss: 9.359471321105957
INFO:agents.father_agent:Step: 135, Training loss: 15.725421905517578
INFO:agents.father_agent:Step: 140, Training loss: 9.579368591308594
INFO:agents.father_agent:Step: 145, Training loss: 9.823066711425781
INFO:agents.father_agent:Step: 150, Training loss: 13.461929321289062
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 155, Training loss: 16.70393943786621
INFO:agents.father_agent:Step: 160, Training loss: 10.075689315795898
INFO:agents.father_agent:Step: 165, Training loss: 13.362235069274902
INFO:agents.father_agent:Step: 170, Training loss: 14.294528007507324
INFO:agents.father_agent:Step: 175, Training loss: 13.490918159484863
INFO:agents.father_agent:Step: 180, Training loss: 12.693074226379395
INFO:agents.father_agent:Step: 185, Training loss: 11.898675918579102
INFO:agents.father_agent:Step: 190, Training loss: 12.135017395019531
INFO:agents.father_agent:Step: 195, Training loss: 15.524585723876953
INFO:agents.father_agent:Step: 200, Training loss: 14.23659610748291
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.8740553855896
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 70.74055480957031
INFO:tools.evaluation_results_class:Average Discounted Reward = 55.013099670410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11.86008358001709
INFO:tools.evaluation_results_class:Current Best Return = 6.896725654602051
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.18891687657431
INFO:tools.evaluation_results_class:Counted Episodes = 3176
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 6.939546585083008
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.39546966552734
INFO:tools.evaluation_results_class:Average Discounted Reward = 55.48424530029297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12.172667503356934
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.18891687657431
INFO:tools.evaluation_results_class:Counted Episodes = 3176
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.8384175300598145
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 70.38417053222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 54.2625617980957
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11.684854507446289
INFO:tools.evaluation_results_class:Current Best Return = 6.8384175300598145
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.79855028692238
INFO:tools.evaluation_results_class:Counted Episodes = 3311
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.116883277893066
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 73.16883087158203
INFO:tools.evaluation_results_class:Average Discounted Reward = 56.58624267578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12.374438285827637
INFO:tools.evaluation_results_class:Current Best Return = 7.116883277893066
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.79855028692238
INFO:tools.evaluation_results_class:Counted Episodes = 3311
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.539083923406146
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.875650882720947
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.75650787353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.18016052246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.400552749633789
INFO:tools.evaluation_results_class:Current Best Return = 7.875650882720947
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 13.705078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 139.05078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 85.000244140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.0868492126464844
INFO:tools.evaluation_results_class:Current Best Return = 13.705078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.35546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 105.5546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 73.19881439208984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.6640065908432007
INFO:tools.evaluation_results_class:Current Best Return = 10.35546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 9.375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 95.75
INFO:tools.evaluation_results_class:Average Discounted Reward = 73.49809265136719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.037499904632568
INFO:tools.evaluation_results_class:Current Best Return = 9.375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.313802242279053
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 75.13802337646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.31559753417969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2582991123199463
INFO:tools.evaluation_results_class:Current Best Return = 7.313802242279053
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.27490234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.7490234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.05623435974121
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.261831283569336
INFO:tools.evaluation_results_class:Current Best Return = 3.27490234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.307942390441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 85.07942962646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 67.68472290039062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3602492809295654
INFO:tools.evaluation_results_class:Current Best Return = 8.307942390441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 13.994140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 141.94140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 86.86143493652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.0761375427246094
INFO:tools.evaluation_results_class:Current Best Return = 13.994140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.69140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 108.9140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.75948333740234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.3253427743911743
INFO:tools.evaluation_results_class:Current Best Return = 10.69140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.010937690734863
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 102.109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.49012756347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.173317909240723
INFO:tools.evaluation_results_class:Current Best Return = 10.010937690734863
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.639974117279053
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 78.39974212646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 61.967288970947266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2538444995880127
INFO:tools.evaluation_results_class:Current Best Return = 7.639974117279053
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.556884765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.56884765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.59619903564453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6759634017944336
INFO:tools.evaluation_results_class:Current Best Return = 3.556884765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 2 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.76924991607666
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.692501068115234
INFO:tools.evaluation_results_class:Average Discounted Reward = 47.84832763671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11.544063568115234
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 70.06852728631947
INFO:tools.evaluation_results_class:Counted Episodes = 4013
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.106428146362305
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 10.686558723449707
INFO:agents.father_agent:Step: 10, Training loss: 17.220584869384766
INFO:agents.father_agent:Step: 15, Training loss: 14.426774024963379
INFO:agents.father_agent:Step: 20, Training loss: 17.053613662719727
INFO:agents.father_agent:Step: 25, Training loss: 17.799774169921875
INFO:agents.father_agent:Step: 30, Training loss: 16.898778915405273
INFO:agents.father_agent:Step: 35, Training loss: 12.334210395812988
INFO:agents.father_agent:Step: 40, Training loss: 23.230575561523438
INFO:agents.father_agent:Step: 45, Training loss: 14.031204223632812
INFO:agents.father_agent:Step: 50, Training loss: 15.535000801086426
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 5.813605785369873
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 60.13605880737305
INFO:tools.evaluation_results_class:Average Discounted Reward = 48.26624298095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11.374676704406738
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 70.06852728631947
INFO:tools.evaluation_results_class:Counted Episodes = 4013
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.470787048339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.70787048339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 45.020111083984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11.284394264221191
INFO:tools.evaluation_results_class:Current Best Return = 5.470787048339844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.01690004828585
INFO:tools.evaluation_results_class:Counted Episodes = 4142
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.871076583862305
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 60.71076583862305
INFO:tools.evaluation_results_class:Average Discounted Reward = 48.397247314453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11.881012916564941
INFO:tools.evaluation_results_class:Current Best Return = 5.871076583862305
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.01690004828585
INFO:tools.evaluation_results_class:Counted Episodes = 4142
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5468184372321128
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 3 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.141292572021484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 53.412925720214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.814056396484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9.867518424987793
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.09276358170496
INFO:tools.evaluation_results_class:Counted Episodes = 4657
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.133419036865234
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Step: 5, Training loss: 14.080788612365723
INFO:agents.father_agent:Step: 10, Training loss: 20.592885971069336
INFO:agents.father_agent:Step: 15, Training loss: 20.137399673461914
INFO:agents.father_agent:Step: 20, Training loss: 20.239234924316406
INFO:agents.father_agent:Step: 25, Training loss: 17.219804763793945
INFO:agents.father_agent:Step: 30, Training loss: 19.967641830444336
INFO:agents.father_agent:Step: 35, Training loss: 14.054363250732422
INFO:agents.father_agent:Step: 40, Training loss: 23.31806182861328
INFO:agents.father_agent:Step: 45, Training loss: 15.181365966796875
INFO:agents.father_agent:Step: 50, Training loss: 15.125816345214844
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 5.086321830749512
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 52.863216400146484
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.27934265136719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10.004573822021484
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.09276358170496
INFO:tools.evaluation_results_class:Counted Episodes = 4657
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.789639472961426
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 49.896392822265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.478824615478516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10.070890426635742
INFO:tools.evaluation_results_class:Current Best Return = 4.789639472961426
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.70637583892618
INFO:tools.evaluation_results_class:Counted Episodes = 4768
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.234270095825195
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.34270095825195
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.25545120239258
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10.233497619628906
INFO:tools.evaluation_results_class:Current Best Return = 5.234270095825195
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.70637583892618
INFO:tools.evaluation_results_class:Counted Episodes = 4768
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5064725129786556
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 4 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.764774799346924
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 49.64774703979492
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.285850524902344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.844220161437988
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.95689487029452
INFO:tools.evaluation_results_class:Counted Episodes = 5127
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 16.809858322143555
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 13.806928634643555
INFO:agents.father_agent:Step: 10, Training loss: 16.945571899414062
INFO:agents.father_agent:Step: 15, Training loss: 16.44725227355957
INFO:agents.father_agent:Step: 20, Training loss: 17.490184783935547
INFO:agents.father_agent:Step: 25, Training loss: 15.90517520904541
INFO:agents.father_agent:Step: 30, Training loss: 17.64284324645996
INFO:agents.father_agent:Step: 35, Training loss: 12.262181282043457
INFO:agents.father_agent:Step: 40, Training loss: 16.5612850189209
INFO:agents.father_agent:Step: 45, Training loss: 14.48943042755127
INFO:agents.father_agent:Step: 50, Training loss: 13.065969467163086
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.818997383117676
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.18997573852539
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.76323699951172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.662772178649902
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.95689487029452
INFO:tools.evaluation_results_class:Counted Episodes = 5127
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.622865676879883
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.228660583496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 39.79035949707031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.654623985290527
INFO:tools.evaluation_results_class:Current Best Return = 4.622865676879883
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.44427393055822
INFO:tools.evaluation_results_class:Counted Episodes = 5213
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.927488803863525
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 51.2748908996582
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.393470764160156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9.24699592590332
INFO:tools.evaluation_results_class:Current Best Return = 4.927488803863525
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.44427393055822
INFO:tools.evaluation_results_class:Counted Episodes = 5213
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5500759209738475
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 5 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.571791172027588
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 47.71791076660156
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.11320877075195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.741582870483398
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.36620739666425
INFO:tools.evaluation_results_class:Counted Episodes = 5516
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.16969108581543
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 13.068004608154297
INFO:agents.father_agent:Step: 10, Training loss: 16.312519073486328
INFO:agents.father_agent:Step: 15, Training loss: 15.234280586242676
INFO:agents.father_agent:Step: 20, Training loss: 14.12731647491455
INFO:agents.father_agent:Step: 25, Training loss: 13.510129928588867
INFO:agents.father_agent:Step: 30, Training loss: 14.55199146270752
INFO:agents.father_agent:Step: 35, Training loss: 12.441812515258789
INFO:agents.father_agent:Step: 40, Training loss: 13.756361961364746
INFO:agents.father_agent:Step: 45, Training loss: 13.042194366455078
INFO:agents.father_agent:Step: 50, Training loss: 11.335603713989258
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.527737617492676
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 47.277374267578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 39.72761535644531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.936323165893555
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.36620739666425
INFO:tools.evaluation_results_class:Counted Episodes = 5516
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.3569135665893555
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.56913757324219
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.077423095703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.8300251960754395
INFO:tools.evaluation_results_class:Current Best Return = 4.3569135665893555
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.44191019244476
INFO:tools.evaluation_results_class:Counted Episodes = 5612
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.6486101150512695
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.48610305786133
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.593902587890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.240032196044922
INFO:tools.evaluation_results_class:Current Best Return = 4.6486101150512695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.44191019244476
INFO:tools.evaluation_results_class:Counted Episodes = 5612
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.551258648392732
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 6 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.362234592437744
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.622344970703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.61420440673828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.290650367736816
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.76901987662783
INFO:tools.evaluation_results_class:Counted Episodes = 5836
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.93552303314209
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 12.54741382598877
INFO:agents.father_agent:Step: 10, Training loss: 14.154956817626953
INFO:agents.father_agent:Step: 15, Training loss: 13.958197593688965
INFO:agents.father_agent:Step: 20, Training loss: 13.153017044067383
INFO:agents.father_agent:Step: 25, Training loss: 13.670077323913574
INFO:agents.father_agent:Step: 30, Training loss: 14.931904792785645
INFO:agents.father_agent:Step: 35, Training loss: 12.097782135009766
INFO:agents.father_agent:Step: 40, Training loss: 15.281193733215332
INFO:agents.father_agent:Step: 45, Training loss: 13.050312042236328
INFO:agents.father_agent:Step: 50, Training loss: 12.426547050476074
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.341158390045166
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.411582946777344
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.51154327392578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.1476616859436035
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.76901987662783
INFO:tools.evaluation_results_class:Counted Episodes = 5836
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.300286769866943
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.002864837646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.9980354309082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.192748069763184
INFO:tools.evaluation_results_class:Current Best Return = 4.300286769866943
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.6314280896982
INFO:tools.evaluation_results_class:Counted Episodes = 5931
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.475805282592773
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.758052825927734
INFO:tools.evaluation_results_class:Average Discounted Reward = 39.485137939453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.736515998840332
INFO:tools.evaluation_results_class:Current Best Return = 4.475805282592773
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.6314280896982
INFO:tools.evaluation_results_class:Counted Episodes = 5931
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5666358031996706
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.903645992279053
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 81.03646087646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.45577239990234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.2628514766693115
INFO:tools.evaluation_results_class:Current Best Return = 7.903645992279053
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 13.626953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 138.26953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 84.52896118164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1635704040527344
INFO:tools.evaluation_results_class:Current Best Return = 13.626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.486979484558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 106.86978912353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 74.09986114501953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.4711847305297852
INFO:tools.evaluation_results_class:Current Best Return = 10.486979484558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 9.450780868530273
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 96.5078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 74.11283874511719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.9100775718688965
INFO:tools.evaluation_results_class:Current Best Return = 9.450780868530273
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.310546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 75.10546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.39754104614258
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.3117637634277344
INFO:tools.evaluation_results_class:Current Best Return = 7.310546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.32958984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.2958984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.54547119140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.3318004608154297
INFO:tools.evaluation_results_class:Current Best Return = 3.32958984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.302734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.02734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.31204605102539
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.3355979919433594
INFO:tools.evaluation_results_class:Current Best Return = 3.302734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2822265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.822265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.12256622314453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.3900742530822754
INFO:tools.evaluation_results_class:Current Best Return = 3.2822265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2666015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.666015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.977581024169922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.3805832862854004
INFO:tools.evaluation_results_class:Current Best Return = 3.2666015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.295166015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.95166015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.246238708496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.3862662315368652
INFO:tools.evaluation_results_class:Current Best Return = 3.295166015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.288330078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.88330078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.175613403320312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.308711528778076
INFO:tools.evaluation_results_class:Current Best Return = 3.288330078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.389973640441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 85.89974212646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 68.33702850341797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.1506547927856445
INFO:tools.evaluation_results_class:Current Best Return = 8.389973640441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.10546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 143.0546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 87.4455337524414
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.0865325927734375
INFO:tools.evaluation_results_class:Current Best Return = 14.10546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.66015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 108.6015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.50350189208984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.3363291025161743
INFO:tools.evaluation_results_class:Current Best Return = 10.66015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 9.844531059265137
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 100.4453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 77.12696075439453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.428173065185547
INFO:tools.evaluation_results_class:Current Best Return = 9.844531059265137
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.738932132720947
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.38932037353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.78807067871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1173906326293945
INFO:tools.evaluation_results_class:Current Best Return = 7.738932132720947
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.548583984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.48583984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.51659393310547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5938315391540527
INFO:tools.evaluation_results_class:Current Best Return = 3.548583984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5712890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.712890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.72718048095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.697554111480713
INFO:tools.evaluation_results_class:Current Best Return = 3.5712890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.563232421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.63232421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.65595245361328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.708404064178467
INFO:tools.evaluation_results_class:Current Best Return = 3.563232421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.557373046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.57373046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.59983825683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.676396369934082
INFO:tools.evaluation_results_class:Current Best Return = 3.557373046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.580810546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.80810546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.807010650634766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6956186294555664
INFO:tools.evaluation_results_class:Current Best Return = 3.580810546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.57421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.7421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.73961639404297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6546478271484375
INFO:tools.evaluation_results_class:Current Best Return = 3.57421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 7 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.32546329498291
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.25463104248047
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.6174201965332
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.777332782745361
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.839809804886045
INFO:tools.evaluation_results_class:Counted Episodes = 6099
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.141650199890137
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 11.962458610534668
INFO:agents.father_agent:Step: 10, Training loss: 13.126469612121582
INFO:agents.father_agent:Step: 15, Training loss: 14.421350479125977
INFO:agents.father_agent:Step: 20, Training loss: 14.020546913146973
INFO:agents.father_agent:Step: 25, Training loss: 13.446184158325195
INFO:agents.father_agent:Step: 30, Training loss: 13.807648658752441
INFO:agents.father_agent:Step: 35, Training loss: 13.499958992004395
INFO:agents.father_agent:Step: 40, Training loss: 14.607792854309082
INFO:agents.father_agent:Step: 45, Training loss: 11.208464622497559
INFO:agents.father_agent:Step: 50, Training loss: 12.857912063598633
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.303000450134277
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.030006408691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.472373962402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.445000171661377
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.839809804886045
INFO:tools.evaluation_results_class:Counted Episodes = 6099
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.099886894226074
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.99886703491211
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.528995513916016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.334041118621826
INFO:tools.evaluation_results_class:Current Best Return = 4.099886894226074
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.661324267443746
INFO:tools.evaluation_results_class:Counted Episodes = 6177
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.474502086639404
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.74502182006836
INFO:tools.evaluation_results_class:Average Discounted Reward = 39.85480880737305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.866154193878174
INFO:tools.evaluation_results_class:Current Best Return = 4.474502086639404
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.661324267443746
INFO:tools.evaluation_results_class:Counted Episodes = 6177
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.575572643303807
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 8 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.1338605880737305
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.33860778808594
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.1983757019043
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.008030891418457
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.358227848101265
INFO:tools.evaluation_results_class:Counted Episodes = 6320
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.92749309539795
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 11.544987678527832
INFO:agents.father_agent:Step: 10, Training loss: 14.609457015991211
INFO:agents.father_agent:Step: 15, Training loss: 13.825765609741211
INFO:agents.father_agent:Step: 20, Training loss: 12.9365234375
INFO:agents.father_agent:Step: 25, Training loss: 12.964675903320312
INFO:agents.father_agent:Step: 30, Training loss: 15.511343955993652
INFO:agents.father_agent:Step: 35, Training loss: 11.888053894042969
INFO:agents.father_agent:Step: 40, Training loss: 16.178220748901367
INFO:agents.father_agent:Step: 45, Training loss: 13.292593955993652
INFO:agents.father_agent:Step: 50, Training loss: 13.197614669799805
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.23306941986084
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.33069610595703
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.08381652832031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.117988586425781
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.358227848101265
INFO:tools.evaluation_results_class:Counted Episodes = 6320
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.035452365875244
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.354522705078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.236656188964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.018265247344971
INFO:tools.evaluation_results_class:Current Best Return = 4.035452365875244
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.9853193815399
INFO:tools.evaluation_results_class:Counted Episodes = 6403
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.250976085662842
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.509761810302734
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.1080436706543
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.190485000610352
INFO:tools.evaluation_results_class:Current Best Return = 4.250976085662842
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.9853193815399
INFO:tools.evaluation_results_class:Counted Episodes = 6403
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.549225215116967
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 9 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.067015647888184
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.67015838623047
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.78337478637695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.816338062286377
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.266214758896936
INFO:tools.evaluation_results_class:Counted Episodes = 6491
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.662188529968262
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 11.90194320678711
INFO:agents.father_agent:Step: 10, Training loss: 10.95092487335205
INFO:agents.father_agent:Step: 15, Training loss: 13.915761947631836
INFO:agents.father_agent:Step: 20, Training loss: 11.905699729919434
INFO:agents.father_agent:Step: 25, Training loss: 13.698018074035645
INFO:agents.father_agent:Step: 30, Training loss: 15.405861854553223
INFO:agents.father_agent:Step: 35, Training loss: 13.76187801361084
INFO:agents.father_agent:Step: 40, Training loss: 14.170613288879395
INFO:agents.father_agent:Step: 45, Training loss: 12.382553100585938
INFO:agents.father_agent:Step: 50, Training loss: 12.663866996765137
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.240024566650391
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.400245666503906
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.32956314086914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.690039157867432
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.266214758896936
INFO:tools.evaluation_results_class:Counted Episodes = 6491
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.103238582611084
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.032386779785156
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.030723571777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.713075637817383
INFO:tools.evaluation_results_class:Current Best Return = 4.103238582611084
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.76630682682074
INFO:tools.evaluation_results_class:Counted Episodes = 6577
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.245704650878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.45704650878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.26240158081055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.835022449493408
INFO:tools.evaluation_results_class:Current Best Return = 4.245704650878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.76630682682074
INFO:tools.evaluation_results_class:Counted Episodes = 6577
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.6138341736610435
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 10 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.069431781768799
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.69432067871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.000450134277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.1595916748046875
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.29185452359483
INFO:tools.evaluation_results_class:Counted Episodes = 6654
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.771453857421875
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 11.722811698913574
INFO:agents.father_agent:Step: 10, Training loss: 15.902156829833984
INFO:agents.father_agent:Step: 15, Training loss: 14.592948913574219
INFO:agents.father_agent:Step: 20, Training loss: 13.663227081298828
INFO:agents.father_agent:Step: 25, Training loss: 12.897780418395996
INFO:agents.father_agent:Step: 30, Training loss: 13.202298164367676
INFO:agents.father_agent:Step: 35, Training loss: 11.376376152038574
INFO:agents.father_agent:Step: 40, Training loss: 13.805639266967773
INFO:agents.father_agent:Step: 45, Training loss: 12.878360748291016
INFO:agents.father_agent:Step: 50, Training loss: 12.011853218078613
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.06131649017334
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.61316680908203
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.859344482421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.41132926940918
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.29185452359483
INFO:tools.evaluation_results_class:Counted Episodes = 6654
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.103350639343262
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.03350830078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.20766830444336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.236675262451172
INFO:tools.evaluation_results_class:Current Best Return = 4.103350639343262
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.849888309754284
INFO:tools.evaluation_results_class:Counted Episodes = 6715
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.170066833496094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.70066833496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.76594543457031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.467131614685059
INFO:tools.evaluation_results_class:Current Best Return = 4.170066833496094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.849888309754284
INFO:tools.evaluation_results_class:Counted Episodes = 6715
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.6198596369965066
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 11 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.187306880950928
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.87306594848633
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.2042236328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.949453830718994
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.49919010455014
INFO:tools.evaluation_results_class:Counted Episodes = 6791
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.61252498626709
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 10.59227180480957
INFO:agents.father_agent:Step: 10, Training loss: 16.752840042114258
INFO:agents.father_agent:Step: 15, Training loss: 12.021263122558594
INFO:agents.father_agent:Step: 20, Training loss: 12.4110746383667
INFO:agents.father_agent:Step: 25, Training loss: 16.983123779296875
INFO:agents.father_agent:Step: 30, Training loss: 12.787884712219238
INFO:agents.father_agent:Step: 35, Training loss: 11.382166862487793
INFO:agents.father_agent:Step: 40, Training loss: 13.831737518310547
INFO:agents.father_agent:Step: 45, Training loss: 13.317861557006836
INFO:agents.father_agent:Step: 50, Training loss: 13.508153915405273
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.08658504486084
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.86585235595703
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.23123550415039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.3251495361328125
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.49919010455014
INFO:tools.evaluation_results_class:Counted Episodes = 6791
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.955169439315796
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.551692962646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.01652145385742
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.988498210906982
INFO:tools.evaluation_results_class:Current Best Return = 3.955169439315796
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.001168224299064
INFO:tools.evaluation_results_class:Counted Episodes = 6848
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.978095769882202
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.78095626831055
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.21147155761719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.165699481964111
INFO:tools.evaluation_results_class:Current Best Return = 3.978095769882202
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.001168224299064
INFO:tools.evaluation_results_class:Counted Episodes = 6848
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.6158738618382715
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.118489265441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 83.18489837646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 66.19708251953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.0432517528533936
INFO:tools.evaluation_results_class:Current Best Return = 8.118489265441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 13.783203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 139.83203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 85.34834289550781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.3182334899902344
INFO:tools.evaluation_results_class:Current Best Return = 13.783203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.52734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 107.2734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 74.48018646240234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.4862314462661743
INFO:tools.evaluation_results_class:Current Best Return = 10.52734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 9.6640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 98.640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.656982421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.6980834007263184
INFO:tools.evaluation_results_class:Current Best Return = 9.6640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.487630367279053
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 76.87630462646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.8312873840332
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.107919931411743
INFO:tools.evaluation_results_class:Current Best Return = 7.487630367279053
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.42724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.2724609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.44474411010742
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.3399219512939453
INFO:tools.evaluation_results_class:Current Best Return = 3.42724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.49072265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.9072265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.005001068115234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.4086055755615234
INFO:tools.evaluation_results_class:Current Best Return = 3.49072265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.464111328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.64111328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.76362609863281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.4425597190856934
INFO:tools.evaluation_results_class:Current Best Return = 3.464111328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.482421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.82421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.93287658691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.4557456970214844
INFO:tools.evaluation_results_class:Current Best Return = 3.482421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.480712890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.80712890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.90845489501953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.3936712741851807
INFO:tools.evaluation_results_class:Current Best Return = 3.480712890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4853515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.853515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.94805145263672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.4021286964416504
INFO:tools.evaluation_results_class:Current Best Return = 3.4853515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.465576171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.65576171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.777591705322266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.3498892784118652
INFO:tools.evaluation_results_class:Current Best Return = 3.465576171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.458984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.58984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.714256286621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.4026145935058594
INFO:tools.evaluation_results_class:Current Best Return = 3.458984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.441162109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.41162109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.56248474121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.4218313694000244
INFO:tools.evaluation_results_class:Current Best Return = 3.441162109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.468017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.68017578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.79825210571289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.4105982780456543
INFO:tools.evaluation_results_class:Current Best Return = 3.468017578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.44482421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.4482421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.58378601074219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.357795238494873
INFO:tools.evaluation_results_class:Current Best Return = 3.44482421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.420573234558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 86.20572662353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 68.70279693603516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.139524459838867
INFO:tools.evaluation_results_class:Current Best Return = 8.420573234558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.05078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 142.5078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 87.14826965332031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.8528900146484375
INFO:tools.evaluation_results_class:Current Best Return = 14.05078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.733073234558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.33072662353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.03449249267578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.31807279586792
INFO:tools.evaluation_results_class:Current Best Return = 10.733073234558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 9.979687690734863
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 101.796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.06825256347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.9402122497558594
INFO:tools.evaluation_results_class:Current Best Return = 9.979687690734863
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.671224117279053
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 78.71224212646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.34553909301758
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.057922124862671
INFO:tools.evaluation_results_class:Current Best Return = 7.671224117279053
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.580322265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.80322265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.82615280151367
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6209897994995117
INFO:tools.evaluation_results_class:Current Best Return = 3.580322265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.619384765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.19384765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.16749572753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6200246810913086
INFO:tools.evaluation_results_class:Current Best Return = 3.619384765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6259765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.259765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.227561950683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5959458351135254
INFO:tools.evaluation_results_class:Current Best Return = 3.6259765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.62060546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.2060546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.1778564453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6080126762390137
INFO:tools.evaluation_results_class:Current Best Return = 3.62060546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.632080078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.32080078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.27678680419922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6339221000671387
INFO:tools.evaluation_results_class:Current Best Return = 3.632080078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.614013671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.14013671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.12043762207031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5890517234802246
INFO:tools.evaluation_results_class:Current Best Return = 3.614013671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.595458984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.95458984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.96063995361328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6217474937438965
INFO:tools.evaluation_results_class:Current Best Return = 3.595458984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.61376953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.1376953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.12269973754883
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.634517192840576
INFO:tools.evaluation_results_class:Current Best Return = 3.61376953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.621337890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.21337890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.19482421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6678948402404785
INFO:tools.evaluation_results_class:Current Best Return = 3.621337890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.613525390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.13525390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.1237678527832
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.661916732788086
INFO:tools.evaluation_results_class:Current Best Return = 3.613525390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.614990234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.14990234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.134361267089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5946874618530273
INFO:tools.evaluation_results_class:Current Best Return = 3.614990234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 12 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.036142826080322
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.361427307128906
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.93891525268555
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.964719295501709
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.79904582911667
INFO:tools.evaluation_results_class:Counted Episodes = 6917
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.140283584594727
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 11.475504875183105
INFO:agents.father_agent:Step: 10, Training loss: 14.961514472961426
INFO:agents.father_agent:Step: 15, Training loss: 14.733649253845215
INFO:agents.father_agent:Step: 20, Training loss: 12.282523155212402
INFO:agents.father_agent:Step: 25, Training loss: 15.32446002960205
INFO:agents.father_agent:Step: 30, Training loss: 13.458183288574219
INFO:agents.father_agent:Step: 35, Training loss: 11.474559783935547
INFO:agents.father_agent:Step: 40, Training loss: 18.532724380493164
INFO:agents.father_agent:Step: 45, Training loss: 14.416315078735352
INFO:agents.father_agent:Step: 50, Training loss: 12.09720230102539
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.050600051879883
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.50600051879883
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.082916259765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.617072105407715
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.79904582911667
INFO:tools.evaluation_results_class:Counted Episodes = 6917
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.957026243209839
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.57026290893555
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.14966583251953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.907478332519531
INFO:tools.evaluation_results_class:Current Best Return = 3.957026243209839
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.18478727975935
INFO:tools.evaluation_results_class:Counted Episodes = 6981
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.056868553161621
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.568687438964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.038692474365234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.909385681152344
INFO:tools.evaluation_results_class:Current Best Return = 4.056868553161621
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.18478727975935
INFO:tools.evaluation_results_class:Counted Episodes = 6981
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.650726482233613
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 13 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.041175842285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.4117546081543
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.080650329589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.668473243713379
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.12395286099673
INFO:tools.evaluation_results_class:Counted Episodes = 7043
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.537664413452148
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 12.76272964477539
INFO:agents.father_agent:Step: 10, Training loss: 13.811945915222168
INFO:agents.father_agent:Step: 15, Training loss: 12.70357608795166
INFO:agents.father_agent:Step: 20, Training loss: 11.261025428771973
INFO:agents.father_agent:Step: 25, Training loss: 18.223342895507812
INFO:agents.father_agent:Step: 30, Training loss: 14.707351684570312
INFO:agents.father_agent:Step: 35, Training loss: 12.086160659790039
INFO:agents.father_agent:Step: 40, Training loss: 14.972431182861328
INFO:agents.father_agent:Step: 45, Training loss: 11.481303215026855
INFO:agents.father_agent:Step: 50, Training loss: 12.53376579284668
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.0177483558654785
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.17748260498047
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.90768814086914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.623708724975586
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.12395286099673
INFO:tools.evaluation_results_class:Counted Episodes = 7043
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9634902477264404
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.63490295410156
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.35564422607422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.477803707122803
INFO:tools.evaluation_results_class:Current Best Return = 3.9634902477264404
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.51564702565548
INFO:tools.evaluation_results_class:Counted Episodes = 7094
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.901607036590576
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.01607131958008
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.7363166809082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.887696743011475
INFO:tools.evaluation_results_class:Current Best Return = 3.901607036590576
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.51564702565548
INFO:tools.evaluation_results_class:Counted Episodes = 7094
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.6732452024686197
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 14 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9902830123901367
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.90283203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.66667938232422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.601088523864746
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.82410928038304
INFO:tools.evaluation_results_class:Counted Episodes = 7101
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.422082901000977
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 12.85810661315918
INFO:agents.father_agent:Step: 10, Training loss: 11.809810638427734
INFO:agents.father_agent:Step: 15, Training loss: 14.15892219543457
INFO:agents.father_agent:Step: 20, Training loss: 12.550533294677734
INFO:agents.father_agent:Step: 25, Training loss: 19.877174377441406
INFO:agents.father_agent:Step: 30, Training loss: 13.521589279174805
INFO:agents.father_agent:Step: 35, Training loss: 12.458663940429688
INFO:agents.father_agent:Step: 40, Training loss: 12.041062355041504
INFO:agents.father_agent:Step: 45, Training loss: 12.158504486083984
INFO:agents.father_agent:Step: 50, Training loss: 12.94845199584961
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.0195746421813965
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.19574737548828
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.90443801879883
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.581366062164307
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.82410928038304
INFO:tools.evaluation_results_class:Counted Episodes = 7101
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9265568256378174
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.265567779541016
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.07968521118164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.44392204284668
INFO:tools.evaluation_results_class:Current Best Return = 3.9265568256378174
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.12370846132365
INFO:tools.evaluation_results_class:Counted Episodes = 7162
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.0445404052734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.44540786743164
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.08634948730469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.746829032897949
INFO:tools.evaluation_results_class:Current Best Return = 4.0445404052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.12370846132365
INFO:tools.evaluation_results_class:Counted Episodes = 7162
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.6746917098633656
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 15 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.922959566116333
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.22959518432617
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.183956146240234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.239596366882324
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.238880421227655
INFO:tools.evaluation_results_class:Counted Episodes = 7217
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.328160285949707
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 12.047123908996582
INFO:agents.father_agent:Step: 10, Training loss: 14.784707069396973
INFO:agents.father_agent:Step: 15, Training loss: 15.324655532836914
INFO:agents.father_agent:Step: 20, Training loss: 12.718656539916992
INFO:agents.father_agent:Step: 25, Training loss: 11.881013870239258
INFO:agents.father_agent:Step: 30, Training loss: 14.456439971923828
INFO:agents.father_agent:Step: 35, Training loss: 13.583819389343262
INFO:agents.father_agent:Step: 40, Training loss: 12.991950988769531
INFO:agents.father_agent:Step: 45, Training loss: 12.996973037719727
INFO:agents.father_agent:Step: 50, Training loss: 12.433505058288574
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.9980602264404297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.9806022644043
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.862735748291016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.598859786987305
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.238880421227655
INFO:tools.evaluation_results_class:Counted Episodes = 7217
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.918488025665283
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.184879302978516
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.033477783203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.302221775054932
INFO:tools.evaluation_results_class:Current Best Return = 3.918488025665283
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.48769759450172
INFO:tools.evaluation_results_class:Counted Episodes = 7275
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.997800588607788
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.97800827026367
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.770484924316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.468173503875732
INFO:tools.evaluation_results_class:Current Best Return = 3.997800588607788
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.48769759450172
INFO:tools.evaluation_results_class:Counted Episodes = 7275
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.657008554735294
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 16 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8792037963867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.79203796386719
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.83411407470703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.032628536224365
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.89842141386411
INFO:tools.evaluation_results_class:Counted Episodes = 7285
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.324291229248047
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 11.21737289428711
INFO:agents.father_agent:Step: 10, Training loss: 11.715767860412598
INFO:agents.father_agent:Step: 15, Training loss: 10.787736892700195
INFO:agents.father_agent:Step: 20, Training loss: 11.487709999084473
INFO:agents.father_agent:Step: 25, Training loss: 17.54779815673828
INFO:agents.father_agent:Step: 30, Training loss: 15.514364242553711
INFO:agents.father_agent:Step: 35, Training loss: 13.507271766662598
INFO:agents.father_agent:Step: 40, Training loss: 14.943313598632812
INFO:agents.father_agent:Step: 45, Training loss: 14.48723030090332
INFO:agents.father_agent:Step: 50, Training loss: 10.36064624786377
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.137268543243408
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.372684478759766
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.125144958496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.19721794128418
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.89842141386411
INFO:tools.evaluation_results_class:Counted Episodes = 7285
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9147138595581055
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.14714050292969
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.07467269897461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.44258975982666
INFO:tools.evaluation_results_class:Current Best Return = 3.9147138595581055
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.12970027247957
INFO:tools.evaluation_results_class:Counted Episodes = 7340
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.980517625808716
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.80517578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.650943756103516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.720738410949707
INFO:tools.evaluation_results_class:Current Best Return = 3.980517625808716
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.12970027247957
INFO:tools.evaluation_results_class:Counted Episodes = 7340
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.6800532420788854
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.391276359558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 85.91275787353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 68.32512664794922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.048074722290039
INFO:tools.evaluation_results_class:Current Best Return = 8.391276359558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.060546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 142.60546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 87.16020202636719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1467247009277344
INFO:tools.evaluation_results_class:Current Best Return = 14.060546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.768229484558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.68228912353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.09456634521484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.2718030214309692
INFO:tools.evaluation_results_class:Current Best Return = 10.768229484558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 9.917187690734863
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 101.171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 77.63641357421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.658766984939575
INFO:tools.evaluation_results_class:Current Best Return = 9.917187690734863
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.714192867279053
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.14192962646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.52824783325195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.068704843521118
INFO:tools.evaluation_results_class:Current Best Return = 7.714192867279053
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.654296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.54296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.491146087646484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6988487243652344
INFO:tools.evaluation_results_class:Current Best Return = 3.654296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.654541015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.54541015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.477020263671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5205512046813965
INFO:tools.evaluation_results_class:Current Best Return = 3.654541015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.62939453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.2939453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.2579345703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5145068168640137
INFO:tools.evaluation_results_class:Current Best Return = 3.62939453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.644775390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.44775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.38508224487305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5083370208740234
INFO:tools.evaluation_results_class:Current Best Return = 3.644775390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.64111328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.4111328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.35792541503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5201258659362793
INFO:tools.evaluation_results_class:Current Best Return = 3.64111328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6455078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.455078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.396236419677734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5349793434143066
INFO:tools.evaluation_results_class:Current Best Return = 3.6455078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.669677734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.69677734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.607818603515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5937681198120117
INFO:tools.evaluation_results_class:Current Best Return = 3.669677734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.66748046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.6748046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.58686828613281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.523707866668701
INFO:tools.evaluation_results_class:Current Best Return = 3.66748046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.68994140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.8994140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.7882080078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5840396881103516
INFO:tools.evaluation_results_class:Current Best Return = 3.68994140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.65869140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.5869140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.505706787109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5768680572509766
INFO:tools.evaluation_results_class:Current Best Return = 3.65869140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.66162109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.6162109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.53156280517578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.594484329223633
INFO:tools.evaluation_results_class:Current Best Return = 3.66162109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6357421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.357421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.30426025390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5391907691955566
INFO:tools.evaluation_results_class:Current Best Return = 3.6357421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.616943359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.16943359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.12451934814453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5561485290527344
INFO:tools.evaluation_results_class:Current Best Return = 3.616943359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.632080078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.32080078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.269325256347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5113635063171387
INFO:tools.evaluation_results_class:Current Best Return = 3.632080078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.64697265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.4697265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.40114212036133
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5374813079833984
INFO:tools.evaluation_results_class:Current Best Return = 3.64697265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.626708984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.26708984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.215553283691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.508847713470459
INFO:tools.evaluation_results_class:Current Best Return = 3.626708984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.520833015441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 87.20833587646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 69.46660614013672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.2326390743255615
INFO:tools.evaluation_results_class:Current Best Return = 8.520833015441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.09375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 142.9375
INFO:tools.evaluation_results_class:Average Discounted Reward = 87.39208221435547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.0888671875
INFO:tools.evaluation_results_class:Current Best Return = 14.09375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.697916984558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 108.97916412353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.834716796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.4530164003372192
INFO:tools.evaluation_results_class:Current Best Return = 10.697916984558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 9.992968559265137
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 101.9296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.16879272460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7866692543029785
INFO:tools.evaluation_results_class:Current Best Return = 9.992968559265137
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.795572757720947
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.95572662353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.34100341796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.0493555068969727
INFO:tools.evaluation_results_class:Current Best Return = 7.795572757720947
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.721435546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.21435546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.0721321105957
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6750879287719727
INFO:tools.evaluation_results_class:Current Best Return = 3.721435546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.63916015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.3916015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.35810089111328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6632518768310547
INFO:tools.evaluation_results_class:Current Best Return = 3.63916015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.663818359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.63818359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.5774040222168
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.679218292236328
INFO:tools.evaluation_results_class:Current Best Return = 3.663818359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.659423828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.59423828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.535484313964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6381583213806152
INFO:tools.evaluation_results_class:Current Best Return = 3.659423828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.636962890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.36962890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.33214569091797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6428627967834473
INFO:tools.evaluation_results_class:Current Best Return = 3.636962890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.643310546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.43310546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.39281463623047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6908884048461914
INFO:tools.evaluation_results_class:Current Best Return = 3.643310546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.68408203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.8408203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.745365142822266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.636035442352295
INFO:tools.evaluation_results_class:Current Best Return = 3.68408203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.694091796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.94091796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.833251953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7279534339904785
INFO:tools.evaluation_results_class:Current Best Return = 3.694091796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6787109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.787109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.69365692138672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.727339267730713
INFO:tools.evaluation_results_class:Current Best Return = 3.6787109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.676513671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.76513671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.68572235107422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6929640769958496
INFO:tools.evaluation_results_class:Current Best Return = 3.676513671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.68359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.8359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.734519958496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6625823974609375
INFO:tools.evaluation_results_class:Current Best Return = 3.68359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6689453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.689453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.625770568847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.729757785797119
INFO:tools.evaluation_results_class:Current Best Return = 3.6689453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.683349609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.83349609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.75620651245117
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7212657928466797
INFO:tools.evaluation_results_class:Current Best Return = 3.683349609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.687255859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.87255859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.77842712402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.721282958984375
INFO:tools.evaluation_results_class:Current Best Return = 3.687255859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6650390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.650390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.57759475708008
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.726667881011963
INFO:tools.evaluation_results_class:Current Best Return = 3.6650390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.65869140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.5869140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.531959533691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6623172760009766
INFO:tools.evaluation_results_class:Current Best Return = 3.65869140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 17 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.008023738861084
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.080238342285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.05419921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.98701548576355
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.56425948592411
INFO:tools.evaluation_results_class:Counted Episodes = 7353
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.471351623535156
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 14.282148361206055
INFO:agents.father_agent:Step: 10, Training loss: 15.686763763427734
INFO:agents.father_agent:Step: 15, Training loss: 13.692278861999512
INFO:agents.father_agent:Step: 20, Training loss: 11.777436256408691
INFO:agents.father_agent:Step: 25, Training loss: 18.379209518432617
INFO:agents.father_agent:Step: 30, Training loss: 15.570337295532227
INFO:agents.father_agent:Step: 35, Training loss: 14.29465389251709
INFO:agents.father_agent:Step: 40, Training loss: 14.233831405639648
INFO:agents.father_agent:Step: 45, Training loss: 12.459959983825684
INFO:agents.father_agent:Step: 50, Training loss: 12.283324241638184
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.9646403789520264
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.64640426635742
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.64265441894531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.18153190612793
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.56425948592411
INFO:tools.evaluation_results_class:Counted Episodes = 7353
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8826944828033447
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.826942443847656
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.896751403808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.9750354290008545
INFO:tools.evaluation_results_class:Current Best Return = 3.8826944828033447
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.76349892008639
INFO:tools.evaluation_results_class:Counted Episodes = 7408
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.07073450088501
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.70734405517578
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.55388259887695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.236627101898193
INFO:tools.evaluation_results_class:Current Best Return = 4.07073450088501
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.76349892008639
INFO:tools.evaluation_results_class:Counted Episodes = 7408
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.7120266278369294
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 18 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8825113773345947
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.82511520385742
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.9528923034668
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.124973297119141
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.235785502559956
INFO:tools.evaluation_results_class:Counted Episodes = 7422
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.817028045654297
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 12.504438400268555
INFO:agents.father_agent:Step: 10, Training loss: 15.709247589111328
INFO:agents.father_agent:Step: 15, Training loss: 14.335042953491211
INFO:agents.father_agent:Step: 20, Training loss: 12.97656536102295
INFO:agents.father_agent:Step: 25, Training loss: 13.335166931152344
INFO:agents.father_agent:Step: 30, Training loss: 14.670626640319824
INFO:agents.father_agent:Step: 35, Training loss: 12.356328964233398
INFO:agents.father_agent:Step: 40, Training loss: 21.269134521484375
INFO:agents.father_agent:Step: 45, Training loss: 12.981359481811523
INFO:agents.father_agent:Step: 50, Training loss: 12.619022369384766
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.061034679412842
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.610347747802734
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.561767578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.306568622589111
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.235785502559956
INFO:tools.evaluation_results_class:Counted Episodes = 7422
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.907278537750244
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.072784423828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.15987777709961
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.930257558822632
INFO:tools.evaluation_results_class:Current Best Return = 3.907278537750244
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.40808134867541
INFO:tools.evaluation_results_class:Counted Episodes = 7474
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8928284645080566
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.92828369140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.007415771484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.179174900054932
INFO:tools.evaluation_results_class:Current Best Return = 3.8928284645080566
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.40808134867541
INFO:tools.evaluation_results_class:Counted Episodes = 7474
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.698624886286231
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 19 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.019384860992432
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.193851470947266
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.255863189697266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.896014928817749
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.96577540106952
INFO:tools.evaluation_results_class:Counted Episodes = 7480
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.769007682800293
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 12.422198295593262
INFO:agents.father_agent:Step: 10, Training loss: 12.711734771728516
INFO:agents.father_agent:Step: 15, Training loss: 12.753933906555176
INFO:agents.father_agent:Step: 20, Training loss: 12.063935279846191
INFO:agents.father_agent:Step: 25, Training loss: 8.898276329040527
INFO:agents.father_agent:Step: 30, Training loss: 10.977813720703125
INFO:agents.father_agent:Step: 35, Training loss: 13.25539779663086
INFO:agents.father_agent:Step: 40, Training loss: 14.219008445739746
INFO:agents.father_agent:Step: 45, Training loss: 15.001683235168457
INFO:agents.father_agent:Step: 50, Training loss: 13.258586883544922
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.9675133228302
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.675132751464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.70901870727539
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.6242120265960693
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.96577540106952
INFO:tools.evaluation_results_class:Counted Episodes = 7480
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8236937522888184
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.2369384765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.399295806884766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.8388798236846924
INFO:tools.evaluation_results_class:Current Best Return = 3.8236937522888184
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.168727562824095
INFO:tools.evaluation_results_class:Counted Episodes = 7521
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.842574119567871
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.425743103027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.63454055786133
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.034251689910889
INFO:tools.evaluation_results_class:Current Best Return = 3.842574119567871
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.168727562824095
INFO:tools.evaluation_results_class:Counted Episodes = 7521
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.683603723471342
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 20 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.993490219116211
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.93490219116211
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.03274917602539
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.877065896987915
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.75169390195297
INFO:tools.evaluation_results_class:Counted Episodes = 7527
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.623001098632812
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 11.982159614562988
INFO:agents.father_agent:Step: 10, Training loss: 12.402889251708984
INFO:agents.father_agent:Step: 15, Training loss: 17.07499885559082
INFO:agents.father_agent:Step: 20, Training loss: 9.90591812133789
INFO:agents.father_agent:Step: 25, Training loss: 9.048330307006836
INFO:agents.father_agent:Step: 30, Training loss: 16.502206802368164
INFO:agents.father_agent:Step: 35, Training loss: 10.79797077178955
INFO:agents.father_agent:Step: 40, Training loss: 13.52518081665039
INFO:agents.father_agent:Step: 45, Training loss: 11.858345031738281
INFO:agents.father_agent:Step: 50, Training loss: 10.91511058807373
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.8316726684570312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.31672668457031
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.62116241455078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.9351308345794678
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.75169390195297
INFO:tools.evaluation_results_class:Counted Episodes = 7527
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.000528812408447
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.005287170410156
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.08695602416992
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.9014008045196533
INFO:tools.evaluation_results_class:Current Best Return = 4.000528812408447
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.93629394660322
INFO:tools.evaluation_results_class:Counted Episodes = 7566
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.889373540878296
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.893733978271484
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.07417678833008
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.9231300354003906
INFO:tools.evaluation_results_class:Current Best Return = 3.889373540878296
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.93629394660322
INFO:tools.evaluation_results_class:Counted Episodes = 7566
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.687375504254901
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 21 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9346189498901367
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.346187591552734
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.538448333740234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7195398807525635
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.542332584863296
INFO:tools.evaluation_results_class:Counted Episodes = 7571
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.038517951965332
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 12.418777465820312
INFO:agents.father_agent:Step: 10, Training loss: 16.035722732543945
INFO:agents.father_agent:Step: 15, Training loss: 15.322339057922363
INFO:agents.father_agent:Step: 20, Training loss: 13.914325714111328
INFO:agents.father_agent:Step: 25, Training loss: 21.97893714904785
INFO:agents.father_agent:Step: 30, Training loss: 13.9631929397583
INFO:agents.father_agent:Step: 35, Training loss: 8.937186241149902
INFO:agents.father_agent:Step: 40, Training loss: 13.66899585723877
INFO:agents.father_agent:Step: 45, Training loss: 14.565977096557617
INFO:agents.father_agent:Step: 50, Training loss: 10.685208320617676
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.029322624206543
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.2932243347168
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.4110107421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7410502433776855
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.542332584863296
INFO:tools.evaluation_results_class:Counted Episodes = 7571
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9677927494049072
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.67792892456055
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.84722137451172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.743014335632324
INFO:tools.evaluation_results_class:Current Best Return = 3.9677927494049072
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.721703693966084
INFO:tools.evaluation_results_class:Counted Episodes = 7607
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.903378486633301
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.033782958984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.262176513671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.781252145767212
INFO:tools.evaluation_results_class:Current Best Return = 3.903378486633301
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.721703693966084
INFO:tools.evaluation_results_class:Counted Episodes = 7607
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.688603575884619
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.411458015441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 86.11458587646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 68.28264617919922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.9843480587005615
INFO:tools.evaluation_results_class:Current Best Return = 8.411458015441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 143.40625
INFO:tools.evaluation_results_class:Average Discounted Reward = 87.86668395996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.175537109375
INFO:tools.evaluation_results_class:Current Best Return = 14.140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.6953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 108.953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.61993408203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.41497802734375
INFO:tools.evaluation_results_class:Current Best Return = 10.6953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.074999809265137
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 102.75
INFO:tools.evaluation_results_class:Average Discounted Reward = 79.16001892089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.484999895095825
INFO:tools.evaluation_results_class:Current Best Return = 10.074999809265137
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.73046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.3046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.57493591308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1265716552734375
INFO:tools.evaluation_results_class:Current Best Return = 7.73046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7587890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.587890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.40512466430664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.740156650543213
INFO:tools.evaluation_results_class:Current Best Return = 3.7587890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.27421188354492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.75531005859375
INFO:tools.evaluation_results_class:Current Best Return = 3.7421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.728759765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.28759765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.154640197753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.77139949798584
INFO:tools.evaluation_results_class:Current Best Return = 3.728759765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.740478515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.40478515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.25186538696289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7829909324645996
INFO:tools.evaluation_results_class:Current Best Return = 3.740478515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.737060546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.37060546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.2261962890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.725541114807129
INFO:tools.evaluation_results_class:Current Best Return = 3.737060546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.73828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.3828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.239585876464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7791595458984375
INFO:tools.evaluation_results_class:Current Best Return = 3.73828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.732666015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.32666015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.17600631713867
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6128592491149902
INFO:tools.evaluation_results_class:Current Best Return = 3.732666015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.73193359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.3193359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.16486358642578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.645425796508789
INFO:tools.evaluation_results_class:Current Best Return = 3.73193359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.723876953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.23876953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.0930061340332
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6134538650512695
INFO:tools.evaluation_results_class:Current Best Return = 3.723876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.740966796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.40966796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.242042541503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6211342811584473
INFO:tools.evaluation_results_class:Current Best Return = 3.740966796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.730712890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.30712890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.16407012939453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6103463172912598
INFO:tools.evaluation_results_class:Current Best Return = 3.730712890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.742919921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.42919921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.2500114440918
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6973376274108887
INFO:tools.evaluation_results_class:Current Best Return = 3.742919921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.742919921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.42919921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.25139617919922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6714587211608887
INFO:tools.evaluation_results_class:Current Best Return = 3.742919921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.750732421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.50732421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.3246955871582
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.637816905975342
INFO:tools.evaluation_results_class:Current Best Return = 3.750732421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.743896484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.43896484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.26603698730469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6524291038513184
INFO:tools.evaluation_results_class:Current Best Return = 3.743896484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.73486328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.3486328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.182342529296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6640772819519043
INFO:tools.evaluation_results_class:Current Best Return = 3.73486328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.71826171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.1826171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.030086517333984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.584685802459717
INFO:tools.evaluation_results_class:Current Best Return = 3.71826171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.719482421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.19482421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.04170227050781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5890345573425293
INFO:tools.evaluation_results_class:Current Best Return = 3.719482421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.70849609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.0849609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.94190216064453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6235218048095703
INFO:tools.evaluation_results_class:Current Best Return = 3.70849609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.712158203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.12158203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.974639892578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.585360050201416
INFO:tools.evaluation_results_class:Current Best Return = 3.712158203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.71337890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.1337890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.99403381347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.599977493286133
INFO:tools.evaluation_results_class:Current Best Return = 3.71337890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.478515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 86.78515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 69.16020965576172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.4409446716308594
INFO:tools.evaluation_results_class:Current Best Return = 8.478515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.03125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 142.3125
INFO:tools.evaluation_results_class:Average Discounted Reward = 86.98269653320312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.8779296875
INFO:tools.evaluation_results_class:Current Best Return = 14.03125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.803385734558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 110.03385162353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.36966705322266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.2517071962356567
INFO:tools.evaluation_results_class:Current Best Return = 10.803385734558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.07421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 102.7421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.78422546386719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.0296478271484375
INFO:tools.evaluation_results_class:Current Best Return = 10.07421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.826822757720947
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.26822662353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.525299072265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1067285537719727
INFO:tools.evaluation_results_class:Current Best Return = 7.826822757720947
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.640380859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.40380859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.347171783447266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6883010864257812
INFO:tools.evaluation_results_class:Current Best Return = 3.640380859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.741455078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.41455078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.24253845214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.688281536102295
INFO:tools.evaluation_results_class:Current Best Return = 3.741455078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.74169921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.4169921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.24714279174805
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6617960929870605
INFO:tools.evaluation_results_class:Current Best Return = 3.74169921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.72900390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.2900390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.13661575317383
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7034168243408203
INFO:tools.evaluation_results_class:Current Best Return = 3.72900390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.72705078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.2705078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.119590759277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6315531730651855
INFO:tools.evaluation_results_class:Current Best Return = 3.72705078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.734130859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.34130859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.182830810546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6722335815429688
INFO:tools.evaluation_results_class:Current Best Return = 3.734130859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.65869140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.5869140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.52479553222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6662235260009766
INFO:tools.evaluation_results_class:Current Best Return = 3.65869140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.653076171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.53076171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.480323791503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6772515773773193
INFO:tools.evaluation_results_class:Current Best Return = 3.653076171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.65380859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.5380859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.49049758911133
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6721439361572266
INFO:tools.evaluation_results_class:Current Best Return = 3.65380859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.662353515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.62353515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.566463470458984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.701669216156006
INFO:tools.evaluation_results_class:Current Best Return = 3.662353515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.653564453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.53564453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.48627853393555
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7283716201782227
INFO:tools.evaluation_results_class:Current Best Return = 3.653564453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.680419921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.80419921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.714351654052734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7687182426452637
INFO:tools.evaluation_results_class:Current Best Return = 3.680419921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.694091796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.94091796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.83338928222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7054924964904785
INFO:tools.evaluation_results_class:Current Best Return = 3.694091796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.699951171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.99951171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.88841247558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.752988338470459
INFO:tools.evaluation_results_class:Current Best Return = 3.699951171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.699951171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.99951171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.88175964355469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.758847713470459
INFO:tools.evaluation_results_class:Current Best Return = 3.699951171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.675048828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.75048828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.66280746459961
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.728635311126709
INFO:tools.evaluation_results_class:Current Best Return = 3.675048828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.677001953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.77001953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.68558883666992
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7460145950317383
INFO:tools.evaluation_results_class:Current Best Return = 3.677001953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.663818359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.63818359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.576480865478516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.691913604736328
INFO:tools.evaluation_results_class:Current Best Return = 3.663818359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.68212890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.8212890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.742889404296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.727571487426758
INFO:tools.evaluation_results_class:Current Best Return = 3.68212890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.67431640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.7431640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.66942596435547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.726449966430664
INFO:tools.evaluation_results_class:Current Best Return = 3.67431640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6845703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.845703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.758262634277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.692984104156494
INFO:tools.evaluation_results_class:Current Best Return = 3.6845703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 22 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.935293436050415
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.35293197631836
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.509925842285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.490891456604004
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.333114581966136
INFO:tools.evaluation_results_class:Counted Episodes = 7619
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.19046401977539
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Step: 5, Training loss: 11.931012153625488
INFO:agents.father_agent:Step: 10, Training loss: 13.601449966430664
INFO:agents.father_agent:Step: 15, Training loss: 14.33788013458252
INFO:agents.father_agent:Step: 20, Training loss: 11.948038101196289
INFO:agents.father_agent:Step: 25, Training loss: 20.020206451416016
INFO:agents.father_agent:Step: 30, Training loss: 14.808988571166992
INFO:agents.father_agent:Step: 35, Training loss: 11.766891479492188
INFO:agents.father_agent:Step: 40, Training loss: 18.935169219970703
INFO:agents.father_agent:Step: 45, Training loss: 13.747230529785156
INFO:agents.father_agent:Step: 50, Training loss: 10.850850105285645
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.0568318367004395
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.56831741333008
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.66664505004883
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7598624229431152
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.333114581966136
INFO:tools.evaluation_results_class:Counted Episodes = 7619
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.924729824066162
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.24729919433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.5244026184082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7700862884521484
INFO:tools.evaluation_results_class:Current Best Return = 3.924729824066162
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.365933064201066
INFO:tools.evaluation_results_class:Counted Episodes = 7679
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.883188009262085
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.831878662109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.129119873046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.5247061252593994
INFO:tools.evaluation_results_class:Current Best Return = 3.883188009262085
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.365933064201066
INFO:tools.evaluation_results_class:Counted Episodes = 7679
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.7061651185018927
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 23 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8751304149627686
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.751304626464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.09934616088867
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.528379440307617
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.13048016701461
INFO:tools.evaluation_results_class:Counted Episodes = 7664
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.51932430267334
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 10.154430389404297
INFO:agents.father_agent:Step: 10, Training loss: 17.147842407226562
INFO:agents.father_agent:Step: 15, Training loss: 10.934098243713379
INFO:agents.father_agent:Step: 20, Training loss: 13.113348007202148
INFO:agents.father_agent:Step: 25, Training loss: 12.328614234924316
INFO:agents.father_agent:Step: 30, Training loss: 12.474437713623047
INFO:agents.father_agent:Step: 35, Training loss: 11.606441497802734
INFO:agents.father_agent:Step: 40, Training loss: 18.05321502685547
INFO:agents.father_agent:Step: 45, Training loss: 13.634919166564941
INFO:agents.father_agent:Step: 50, Training loss: 11.076075553894043
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.9239301681518555
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.23929977416992
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.47358703613281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.675450563430786
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.13048016701461
INFO:tools.evaluation_results_class:Counted Episodes = 7664
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.819220781326294
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.19220733642578
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.52549362182617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.760565757751465
INFO:tools.evaluation_results_class:Current Best Return = 3.819220781326294
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.256623376623374
INFO:tools.evaluation_results_class:Counted Episodes = 7700
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7903895378112793
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.90389633178711
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.35344314575195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.548011541366577
INFO:tools.evaluation_results_class:Current Best Return = 3.7903895378112793
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.256623376623374
INFO:tools.evaluation_results_class:Counted Episodes = 7700
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.696302276428585
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 24 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.009626865386963
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.09626770019531
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.28649139404297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.333327531814575
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.0273188500065
INFO:tools.evaluation_results_class:Counted Episodes = 7687
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 16.079185485839844
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 11.155240058898926
INFO:agents.father_agent:Step: 10, Training loss: 14.071497917175293
INFO:agents.father_agent:Step: 15, Training loss: 15.49160385131836
INFO:agents.father_agent:Step: 20, Training loss: 12.958218574523926
INFO:agents.father_agent:Step: 25, Training loss: 14.186689376831055
INFO:agents.father_agent:Step: 30, Training loss: 11.795801162719727
INFO:agents.father_agent:Step: 35, Training loss: 12.021380424499512
INFO:agents.father_agent:Step: 40, Training loss: 12.903682708740234
INFO:agents.father_agent:Step: 45, Training loss: 14.904329299926758
INFO:agents.father_agent:Step: 50, Training loss: 11.341910362243652
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.890334367752075
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.903343200683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.24964141845703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.695141553878784
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.0273188500065
INFO:tools.evaluation_results_class:Counted Episodes = 7687
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.948704719543457
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.48704528808594
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.782005310058594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.547887086868286
INFO:tools.evaluation_results_class:Current Best Return = 3.948704719543457
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.15751295336788
INFO:tools.evaluation_results_class:Counted Episodes = 7720
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.843134641647339
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.43134689331055
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.77834701538086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.6926214694976807
INFO:tools.evaluation_results_class:Current Best Return = 3.843134641647339
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.15751295336788
INFO:tools.evaluation_results_class:Counted Episodes = 7720
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.7148163736582087
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 25 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.958301067352295
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.5830078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.815940856933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.6545677185058594
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.87723387723388
INFO:tools.evaluation_results_class:Counted Episodes = 7722
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.086247444152832
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 11.698356628417969
INFO:agents.father_agent:Step: 10, Training loss: 14.150506973266602
INFO:agents.father_agent:Step: 15, Training loss: 11.863848686218262
INFO:agents.father_agent:Step: 20, Training loss: 10.06395435333252
INFO:agents.father_agent:Step: 25, Training loss: 17.361793518066406
INFO:agents.father_agent:Step: 30, Training loss: 15.417339324951172
INFO:agents.father_agent:Step: 35, Training loss: 10.309342384338379
INFO:agents.father_agent:Step: 40, Training loss: 16.503564834594727
INFO:agents.father_agent:Step: 45, Training loss: 13.99160385131836
INFO:agents.father_agent:Step: 50, Training loss: 11.77932357788086
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.9158248901367188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.15824890136719
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.44789505004883
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.4575607776641846
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.87723387723388
INFO:tools.evaluation_results_class:Counted Episodes = 7722
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9821038246154785
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.82103729248047
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.04029083251953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.643686532974243
INFO:tools.evaluation_results_class:Current Best Return = 3.9821038246154785
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.93330758336552
INFO:tools.evaluation_results_class:Counted Episodes = 7767
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.913222551345825
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.132225036621094
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.46566390991211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.6824400424957275
INFO:tools.evaluation_results_class:Current Best Return = 3.913222551345825
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.93330758336552
INFO:tools.evaluation_results_class:Counted Episodes = 7767
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.6787629252378857
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 26 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.009542465209961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.095420837402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.341453552246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.166640281677246
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.72688588007737
INFO:tools.evaluation_results_class:Counted Episodes = 7755
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.3449125289917
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 10.913480758666992
INFO:agents.father_agent:Step: 10, Training loss: 10.795317649841309
INFO:agents.father_agent:Step: 15, Training loss: 11.093303680419922
INFO:agents.father_agent:Step: 20, Training loss: 10.977189064025879
INFO:agents.father_agent:Step: 25, Training loss: 12.558709144592285
INFO:agents.father_agent:Step: 30, Training loss: 12.735636711120605
INFO:agents.father_agent:Step: 35, Training loss: 12.590200424194336
INFO:agents.father_agent:Step: 40, Training loss: 9.932382583618164
INFO:agents.father_agent:Step: 45, Training loss: 11.815778732299805
INFO:agents.father_agent:Step: 50, Training loss: 12.167707443237305
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.02940034866333
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.294002532958984
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.5145378112793
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.4008119106292725
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.72688588007737
INFO:tools.evaluation_results_class:Counted Episodes = 7755
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8235368728637695
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.23537063598633
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.648094177246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.407644271850586
INFO:tools.evaluation_results_class:Current Best Return = 3.8235368728637695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.81211498973306
INFO:tools.evaluation_results_class:Counted Episodes = 7792
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7827258110046387
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.8272590637207
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.30797576904297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.249891757965088
INFO:tools.evaluation_results_class:Current Best Return = 3.7827258110046387
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.81211498973306
INFO:tools.evaluation_results_class:Counted Episodes = 7792
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.685621070834967
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.393229484558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 85.93228912353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 68.19805145263672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.0862560272216797
INFO:tools.evaluation_results_class:Current Best Return = 8.393229484558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 143.40625
INFO:tools.evaluation_results_class:Average Discounted Reward = 88.02449035644531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.722412109375
INFO:tools.evaluation_results_class:Current Best Return = 14.140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.772135734558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.72135162353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.2283706665039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.26448392868042
INFO:tools.evaluation_results_class:Current Best Return = 10.772135734558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.200780868530273
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 104.0078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 80.13761901855469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.6120307445526123
INFO:tools.evaluation_results_class:Current Best Return = 10.200780868530273
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.706380367279053
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.06380462646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.38411331176758
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.146209478378296
INFO:tools.evaluation_results_class:Current Best Return = 7.706380367279053
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7646484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.47110366821289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7361130714416504
INFO:tools.evaluation_results_class:Current Best Return = 3.7646484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.831787109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.31787109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.05839157104492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8073983192443848
INFO:tools.evaluation_results_class:Current Best Return = 3.831787109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.817138671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.17138671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.91858673095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.817880392074585
INFO:tools.evaluation_results_class:Current Best Return = 3.817138671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.822509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.22509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.97577667236328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8095617294311523
INFO:tools.evaluation_results_class:Current Best Return = 3.822509765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.821044921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.21044921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.95997619628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.81880521774292
INFO:tools.evaluation_results_class:Current Best Return = 3.821044921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.950740814208984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.80657958984375
INFO:tools.evaluation_results_class:Current Best Return = 3.8203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.787353515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.87353515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.673805236816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.853951930999756
INFO:tools.evaluation_results_class:Current Best Return = 3.787353515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.78369140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.8369140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.64476776123047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8330936431884766
INFO:tools.evaluation_results_class:Current Best Return = 3.78369140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.77880859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.7880859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.59758758544922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8412113189697266
INFO:tools.evaluation_results_class:Current Best Return = 3.77880859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.789794921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.89794921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.68913650512695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8432650566101074
INFO:tools.evaluation_results_class:Current Best Return = 3.789794921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.783935546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.83935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.644775390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8461389541625977
INFO:tools.evaluation_results_class:Current Best Return = 3.783935546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.775146484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.75146484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.54045867919922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.726052761077881
INFO:tools.evaluation_results_class:Current Best Return = 3.775146484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.779052734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.79052734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.57956314086914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6877546310424805
INFO:tools.evaluation_results_class:Current Best Return = 3.779052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.78076171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.8076171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.59326934814453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.693633556365967
INFO:tools.evaluation_results_class:Current Best Return = 3.78076171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.77001953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.7001953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.49610137939453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.697108745574951
INFO:tools.evaluation_results_class:Current Best Return = 3.77001953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.773681640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.73681640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.52859115600586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.714160919189453
INFO:tools.evaluation_results_class:Current Best Return = 3.773681640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.79931640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.9931640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.751956939697266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.738534927368164
INFO:tools.evaluation_results_class:Current Best Return = 3.79931640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.792236328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.92236328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.68684387207031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.732957363128662
INFO:tools.evaluation_results_class:Current Best Return = 3.792236328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.797607421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.97607421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.738487243652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7712931632995605
INFO:tools.evaluation_results_class:Current Best Return = 3.797607421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7900390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.900390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.665626525878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.742537021636963
INFO:tools.evaluation_results_class:Current Best Return = 3.7900390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.792724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.92724609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.6917724609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7312068939208984
INFO:tools.evaluation_results_class:Current Best Return = 3.792724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.76318359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.6318359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.42974090576172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.691476821899414
INFO:tools.evaluation_results_class:Current Best Return = 3.76318359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.76953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.6953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.478729248046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6988372802734375
INFO:tools.evaluation_results_class:Current Best Return = 3.76953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.760498046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.60498046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.399391174316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7231569290161133
INFO:tools.evaluation_results_class:Current Best Return = 3.760498046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.76171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.6171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.41722869873047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6771087646484375
INFO:tools.evaluation_results_class:Current Best Return = 3.76171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.751953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.51953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.325828552246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7001914978027344
INFO:tools.evaluation_results_class:Current Best Return = 3.751953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.491536140441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 86.91536712646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 69.16024017333984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3228447437286377
INFO:tools.evaluation_results_class:Current Best Return = 8.491536140441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.224609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 144.24609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 88.12739562988281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.8694725036621094
INFO:tools.evaluation_results_class:Current Best Return = 14.224609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.700520515441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.00521087646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.88610076904297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.496249794960022
INFO:tools.evaluation_results_class:Current Best Return = 10.700520515441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 9.997655868530273
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 101.9765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.1427230834961
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.814838409423828
INFO:tools.evaluation_results_class:Current Best Return = 9.997655868530273
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.826822757720947
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.26822662353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.42539978027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.249957799911499
INFO:tools.evaluation_results_class:Current Best Return = 7.826822757720947
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.66796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.6796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.60702133178711
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6568450927734375
INFO:tools.evaluation_results_class:Current Best Return = 3.66796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.638916015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.38916015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.33936309814453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7414450645446777
INFO:tools.evaluation_results_class:Current Best Return = 3.638916015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.64892578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.4892578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.42978286743164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.786902904510498
INFO:tools.evaluation_results_class:Current Best Return = 3.64892578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.631103515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.31103515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.266090393066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7640624046325684
INFO:tools.evaluation_results_class:Current Best Return = 3.631103515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.650390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.50390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.448524475097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7195701599121094
INFO:tools.evaluation_results_class:Current Best Return = 3.650390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.634521484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.34521484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.29808807373047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7362990379333496
INFO:tools.evaluation_results_class:Current Best Return = 3.634521484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.34375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.190643310546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.720458984375
INFO:tools.evaluation_results_class:Current Best Return = 3.734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.716796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.16796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.02973175048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6746788024902344
INFO:tools.evaluation_results_class:Current Best Return = 3.716796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.71875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.1875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.04279327392578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.67578125
INFO:tools.evaluation_results_class:Current Best Return = 3.71875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.726806640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.26806640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.127708435058594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.678539276123047
INFO:tools.evaluation_results_class:Current Best Return = 3.726806640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.720703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.20703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.06256103515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6949424743652344
INFO:tools.evaluation_results_class:Current Best Return = 3.720703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.642822265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.42822265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.383697509765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7105588912963867
INFO:tools.evaluation_results_class:Current Best Return = 3.642822265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6513671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.513671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.459922790527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6846070289611816
INFO:tools.evaluation_results_class:Current Best Return = 3.6513671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.64111328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.4111328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.37056350708008
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6944422721862793
INFO:tools.evaluation_results_class:Current Best Return = 3.64111328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.649658203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.49658203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.446014404296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.720766544342041
INFO:tools.evaluation_results_class:Current Best Return = 3.649658203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.667724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.67724609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.61067199707031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7399349212646484
INFO:tools.evaluation_results_class:Current Best Return = 3.667724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.69775390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.9775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.8701286315918
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7567920684814453
INFO:tools.evaluation_results_class:Current Best Return = 3.69775390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.715087890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.15087890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.02512741088867
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.754030704498291
INFO:tools.evaluation_results_class:Current Best Return = 3.715087890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.677734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.77734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.68959426879883
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7242698669433594
INFO:tools.evaluation_results_class:Current Best Return = 3.677734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.70068359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.0068359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.89807891845703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.752206802368164
INFO:tools.evaluation_results_class:Current Best Return = 3.70068359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6923828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.923828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.81572723388672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7349610328674316
INFO:tools.evaluation_results_class:Current Best Return = 3.6923828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6962890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.962890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.86284255981445
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.794966220855713
INFO:tools.evaluation_results_class:Current Best Return = 3.6962890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.681640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.81640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.729759216308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7321434020996094
INFO:tools.evaluation_results_class:Current Best Return = 3.681640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.67138671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.7138671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.6363410949707
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7484583854675293
INFO:tools.evaluation_results_class:Current Best Return = 3.67138671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.680419921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.80419921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.71874237060547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7472338676452637
INFO:tools.evaluation_results_class:Current Best Return = 3.680419921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.676025390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.76025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.6877326965332
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.714132308959961
INFO:tools.evaluation_results_class:Current Best Return = 3.676025390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 27 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.000641822814941
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.00642013549805
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.317909240722656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.145955801010132
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.57946084724005
INFO:tools.evaluation_results_class:Counted Episodes = 7790
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.853139877319336
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 12.280351638793945
INFO:agents.father_agent:Step: 10, Training loss: 13.385631561279297
INFO:agents.father_agent:Step: 15, Training loss: 11.232040405273438
INFO:agents.father_agent:Step: 20, Training loss: 11.552724838256836
INFO:agents.father_agent:Step: 25, Training loss: 16.131864547729492
INFO:agents.father_agent:Step: 30, Training loss: 13.606802940368652
INFO:agents.father_agent:Step: 35, Training loss: 11.484599113464355
INFO:agents.father_agent:Step: 40, Training loss: 11.29023551940918
INFO:agents.father_agent:Step: 45, Training loss: 13.022455215454102
INFO:agents.father_agent:Step: 50, Training loss: 14.048689842224121
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.9596920013427734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.596920013427734
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.900482177734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.232778310775757
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.57946084724005
INFO:tools.evaluation_results_class:Counted Episodes = 7790
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8539512157440186
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.539512634277344
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.918453216552734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.591334104537964
INFO:tools.evaluation_results_class:Current Best Return = 3.8539512157440186
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.609600408528024
INFO:tools.evaluation_results_class:Counted Episodes = 7833
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8976125717163086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.97612762451172
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.38459396362305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.382852554321289
INFO:tools.evaluation_results_class:Current Best Return = 3.8976125717163086
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.609600408528024
INFO:tools.evaluation_results_class:Counted Episodes = 7833
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.6986269682637323
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 28 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.968898057937622
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.68898010253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.02910232543945
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3210601806640625
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.47958530654038
INFO:tools.evaluation_results_class:Counted Episodes = 7813
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.771306037902832
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 11.906819343566895
INFO:agents.father_agent:Step: 10, Training loss: 9.034263610839844
INFO:agents.father_agent:Step: 15, Training loss: 17.43106460571289
INFO:agents.father_agent:Step: 20, Training loss: 13.359042167663574
INFO:agents.father_agent:Step: 25, Training loss: 14.206218719482422
INFO:agents.father_agent:Step: 30, Training loss: 13.087600708007812
INFO:agents.father_agent:Step: 35, Training loss: 11.957799911499023
INFO:agents.father_agent:Step: 40, Training loss: 13.616219520568848
INFO:agents.father_agent:Step: 45, Training loss: 13.110762596130371
INFO:agents.father_agent:Step: 50, Training loss: 10.23864459991455
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.93088436126709
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.30884552001953
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.62309265136719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.293572187423706
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.47958530654038
INFO:tools.evaluation_results_class:Counted Episodes = 7813
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8066980838775635
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.06698226928711
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.51320266723633
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.542922258377075
INFO:tools.evaluation_results_class:Current Best Return = 3.8066980838775635
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.5138163759073
INFO:tools.evaluation_results_class:Counted Episodes = 7853
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6444671154022217
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.444671630859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.14012908935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.461014986038208
INFO:tools.evaluation_results_class:Current Best Return = 3.6444671154022217
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.5138163759073
INFO:tools.evaluation_results_class:Counted Episodes = 7853
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.6828572740810537
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 29 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9910531044006348
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.91053009033203
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.20454788208008
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.1778340339660645
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.43149284253579
INFO:tools.evaluation_results_class:Counted Episodes = 7824
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.176962852478027
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 10.349184036254883
INFO:agents.father_agent:Step: 10, Training loss: 12.859789848327637
INFO:agents.father_agent:Step: 15, Training loss: 11.889751434326172
INFO:agents.father_agent:Step: 20, Training loss: 11.533430099487305
INFO:agents.father_agent:Step: 25, Training loss: 12.100089073181152
INFO:agents.father_agent:Step: 30, Training loss: 12.701339721679688
INFO:agents.father_agent:Step: 35, Training loss: 11.9659423828125
INFO:agents.father_agent:Step: 40, Training loss: 12.060240745544434
INFO:agents.father_agent:Step: 45, Training loss: 13.533892631530762
INFO:agents.father_agent:Step: 50, Training loss: 10.659199714660645
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.8684816360473633
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.684814453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.07520294189453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.2916243076324463
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.43149284253579
INFO:tools.evaluation_results_class:Counted Episodes = 7824
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9544122219085693
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.544124603271484
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.9046745300293
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.1444900035858154
INFO:tools.evaluation_results_class:Current Best Return = 3.9544122219085693
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.5138163759073
INFO:tools.evaluation_results_class:Counted Episodes = 7853
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8265631198883057
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.26563262939453
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.7573127746582
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.2692954540252686
INFO:tools.evaluation_results_class:Current Best Return = 3.8265631198883057
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.5138163759073
INFO:tools.evaluation_results_class:Counted Episodes = 7853
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.683582845881509
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 30 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9355862140655518
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.35586166381836
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.78703689575195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.2453746795654297
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.23592936094524
INFO:tools.evaluation_results_class:Counted Episodes = 7871
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.92676067352295
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 8.971089363098145
INFO:agents.father_agent:Step: 10, Training loss: 12.782005310058594
INFO:agents.father_agent:Step: 15, Training loss: 10.226635932922363
INFO:agents.father_agent:Step: 20, Training loss: 10.667305946350098
INFO:agents.father_agent:Step: 25, Training loss: 13.45332145690918
INFO:agents.father_agent:Step: 30, Training loss: 8.66101360321045
INFO:agents.father_agent:Step: 35, Training loss: 10.511832237243652
INFO:agents.father_agent:Step: 40, Training loss: 12.349111557006836
INFO:agents.father_agent:Step: 45, Training loss: 10.951716423034668
INFO:agents.father_agent:Step: 50, Training loss: 11.501697540283203
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.7667386531829834
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.66738510131836
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.2685661315918
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.203624725341797
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.23592936094524
INFO:tools.evaluation_results_class:Counted Episodes = 7871
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8944971561431885
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.94497299194336
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.37795639038086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.252373456954956
INFO:tools.evaluation_results_class:Current Best Return = 3.8944971561431885
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.2741302972802
INFO:tools.evaluation_results_class:Counted Episodes = 7905
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.782289743423462
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.82289505004883
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.42066955566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.219268798828125
INFO:tools.evaluation_results_class:Current Best Return = 3.782289743423462
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.2741302972802
INFO:tools.evaluation_results_class:Counted Episodes = 7905
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.6861866420966556
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 31 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.904085159301758
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.04085159301758
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.469154357910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.214855194091797
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.188530829738646
INFO:tools.evaluation_results_class:Counted Episodes = 7882
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.348124504089355
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 10.574913024902344
INFO:agents.father_agent:Step: 10, Training loss: 16.098451614379883
INFO:agents.father_agent:Step: 15, Training loss: 15.210659980773926
INFO:agents.father_agent:Step: 20, Training loss: 15.408016204833984
INFO:agents.father_agent:Step: 25, Training loss: 8.233384132385254
INFO:agents.father_agent:Step: 30, Training loss: 11.67288589477539
INFO:agents.father_agent:Step: 35, Training loss: 11.231165885925293
INFO:agents.father_agent:Step: 40, Training loss: 13.460082054138184
INFO:agents.father_agent:Step: 45, Training loss: 15.222073554992676
INFO:agents.father_agent:Step: 50, Training loss: 11.782024383544922
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.010784149169922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.10784149169922
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.4501838684082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.4021925926208496
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.188530829738646
INFO:tools.evaluation_results_class:Counted Episodes = 7882
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.983724355697632
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.83724594116211
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.280906677246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.488379955291748
INFO:tools.evaluation_results_class:Current Best Return = 3.983724355697632
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.17083017915721
INFO:tools.evaluation_results_class:Counted Episodes = 7926
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.635124921798706
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.35124969482422
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.10334777832031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.155536413192749
INFO:tools.evaluation_results_class:Current Best Return = 3.635124921798706
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.17083017915721
INFO:tools.evaluation_results_class:Counted Episodes = 7926
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.6646235874431037
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.409504890441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 86.09505462646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 68.16641235351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.188425302505493
INFO:tools.evaluation_results_class:Current Best Return = 8.409504890441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.248046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 144.48046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 88.87000274658203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.7841758728027344
INFO:tools.evaluation_results_class:Current Best Return = 14.248046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.766926765441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.66927337646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.05695343017578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.3662500381469727
INFO:tools.evaluation_results_class:Current Best Return = 10.766926765441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.276562690734863
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 104.765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 80.70680236816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3844504356384277
INFO:tools.evaluation_results_class:Current Best Return = 10.276562690734863
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.722005367279053
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.22005462646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.39168167114258
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.121286630630493
INFO:tools.evaluation_results_class:Current Best Return = 7.722005367279053
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.80810546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.0810546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.839927673339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7981371879577637
INFO:tools.evaluation_results_class:Current Best Return = 3.80810546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.78369140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.8369140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.64042663574219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7442264556884766
INFO:tools.evaluation_results_class:Current Best Return = 3.78369140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.79736328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.9736328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.76033401489258
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7382349967956543
INFO:tools.evaluation_results_class:Current Best Return = 3.79736328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.791015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.91015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.70804977416992
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7346458435058594
INFO:tools.evaluation_results_class:Current Best Return = 3.791015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.782470703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.82470703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.63361358642578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.743452548980713
INFO:tools.evaluation_results_class:Current Best Return = 3.782470703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.78125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.8125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.616920471191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.73779296875
INFO:tools.evaluation_results_class:Current Best Return = 3.78125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8388671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.388671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.11612319946289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8544068336486816
INFO:tools.evaluation_results_class:Current Best Return = 3.8388671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.830322265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.30322265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.04142379760742
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8498716354370117
INFO:tools.evaluation_results_class:Current Best Return = 3.830322265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.85400390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.5400390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.25136947631836
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8761463165283203
INFO:tools.evaluation_results_class:Current Best Return = 3.85400390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.827392578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.27392578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.01578140258789
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8679118156433105
INFO:tools.evaluation_results_class:Current Best Return = 3.827392578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.824951171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.24951171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.99482727050781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.825070858001709
INFO:tools.evaluation_results_class:Current Best Return = 3.824951171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.796630859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.96630859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.756229400634766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8397445678710938
INFO:tools.evaluation_results_class:Current Best Return = 3.796630859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.802734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.02734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.81357192993164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8639183044433594
INFO:tools.evaluation_results_class:Current Best Return = 3.802734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7880859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.880859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.679039001464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.843763828277588
INFO:tools.evaluation_results_class:Current Best Return = 3.7880859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.80615234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.0615234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.84186553955078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.879903793334961
INFO:tools.evaluation_results_class:Current Best Return = 3.80615234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.80078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.0078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.795005798339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8714447021484375
INFO:tools.evaluation_results_class:Current Best Return = 3.80078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.798095703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.98095703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.74955368041992
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7451233863830566
INFO:tools.evaluation_results_class:Current Best Return = 3.798095703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.66563415527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.74212646484375
INFO:tools.evaluation_results_class:Current Best Return = 3.7890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7978515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.978515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.745262145996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7457566261291504
INFO:tools.evaluation_results_class:Current Best Return = 3.7978515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.791015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.91015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.68885803222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7473411560058594
INFO:tools.evaluation_results_class:Current Best Return = 3.791015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.78857421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.8857421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.66130065917969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.751197338104248
INFO:tools.evaluation_results_class:Current Best Return = 3.78857421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.79817199707031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.76507568359375
INFO:tools.evaluation_results_class:Current Best Return = 3.8046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.806640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.06640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.816139221191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8068504333496094
INFO:tools.evaluation_results_class:Current Best Return = 3.806640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.804443359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.04443359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.79835510253906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7837791442871094
INFO:tools.evaluation_results_class:Current Best Return = 3.804443359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.804443359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.04443359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.80180358886719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7788963317871094
INFO:tools.evaluation_results_class:Current Best Return = 3.804443359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.801025390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.01025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.76578140258789
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.772665023803711
INFO:tools.evaluation_results_class:Current Best Return = 3.801025390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7783203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.783203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.55943298339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.742361545562744
INFO:tools.evaluation_results_class:Current Best Return = 3.7783203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.774658203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.74658203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.53132629394531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.702394962310791
INFO:tools.evaluation_results_class:Current Best Return = 3.774658203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7861328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.861328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.63279724121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7272095680236816
INFO:tools.evaluation_results_class:Current Best Return = 3.7861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.779296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.79296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.570953369140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7208213806152344
INFO:tools.evaluation_results_class:Current Best Return = 3.779296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.775390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.75390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.5390510559082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7483787536621094
INFO:tools.evaluation_results_class:Current Best Return = 3.775390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.530598640441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 87.30599212646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 69.49933624267578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.2555739879608154
INFO:tools.evaluation_results_class:Current Best Return = 8.530598640441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.169921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 143.69921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 88.01602172851562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.9066734313964844
INFO:tools.evaluation_results_class:Current Best Return = 14.169921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.782551765441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.82552337646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.34069061279297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.3837060928344727
INFO:tools.evaluation_results_class:Current Best Return = 10.782551765441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.0546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 102.546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.56712341308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.87982177734375
INFO:tools.evaluation_results_class:Current Best Return = 10.0546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.777994632720947
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.77994537353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.145111083984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.089385747909546
INFO:tools.evaluation_results_class:Current Best Return = 7.777994632720947
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.664794921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.64794921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.57493591308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7355380058288574
INFO:tools.evaluation_results_class:Current Best Return = 3.664794921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6708984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.708984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.630470275878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6753830909729004
INFO:tools.evaluation_results_class:Current Best Return = 3.6708984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6591796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.591796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.532318115234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.668509006500244
INFO:tools.evaluation_results_class:Current Best Return = 3.6591796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.677978515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.77978515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.699214935302734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6426405906677246
INFO:tools.evaluation_results_class:Current Best Return = 3.677978515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.671142578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.71142578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.64445495605469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.661139965057373
INFO:tools.evaluation_results_class:Current Best Return = 3.671142578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.66796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.6796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.60530090332031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6470794677734375
INFO:tools.evaluation_results_class:Current Best Return = 3.66796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.628662109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.28662109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.24818801879883
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7637200355529785
INFO:tools.evaluation_results_class:Current Best Return = 3.628662109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6142578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.142578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.11083984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7540345191955566
INFO:tools.evaluation_results_class:Current Best Return = 3.6142578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.61474609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.1474609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.12644958496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7519702911376953
INFO:tools.evaluation_results_class:Current Best Return = 3.61474609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6259765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.259765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.235870361328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7375473976135254
INFO:tools.evaluation_results_class:Current Best Return = 3.6259765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.61865234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.1865234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.161224365234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.746664047241211
INFO:tools.evaluation_results_class:Current Best Return = 3.61865234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.718017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.18017578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.038238525390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6599879264831543
INFO:tools.evaluation_results_class:Current Best Return = 3.718017578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.712646484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.12646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.002357482910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.684762477874756
INFO:tools.evaluation_results_class:Current Best Return = 3.712646484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.699462890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.99462890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.87181854248047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6911721229553223
INFO:tools.evaluation_results_class:Current Best Return = 3.699462890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.699951171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.99951171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.88157653808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.644101619720459
INFO:tools.evaluation_results_class:Current Best Return = 3.699951171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.700439453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.00439453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.884891510009766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.646836280822754
INFO:tools.evaluation_results_class:Current Best Return = 3.700439453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.625732421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.25732421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.23585510253906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.689269542694092
INFO:tools.evaluation_results_class:Current Best Return = 3.625732421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.629638671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.29638671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.272117614746094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.708779811859131
INFO:tools.evaluation_results_class:Current Best Return = 3.629638671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6455078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.455078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.41741943359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7029480934143066
INFO:tools.evaluation_results_class:Current Best Return = 3.6455078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.62646484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.2646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.24156951904297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.734983444213867
INFO:tools.evaluation_results_class:Current Best Return = 3.62646484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.61279296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.1279296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.124977111816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6733126640319824
INFO:tools.evaluation_results_class:Current Best Return = 3.61279296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.67724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.7724609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.69012451171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8025684356689453
INFO:tools.evaluation_results_class:Current Best Return = 3.67724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.665283203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.65283203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.57841491699219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7417244911193848
INFO:tools.evaluation_results_class:Current Best Return = 3.665283203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.71875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.6468391418457
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.775146484375
INFO:tools.evaluation_results_class:Current Best Return = 3.671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.669189453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.69189453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.614288330078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7370004653930664
INFO:tools.evaluation_results_class:Current Best Return = 3.669189453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6669921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.669921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.59727478027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7748475074768066
INFO:tools.evaluation_results_class:Current Best Return = 3.6669921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.664306640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.64306640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.578529357910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.751811981201172
INFO:tools.evaluation_results_class:Current Best Return = 3.664306640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.66943359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.6943359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.62690353393555
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.736429214477539
INFO:tools.evaluation_results_class:Current Best Return = 3.66943359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6708984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.708984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.63655471801758
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7676682472229004
INFO:tools.evaluation_results_class:Current Best Return = 3.6708984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6572265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.572265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.51471710205078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7106308937072754
INFO:tools.evaluation_results_class:Current Best Return = 3.6572265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.661865234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.61865234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.559959411621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.710127830505371
INFO:tools.evaluation_results_class:Current Best Return = 3.661865234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 32 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.995184898376465
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.95185089111328
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.363948822021484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3456432819366455
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.141409021794225
INFO:tools.evaluation_results_class:Counted Episodes = 7892
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.32319450378418
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 9.913919448852539
INFO:agents.father_agent:Step: 10, Training loss: 16.106061935424805
INFO:agents.father_agent:Step: 15, Training loss: 11.393389701843262
INFO:agents.father_agent:Step: 20, Training loss: 10.113082885742188
INFO:agents.father_agent:Step: 25, Training loss: 9.492073059082031
INFO:agents.father_agent:Step: 30, Training loss: 11.069916725158691
INFO:agents.father_agent:Step: 35, Training loss: 9.294875144958496
INFO:agents.father_agent:Step: 40, Training loss: 13.29110050201416
INFO:agents.father_agent:Step: 45, Training loss: 10.001537322998047
INFO:agents.father_agent:Step: 50, Training loss: 11.052348136901855
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.853142499923706
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.53142547607422
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.03236389160156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3918893337249756
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.141409021794225
INFO:tools.evaluation_results_class:Counted Episodes = 7892
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8331236839294434
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.331233978271484
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.89182662963867
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.197171211242676
INFO:tools.evaluation_results_class:Current Best Return = 3.8331236839294434
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.07727158318651
INFO:tools.evaluation_results_class:Counted Episodes = 7946
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.802542209625244
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.025421142578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.63374710083008
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.922877788543701
INFO:tools.evaluation_results_class:Current Best Return = 3.802542209625244
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.07727158318651
INFO:tools.evaluation_results_class:Counted Episodes = 7946
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.674994568294589
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 33 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9433724880218506
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.43372344970703
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.88398742675781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.164027452468872
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.995838062807415
INFO:tools.evaluation_results_class:Counted Episodes = 7929
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.702738761901855
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 13.344286918640137
INFO:agents.father_agent:Step: 10, Training loss: 10.652036666870117
INFO:agents.father_agent:Step: 15, Training loss: 16.030651092529297
INFO:agents.father_agent:Step: 20, Training loss: 9.971614837646484
INFO:agents.father_agent:Step: 25, Training loss: 12.065041542053223
INFO:agents.father_agent:Step: 30, Training loss: 13.618525505065918
INFO:agents.father_agent:Step: 35, Training loss: 10.013762474060059
INFO:agents.father_agent:Step: 40, Training loss: 11.689702987670898
INFO:agents.father_agent:Step: 45, Training loss: 10.760406494140625
INFO:agents.father_agent:Step: 50, Training loss: 11.168633460998535
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.885988235473633
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.85988235473633
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.40278625488281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.092310905456543
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.995838062807415
INFO:tools.evaluation_results_class:Counted Episodes = 7929
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.803163528442383
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.03163528442383
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.61557388305664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.269314765930176
INFO:tools.evaluation_results_class:Current Best Return = 3.803163528442383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.984182776801404
INFO:tools.evaluation_results_class:Counted Episodes = 7966
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.951167345046997
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.51167297363281
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.98514938354492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.2837061882019043
INFO:tools.evaluation_results_class:Current Best Return = 3.951167345046997
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.984182776801404
INFO:tools.evaluation_results_class:Counted Episodes = 7966
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.6893875593738947
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 34 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.843324899673462
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.433250427246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.98454666137695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.0502641201019287
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.94911838790932
INFO:tools.evaluation_results_class:Counted Episodes = 7940
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.549585342407227
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 9.737926483154297
INFO:agents.father_agent:Step: 10, Training loss: 9.422830581665039
INFO:agents.father_agent:Step: 15, Training loss: 11.892462730407715
INFO:agents.father_agent:Step: 20, Training loss: 9.540024757385254
INFO:agents.father_agent:Step: 25, Training loss: 11.331046104431152
INFO:agents.father_agent:Step: 30, Training loss: 10.58960247039795
INFO:agents.father_agent:Step: 35, Training loss: 10.200784683227539
INFO:agents.father_agent:Step: 40, Training loss: 18.09498405456543
INFO:agents.father_agent:Step: 45, Training loss: 11.681300163269043
INFO:agents.father_agent:Step: 50, Training loss: 10.00086498260498
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.864231824874878
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.64231872558594
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.21327209472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.2327003479003906
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.94911838790932
INFO:tools.evaluation_results_class:Counted Episodes = 7940
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.897564649581909
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.97564697265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.46178436279297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3337197303771973
INFO:tools.evaluation_results_class:Current Best Return = 3.897564649581909
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.984182776801404
INFO:tools.evaluation_results_class:Counted Episodes = 7966
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.78232479095459
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.82324981689453
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.47308349609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.195650577545166
INFO:tools.evaluation_results_class:Current Best Return = 3.78232479095459
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.984182776801404
INFO:tools.evaluation_results_class:Counted Episodes = 7966
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.6692528172044403
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 35 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.868553400039673
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.6855354309082
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.20996856689453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.909137010574341
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.90264150943396
INFO:tools.evaluation_results_class:Counted Episodes = 7950
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.532727241516113
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 11.529606819152832
INFO:agents.father_agent:Step: 10, Training loss: 12.840780258178711
INFO:agents.father_agent:Step: 15, Training loss: 11.935450553894043
INFO:agents.father_agent:Step: 20, Training loss: 9.654921531677246
INFO:agents.father_agent:Step: 25, Training loss: 9.048511505126953
INFO:agents.father_agent:Step: 30, Training loss: 8.905630111694336
INFO:agents.father_agent:Step: 35, Training loss: 11.891356468200684
INFO:agents.father_agent:Step: 40, Training loss: 11.047137260437012
INFO:agents.father_agent:Step: 45, Training loss: 12.226306915283203
INFO:agents.father_agent:Step: 50, Training loss: 11.148148536682129
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.890566110610962
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.90565872192383
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.463138580322266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8272695541381836
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.90264150943396
INFO:tools.evaluation_results_class:Counted Episodes = 7950
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.72344970703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.2344970703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.976531982421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.367783308029175
INFO:tools.evaluation_results_class:Current Best Return = 3.72344970703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.984182776801404
INFO:tools.evaluation_results_class:Counted Episodes = 7966
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6559126377105713
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.55912780761719
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.265872955322266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.341684341430664
INFO:tools.evaluation_results_class:Current Best Return = 3.6559126377105713
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.984182776801404
INFO:tools.evaluation_results_class:Counted Episodes = 7966
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.667075116090305
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 36 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8715827465057373
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.71582794189453
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.266597747802734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.001316785812378
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.80862804113369
INFO:tools.evaluation_results_class:Counted Episodes = 7974
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.335384368896484
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 11.7511625289917
INFO:agents.father_agent:Step: 10, Training loss: 14.102203369140625
INFO:agents.father_agent:Step: 15, Training loss: 11.083321571350098
INFO:agents.father_agent:Step: 20, Training loss: 9.113251686096191
INFO:agents.father_agent:Step: 25, Training loss: 13.543654441833496
INFO:agents.father_agent:Step: 30, Training loss: 12.8087797164917
INFO:agents.father_agent:Step: 35, Training loss: 13.515992164611816
INFO:agents.father_agent:Step: 40, Training loss: 16.43704605102539
INFO:agents.father_agent:Step: 45, Training loss: 14.330245971679688
INFO:agents.father_agent:Step: 50, Training loss: 10.602526664733887
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.90895414352417
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.089542388916016
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.57016372680664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.1075870990753174
INFO:tools.evaluation_results_class:Current Best Return = 6.939546585083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.80862804113369
INFO:tools.evaluation_results_class:Counted Episodes = 7974
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7833104133605957
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.83310317993164
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.465789794921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.157349109649658
INFO:tools.evaluation_results_class:Current Best Return = 3.7833104133605957
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.86575753784562
INFO:tools.evaluation_results_class:Counted Episodes = 7993
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.600400447845459
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.00400161743164
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.811553955078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.1867482662200928
INFO:tools.evaluation_results_class:Current Best Return = 3.600400447845459
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.86575753784562
INFO:tools.evaluation_results_class:Counted Episodes = 7993
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.667306179939101
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.346354484558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 85.46353912353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 67.78146362304688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3331634998321533
INFO:tools.evaluation_results_class:Current Best Return = 8.346354484558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 142.17578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 87.24267578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1149253845214844
INFO:tools.evaluation_results_class:Current Best Return = 14.017578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.680989265441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 108.80989837646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.31620788574219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.3787012100219727
INFO:tools.evaluation_results_class:Current Best Return = 10.680989265441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.193750381469727
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 103.9375
INFO:tools.evaluation_results_class:Average Discounted Reward = 79.93024444580078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.889023542404175
INFO:tools.evaluation_results_class:Current Best Return = 10.193750381469727
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.677734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 78.77734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.11422348022461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.110337495803833
INFO:tools.evaluation_results_class:Current Best Return = 7.677734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.742919921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.42919921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.27684020996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8115954399108887
INFO:tools.evaluation_results_class:Current Best Return = 3.742919921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.807373046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.07373046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.8319091796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.804936408996582
INFO:tools.evaluation_results_class:Current Best Return = 3.807373046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.821533203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.21533203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.96199035644531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7979836463928223
INFO:tools.evaluation_results_class:Current Best Return = 3.821533203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8115234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.115234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.86951446533203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8004136085510254
INFO:tools.evaluation_results_class:Current Best Return = 3.8115234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8115234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.115234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.87042236328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8204331398010254
INFO:tools.evaluation_results_class:Current Best Return = 3.8115234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.822509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.22509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.9677848815918
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8105380535125732
INFO:tools.evaluation_results_class:Current Best Return = 3.822509765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.79296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.9296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.728919982910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7691497802734375
INFO:tools.evaluation_results_class:Current Best Return = 3.79296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.794677734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.94677734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.74391174316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7364072799682617
INFO:tools.evaluation_results_class:Current Best Return = 3.794677734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.78857421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.8857421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.68928909301758
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.744361400604248
INFO:tools.evaluation_results_class:Current Best Return = 3.78857421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.794189453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.94189453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.737823486328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7650156021118164
INFO:tools.evaluation_results_class:Current Best Return = 3.794189453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.784912109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.84912109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.653350830078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.753298282623291
INFO:tools.evaluation_results_class:Current Best Return = 3.784912109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.83349609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.3349609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.07497787475586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8638782501220703
INFO:tools.evaluation_results_class:Current Best Return = 3.83349609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.844970703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.44970703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.176971435546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.882460117340088
INFO:tools.evaluation_results_class:Current Best Return = 3.844970703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.841064453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.41064453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.14265441894531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8656091690063477
INFO:tools.evaluation_results_class:Current Best Return = 3.841064453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.843994140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.43994140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.16847229003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8806915283203125
INFO:tools.evaluation_results_class:Current Best Return = 3.843994140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.83935546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.3935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.1273078918457
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8672595024108887
INFO:tools.evaluation_results_class:Current Best Return = 3.83935546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.017578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.80540466308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8684144020080566
INFO:tools.evaluation_results_class:Current Best Return = 3.8017578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.801025390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.01025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.79777908325195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.890829086303711
INFO:tools.evaluation_results_class:Current Best Return = 3.801025390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.80615234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.0615234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.84398651123047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.887228012084961
INFO:tools.evaluation_results_class:Current Best Return = 3.80615234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.805908203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.05908203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.841392517089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8937249183654785
INFO:tools.evaluation_results_class:Current Best Return = 3.805908203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.017578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.804351806640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8957581520080566
INFO:tools.evaluation_results_class:Current Best Return = 3.8017578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.80078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.0078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.776763916015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7566986083984375
INFO:tools.evaluation_results_class:Current Best Return = 3.80078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.96875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.74342727661133
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.754150390625
INFO:tools.evaluation_results_class:Current Best Return = 3.796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.803466796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.03466796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.79695129394531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7628884315490723
INFO:tools.evaluation_results_class:Current Best Return = 3.803466796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.795166015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.95166015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.72715759277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7439322471618652
INFO:tools.evaluation_results_class:Current Best Return = 3.795166015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.79833984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.9833984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.75541687011719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7606029510498047
INFO:tools.evaluation_results_class:Current Best Return = 3.79833984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.818359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.18359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.924537658691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8005027770996094
INFO:tools.evaluation_results_class:Current Best Return = 3.818359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.818359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.18359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.922821044921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8088035583496094
INFO:tools.evaluation_results_class:Current Best Return = 3.818359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.81787109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.1787109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.919559478759766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.786165237426758
INFO:tools.evaluation_results_class:Current Best Return = 3.81787109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.81103515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.1103515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.85977554321289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.803647994995117
INFO:tools.evaluation_results_class:Current Best Return = 3.81103515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.809326171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.09326171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.843101501464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7832236289978027
INFO:tools.evaluation_results_class:Current Best Return = 3.809326171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.779541015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.79541015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.57661437988281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7226386070251465
INFO:tools.evaluation_results_class:Current Best Return = 3.779541015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.78466796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.8466796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.618736267089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.77052640914917
INFO:tools.evaluation_results_class:Current Best Return = 3.78466796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.78515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.8515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.62432098388672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7404632568359375
INFO:tools.evaluation_results_class:Current Best Return = 3.78515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7900390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.900390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.66801452636719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.728865146636963
INFO:tools.evaluation_results_class:Current Best Return = 3.7900390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.782958984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.82958984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.60931396484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7456183433532715
INFO:tools.evaluation_results_class:Current Best Return = 3.782958984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.466145515441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 86.66146087646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 69.0348129272461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.1316661834716797
INFO:tools.evaluation_results_class:Current Best Return = 8.466145515441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.138671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 143.38671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 87.5206527709961
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.8694419860839844
INFO:tools.evaluation_results_class:Current Best Return = 14.138671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.802083015441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 110.02083587646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.56128692626953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.317599892616272
INFO:tools.evaluation_results_class:Current Best Return = 10.802083015441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.056249618530273
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 102.5625
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.43925476074219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.990586042404175
INFO:tools.evaluation_results_class:Current Best Return = 10.056249618530273
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.779296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.79296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.0925407409668
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.161576509475708
INFO:tools.evaluation_results_class:Current Best Return = 7.779296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6201171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.201171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.19239044189453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6677002906799316
INFO:tools.evaluation_results_class:Current Best Return = 3.6201171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6435546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.435546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.3984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.729391574859619
INFO:tools.evaluation_results_class:Current Best Return = 3.6435546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.64794921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.4794921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.438812255859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7388529777526855
INFO:tools.evaluation_results_class:Current Best Return = 3.64794921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.65673828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.5673828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.51349639892578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.746917247772217
INFO:tools.evaluation_results_class:Current Best Return = 3.65673828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6435546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.435546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.398040771484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.744040012359619
INFO:tools.evaluation_results_class:Current Best Return = 3.6435546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.669677734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.69677734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.631675720214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7441587448120117
INFO:tools.evaluation_results_class:Current Best Return = 3.669677734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.67138671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.7138671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.64313507080078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6351771354675293
INFO:tools.evaluation_results_class:Current Best Return = 3.67138671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6708984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.708984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.63649368286133
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6040940284729004
INFO:tools.evaluation_results_class:Current Best Return = 3.6708984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.67626953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.7626953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.689430236816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.669124126434326
INFO:tools.evaluation_results_class:Current Best Return = 3.67626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.66943359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.6943359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.62841033935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.658792495727539
INFO:tools.evaluation_results_class:Current Best Return = 3.66943359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.665283203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.65283203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.59395217895508
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6523690223693848
INFO:tools.evaluation_results_class:Current Best Return = 3.665283203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6376953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.376953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.33677673339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.737875461578369
INFO:tools.evaluation_results_class:Current Best Return = 3.6376953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.61865234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.1865234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.16273498535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.722249984741211
INFO:tools.evaluation_results_class:Current Best Return = 3.61865234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.62548828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.2548828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.222164154052734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.686400890350342
INFO:tools.evaluation_results_class:Current Best Return = 3.62548828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.628173828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.28173828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.243370056152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.692556142807007
INFO:tools.evaluation_results_class:Current Best Return = 3.628173828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.621337890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.21337890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.18828582763672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7474846839904785
INFO:tools.evaluation_results_class:Current Best Return = 3.621337890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.723876953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.23876953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.102638244628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6622817516326904
INFO:tools.evaluation_results_class:Current Best Return = 3.723876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.71826171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.1826171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.05329132080078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.670623302459717
INFO:tools.evaluation_results_class:Current Best Return = 3.71826171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.703857421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.03857421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.92631149291992
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.621039867401123
INFO:tools.evaluation_results_class:Current Best Return = 3.703857421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.705810546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.05810546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.93719482421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6759042739868164
INFO:tools.evaluation_results_class:Current Best Return = 3.705810546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.718505859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.18505859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.04816436767578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6710052490234375
INFO:tools.evaluation_results_class:Current Best Return = 3.718505859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6376953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.376953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.35033416748047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.682211399078369
INFO:tools.evaluation_results_class:Current Best Return = 3.6376953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.621826171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.21826171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.21365737915039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6951193809509277
INFO:tools.evaluation_results_class:Current Best Return = 3.621826171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6337890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.337890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.31682586669922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.691084384918213
INFO:tools.evaluation_results_class:Current Best Return = 3.6337890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.63427734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.3427734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.31982421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6538448333740234
INFO:tools.evaluation_results_class:Current Best Return = 3.63427734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.63134765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.3134765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.29548645019531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.699544906616211
INFO:tools.evaluation_results_class:Current Best Return = 3.63134765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.659912109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.59912109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.546302795410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.696596622467041
INFO:tools.evaluation_results_class:Current Best Return = 3.659912109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.67724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.7724609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.69512939453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7459278106689453
INFO:tools.evaluation_results_class:Current Best Return = 3.67724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.658203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.58203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.52177810668945
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7166709899902344
INFO:tools.evaluation_results_class:Current Best Return = 3.658203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6591796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.591796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.53150939941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.756887912750244
INFO:tools.evaluation_results_class:Current Best Return = 3.6591796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.674560546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.74560546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.67076873779297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.733689308166504
INFO:tools.evaluation_results_class:Current Best Return = 3.674560546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.652587890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.52587890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.47930908203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.697420597076416
INFO:tools.evaluation_results_class:Current Best Return = 3.652587890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6669921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.669921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.60634994506836
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6962342262268066
INFO:tools.evaluation_results_class:Current Best Return = 3.6669921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.66796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.6796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.619205474853516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7491302490234375
INFO:tools.evaluation_results_class:Current Best Return = 3.66796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.677490234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.77490234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.70038986206055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7258214950561523
INFO:tools.evaluation_results_class:Current Best Return = 3.677490234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.68017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.8017578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.731597900390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.751227855682373
INFO:tools.evaluation_results_class:Current Best Return = 3.68017578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 37 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
2025-08-20 16:49:07.405411: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-20 16:49:07.407408: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 16:49:07.437808: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-20 16:49:07.437849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-20 16:49:07.439102: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-20 16:49:07.444732: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 16:49:07.444915: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-20 16:49:07.980340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/obstacles-10-2/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/obstacles-10-2/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"penalty"}min=? [F ((x = 10) & (y = 10))] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 326 states and 829 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"rewardmodel_penalty"}min=? [F ((x = 10) & (y = 10))] 
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -393.2737121582031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -158.87246704101562
INFO:tools.evaluation_results_class:Average Discounted Reward = -104.58831024169922
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7325038880248833
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 77736.1953125
INFO:tools.evaluation_results_class:Current Best Return = -393.2737121582031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7325038880248833
INFO:tools.evaluation_results_class:Average Episode Length = 377.47122861586314
INFO:tools.evaluation_results_class:Counted Episodes = 643
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1503.9041748046875
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 3.7660858631134033
INFO:agents.father_agent:Step: 10, Training loss: 4.377209186553955
INFO:agents.father_agent:Step: 15, Training loss: 3.949558973312378
INFO:agents.father_agent:Step: 20, Training loss: 4.437240123748779
INFO:agents.father_agent:Step: 25, Training loss: 3.5152840614318848
INFO:agents.father_agent:Step: 30, Training loss: 4.620084762573242
INFO:agents.father_agent:Step: 35, Training loss: 4.060186386108398
INFO:agents.father_agent:Step: 40, Training loss: 3.9623072147369385
INFO:agents.father_agent:Step: 45, Training loss: 3.6708760261535645
INFO:agents.father_agent:Step: 50, Training loss: 3.5356600284576416
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 55, Training loss: 4.554797649383545
INFO:agents.father_agent:Step: 60, Training loss: 4.6571502685546875
INFO:agents.father_agent:Step: 65, Training loss: 3.7865920066833496
INFO:agents.father_agent:Step: 70, Training loss: 5.281468391418457
INFO:agents.father_agent:Step: 75, Training loss: 3.1983182430267334
INFO:agents.father_agent:Step: 80, Training loss: 5.80916166305542
INFO:agents.father_agent:Step: 85, Training loss: 5.547447681427002
INFO:agents.father_agent:Step: 90, Training loss: 4.385189533233643
INFO:agents.father_agent:Step: 95, Training loss: 4.539398193359375
INFO:agents.father_agent:Step: 100, Training loss: 5.51156759262085
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -86.68964385986328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.175048828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 124.09156799316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9995771670190274
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7368.7451171875
INFO:tools.evaluation_results_class:Current Best Return = -86.68964385986328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9995771670190274
INFO:tools.evaluation_results_class:Average Episode Length = 113.35644820295983
INFO:tools.evaluation_results_class:Counted Episodes = 2365
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 105, Training loss: 5.778448581695557
INFO:agents.father_agent:Step: 110, Training loss: 5.348533630371094
INFO:agents.father_agent:Step: 115, Training loss: 5.711094856262207
INFO:agents.father_agent:Step: 120, Training loss: 5.7249908447265625
INFO:agents.father_agent:Step: 125, Training loss: 5.10428524017334
INFO:agents.father_agent:Step: 130, Training loss: 5.416223526000977
INFO:agents.father_agent:Step: 135, Training loss: 6.139572620391846
INFO:agents.father_agent:Step: 140, Training loss: 5.716961860656738
INFO:agents.father_agent:Step: 145, Training loss: 6.600017547607422
INFO:agents.father_agent:Step: 150, Training loss: 5.797442436218262
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 4.69%
INFO:agents.father_agent:Step: 155, Training loss: 4.963231086730957
INFO:agents.father_agent:Step: 160, Training loss: 4.442915439605713
INFO:agents.father_agent:Step: 165, Training loss: 5.576053142547607
INFO:agents.father_agent:Step: 170, Training loss: 3.682980537414551
INFO:agents.father_agent:Step: 175, Training loss: 4.072858810424805
INFO:agents.father_agent:Step: 180, Training loss: 6.157683372497559
INFO:agents.father_agent:Step: 185, Training loss: 4.4211344718933105
INFO:agents.father_agent:Step: 190, Training loss: 5.0894694328308105
INFO:agents.father_agent:Step: 195, Training loss: 4.948641300201416
INFO:agents.father_agent:Step: 200, Training loss: 4.654975891113281
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.72483825683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.13009643554688
INFO:tools.evaluation_results_class:Average Discounted Reward = 130.972412109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9995466908431551
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6151.822265625
INFO:tools.evaluation_results_class:Current Best Return = -76.72483825683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9995771670190274
INFO:tools.evaluation_results_class:Average Episode Length = 118.92520398912058
INFO:tools.evaluation_results_class:Counted Episodes = 2206
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -74.60855865478516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.1031494140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 132.6909637451172
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9990990990990991
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5105.5986328125
INFO:tools.evaluation_results_class:Current Best Return = -74.60855865478516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9995771670190274
INFO:tools.evaluation_results_class:Average Episode Length = 118.27252252252252
INFO:tools.evaluation_results_class:Counted Episodes = 2220
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -88.71925354003906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 231.0218505859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 122.83993530273438
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9991909385113269
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8534.578125
INFO:tools.evaluation_results_class:Current Best Return = -88.71925354003906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9991909385113269
INFO:tools.evaluation_results_class:Average Episode Length = 116.75485436893204
INFO:tools.evaluation_results_class:Counted Episodes = 2472
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.45287322998047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.1554412841797
INFO:tools.evaluation_results_class:Average Discounted Reward = 132.02407836914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9987760097919217
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4931.0556640625
INFO:tools.evaluation_results_class:Current Best Return = -75.45287322998047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9987760097919217
INFO:tools.evaluation_results_class:Average Episode Length = 117.30069359445125
INFO:tools.evaluation_results_class:Counted Episodes = 2451
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 125.92563085264055
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -78.82477569580078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.1752166748047
INFO:tools.evaluation_results_class:Average Discounted Reward = 127.24951934814453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5001.99951171875
INFO:tools.evaluation_results_class:Current Best Return = -78.82477569580078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.57522123893806
INFO:tools.evaluation_results_class:Counted Episodes = 1130
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -96.48233032226562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 223.2349853515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 113.19332122802734
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9991166077738516
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9061.30078125
INFO:tools.evaluation_results_class:Current Best Return = -96.48233032226562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9991166077738516
INFO:tools.evaluation_results_class:Average Episode Length = 116.75618374558304
INFO:tools.evaluation_results_class:Counted Episodes = 1132
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.35818481445312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.64181518554688
INFO:tools.evaluation_results_class:Average Discounted Reward = 140.96763610839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4030.0615234375
INFO:tools.evaluation_results_class:Current Best Return = -72.35818481445312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.46677471636953
INFO:tools.evaluation_results_class:Counted Episodes = 1234
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.15987396240234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.84011840820312
INFO:tools.evaluation_results_class:Average Discounted Reward = 144.97328186035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3517.530517578125
INFO:tools.evaluation_results_class:Current Best Return = -69.15987396240234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.24137931034483
INFO:tools.evaluation_results_class:Counted Episodes = 1276
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.09051513671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.90948486328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 135.1727294921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3889.042724609375
INFO:tools.evaluation_results_class:Current Best Return = -73.09051513671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 113.05086206896551
INFO:tools.evaluation_results_class:Counted Episodes = 1160
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -123.06912231445312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 196.93087768554688
INFO:tools.evaluation_results_class:Average Discounted Reward = 91.55113983154297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15739.7197265625
INFO:tools.evaluation_results_class:Current Best Return = -123.06912231445312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 120.78248847926267
INFO:tools.evaluation_results_class:Counted Episodes = 1085
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.518550872802734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.4814453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 140.3347625732422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3292.364501953125
INFO:tools.evaluation_results_class:Current Best Return = -62.518550872802734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 117.15989399293287
INFO:tools.evaluation_results_class:Counted Episodes = 1132
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -90.19055938720703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.52972412109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 120.96610260009766
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9991258741258742
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7991.06640625
INFO:tools.evaluation_results_class:Current Best Return = -90.19055938720703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9991258741258742
INFO:tools.evaluation_results_class:Average Episode Length = 116.46503496503496
INFO:tools.evaluation_results_class:Counted Episodes = 1144
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.54508972167969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.4549102783203
INFO:tools.evaluation_results_class:Average Discounted Reward = 143.03440856933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3817.815185546875
INFO:tools.evaluation_results_class:Current Best Return = -69.54508972167969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.54828411811653
INFO:tools.evaluation_results_class:Counted Episodes = 1253
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -65.92568969726562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.07431030273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 148.17193603515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3317.841064453125
INFO:tools.evaluation_results_class:Current Best Return = -65.92568969726562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.55889328063242
INFO:tools.evaluation_results_class:Counted Episodes = 1265
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.22022247314453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.77976989746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 141.2125701904297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2638.761474609375
INFO:tools.evaluation_results_class:Current Best Return = -64.22022247314453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.06341045415596
INFO:tools.evaluation_results_class:Counted Episodes = 1167
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.24118041992188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 200.75881958007812
INFO:tools.evaluation_results_class:Average Discounted Reward = 94.20057678222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16062.1064453125
INFO:tools.evaluation_results_class:Current Best Return = -119.24118041992188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.5443279313632
INFO:tools.evaluation_results_class:Counted Episodes = 1049
INFO:robust_rl.robust_rl_trainer:Iteration 2 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -88.87125396728516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 231.1287384033203
INFO:tools.evaluation_results_class:Average Discounted Reward = 118.9510498046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8446.9775390625
INFO:tools.evaluation_results_class:Current Best Return = -74.60855865478516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 123.09878277153558
INFO:tools.evaluation_results_class:Counted Episodes = 2136
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.078796863555908
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 4.69%
INFO:agents.father_agent:Step: 5, Training loss: 6.004781246185303
INFO:agents.father_agent:Step: 10, Training loss: 4.771285533905029
INFO:agents.father_agent:Step: 15, Training loss: 6.899343013763428
INFO:agents.father_agent:Step: 20, Training loss: 4.157624244689941
INFO:agents.father_agent:Step: 25, Training loss: 5.9354658126831055
INFO:agents.father_agent:Step: 30, Training loss: 6.068286418914795
INFO:agents.father_agent:Step: 35, Training loss: 5.158822536468506
INFO:agents.father_agent:Step: 40, Training loss: 4.935036659240723
INFO:agents.father_agent:Step: 45, Training loss: 5.8212080001831055
INFO:agents.father_agent:Step: 50, Training loss: 5.744472503662109
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -68.43793487548828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.56207275390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 159.47752380371094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5379.0390625
INFO:tools.evaluation_results_class:Current Best Return = -68.43793487548828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.6768039811999
INFO:tools.evaluation_results_class:Counted Episodes = 3617
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.55957794189453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.44041442871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 137.44937133789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7396.23388671875
INFO:tools.evaluation_results_class:Current Best Return = -82.55957794189453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.8805211312361
INFO:tools.evaluation_results_class:Counted Episodes = 3147
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.17606353759766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.82394409179688
INFO:tools.evaluation_results_class:Average Discounted Reward = 159.6692657470703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5112.58935546875
INFO:tools.evaluation_results_class:Current Best Return = -68.17606353759766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.70746108427268
INFO:tools.evaluation_results_class:Counted Episodes = 3726
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 89.57406221192868
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 3 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.36083984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.63916015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 155.6860809326172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5925.3115234375
INFO:tools.evaluation_results_class:Current Best Return = -68.43793487548828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.99524475524476
INFO:tools.evaluation_results_class:Counted Episodes = 3575
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.94808292388916
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 5.074061870574951
INFO:agents.father_agent:Step: 10, Training loss: 4.989691257476807
INFO:agents.father_agent:Step: 15, Training loss: 5.046444416046143
INFO:agents.father_agent:Step: 20, Training loss: 4.715673923492432
INFO:agents.father_agent:Step: 25, Training loss: 4.348928928375244
INFO:agents.father_agent:Step: 30, Training loss: 4.423214435577393
INFO:agents.father_agent:Step: 35, Training loss: 4.198939323425293
INFO:agents.father_agent:Step: 40, Training loss: 3.860957622528076
INFO:agents.father_agent:Step: 45, Training loss: 3.9055590629577637
INFO:agents.father_agent:Step: 50, Training loss: 3.250270366668701
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -56.97636413574219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.02362060546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.9310760498047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3652.099609375
INFO:tools.evaluation_results_class:Current Best Return = -56.97636413574219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.55542424958638
INFO:tools.evaluation_results_class:Counted Episodes = 4231
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.81352996826172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.18646240234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 151.03627014160156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5298.6357421875
INFO:tools.evaluation_results_class:Current Best Return = -73.81352996826172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 82.65585686329327
INFO:tools.evaluation_results_class:Counted Episodes = 3577
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.00846481323242
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.9915466308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.08348083496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3604.2724609375
INFO:tools.evaluation_results_class:Current Best Return = -58.00846481323242
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.43697094486387
INFO:tools.evaluation_results_class:Counted Episodes = 4371
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 71.16463801735965
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 4 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.06757736206055
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.93243408203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.6911163330078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3443.95556640625
INFO:tools.evaluation_results_class:Current Best Return = -56.97636413574219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.57522957381681
INFO:tools.evaluation_results_class:Counted Episodes = 4247
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.023878812789917
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 3.219731092453003
INFO:agents.father_agent:Step: 10, Training loss: 3.1521880626678467
INFO:agents.father_agent:Step: 15, Training loss: 2.6407470703125
INFO:agents.father_agent:Step: 20, Training loss: 2.3412413597106934
INFO:agents.father_agent:Step: 25, Training loss: 2.386458396911621
INFO:agents.father_agent:Step: 30, Training loss: 1.721901774406433
INFO:agents.father_agent:Step: 35, Training loss: 1.8025174140930176
INFO:agents.father_agent:Step: 40, Training loss: 1.4543629884719849
INFO:agents.father_agent:Step: 45, Training loss: 1.2417079210281372
INFO:agents.father_agent:Step: 50, Training loss: 1.0018460750579834
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -52.159969329833984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.84002685546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.1223602294922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3058.542724609375
INFO:tools.evaluation_results_class:Current Best Return = -52.159969329833984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.903721533049435
INFO:tools.evaluation_results_class:Counted Episodes = 5401
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.10264587402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.8973388671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.31228637695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3546.36767578125
INFO:tools.evaluation_results_class:Current Best Return = -58.10264587402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.24538893344025
INFO:tools.evaluation_results_class:Counted Episodes = 4988
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.60417556762695
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.39581298828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.54393005371094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3097.531494140625
INFO:tools.evaluation_results_class:Current Best Return = -53.60417556762695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.11218784942229
INFO:tools.evaluation_results_class:Counted Episodes = 5366
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 69.95762972564874
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 5 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.63386154174805
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.36614990234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.6836700439453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3210.416015625
INFO:tools.evaluation_results_class:Current Best Return = -52.159969329833984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.08539534883721
INFO:tools.evaluation_results_class:Counted Episodes = 5375
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.9539048671722412
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 1.02039635181427
INFO:agents.father_agent:Step: 10, Training loss: 0.8427858352661133
INFO:agents.father_agent:Step: 15, Training loss: 0.9598965644836426
INFO:agents.father_agent:Step: 20, Training loss: 1.1626837253570557
INFO:agents.father_agent:Step: 25, Training loss: 0.9109408855438232
INFO:agents.father_agent:Step: 30, Training loss: 0.903562605381012
INFO:agents.father_agent:Step: 35, Training loss: 0.8620951771736145
INFO:agents.father_agent:Step: 40, Training loss: 0.75626540184021
INFO:agents.father_agent:Step: 45, Training loss: 0.7801928520202637
INFO:agents.father_agent:Step: 50, Training loss: 0.7629469633102417
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -60.328731536865234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.6712646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.5890350341797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3355.630859375
INFO:tools.evaluation_results_class:Current Best Return = -52.159969329833984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.81212862635442
INFO:tools.evaluation_results_class:Counted Episodes = 5722
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.372093200683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.6278991699219
INFO:tools.evaluation_results_class:Average Discounted Reward = 188.1817626953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3559.688232421875
INFO:tools.evaluation_results_class:Current Best Return = -62.372093200683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.32323778619073
INFO:tools.evaluation_results_class:Counted Episodes = 5547
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.0561637878418
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.94384765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.48223876953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3608.294921875
INFO:tools.evaluation_results_class:Current Best Return = -63.0561637878418
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.29467845380482
INFO:tools.evaluation_results_class:Counted Episodes = 5769
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 85.41096784291474
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 6 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -61.76408386230469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.23590087890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.48648071289062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3444.16162109375
INFO:tools.evaluation_results_class:Current Best Return = -52.159969329833984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.546195181140575
INFO:tools.evaluation_results_class:Counted Episodes = 5769
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.7887480854988098
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 0.7060583829879761
INFO:agents.father_agent:Step: 10, Training loss: 0.845796525478363
INFO:agents.father_agent:Step: 15, Training loss: 0.8982289433479309
INFO:agents.father_agent:Step: 20, Training loss: 0.7964140772819519
INFO:agents.father_agent:Step: 25, Training loss: 0.8248129487037659
INFO:agents.father_agent:Step: 30, Training loss: 0.8016839623451233
INFO:agents.father_agent:Step: 35, Training loss: 0.8467890620231628
INFO:agents.father_agent:Step: 40, Training loss: 0.8468490839004517
INFO:agents.father_agent:Step: 45, Training loss: 0.68515545129776
INFO:agents.father_agent:Step: 50, Training loss: 0.7309263944625854
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -49.49110412597656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.5088806152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.64361572265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2538.856201171875
INFO:tools.evaluation_results_class:Current Best Return = -49.49110412597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.79630275549355
INFO:tools.evaluation_results_class:Counted Episodes = 5734
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.55609130859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.44390869140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.80996704101562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2872.281005859375
INFO:tools.evaluation_results_class:Current Best Return = -53.55609130859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.87904813025586
INFO:tools.evaluation_results_class:Counted Episodes = 5589
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.20365524291992
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.7963562011719
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.5054473876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3186.205322265625
INFO:tools.evaluation_results_class:Current Best Return = -57.20365524291992
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.56173062997828
INFO:tools.evaluation_results_class:Counted Episodes = 5524
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 75.3575638245608
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -33.06167221069336
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 286.9383239746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.54786682128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1540.071044921875
INFO:tools.evaluation_results_class:Current Best Return = -33.06167221069336
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.14831130690162
INFO:tools.evaluation_results_class:Counted Episodes = 2724
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.04676055908203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.9532470703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.25289916992188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2491.731689453125
INFO:tools.evaluation_results_class:Current Best Return = -50.04676055908203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.49521354933726
INFO:tools.evaluation_results_class:Counted Episodes = 2716
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.95613098144531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.04388427734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.5854034423828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3252.531494140625
INFO:tools.evaluation_results_class:Current Best Return = -54.95613098144531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.9888529306005
INFO:tools.evaluation_results_class:Counted Episodes = 2781
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.163185119628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.8368225097656
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.829345703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2945.960205078125
INFO:tools.evaluation_results_class:Current Best Return = -50.163185119628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.10770893371758
INFO:tools.evaluation_results_class:Counted Episodes = 2776
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.03199005126953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.968017578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.12704467773438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2282.715087890625
INFO:tools.evaluation_results_class:Current Best Return = -48.03199005126953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.58087968011632
INFO:tools.evaluation_results_class:Counted Episodes = 2751
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.23716735839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.7628173828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.47421264648438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2402.280517578125
INFO:tools.evaluation_results_class:Current Best Return = -49.23716735839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.233046084675905
INFO:tools.evaluation_results_class:Counted Episodes = 2669
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.60187911987305
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.39813232421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.2868194580078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2546.077880859375
INFO:tools.evaluation_results_class:Current Best Return = -50.60187911987305
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.295864661654136
INFO:tools.evaluation_results_class:Counted Episodes = 2660
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.87147521972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.1285095214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.04136657714844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3173.303466796875
INFO:tools.evaluation_results_class:Current Best Return = -59.87147521972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.95093372391066
INFO:tools.evaluation_results_class:Counted Episodes = 2731
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.565486907958984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.43450927734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.62632751464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3247.856201171875
INFO:tools.evaluation_results_class:Current Best Return = -60.565486907958984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.751550529004014
INFO:tools.evaluation_results_class:Counted Episodes = 2741
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.94363784790039
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.0563659667969
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.5403289794922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3600.970947265625
INFO:tools.evaluation_results_class:Current Best Return = -63.94363784790039
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.50145454545454
INFO:tools.evaluation_results_class:Counted Episodes = 2750
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -61.89784622192383
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.1021423339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.17034912109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3749.785888671875
INFO:tools.evaluation_results_class:Current Best Return = -61.89784622192383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.69901495804451
INFO:tools.evaluation_results_class:Counted Episodes = 2741
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -35.0971794128418
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.90283203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.2073516845703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1527.1595458984375
INFO:tools.evaluation_results_class:Current Best Return = -35.0971794128418
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.67878338278932
INFO:tools.evaluation_results_class:Counted Episodes = 2696
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -45.793846130371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.2061462402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.74188232421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2156.471923828125
INFO:tools.evaluation_results_class:Current Best Return = -45.793846130371094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.75157582499073
INFO:tools.evaluation_results_class:Counted Episodes = 2697
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.493141174316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.5068664550781
INFO:tools.evaluation_results_class:Average Discounted Reward = 188.50686645507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3757.064697265625
INFO:tools.evaluation_results_class:Current Best Return = -63.493141174316406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.14151624548737
INFO:tools.evaluation_results_class:Counted Episodes = 2770
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.056312561035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.9436950683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.78665161132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3898.49072265625
INFO:tools.evaluation_results_class:Current Best Return = -60.056312561035156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.072812051649926
INFO:tools.evaluation_results_class:Counted Episodes = 2788
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -41.90852355957031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.0914611816406
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.0483856201172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1814.5513916015625
INFO:tools.evaluation_results_class:Current Best Return = -41.90852355957031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.94035858031467
INFO:tools.evaluation_results_class:Counted Episodes = 2733
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.48447036743164
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.5155334472656
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.0243377685547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1936.9302978515625
INFO:tools.evaluation_results_class:Current Best Return = -43.48447036743164
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.6064393939394
INFO:tools.evaluation_results_class:Counted Episodes = 2640
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.716434478759766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.2835693359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.00399780273438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1931.7647705078125
INFO:tools.evaluation_results_class:Current Best Return = -43.716434478759766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.4761188416698
INFO:tools.evaluation_results_class:Counted Episodes = 2659
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.54893493652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.45106506347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.9135284423828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3867.72998046875
INFO:tools.evaluation_results_class:Current Best Return = -73.54893493652344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.248826291079816
INFO:tools.evaluation_results_class:Counted Episodes = 2769
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.96441650390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.03558349609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.24221801757812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3915.509765625
INFO:tools.evaluation_results_class:Current Best Return = -74.96441650390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.6477850399419
INFO:tools.evaluation_results_class:Counted Episodes = 2754
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.65860748291016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.3413848876953
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.97418212890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4537.5595703125
INFO:tools.evaluation_results_class:Current Best Return = -74.65860748291016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.98095238095238
INFO:tools.evaluation_results_class:Counted Episodes = 2730
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.12921905517578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.8707733154297
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.23202514648438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4344.61328125
INFO:tools.evaluation_results_class:Current Best Return = -76.12921905517578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.39274047186933
INFO:tools.evaluation_results_class:Counted Episodes = 2755
INFO:robust_rl.robust_rl_trainer:Iteration 7 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.1641960144043
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.8358154296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.26300048828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2646.91455078125
INFO:tools.evaluation_results_class:Current Best Return = -49.49110412597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.687838073634616
INFO:tools.evaluation_results_class:Counted Episodes = 5731
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.6327174305915833
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 0.719000518321991
INFO:agents.father_agent:Step: 10, Training loss: 0.6213974356651306
INFO:agents.father_agent:Step: 15, Training loss: 0.5991211533546448
INFO:agents.father_agent:Step: 20, Training loss: 0.6156961917877197
INFO:agents.father_agent:Step: 25, Training loss: 0.7098395824432373
INFO:agents.father_agent:Step: 30, Training loss: 0.5941786170005798
INFO:agents.father_agent:Step: 35, Training loss: 0.5378088355064392
INFO:agents.father_agent:Step: 40, Training loss: 0.6098272204399109
INFO:agents.father_agent:Step: 45, Training loss: 0.5644894242286682
INFO:agents.father_agent:Step: 50, Training loss: 0.5522558689117432
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -52.710655212402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.2893371582031
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.65911865234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2506.364501953125
INFO:tools.evaluation_results_class:Current Best Return = -49.49110412597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.37271028037383
INFO:tools.evaluation_results_class:Counted Episodes = 5350
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.918418884277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.0815734863281
INFO:tools.evaluation_results_class:Average Discounted Reward = 188.8311767578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3037.095947265625
INFO:tools.evaluation_results_class:Current Best Return = -57.918418884277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.4980894153611
INFO:tools.evaluation_results_class:Counted Episodes = 5234
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.86927032470703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.1307373046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.81434631347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3622.61962890625
INFO:tools.evaluation_results_class:Current Best Return = -66.86927032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.35761460761461
INFO:tools.evaluation_results_class:Counted Episodes = 5148
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 89.30246869951631
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 8 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.31903076171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.68096923828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.0113983154297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2634.285400390625
INFO:tools.evaluation_results_class:Current Best Return = -49.49110412597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.46231343283582
INFO:tools.evaluation_results_class:Counted Episodes = 5360
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.5174193382263184
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 0.6262542605400085
INFO:agents.father_agent:Step: 10, Training loss: 0.7626241445541382
INFO:agents.father_agent:Step: 15, Training loss: 0.7037907242774963
INFO:agents.father_agent:Step: 20, Training loss: 0.7378193736076355
INFO:agents.father_agent:Step: 25, Training loss: 0.7532130479812622
INFO:agents.father_agent:Step: 30, Training loss: 0.9351863861083984
INFO:agents.father_agent:Step: 35, Training loss: 0.8785939812660217
INFO:agents.father_agent:Step: 40, Training loss: 0.9010456204414368
INFO:agents.father_agent:Step: 45, Training loss: 0.8299438953399658
INFO:agents.father_agent:Step: 50, Training loss: 0.7497539520263672
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -47.29558181762695
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.70440673828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.5134735107422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2226.774658203125
INFO:tools.evaluation_results_class:Current Best Return = -47.29558181762695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.24378453038674
INFO:tools.evaluation_results_class:Counted Episodes = 5792
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.518280029296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.4817199707031
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.01939392089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2457.958740234375
INFO:tools.evaluation_results_class:Current Best Return = -49.518280029296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.5264465743699
INFO:tools.evaluation_results_class:Counted Episodes = 5634
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.4930534362793
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.5069580078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.0569305419922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3451.9658203125
INFO:tools.evaluation_results_class:Current Best Return = -57.4930534362793
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.700759681304426
INFO:tools.evaluation_results_class:Counted Episodes = 5397
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 72.12458941998831
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 9 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.44363784790039
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.5563659667969
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.35177612304688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2268.650390625
INFO:tools.evaluation_results_class:Current Best Return = -47.29558181762695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.359134199134196
INFO:tools.evaluation_results_class:Counted Episodes = 5775
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.6651901602745056
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 0.5358887910842896
INFO:agents.father_agent:Step: 10, Training loss: 0.5244321227073669
INFO:agents.father_agent:Step: 15, Training loss: 0.6777076721191406
INFO:agents.father_agent:Step: 20, Training loss: 0.5082359910011292
INFO:agents.father_agent:Step: 25, Training loss: 0.41792842745780945
INFO:agents.father_agent:Step: 30, Training loss: 0.4010908603668213
INFO:agents.father_agent:Step: 35, Training loss: 0.4438234269618988
INFO:agents.father_agent:Step: 40, Training loss: 0.5238206386566162
INFO:agents.father_agent:Step: 45, Training loss: 0.551973819732666
INFO:agents.father_agent:Step: 50, Training loss: 0.5082786679267883
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -49.67335510253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.3266296386719
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.4209747314453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2251.9921875
INFO:tools.evaluation_results_class:Current Best Return = -47.29558181762695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.99910410320731
INFO:tools.evaluation_results_class:Counted Episodes = 5581
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.96644592285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.0335693359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.82513427734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2519.98095703125
INFO:tools.evaluation_results_class:Current Best Return = -51.96644592285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.402128811113116
INFO:tools.evaluation_results_class:Counted Episodes = 5543
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.8498764038086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.15011596679688
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.29840087890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3788.600830078125
INFO:tools.evaluation_results_class:Current Best Return = -64.8498764038086
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.80242011722443
INFO:tools.evaluation_results_class:Counted Episodes = 5289
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 79.79443924556283
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 10 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.84248352050781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.15753173828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.32247924804688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2332.455810546875
INFO:tools.evaluation_results_class:Current Best Return = -47.29558181762695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.00929733595566
INFO:tools.evaluation_results_class:Counted Episodes = 5593
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.5006062388420105
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 0.4402282238006592
INFO:agents.father_agent:Step: 10, Training loss: 0.4406912326812744
INFO:agents.father_agent:Step: 15, Training loss: 0.4788762629032135
INFO:agents.father_agent:Step: 20, Training loss: 0.5341313481330872
INFO:agents.father_agent:Step: 25, Training loss: 0.4762406051158905
INFO:agents.father_agent:Step: 30, Training loss: 0.5775080919265747
INFO:agents.father_agent:Step: 35, Training loss: 0.49452054500579834
INFO:agents.father_agent:Step: 40, Training loss: 0.5388474464416504
INFO:agents.father_agent:Step: 45, Training loss: 0.5729962587356567
INFO:agents.father_agent:Step: 50, Training loss: 0.4923686683177948
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -49.3437385559082
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.65625
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.259033203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2271.3095703125
INFO:tools.evaluation_results_class:Current Best Return = -47.29558181762695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.23192239858906
INFO:tools.evaluation_results_class:Counted Episodes = 5670
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.851871490478516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.14813232421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.2353973388672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2357.990966796875
INFO:tools.evaluation_results_class:Current Best Return = -49.851871490478516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.70811472542847
INFO:tools.evaluation_results_class:Counted Episodes = 5718
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.79692840576172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.2030792236328
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.9784393310547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4215.08154296875
INFO:tools.evaluation_results_class:Current Best Return = -74.79692840576172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.177094767654594
INFO:tools.evaluation_results_class:Counted Episodes = 5466
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 91.33587547921027
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 11 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.55950164794922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.4405059814453
INFO:tools.evaluation_results_class:Average Discounted Reward = 135.62051391601562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3837.46923828125
INFO:tools.evaluation_results_class:Current Best Return = -47.29558181762695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.11189993238675
INFO:tools.evaluation_results_class:Counted Episodes = 2958
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.0789602994918823
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Step: 5, Training loss: 1.223170518875122
INFO:agents.father_agent:Step: 10, Training loss: 1.2336246967315674
INFO:agents.father_agent:Step: 15, Training loss: 1.1119459867477417
INFO:agents.father_agent:Step: 20, Training loss: 1.1733828783035278
INFO:agents.father_agent:Step: 25, Training loss: 1.0191401243209839
INFO:agents.father_agent:Step: 30, Training loss: 0.7259342670440674
INFO:agents.father_agent:Step: 35, Training loss: 0.7431893348693848
INFO:agents.father_agent:Step: 40, Training loss: 0.688649594783783
INFO:agents.father_agent:Step: 45, Training loss: 0.7149778604507446
INFO:agents.father_agent:Step: 50, Training loss: 0.5578823089599609
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Step: 55, Training loss: 0.6172873973846436
INFO:agents.father_agent:Step: 60, Training loss: 0.5887354612350464
INFO:agents.father_agent:Step: 65, Training loss: 0.5597726702690125
INFO:agents.father_agent:Step: 70, Training loss: 0.6665380001068115
INFO:agents.father_agent:Step: 75, Training loss: 0.6660518646240234
INFO:agents.father_agent:Step: 80, Training loss: 0.694037914276123
INFO:agents.father_agent:Step: 85, Training loss: 0.6295615434646606
INFO:agents.father_agent:Step: 90, Training loss: 0.5428694486618042
INFO:agents.father_agent:Step: 95, Training loss: 0.48013758659362793
INFO:agents.father_agent:Step: 100, Training loss: 0.5008645057678223
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.370174407958984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.62982177734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.29832458496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2446.342529296875
INFO:tools.evaluation_results_class:Current Best Return = -47.29558181762695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.94285223957439
INFO:tools.evaluation_results_class:Counted Episodes = 5827
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -51.731895446777344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.2680969238281
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.50094604492188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2343.35595703125
INFO:tools.evaluation_results_class:Current Best Return = -47.29558181762695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.2798275862069
INFO:tools.evaluation_results_class:Counted Episodes = 5800
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.101890563964844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.8981018066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.47222900390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2594.08203125
INFO:tools.evaluation_results_class:Current Best Return = -54.101890563964844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.146768324380524
INFO:tools.evaluation_results_class:Counted Episodes = 5771
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -65.39369201660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.60630798339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.60899353027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3845.26513671875
INFO:tools.evaluation_results_class:Current Best Return = -65.39369201660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.1225317348378
INFO:tools.evaluation_results_class:Counted Episodes = 5672
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 79.454238990414
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -31.536165237426758
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 288.4638366699219
INFO:tools.evaluation_results_class:Average Discounted Reward = 218.49740600585938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1295.25146484375
INFO:tools.evaluation_results_class:Current Best Return = -31.536165237426758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.91046348314607
INFO:tools.evaluation_results_class:Counted Episodes = 2848
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.16160202026367
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.8384094238281
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.3377685546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2503.855224609375
INFO:tools.evaluation_results_class:Current Best Return = -56.16160202026367
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.62745799070433
INFO:tools.evaluation_results_class:Counted Episodes = 2797
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.65742874145508
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.3425598144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.29534912109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2638.02294921875
INFO:tools.evaluation_results_class:Current Best Return = -55.65742874145508
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.370280568063734
INFO:tools.evaluation_results_class:Counted Episodes = 2887
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.1159553527832
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.884033203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.52699279785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2481.514892578125
INFO:tools.evaluation_results_class:Current Best Return = -49.1159553527832
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.03599861543787
INFO:tools.evaluation_results_class:Counted Episodes = 2889
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.79048538208008
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.2095031738281
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.51597595214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2446.095947265625
INFO:tools.evaluation_results_class:Current Best Return = -54.79048538208008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.73941937740469
INFO:tools.evaluation_results_class:Counted Episodes = 2859
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.881988525390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.1180114746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.17701721191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2263.81884765625
INFO:tools.evaluation_results_class:Current Best Return = -49.881988525390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.454974582425564
INFO:tools.evaluation_results_class:Counted Episodes = 2754
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.24061965942383
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.7593688964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.01979064941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2250.164794921875
INFO:tools.evaluation_results_class:Current Best Return = -50.24061965942383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.30916305916306
INFO:tools.evaluation_results_class:Counted Episodes = 2772
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.73671340942383
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.2632751464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.28260803222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2520.5791015625
INFO:tools.evaluation_results_class:Current Best Return = -52.73671340942383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.84019711369236
INFO:tools.evaluation_results_class:Counted Episodes = 2841
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.96287155151367
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.0371398925781
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.3754425048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2542.210205078125
INFO:tools.evaluation_results_class:Current Best Return = -53.96287155151367
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.72714535901927
INFO:tools.evaluation_results_class:Counted Episodes = 2855
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.0
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.0
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.51992797851562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2803.702880859375
INFO:tools.evaluation_results_class:Current Best Return = -59.0
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.830354013319315
INFO:tools.evaluation_results_class:Counted Episodes = 2853
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.674072265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.325927734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.76353454589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2777.95458984375
INFO:tools.evaluation_results_class:Current Best Return = -60.674072265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.04303350970017
INFO:tools.evaluation_results_class:Counted Episodes = 2835
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -47.18686294555664
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.8131408691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.06410217285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2509.490966796875
INFO:tools.evaluation_results_class:Current Best Return = -47.18686294555664
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.8510713031261
INFO:tools.evaluation_results_class:Counted Episodes = 2847
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.593257904052734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.40673828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.9767303466797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2856.581298828125
INFO:tools.evaluation_results_class:Current Best Return = -60.593257904052734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.922725676150336
INFO:tools.evaluation_results_class:Counted Episodes = 2847
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.292724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.707275390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.07847595214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2756.459716796875
INFO:tools.evaluation_results_class:Current Best Return = -60.292724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.139477401129945
INFO:tools.evaluation_results_class:Counted Episodes = 2832
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -45.8428840637207
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.1571044921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.28871154785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2394.578857421875
INFO:tools.evaluation_results_class:Current Best Return = -45.8428840637207
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.869244288224955
INFO:tools.evaluation_results_class:Counted Episodes = 2845
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.711822509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.2881774902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.13784790039062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2936.6943359375
INFO:tools.evaluation_results_class:Current Best Return = -62.711822509765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.929275158339195
INFO:tools.evaluation_results_class:Counted Episodes = 2842
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -37.215267181396484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.78472900390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.9529266357422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1696.7476806640625
INFO:tools.evaluation_results_class:Current Best Return = -37.215267181396484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.35347322720695
INFO:tools.evaluation_results_class:Counted Episodes = 2764
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -47.118717193603516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.88128662109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.8504180908203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2074.568603515625
INFO:tools.evaluation_results_class:Current Best Return = -47.118717193603516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.73707210487982
INFO:tools.evaluation_results_class:Counted Episodes = 2746
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.63530731201172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.3647003173828
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.25233459472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3992.13037109375
INFO:tools.evaluation_results_class:Current Best Return = -67.63530731201172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.02713178294574
INFO:tools.evaluation_results_class:Counted Episodes = 2838
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.06210327148438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.93789672851562
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.4491729736328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4058.825439453125
INFO:tools.evaluation_results_class:Current Best Return = -64.06210327148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.16713981547196
INFO:tools.evaluation_results_class:Counted Episodes = 2818
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -45.38591384887695
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.61407470703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.19764709472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2043.861328125
INFO:tools.evaluation_results_class:Current Best Return = -45.38591384887695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.924182536830756
INFO:tools.evaluation_results_class:Counted Episodes = 2783
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -40.6519660949707
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.3480224609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.2411346435547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1702.8453369140625
INFO:tools.evaluation_results_class:Current Best Return = -40.6519660949707
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.15839764792356
INFO:tools.evaluation_results_class:Counted Episodes = 2721
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.2445182800293
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.7554931640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.27308654785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1839.233154296875
INFO:tools.evaluation_results_class:Current Best Return = -43.2445182800293
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.83686361947232
INFO:tools.evaluation_results_class:Counted Episodes = 2691
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.43936157226562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.56063842773438
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.55093383789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3915.924072265625
INFO:tools.evaluation_results_class:Current Best Return = -75.43936157226562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.24858156028369
INFO:tools.evaluation_results_class:Counted Episodes = 2820
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.91410064697266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.0858917236328
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.77639770507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3748.00146484375
INFO:tools.evaluation_results_class:Current Best Return = -75.91410064697266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.49176807444524
INFO:tools.evaluation_results_class:Counted Episodes = 2794
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.27088928222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.72911071777344
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.73191833496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4391.373046875
INFO:tools.evaluation_results_class:Current Best Return = -77.27088928222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.251770538243626
INFO:tools.evaluation_results_class:Counted Episodes = 2824
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -78.4629135131836
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.53707885742188
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.36126708984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4403.9267578125
INFO:tools.evaluation_results_class:Current Best Return = -78.4629135131836
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.53421712647796
INFO:tools.evaluation_results_class:Counted Episodes = 2791
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.0148696899414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.98513793945312
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.18809509277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4646.8125
INFO:tools.evaluation_results_class:Current Best Return = -73.0148696899414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.184070796460176
INFO:tools.evaluation_results_class:Counted Episodes = 2825
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.92782592773438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.07217407226562
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.4398956298828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4363.00341796875
INFO:tools.evaluation_results_class:Current Best Return = -77.92782592773438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.799281867145424
INFO:tools.evaluation_results_class:Counted Episodes = 2785
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -78.70448303222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.29551696777344
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.28448486328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4388.31201171875
INFO:tools.evaluation_results_class:Current Best Return = -78.70448303222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.37660028449502
INFO:tools.evaluation_results_class:Counted Episodes = 2812
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.26676177978516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.73324584960938
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.8403778076172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4961.24951171875
INFO:tools.evaluation_results_class:Current Best Return = -75.26676177978516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.45506419400856
INFO:tools.evaluation_results_class:Counted Episodes = 2804
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.2071304321289
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.79287719726562
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.56307983398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4547.46484375
INFO:tools.evaluation_results_class:Current Best Return = -80.2071304321289
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.685204991087346
INFO:tools.evaluation_results_class:Counted Episodes = 2805
INFO:robust_rl.robust_rl_trainer:Iteration 12 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.77198028564453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.22802734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.17547607421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2404.049072265625
INFO:tools.evaluation_results_class:Current Best Return = -47.29558181762695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.79661306876497
INFO:tools.evaluation_results_class:Counted Episodes = 5846
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.4700681269168854
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Step: 5, Training loss: 0.4496517777442932
INFO:agents.father_agent:Step: 10, Training loss: 0.4304109215736389
INFO:agents.father_agent:Step: 15, Training loss: 0.4171166718006134
INFO:agents.father_agent:Step: 20, Training loss: 0.5254150032997131
INFO:agents.father_agent:Step: 25, Training loss: 0.4848574101924896
INFO:agents.father_agent:Step: 30, Training loss: 0.7257032990455627
INFO:agents.father_agent:Step: 35, Training loss: 0.6600556373596191
INFO:agents.father_agent:Step: 40, Training loss: 0.693328857421875
INFO:agents.father_agent:Step: 45, Training loss: 0.7014090418815613
INFO:agents.father_agent:Step: 50, Training loss: 0.7108084559440613
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -52.369625091552734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.63037109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.13075256347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2374.371826171875
INFO:tools.evaluation_results_class:Current Best Return = -47.29558181762695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.052254456483745
INFO:tools.evaluation_results_class:Counted Episodes = 5722
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.937347412109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.0626525878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.55528259277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2477.040771484375
INFO:tools.evaluation_results_class:Current Best Return = -53.937347412109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.17772679138722
INFO:tools.evaluation_results_class:Counted Episodes = 5666
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.53958892822266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.4604034423828
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.0140838623047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4047.000244140625
INFO:tools.evaluation_results_class:Current Best Return = -69.53958892822266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.9183088498746
INFO:tools.evaluation_results_class:Counted Episodes = 5582
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 81.17462046131199
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 13 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.54444122314453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.45556640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.35104370117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2383.08251953125
INFO:tools.evaluation_results_class:Current Best Return = -47.29558181762695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.70864498173595
INFO:tools.evaluation_results_class:Counted Episodes = 5749
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.7050663828849792
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Step: 5, Training loss: 0.8302675485610962
INFO:agents.father_agent:Step: 10, Training loss: 0.662146270275116
INFO:agents.father_agent:Step: 15, Training loss: 0.6034337282180786
INFO:agents.father_agent:Step: 20, Training loss: 0.5356353521347046
INFO:agents.father_agent:Step: 25, Training loss: 0.5198078155517578
INFO:agents.father_agent:Step: 30, Training loss: 0.4619009494781494
INFO:agents.father_agent:Step: 35, Training loss: 0.43910759687423706
INFO:agents.father_agent:Step: 40, Training loss: 0.38037580251693726
INFO:agents.father_agent:Step: 45, Training loss: 0.38533705472946167
INFO:agents.father_agent:Step: 50, Training loss: 0.36398839950561523
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -46.15875244140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.84124755859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.62692260742188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2114.350830078125
INFO:tools.evaluation_results_class:Current Best Return = -46.15875244140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.0176975643946
INFO:tools.evaluation_results_class:Counted Episodes = 5707
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -47.73026657104492
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.2697448730469
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.88743591308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2264.982666015625
INFO:tools.evaluation_results_class:Current Best Return = -47.73026657104492
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.39188011848754
INFO:tools.evaluation_results_class:Counted Episodes = 5739
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.32882308959961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.6711730957031
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.50498962402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3702.060302734375
INFO:tools.evaluation_results_class:Current Best Return = -62.32882308959961
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.93986474136355
INFO:tools.evaluation_results_class:Counted Episodes = 5471
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 71.6636856323478
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 14 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.792625427246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.2073669433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.36680603027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2241.263916015625
INFO:tools.evaluation_results_class:Current Best Return = -46.15875244140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.995610184372254
INFO:tools.evaluation_results_class:Counted Episodes = 5695
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.3470590114593506
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Step: 5, Training loss: 0.3646794557571411
INFO:agents.father_agent:Step: 10, Training loss: 0.3925158679485321
INFO:agents.father_agent:Step: 15, Training loss: 0.3831632137298584
INFO:agents.father_agent:Step: 20, Training loss: 0.4136878252029419
INFO:agents.father_agent:Step: 25, Training loss: 0.38792017102241516
INFO:agents.father_agent:Step: 30, Training loss: 0.34341564774513245
INFO:agents.father_agent:Step: 35, Training loss: 0.33847740292549133
INFO:agents.father_agent:Step: 40, Training loss: 0.3407600522041321
INFO:agents.father_agent:Step: 45, Training loss: 0.3680453598499298
INFO:agents.father_agent:Step: 50, Training loss: 0.3645196855068207
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -55.88823318481445
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.11175537109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.98519897460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2561.59326171875
INFO:tools.evaluation_results_class:Current Best Return = -46.15875244140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.260906323977004
INFO:tools.evaluation_results_class:Counted Episodes = 5914
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.733680725097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.2663269042969
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.73104858398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2581.0126953125
INFO:tools.evaluation_results_class:Current Best Return = -56.733680725097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.7528600269179
INFO:tools.evaluation_results_class:Counted Episodes = 5944
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.32183074951172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.67816162109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.38522338867188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4212.91845703125
INFO:tools.evaluation_results_class:Current Best Return = -74.32183074951172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.525452681926886
INFO:tools.evaluation_results_class:Counted Episodes = 5854
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 85.42366844815659
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 15 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.846675872802734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.1533203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.18560791015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2567.784912109375
INFO:tools.evaluation_results_class:Current Best Return = -46.15875244140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.24538839059063
INFO:tools.evaluation_results_class:Counted Episodes = 5909
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.3953658938407898
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Step: 5, Training loss: 0.4143943786621094
INFO:agents.father_agent:Step: 10, Training loss: 0.48070651292800903
INFO:agents.father_agent:Step: 15, Training loss: 0.4467065930366516
INFO:agents.father_agent:Step: 20, Training loss: 0.5737141966819763
INFO:agents.father_agent:Step: 25, Training loss: 0.574434757232666
INFO:agents.father_agent:Step: 30, Training loss: 0.5958853363990784
INFO:agents.father_agent:Step: 35, Training loss: 0.5178943276405334
INFO:agents.father_agent:Step: 40, Training loss: 0.4645881950855255
INFO:agents.father_agent:Step: 45, Training loss: 0.4504500925540924
INFO:agents.father_agent:Step: 50, Training loss: 0.3670501112937927
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -50.150394439697266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.849609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.47567749023438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2250.24072265625
INFO:tools.evaluation_results_class:Current Best Return = -46.15875244140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.853717026378895
INFO:tools.evaluation_results_class:Counted Episodes = 5838
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.5059814453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.4940185546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.51644897460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2296.808837890625
INFO:tools.evaluation_results_class:Current Best Return = -50.5059814453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.43113465481886
INFO:tools.evaluation_results_class:Counted Episodes = 5852
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.18462371826172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.8153839111328
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.261474609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3969.072509765625
INFO:tools.evaluation_results_class:Current Best Return = -66.18462371826172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.66232607920086
INFO:tools.evaluation_results_class:Counted Episodes = 5606
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 73.67016608540175
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 16 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.50894546508789
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.4910583496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.3435821533203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2268.263671875
INFO:tools.evaluation_results_class:Current Best Return = -46.15875244140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.763332765377406
INFO:tools.evaluation_results_class:Counted Episodes = 5869
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.47244757413864136
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Step: 5, Training loss: 0.2717856168746948
INFO:agents.father_agent:Step: 10, Training loss: 0.33299916982650757
INFO:agents.father_agent:Step: 15, Training loss: 0.2691470980644226
INFO:agents.father_agent:Step: 20, Training loss: 0.2725145220756531
INFO:agents.father_agent:Step: 25, Training loss: 0.2781127989292145
INFO:agents.father_agent:Step: 30, Training loss: 0.3071436285972595
INFO:agents.father_agent:Step: 35, Training loss: 0.27864500880241394
INFO:agents.father_agent:Step: 40, Training loss: 0.28668686747550964
INFO:agents.father_agent:Step: 45, Training loss: 0.31693166494369507
INFO:agents.father_agent:Step: 50, Training loss: 0.22847342491149902
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -54.796630859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.203369140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.50991821289062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2429.2373046875
INFO:tools.evaluation_results_class:Current Best Return = -46.15875244140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.543417853421325
INFO:tools.evaluation_results_class:Counted Episodes = 5758
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.238121032714844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.7618713378906
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.93026733398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2450.025146484375
INFO:tools.evaluation_results_class:Current Best Return = -55.238121032714844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.56271837875612
INFO:tools.evaluation_results_class:Counted Episodes = 5724
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.84161376953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.15838623046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.32199096679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4166.30419921875
INFO:tools.evaluation_results_class:Current Best Return = -74.84161376953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.85091169109761
INFO:tools.evaluation_results_class:Counted Episodes = 5594
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 85.88343494529282
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -44.79547119140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.20452880859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 206.6479949951172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2054.940673828125
INFO:tools.evaluation_results_class:Current Best Return = -44.79547119140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.29794762915782
INFO:tools.evaluation_results_class:Counted Episodes = 2826
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.28953170776367
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.7104797363281
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.25439453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2375.083740234375
INFO:tools.evaluation_results_class:Current Best Return = -54.28953170776367
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.148014440433215
INFO:tools.evaluation_results_class:Counted Episodes = 2770
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.282405853271484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.71759033203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.09976196289062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2517.94677734375
INFO:tools.evaluation_results_class:Current Best Return = -55.282405853271484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.001382647770484
INFO:tools.evaluation_results_class:Counted Episodes = 2893
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.12386703491211
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.8761291503906
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.53384399414062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2400.128662109375
INFO:tools.evaluation_results_class:Current Best Return = -51.12386703491211
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.47976273551989
INFO:tools.evaluation_results_class:Counted Episodes = 2866
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.343467712402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.6565246582031
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.65325927734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2326.427978515625
INFO:tools.evaluation_results_class:Current Best Return = -53.343467712402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.708245981830885
INFO:tools.evaluation_results_class:Counted Episodes = 2862
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -40.902286529541016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.09771728515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.55030822753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1751.345947265625
INFO:tools.evaluation_results_class:Current Best Return = -40.902286529541016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.61532873229204
INFO:tools.evaluation_results_class:Counted Episodes = 2753
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -41.63874816894531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.36126708984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.9767608642578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1760.2825927734375
INFO:tools.evaluation_results_class:Current Best Return = -41.63874816894531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.57538237436271
INFO:tools.evaluation_results_class:Counted Episodes = 2746
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -45.500526428222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.4994812011719
INFO:tools.evaluation_results_class:Average Discounted Reward = 206.5512237548828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2064.84716796875
INFO:tools.evaluation_results_class:Current Best Return = -45.500526428222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.74464348436951
INFO:tools.evaluation_results_class:Counted Episodes = 2847
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -42.281681060791016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.71832275390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.01742553710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1919.9447021484375
INFO:tools.evaluation_results_class:Current Best Return = -42.281681060791016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.05188845746559
INFO:tools.evaluation_results_class:Counted Episodes = 2833
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.86343002319336
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.1365661621094
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.35260009765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2572.69580078125
INFO:tools.evaluation_results_class:Current Best Return = -58.86343002319336
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.05772615276311
INFO:tools.evaluation_results_class:Counted Episodes = 2841
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.228370666503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.7716369628906
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.6552276611328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2539.41943359375
INFO:tools.evaluation_results_class:Current Best Return = -59.228370666503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.33900709219858
INFO:tools.evaluation_results_class:Counted Episodes = 2820
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.20871353149414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.7912902832031
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.87435913085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2733.97900390625
INFO:tools.evaluation_results_class:Current Best Return = -60.20871353149414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.88334504567815
INFO:tools.evaluation_results_class:Counted Episodes = 2846
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.89705276489258
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.1029357910156
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.9442901611328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2547.19482421875
INFO:tools.evaluation_results_class:Current Best Return = -57.89705276489258
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.25949591764288
INFO:tools.evaluation_results_class:Counted Episodes = 2817
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.89163589477539
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.1083679199219
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.6655731201172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2564.145263671875
INFO:tools.evaluation_results_class:Current Best Return = -57.89163589477539
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.540418118466896
INFO:tools.evaluation_results_class:Counted Episodes = 2870
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.08694076538086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.9130554199219
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.0929718017578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2725.518798828125
INFO:tools.evaluation_results_class:Current Best Return = -60.08694076538086
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.821189721928896
INFO:tools.evaluation_results_class:Counted Episodes = 2841
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.240577697753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.7594299316406
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.9449462890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2508.5673828125
INFO:tools.evaluation_results_class:Current Best Return = -57.240577697753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.85734413525889
INFO:tools.evaluation_results_class:Counted Episodes = 2839
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.72611999511719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.2738952636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.588623046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2543.50732421875
INFO:tools.evaluation_results_class:Current Best Return = -58.72611999511719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.89460697920338
INFO:tools.evaluation_results_class:Counted Episodes = 2837
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.716102600097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.2839050292969
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.33511352539062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2519.572021484375
INFO:tools.evaluation_results_class:Current Best Return = -57.716102600097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.036723163841806
INFO:tools.evaluation_results_class:Counted Episodes = 2832
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.769012451171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.2309875488281
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.28257751464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2548.231689453125
INFO:tools.evaluation_results_class:Current Best Return = -57.769012451171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.0056338028169
INFO:tools.evaluation_results_class:Counted Episodes = 2840
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.2458610534668
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.754150390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.91709899902344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2528.659423828125
INFO:tools.evaluation_results_class:Current Best Return = -58.2458610534668
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.02007749207468
INFO:tools.evaluation_results_class:Counted Episodes = 2839
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.66172790527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.3382568359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.19918823242188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2480.351318359375
INFO:tools.evaluation_results_class:Current Best Return = -56.66172790527344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.069488536155205
INFO:tools.evaluation_results_class:Counted Episodes = 2835
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -40.22511291503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.7748718261719
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.14877319335938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1776.954345703125
INFO:tools.evaluation_results_class:Current Best Return = -40.22511291503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.43853695324284
INFO:tools.evaluation_results_class:Counted Episodes = 2652
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -47.178260803222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.8217468261719
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.1509552001953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2021.24609375
INFO:tools.evaluation_results_class:Current Best Return = -47.178260803222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.25611132004513
INFO:tools.evaluation_results_class:Counted Episodes = 2659
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.28024291992188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.71975708007812
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.29049682617188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4241.01123046875
INFO:tools.evaluation_results_class:Current Best Return = -76.28024291992188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.31708197905381
INFO:tools.evaluation_results_class:Counted Episodes = 2769
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.64405059814453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.35595703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.83132934570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4365.1953125
INFO:tools.evaluation_results_class:Current Best Return = -73.64405059814453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.29354136429608
INFO:tools.evaluation_results_class:Counted Episodes = 2756
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -46.977230072021484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.02276611328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.42724609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2008.3812255859375
INFO:tools.evaluation_results_class:Current Best Return = -46.977230072021484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.23356591994124
INFO:tools.evaluation_results_class:Counted Episodes = 2723
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -40.1669807434082
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.8330078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.09823608398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1637.11083984375
INFO:tools.evaluation_results_class:Current Best Return = -40.1669807434082
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.62759140595552
INFO:tools.evaluation_results_class:Counted Episodes = 2653
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -42.45866775512695
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.54132080078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.64407348632812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1753.148193359375
INFO:tools.evaluation_results_class:Current Best Return = -42.45866775512695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.10704761904762
INFO:tools.evaluation_results_class:Counted Episodes = 2625
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.43254089355469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.5674591064453
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.9635009765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3755.57177734375
INFO:tools.evaluation_results_class:Current Best Return = -80.43254089355469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.951736745886656
INFO:tools.evaluation_results_class:Counted Episodes = 2735
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.42872619628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.57127380371094
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.18446350097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3750.9189453125
INFO:tools.evaluation_results_class:Current Best Return = -81.42872619628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.93823099415204
INFO:tools.evaluation_results_class:Counted Episodes = 2736
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -86.93423461914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.06576538085938
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.30702209472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4437.44970703125
INFO:tools.evaluation_results_class:Current Best Return = -86.93423461914062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.86335403726708
INFO:tools.evaluation_results_class:Counted Episodes = 2737
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -86.69463348388672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.30535888671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.63697814941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4368.5869140625
INFO:tools.evaluation_results_class:Current Best Return = -86.69463348388672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.663991244071504
INFO:tools.evaluation_results_class:Counted Episodes = 2741
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -86.17439270019531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.8256072998047
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.5801544189453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5312.1787109375
INFO:tools.evaluation_results_class:Current Best Return = -86.17439270019531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.00735835172921
INFO:tools.evaluation_results_class:Counted Episodes = 2718
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.02395629882812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.97604370117188
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.23316955566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4234.26416015625
INFO:tools.evaluation_results_class:Current Best Return = -83.02395629882812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.387295825771325
INFO:tools.evaluation_results_class:Counted Episodes = 2755
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -85.5411605834961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.45883178710938
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.38140869140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4415.36181640625
INFO:tools.evaluation_results_class:Current Best Return = -85.5411605834961
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.99304793267471
INFO:tools.evaluation_results_class:Counted Episodes = 2733
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.9798355102539
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.02015686035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.4896697998047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5093.28173828125
INFO:tools.evaluation_results_class:Current Best Return = -82.9798355102539
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.85740469208211
INFO:tools.evaluation_results_class:Counted Episodes = 2728
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.80036926269531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.1996307373047
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.39353942871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4374.80712890625
INFO:tools.evaluation_results_class:Current Best Return = -84.80036926269531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.673126142595976
INFO:tools.evaluation_results_class:Counted Episodes = 2735
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -87.89107513427734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 232.1089324951172
INFO:tools.evaluation_results_class:Average Discounted Reward = 165.82327270507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4264.6103515625
INFO:tools.evaluation_results_class:Current Best Return = -87.89107513427734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.508925318761385
INFO:tools.evaluation_results_class:Counted Episodes = 2745
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -85.34809112548828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.6519012451172
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.120849609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4489.5595703125
INFO:tools.evaluation_results_class:Current Best Return = -85.34809112548828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.4994555353902
INFO:tools.evaluation_results_class:Counted Episodes = 2755
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -85.54721069335938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.45278930664062
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.7505645751953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4583.0556640625
INFO:tools.evaluation_results_class:Current Best Return = -85.54721069335938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.65257017863653
INFO:tools.evaluation_results_class:Counted Episodes = 2743
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -86.78602600097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.21397399902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.27598571777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4484.56787109375
INFO:tools.evaluation_results_class:Current Best Return = -86.78602600097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.96086320409656
INFO:tools.evaluation_results_class:Counted Episodes = 2734
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -85.20706176757812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.79293823242188
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.33787536621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4345.2919921875
INFO:tools.evaluation_results_class:Current Best Return = -85.20706176757812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.4363173216885
INFO:tools.evaluation_results_class:Counted Episodes = 2748
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 17 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.86376953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.13623046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.66595458984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4045.56640625
INFO:tools.evaluation_results_class:Current Best Return = -46.15875244140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.656332205937616
INFO:tools.evaluation_results_class:Counted Episodes = 5322
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.695128858089447
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Step: 5, Training loss: 0.8959740400314331
INFO:agents.father_agent:Step: 10, Training loss: 0.794123649597168
INFO:agents.father_agent:Step: 15, Training loss: 0.8749827742576599
INFO:agents.father_agent:Step: 20, Training loss: 0.843886137008667
INFO:agents.father_agent:Step: 25, Training loss: 0.833879828453064
INFO:agents.father_agent:Step: 30, Training loss: 0.7611289620399475
INFO:agents.father_agent:Step: 35, Training loss: 0.6197221875190735
INFO:agents.father_agent:Step: 40, Training loss: 0.6290878057479858
INFO:agents.father_agent:Step: 45, Training loss: 0.4822131395339966
INFO:agents.father_agent:Step: 50, Training loss: 0.42622631788253784
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 4.69%
INFO:agents.father_agent:Step: 55, Training loss: 0.36097782850265503
INFO:agents.father_agent:Step: 60, Training loss: 0.34259936213493347
INFO:agents.father_agent:Step: 65, Training loss: 0.2716120481491089
INFO:agents.father_agent:Step: 70, Training loss: 0.33539676666259766
INFO:agents.father_agent:Step: 75, Training loss: 0.3406367897987366
INFO:agents.father_agent:Step: 80, Training loss: 0.3197905421257019
INFO:agents.father_agent:Step: 85, Training loss: 0.40393373370170593
INFO:agents.father_agent:Step: 90, Training loss: 0.33539673686027527
INFO:agents.father_agent:Step: 95, Training loss: 0.3799993097782135
INFO:agents.father_agent:Step: 100, Training loss: 0.4163505733013153
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.94247055053711
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.0575256347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.53782653808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2321.86328125
INFO:tools.evaluation_results_class:Current Best Return = -46.15875244140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.911159657746225
INFO:tools.evaluation_results_class:Counted Episodes = 5493
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 4.69%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -54.206260681152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.7937316894531
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.4363555908203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2373.338134765625
INFO:tools.evaluation_results_class:Current Best Return = -46.15875244140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.93409794283634
INFO:tools.evaluation_results_class:Counted Episodes = 5493
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.51421356201172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.48577880859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.1438446044922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2436.763916015625
INFO:tools.evaluation_results_class:Current Best Return = -54.51421356201172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.93259431432147
INFO:tools.evaluation_results_class:Counted Episodes = 5593
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.38385772705078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.61614990234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.1861114501953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4123.36669921875
INFO:tools.evaluation_results_class:Current Best Return = -74.38385772705078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.164243102162565
INFO:tools.evaluation_results_class:Counted Episodes = 5364
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 81.60536032340782
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 18 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -40.450645446777344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.5493469238281
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.87701416015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2056.26708984375
INFO:tools.evaluation_results_class:Current Best Return = -40.450645446777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.3999646830302
INFO:tools.evaluation_results_class:Counted Episodes = 5663
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.5295957326889038
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Step: 5, Training loss: 0.4488394856452942
INFO:agents.father_agent:Step: 10, Training loss: 0.3039373755455017
INFO:agents.father_agent:Step: 15, Training loss: 0.3329503536224365
INFO:agents.father_agent:Step: 20, Training loss: 0.27991431951522827
INFO:agents.father_agent:Step: 25, Training loss: 0.3742349445819855
INFO:agents.father_agent:Step: 30, Training loss: 0.4628935754299164
INFO:agents.father_agent:Step: 35, Training loss: 0.4860040843486786
INFO:agents.father_agent:Step: 40, Training loss: 0.5206195116043091
INFO:agents.father_agent:Step: 45, Training loss: 0.5120694637298584
INFO:agents.father_agent:Step: 50, Training loss: 0.5460885167121887
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Step: 55, Training loss: 0.45264706015586853
INFO:agents.father_agent:Step: 60, Training loss: 0.36745938658714294
INFO:agents.father_agent:Step: 65, Training loss: 0.36652863025665283
INFO:agents.father_agent:Step: 70, Training loss: 0.37922802567481995
INFO:agents.father_agent:Step: 75, Training loss: 0.3250153064727783
INFO:agents.father_agent:Step: 80, Training loss: 0.3809502124786377
INFO:agents.father_agent:Step: 85, Training loss: 0.43478596210479736
INFO:agents.father_agent:Step: 90, Training loss: 0.40827232599258423
INFO:agents.father_agent:Step: 95, Training loss: 0.34841400384902954
INFO:agents.father_agent:Step: 100, Training loss: 0.4749546945095062
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.64183044433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.358154296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.28286743164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2500.607177734375
INFO:tools.evaluation_results_class:Current Best Return = -40.450645446777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.228248014867376
INFO:tools.evaluation_results_class:Counted Episodes = 5919
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 3.12%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -55.11750793457031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.8824768066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.07394409179688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2458.4716796875
INFO:tools.evaluation_results_class:Current Best Return = -40.450645446777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.947288903810644
INFO:tools.evaluation_results_class:Counted Episodes = 5957
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.7199592590332
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.280029296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.69171142578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2510.814453125
INFO:tools.evaluation_results_class:Current Best Return = -54.7199592590332
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.56708945260347
INFO:tools.evaluation_results_class:Counted Episodes = 5992
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.41989135742188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.58010864257812
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.0576934814453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4331.54638671875
INFO:tools.evaluation_results_class:Current Best Return = -77.41989135742188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.00371433395239
INFO:tools.evaluation_results_class:Counted Episodes = 5923
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 86.17244324508933
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 19 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.01789474487305
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.98211669921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.9402618408203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3286.2890625
INFO:tools.evaluation_results_class:Current Best Return = -40.450645446777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.51580958999305
INFO:tools.evaluation_results_class:Counted Episodes = 5756
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.5835400819778442
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Step: 5, Training loss: 0.6556404232978821
INFO:agents.father_agent:Step: 10, Training loss: 0.5134520530700684
INFO:agents.father_agent:Step: 15, Training loss: 0.6639289855957031
INFO:agents.father_agent:Step: 20, Training loss: 0.4547645151615143
INFO:agents.father_agent:Step: 25, Training loss: 0.5442897081375122
INFO:agents.father_agent:Step: 30, Training loss: 0.4768800139427185
INFO:agents.father_agent:Step: 35, Training loss: 0.3871113955974579
INFO:agents.father_agent:Step: 40, Training loss: 0.3228730857372284
INFO:agents.father_agent:Step: 45, Training loss: 0.30343785881996155
INFO:agents.father_agent:Step: 50, Training loss: 0.3238477408885956
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Step: 55, Training loss: 0.21293897926807404
INFO:agents.father_agent:Step: 60, Training loss: 0.24560114741325378
INFO:agents.father_agent:Step: 65, Training loss: 0.2636822462081909
INFO:agents.father_agent:Step: 70, Training loss: 0.2552618384361267
INFO:agents.father_agent:Step: 75, Training loss: 0.24656876921653748
INFO:agents.father_agent:Step: 80, Training loss: 0.22843587398529053
INFO:agents.father_agent:Step: 85, Training loss: 0.28888657689094543
INFO:agents.father_agent:Step: 90, Training loss: 0.26168036460876465
INFO:agents.father_agent:Step: 95, Training loss: 0.3606037199497223
INFO:agents.father_agent:Step: 100, Training loss: 0.34545809030532837
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.544212341308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.4557800292969
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.75462341308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2668.057373046875
INFO:tools.evaluation_results_class:Current Best Return = -40.450645446777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.12452504317789
INFO:tools.evaluation_results_class:Counted Episodes = 5790
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -56.29656982421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.70343017578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.6923370361328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2568.3251953125
INFO:tools.evaluation_results_class:Current Best Return = -40.450645446777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.17215233499914
INFO:tools.evaluation_results_class:Counted Episodes = 5803
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.55840301513672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.44158935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.50030517578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2712.549560546875
INFO:tools.evaluation_results_class:Current Best Return = -59.55840301513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.45321038251366
INFO:tools.evaluation_results_class:Counted Episodes = 5856
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.8480453491211
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.15196228027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.33609008789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4406.615234375
INFO:tools.evaluation_results_class:Current Best Return = -79.8480453491211
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.44220590746116
INFO:tools.evaluation_results_class:Counted Episodes = 5857
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 89.08106071826708
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 20 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.22007751464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.77992248535156
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.91925048828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3408.03466796875
INFO:tools.evaluation_results_class:Current Best Return = -40.450645446777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 80.37276286353467
INFO:tools.evaluation_results_class:Counted Episodes = 3576
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.688350260257721
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Step: 5, Training loss: 0.8841729760169983
INFO:agents.father_agent:Step: 10, Training loss: 1.0741398334503174
INFO:agents.father_agent:Step: 15, Training loss: 1.1419811248779297
INFO:agents.father_agent:Step: 20, Training loss: 0.9088780283927917
INFO:agents.father_agent:Step: 25, Training loss: 0.5496664643287659
INFO:agents.father_agent:Step: 30, Training loss: 0.5072747468948364
INFO:agents.father_agent:Step: 35, Training loss: 0.5249332189559937
INFO:agents.father_agent:Step: 40, Training loss: 0.4403995871543884
INFO:agents.father_agent:Step: 45, Training loss: 0.3669548034667969
INFO:agents.father_agent:Step: 50, Training loss: 0.42887768149375916
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 4.69%
INFO:agents.father_agent:Step: 55, Training loss: 0.444274365901947
INFO:agents.father_agent:Step: 60, Training loss: 0.5334562063217163
INFO:agents.father_agent:Step: 65, Training loss: 0.5335034728050232
INFO:agents.father_agent:Step: 70, Training loss: 0.5596798062324524
INFO:agents.father_agent:Step: 75, Training loss: 0.5772886276245117
INFO:agents.father_agent:Step: 80, Training loss: 0.46270179748535156
INFO:agents.father_agent:Step: 85, Training loss: 0.40395045280456543
INFO:agents.father_agent:Step: 90, Training loss: 0.3407709300518036
INFO:agents.father_agent:Step: 95, Training loss: 0.33022433519363403
INFO:agents.father_agent:Step: 100, Training loss: 0.3410016894340515
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.5679817199707
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.4320068359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.87405395507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2501.5556640625
INFO:tools.evaluation_results_class:Current Best Return = -40.450645446777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.87544006705784
INFO:tools.evaluation_results_class:Counted Episodes = 5965
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 4.69%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -57.71224594116211
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.2877502441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.7998504638672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2515.065673828125
INFO:tools.evaluation_results_class:Current Best Return = -40.450645446777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.94422980010079
INFO:tools.evaluation_results_class:Counted Episodes = 5953
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.83378219604492
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.1662292480469
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.67677307128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2619.683837890625
INFO:tools.evaluation_results_class:Current Best Return = -58.83378219604492
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.99864864864865
INFO:tools.evaluation_results_class:Counted Episodes = 5920
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.1552505493164
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.84475708007812
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.26785278320312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4489.6630859375
INFO:tools.evaluation_results_class:Current Best Return = -81.1552505493164
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.51119652406417
INFO:tools.evaluation_results_class:Counted Episodes = 5984
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 88.70953677607568
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 21 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -93.05077362060547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 226.94923400878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 156.68051147460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4892.38916015625
INFO:tools.evaluation_results_class:Current Best Return = -40.450645446777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.607107986721346
INFO:tools.evaluation_results_class:Counted Episodes = 5121
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.036417007446289
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Step: 5, Training loss: 1.4635329246520996
INFO:agents.father_agent:Step: 10, Training loss: 1.385373592376709
INFO:agents.father_agent:Step: 15, Training loss: 1.2776669263839722
INFO:agents.father_agent:Step: 20, Training loss: 1.1753495931625366
INFO:agents.father_agent:Step: 25, Training loss: 1.0977901220321655
INFO:agents.father_agent:Step: 30, Training loss: 0.9980406761169434
INFO:agents.father_agent:Step: 35, Training loss: 0.9554739594459534
INFO:agents.father_agent:Step: 40, Training loss: 0.876715898513794
INFO:agents.father_agent:Step: 45, Training loss: 0.75789874792099
INFO:agents.father_agent:Step: 50, Training loss: 0.5464513897895813
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Step: 55, Training loss: 0.4899144768714905
INFO:agents.father_agent:Step: 60, Training loss: 0.35447752475738525
INFO:agents.father_agent:Step: 65, Training loss: 0.28724369406700134
INFO:agents.father_agent:Step: 70, Training loss: 0.21160373091697693
INFO:agents.father_agent:Step: 75, Training loss: 0.2093382477760315
INFO:agents.father_agent:Step: 80, Training loss: 0.21291117370128632
INFO:agents.father_agent:Step: 85, Training loss: 0.21708661317825317
INFO:agents.father_agent:Step: 90, Training loss: 0.2655916213989258
INFO:agents.father_agent:Step: 95, Training loss: 0.27407410740852356
INFO:agents.father_agent:Step: 100, Training loss: 0.3078380823135376
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.26658248901367
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.7334289550781
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.82728576660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2473.214599609375
INFO:tools.evaluation_results_class:Current Best Return = -40.450645446777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.688474084813336
INFO:tools.evaluation_results_class:Counted Episodes = 5518
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -56.20915603637695
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.79083251953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.13514709472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2472.924560546875
INFO:tools.evaluation_results_class:Current Best Return = -40.450645446777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.60322055364574
INFO:tools.evaluation_results_class:Counted Episodes = 5527
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.09418869018555
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.90582275390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.560791015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2613.5205078125
INFO:tools.evaluation_results_class:Current Best Return = -58.09418869018555
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.41645615301335
INFO:tools.evaluation_results_class:Counted Episodes = 5542
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.49056243896484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.50942993164062
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.48193359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4248.2939453125
INFO:tools.evaluation_results_class:Current Best Return = -77.49056243896484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.17408832691955
INFO:tools.evaluation_results_class:Counted Episodes = 5457
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 84.11255741957316
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -33.61714553833008
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 286.3828430175781
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.0763397216797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1420.6982421875
INFO:tools.evaluation_results_class:Current Best Return = -33.61714553833008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.413155949741316
INFO:tools.evaluation_results_class:Counted Episodes = 2706
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.36660385131836
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.6333923339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.14894104003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2474.711181640625
INFO:tools.evaluation_results_class:Current Best Return = -56.36660385131836
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.32082551594747
INFO:tools.evaluation_results_class:Counted Episodes = 2665
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.60493850708008
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.3950500488281
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.55047607421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2499.63623046875
INFO:tools.evaluation_results_class:Current Best Return = -51.60493850708008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.5039941902687
INFO:tools.evaluation_results_class:Counted Episodes = 2754
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.2978401184082
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.7021484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.2698974609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2360.576416015625
INFO:tools.evaluation_results_class:Current Best Return = -48.2978401184082
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.89864617636297
INFO:tools.evaluation_results_class:Counted Episodes = 2733
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.70708465576172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.29290771484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.636962890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2439.179443359375
INFO:tools.evaluation_results_class:Current Best Return = -56.70708465576172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.65157048940833
INFO:tools.evaluation_results_class:Counted Episodes = 2738
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.31467819213867
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.6853332519531
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.83486938476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2184.10498046875
INFO:tools.evaluation_results_class:Current Best Return = -49.31467819213867
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.006515906477574
INFO:tools.evaluation_results_class:Counted Episodes = 2609
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -47.79816818237305
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.20184326171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.02734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2070.24560546875
INFO:tools.evaluation_results_class:Current Best Return = -47.79816818237305
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.10835558946967
INFO:tools.evaluation_results_class:Counted Episodes = 2621
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.1181526184082
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.8818359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.92422485351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2298.87890625
INFO:tools.evaluation_results_class:Current Best Return = -52.1181526184082
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.513604174431606
INFO:tools.evaluation_results_class:Counted Episodes = 2683
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.214576721191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.7854309082031
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.06689453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2347.713623046875
INFO:tools.evaluation_results_class:Current Best Return = -51.214576721191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.168936326831066
INFO:tools.evaluation_results_class:Counted Episodes = 2717
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.160545349121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.8394470214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.74935913085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2723.259765625
INFO:tools.evaluation_results_class:Current Best Return = -58.160545349121094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.13703159441587
INFO:tools.evaluation_results_class:Counted Episodes = 2722
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -61.203739166259766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.7962646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.35653686523438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2723.64794921875
INFO:tools.evaluation_results_class:Current Best Return = -61.203739166259766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.889345794392526
INFO:tools.evaluation_results_class:Counted Episodes = 2675
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.81894302368164
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.1810607910156
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.9091033935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2528.243896484375
INFO:tools.evaluation_results_class:Current Best Return = -49.81894302368164
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.93734061930783
INFO:tools.evaluation_results_class:Counted Episodes = 2745
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.61892318725586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.3810729980469
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.96437072753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2503.032958984375
INFO:tools.evaluation_results_class:Current Best Return = -57.61892318725586
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.43339517625232
INFO:tools.evaluation_results_class:Counted Episodes = 2695
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.865997314453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.1340026855469
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.0286865234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2698.291259765625
INFO:tools.evaluation_results_class:Current Best Return = -60.865997314453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.55419450631032
INFO:tools.evaluation_results_class:Counted Episodes = 2694
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.75193786621094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.248046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.3546905517578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2415.845458984375
INFO:tools.evaluation_results_class:Current Best Return = -48.75193786621094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.35289774824658
INFO:tools.evaluation_results_class:Counted Episodes = 2709
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.03361511230469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.96636962890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.4330596923828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2668.4130859375
INFO:tools.evaluation_results_class:Current Best Return = -59.03361511230469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.65422977465829
INFO:tools.evaluation_results_class:Counted Episodes = 2707
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.99256896972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.0074157714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.4053955078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2684.099609375
INFO:tools.evaluation_results_class:Current Best Return = -58.99256896972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.63447251114413
INFO:tools.evaluation_results_class:Counted Episodes = 2692
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.54003143310547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.4599609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.9259796142578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2602.1455078125
INFO:tools.evaluation_results_class:Current Best Return = -58.54003143310547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.65863602668644
INFO:tools.evaluation_results_class:Counted Episodes = 2698
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.26127243041992
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.7387390136719
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.6032257080078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2656.639404296875
INFO:tools.evaluation_results_class:Current Best Return = -59.26127243041992
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.40539541759054
INFO:tools.evaluation_results_class:Counted Episodes = 2706
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.02647018432617
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.9735412597656
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.14772033691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2672.536865234375
INFO:tools.evaluation_results_class:Current Best Return = -60.02647018432617
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.13235294117647
INFO:tools.evaluation_results_class:Counted Episodes = 2720
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.36903762817383
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.6309509277344
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.9949493408203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2661.01806640625
INFO:tools.evaluation_results_class:Current Best Return = -60.36903762817383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.004402054292
INFO:tools.evaluation_results_class:Counted Episodes = 2726
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.07321548461914
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.9267883300781
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.93641662597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2706.25634765625
INFO:tools.evaluation_results_class:Current Best Return = -59.07321548461914
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.1607799852833
INFO:tools.evaluation_results_class:Counted Episodes = 2718
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.23161697387695
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.76837158203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.91268920898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2704.272216796875
INFO:tools.evaluation_results_class:Current Best Return = -59.23161697387695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.06397058823529
INFO:tools.evaluation_results_class:Counted Episodes = 2720
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.48481369018555
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.51519775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.9109649658203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2705.0400390625
INFO:tools.evaluation_results_class:Current Best Return = -59.48481369018555
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.87010611050128
INFO:tools.evaluation_results_class:Counted Episodes = 2733
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.74934768676758
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.2506408691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.42222595214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2561.959228515625
INFO:tools.evaluation_results_class:Current Best Return = -58.74934768676758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.9292364990689
INFO:tools.evaluation_results_class:Counted Episodes = 2685
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.79933547973633
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.2006530761719
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.164794921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2817.29443359375
INFO:tools.evaluation_results_class:Current Best Return = -60.79933547973633
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.376244928070825
INFO:tools.evaluation_results_class:Counted Episodes = 2711
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -40.74094009399414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.2590637207031
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.4058380126953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1820.8619384765625
INFO:tools.evaluation_results_class:Current Best Return = -40.74094009399414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.59059367771781
INFO:tools.evaluation_results_class:Counted Episodes = 2594
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -47.541378021240234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.4586181640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.4090576171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2066.59619140625
INFO:tools.evaluation_results_class:Current Best Return = -47.541378021240234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.583524904214556
INFO:tools.evaluation_results_class:Counted Episodes = 2610
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.76795959472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.23204040527344
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.47914123535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4217.25146484375
INFO:tools.evaluation_results_class:Current Best Return = -74.76795959472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.259300184162065
INFO:tools.evaluation_results_class:Counted Episodes = 2715
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.48143005371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.51856994628906
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.30235290527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4292.62255859375
INFO:tools.evaluation_results_class:Current Best Return = -70.48143005371094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.68681718863802
INFO:tools.evaluation_results_class:Counted Episodes = 2746
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -45.787353515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.212646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.3267364501953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1993.8739013671875
INFO:tools.evaluation_results_class:Current Best Return = -45.787353515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.309747835905156
INFO:tools.evaluation_results_class:Counted Episodes = 2657
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -42.26076889038086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.7392272949219
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.406982421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1732.358642578125
INFO:tools.evaluation_results_class:Current Best Return = -42.26076889038086
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.31871574001566
INFO:tools.evaluation_results_class:Counted Episodes = 2554
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -42.207401275634766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.7926025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.36256408691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1785.2886962890625
INFO:tools.evaluation_results_class:Current Best Return = -42.207401275634766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.479568234387045
INFO:tools.evaluation_results_class:Counted Episodes = 2594
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -78.67886352539062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.32113647460938
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.1855926513672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3751.6533203125
INFO:tools.evaluation_results_class:Current Best Return = -78.67886352539062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.45417590539542
INFO:tools.evaluation_results_class:Counted Episodes = 2706
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.96443176269531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.0355682373047
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.48757934570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3754.41650390625
INFO:tools.evaluation_results_class:Current Best Return = -76.96443176269531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.62245276028158
INFO:tools.evaluation_results_class:Counted Episodes = 2699
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.81236267089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.18763732910156
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.14227294921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4429.02880859375
INFO:tools.evaluation_results_class:Current Best Return = -84.81236267089844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.86820551005212
INFO:tools.evaluation_results_class:Counted Episodes = 2686
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.1901626586914
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.80984497070312
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.01950073242188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4566.27880859375
INFO:tools.evaluation_results_class:Current Best Return = -84.1901626586914
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.58342582315945
INFO:tools.evaluation_results_class:Counted Episodes = 2703
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.35688018798828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.64312744140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.34982299804688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5173.103515625
INFO:tools.evaluation_results_class:Current Best Return = -81.35688018798828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.477440828402365
INFO:tools.evaluation_results_class:Counted Episodes = 2704
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.27827453613281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.7217254638672
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.57679748535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4509.90625
INFO:tools.evaluation_results_class:Current Best Return = -82.27827453613281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.727715355805245
INFO:tools.evaluation_results_class:Counted Episodes = 2670
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.47161102294922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.5283966064453
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.8971405029297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4324.16259765625
INFO:tools.evaluation_results_class:Current Best Return = -84.47161102294922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.42219764011799
INFO:tools.evaluation_results_class:Counted Episodes = 2712
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.76248931884766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.2375030517578
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.5618438720703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4995.98291015625
INFO:tools.evaluation_results_class:Current Best Return = -82.76248931884766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.91014168530947
INFO:tools.evaluation_results_class:Counted Episodes = 2682
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.77645874023438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.22354125976562
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.41644287109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4477.9482421875
INFO:tools.evaluation_results_class:Current Best Return = -83.77645874023438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.50464166357222
INFO:tools.evaluation_results_class:Counted Episodes = 2693
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.81720733642578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.18280029296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.3196563720703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4303.83544921875
INFO:tools.evaluation_results_class:Current Best Return = -81.81720733642578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.367816091954026
INFO:tools.evaluation_results_class:Counted Episodes = 2697
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.21672821044922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.78326416015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.5276336669922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4395.29150390625
INFO:tools.evaluation_results_class:Current Best Return = -83.21672821044922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.837546468401484
INFO:tools.evaluation_results_class:Counted Episodes = 2690
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -85.81543731689453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.1845703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.4385986328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4571.23779296875
INFO:tools.evaluation_results_class:Current Best Return = -85.81543731689453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.673005219985086
INFO:tools.evaluation_results_class:Counted Episodes = 2682
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.41988372802734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.5801239013672
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.3611297607422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4343.8115234375
INFO:tools.evaluation_results_class:Current Best Return = -81.41988372802734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.66320474777448
INFO:tools.evaluation_results_class:Counted Episodes = 2696
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.0702133178711
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.92977905273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.20120239257812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4404.84619140625
INFO:tools.evaluation_results_class:Current Best Return = -84.0702133178711
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.4419807834442
INFO:tools.evaluation_results_class:Counted Episodes = 2706
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.92684936523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.07315063476562
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.37686157226562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4569.1181640625
INFO:tools.evaluation_results_class:Current Best Return = -84.92684936523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.50241366505756
INFO:tools.evaluation_results_class:Counted Episodes = 2693
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.59248352050781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.4075164794922
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.8093719482422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4363.27099609375
INFO:tools.evaluation_results_class:Current Best Return = -83.59248352050781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.25202652910833
INFO:tools.evaluation_results_class:Counted Episodes = 2714
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.8294677734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.1705322265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.998291015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4342.18408203125
INFO:tools.evaluation_results_class:Current Best Return = -83.8294677734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.84293193717277
INFO:tools.evaluation_results_class:Counted Episodes = 2674
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.10047912597656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.89952087402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.99168395996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4423.38427734375
INFO:tools.evaluation_results_class:Current Best Return = -83.10047912597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.54690396737115
INFO:tools.evaluation_results_class:Counted Episodes = 2697
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.07691955566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.92308044433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.1511688232422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4427.65478515625
INFO:tools.evaluation_results_class:Current Best Return = -82.07691955566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.30769230769231
INFO:tools.evaluation_results_class:Counted Episodes = 2704
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 22 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -37.986839294433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.0131530761719
INFO:tools.evaluation_results_class:Average Discounted Reward = 206.18826293945312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1859.9371337890625
INFO:tools.evaluation_results_class:Current Best Return = -37.986839294433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.35533191407006
INFO:tools.evaluation_results_class:Counted Episodes = 5167
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.22805136442184448
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 0.35319703817367554
INFO:agents.father_agent:Step: 10, Training loss: 0.43617257475852966
INFO:agents.father_agent:Step: 15, Training loss: 0.5103280544281006
INFO:agents.father_agent:Step: 20, Training loss: 0.5648506283760071
INFO:agents.father_agent:Step: 25, Training loss: 0.6035590767860413
INFO:agents.father_agent:Step: 30, Training loss: 0.5776205658912659
INFO:agents.father_agent:Step: 35, Training loss: 0.5108980536460876
INFO:agents.father_agent:Step: 40, Training loss: 0.5252548456192017
INFO:agents.father_agent:Step: 45, Training loss: 0.42093098163604736
INFO:agents.father_agent:Step: 50, Training loss: 0.3190276622772217
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Step: 55, Training loss: 0.4366341233253479
INFO:agents.father_agent:Step: 60, Training loss: 0.31209614872932434
INFO:agents.father_agent:Step: 65, Training loss: 0.3942684531211853
INFO:agents.father_agent:Step: 70, Training loss: 0.3868531584739685
INFO:agents.father_agent:Step: 75, Training loss: 0.3847738206386566
INFO:agents.father_agent:Step: 80, Training loss: 0.3208844065666199
INFO:agents.father_agent:Step: 85, Training loss: 0.40645262598991394
INFO:agents.father_agent:Step: 90, Training loss: 0.32523250579833984
INFO:agents.father_agent:Step: 95, Training loss: 0.33845794200897217
INFO:agents.father_agent:Step: 100, Training loss: 0.2999650239944458
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.18062973022461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.8193664550781
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.26678466796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2494.907958984375
INFO:tools.evaluation_results_class:Current Best Return = -37.986839294433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.863186167533996
INFO:tools.evaluation_results_class:Counted Episodes = 5957
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -56.15993881225586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.8400573730469
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.80628967285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2514.221435546875
INFO:tools.evaluation_results_class:Current Best Return = -37.986839294433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.250971119743284
INFO:tools.evaluation_results_class:Counted Episodes = 5921
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.53022766113281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.46978759765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.13156127929688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2558.36083984375
INFO:tools.evaluation_results_class:Current Best Return = -56.53022766113281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.56195724782899
INFO:tools.evaluation_results_class:Counted Episodes = 5988
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -78.32355499267578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.67645263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.30455017089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4510.33154296875
INFO:tools.evaluation_results_class:Current Best Return = -78.32355499267578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.922947226437365
INFO:tools.evaluation_results_class:Counted Episodes = 5931
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 83.60246519355493
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 23 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.00194549560547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.998046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.5135498046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2759.226806640625
INFO:tools.evaluation_results_class:Current Best Return = -37.986839294433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.365369649805444
INFO:tools.evaluation_results_class:Counted Episodes = 5140
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.104110598564148
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 1.2502944469451904
INFO:agents.father_agent:Step: 10, Training loss: 1.363745093345642
INFO:agents.father_agent:Step: 15, Training loss: 1.2798672914505005
INFO:agents.father_agent:Step: 20, Training loss: 1.2427890300750732
INFO:agents.father_agent:Step: 25, Training loss: 0.9619932174682617
INFO:agents.father_agent:Step: 30, Training loss: 0.9199932813644409
INFO:agents.father_agent:Step: 35, Training loss: 0.7554192543029785
INFO:agents.father_agent:Step: 40, Training loss: 0.5438040494918823
INFO:agents.father_agent:Step: 45, Training loss: 0.3583238124847412
INFO:agents.father_agent:Step: 50, Training loss: 0.28262636065483093
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 55, Training loss: 0.2470674067735672
INFO:agents.father_agent:Step: 60, Training loss: 0.21428444981575012
INFO:agents.father_agent:Step: 65, Training loss: 0.21352103352546692
INFO:agents.father_agent:Step: 70, Training loss: 0.27548056840896606
INFO:agents.father_agent:Step: 75, Training loss: 0.30246293544769287
INFO:agents.father_agent:Step: 80, Training loss: 0.4510419964790344
INFO:agents.father_agent:Step: 85, Training loss: 0.4060567021369934
INFO:agents.father_agent:Step: 90, Training loss: 0.30100446939468384
INFO:agents.father_agent:Step: 95, Training loss: 0.3218556344509125
INFO:agents.father_agent:Step: 100, Training loss: 0.3306446969509125
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.593177795410156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.4068298339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.28575134277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2531.00048828125
INFO:tools.evaluation_results_class:Current Best Return = -37.986839294433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.818653810495086
INFO:tools.evaluation_results_class:Counted Episodes = 5393
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -53.25884246826172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.74114990234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.443115234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2453.84814453125
INFO:tools.evaluation_results_class:Current Best Return = -37.986839294433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.700981299759306
INFO:tools.evaluation_results_class:Counted Episodes = 5401
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.854026794433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.1459655761719
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.24960327148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2632.44287109375
INFO:tools.evaluation_results_class:Current Best Return = -55.854026794433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.59460458240946
INFO:tools.evaluation_results_class:Counted Episodes = 5412
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.17543029785156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.82456970214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.76206970214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4064.973876953125
INFO:tools.evaluation_results_class:Current Best Return = -74.17543029785156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.69568494441304
INFO:tools.evaluation_results_class:Counted Episodes = 5307
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 79.62477900129842
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 24 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -37.576419830322266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.423583984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 206.20156860351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1799.751953125
INFO:tools.evaluation_results_class:Current Best Return = -37.576419830322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.69607651766543
INFO:tools.evaluation_results_class:Counted Episodes = 5123
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.6435332894325256
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Step: 5, Training loss: 0.6312924027442932
INFO:agents.father_agent:Step: 10, Training loss: 0.6581045985221863
INFO:agents.father_agent:Step: 15, Training loss: 0.7677043676376343
INFO:agents.father_agent:Step: 20, Training loss: 0.727154016494751
INFO:agents.father_agent:Step: 25, Training loss: 0.6510680913925171
INFO:agents.father_agent:Step: 30, Training loss: 0.6292230486869812
INFO:agents.father_agent:Step: 35, Training loss: 0.5229057669639587
INFO:agents.father_agent:Step: 40, Training loss: 0.4175431430339813
INFO:agents.father_agent:Step: 45, Training loss: 0.4016290605068207
INFO:agents.father_agent:Step: 50, Training loss: 0.34419113397598267
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Step: 55, Training loss: 0.3050190508365631
INFO:agents.father_agent:Step: 60, Training loss: 0.2713877558708191
INFO:agents.father_agent:Step: 65, Training loss: 0.2544076144695282
INFO:agents.father_agent:Step: 70, Training loss: 0.24931016564369202
INFO:agents.father_agent:Step: 75, Training loss: 0.2211717665195465
INFO:agents.father_agent:Step: 80, Training loss: 0.2553008198738098
INFO:agents.father_agent:Step: 85, Training loss: 0.24506449699401855
INFO:agents.father_agent:Step: 90, Training loss: 0.329911470413208
INFO:agents.father_agent:Step: 95, Training loss: 0.40052446722984314
INFO:agents.father_agent:Step: 100, Training loss: 0.3928050994873047
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.0120735168457
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.9879150390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.00672912597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2478.494873046875
INFO:tools.evaluation_results_class:Current Best Return = -37.576419830322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.93744283885129
INFO:tools.evaluation_results_class:Counted Episodes = 5467
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -58.31608581542969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.68389892578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.80836486816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2526.857666015625
INFO:tools.evaluation_results_class:Current Best Return = -37.576419830322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.942430315175805
INFO:tools.evaluation_results_class:Counted Episodes = 5489
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.127201080322266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.872802734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.25555419921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2664.437744140625
INFO:tools.evaluation_results_class:Current Best Return = -58.127201080322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.6634004717837
INFO:tools.evaluation_results_class:Counted Episodes = 5511
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.5813980102539
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.41860961914062
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.20326232910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4098.95703125
INFO:tools.evaluation_results_class:Current Best Return = -76.5813980102539
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.15766343160593
INFO:tools.evaluation_results_class:Counted Episodes = 5461
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 84.18584396308236
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 25 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.72490310668945
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.27508544921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.18482971191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2564.55029296875
INFO:tools.evaluation_results_class:Current Best Return = -37.576419830322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.23288676821435
INFO:tools.evaluation_results_class:Counted Episodes = 5449
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.38555800914764404
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Step: 5, Training loss: 0.3964656591415405
INFO:agents.father_agent:Step: 10, Training loss: 0.4330929219722748
INFO:agents.father_agent:Step: 15, Training loss: 0.4940577745437622
INFO:agents.father_agent:Step: 20, Training loss: 0.4066663980484009
INFO:agents.father_agent:Step: 25, Training loss: 0.33597245812416077
INFO:agents.father_agent:Step: 30, Training loss: 0.3223446011543274
INFO:agents.father_agent:Step: 35, Training loss: 0.3144742250442505
INFO:agents.father_agent:Step: 40, Training loss: 0.3185190260410309
INFO:agents.father_agent:Step: 45, Training loss: 0.389446496963501
INFO:agents.father_agent:Step: 50, Training loss: 0.31407612562179565
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -58.48931121826172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.51068115234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.44825744628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2541.673828125
INFO:tools.evaluation_results_class:Current Best Return = -37.576419830322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.331920155449566
INFO:tools.evaluation_results_class:Counted Episodes = 5661
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -61.13107681274414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.8689270019531
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.31979370117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2691.823974609375
INFO:tools.evaluation_results_class:Current Best Return = -61.13107681274414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.06694855532065
INFO:tools.evaluation_results_class:Counted Episodes = 5676
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.33576202392578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.6642303466797
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.86668395996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4394.3388671875
INFO:tools.evaluation_results_class:Current Best Return = -82.33576202392578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.462673302523996
INFO:tools.evaluation_results_class:Counted Episodes = 5626
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 88.62175670560366
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 26 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.98914337158203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.01084899902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 130.0634307861328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3516.502685546875
INFO:tools.evaluation_results_class:Current Best Return = -37.576419830322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.94029850746269
INFO:tools.evaluation_results_class:Counted Episodes = 2948
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.7407776117324829
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 0.7882035970687866
INFO:agents.father_agent:Step: 10, Training loss: 1.0139052867889404
INFO:agents.father_agent:Step: 15, Training loss: 1.0112025737762451
INFO:agents.father_agent:Step: 20, Training loss: 1.0847338438034058
INFO:agents.father_agent:Step: 25, Training loss: 0.8398797512054443
INFO:agents.father_agent:Step: 30, Training loss: 0.7663730382919312
INFO:agents.father_agent:Step: 35, Training loss: 0.5333168506622314
INFO:agents.father_agent:Step: 40, Training loss: 0.4457056522369385
INFO:agents.father_agent:Step: 45, Training loss: 0.3748757839202881
INFO:agents.father_agent:Step: 50, Training loss: 0.22744615375995636
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Step: 55, Training loss: 0.2645533084869385
INFO:agents.father_agent:Step: 60, Training loss: 0.26688575744628906
INFO:agents.father_agent:Step: 65, Training loss: 0.39163821935653687
INFO:agents.father_agent:Step: 70, Training loss: 0.2815793752670288
INFO:agents.father_agent:Step: 75, Training loss: 0.39441022276878357
INFO:agents.father_agent:Step: 80, Training loss: 0.3604167103767395
INFO:agents.father_agent:Step: 85, Training loss: 0.3412705957889557
INFO:agents.father_agent:Step: 90, Training loss: 0.4148602783679962
INFO:agents.father_agent:Step: 95, Training loss: 0.28100666403770447
INFO:agents.father_agent:Step: 100, Training loss: 0.26513671875
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.36788558959961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.6321105957031
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.82595825195312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2340.2939453125
INFO:tools.evaluation_results_class:Current Best Return = -37.576419830322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.23991031390135
INFO:tools.evaluation_results_class:Counted Episodes = 5798
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -50.25402069091797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.7459716796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.98297119140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2327.723388671875
INFO:tools.evaluation_results_class:Current Best Return = -37.576419830322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.21251945357081
INFO:tools.evaluation_results_class:Counted Episodes = 5783
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.311946868896484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.68804931640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.0676727294922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2453.098388671875
INFO:tools.evaluation_results_class:Current Best Return = -52.311946868896484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.215363273799206
INFO:tools.evaluation_results_class:Counted Episodes = 5767
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.45555114746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.54444885253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.55203247070312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4238.625
INFO:tools.evaluation_results_class:Current Best Return = -73.45555114746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.790789003927166
INFO:tools.evaluation_results_class:Counted Episodes = 5602
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 75.20214481271458
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -26.105802536010742
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.8941955566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.93338012695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 850.138427734375
INFO:tools.evaluation_results_class:Current Best Return = -26.105802536010742
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.17374380750177
INFO:tools.evaluation_results_class:Counted Episodes = 2826
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.34025192260742
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.6597595214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.4324951171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2343.45947265625
INFO:tools.evaluation_results_class:Current Best Return = -52.34025192260742
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.780679785330946
INFO:tools.evaluation_results_class:Counted Episodes = 2795
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -45.589752197265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.4102478027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.67803955078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2176.6845703125
INFO:tools.evaluation_results_class:Current Best Return = -45.589752197265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.88067400275103
INFO:tools.evaluation_results_class:Counted Episodes = 2908
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -40.15610885620117
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.8439025878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.86257934570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1968.8841552734375
INFO:tools.evaluation_results_class:Current Best Return = -40.15610885620117
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.545327266363316
INFO:tools.evaluation_results_class:Counted Episodes = 2857
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.95838928222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.0415954589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.73382568359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2369.501708984375
INFO:tools.evaluation_results_class:Current Best Return = -52.95838928222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.059639389736475
INFO:tools.evaluation_results_class:Counted Episodes = 2884
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.65423583984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.34576416015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.02432250976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2402.50927734375
INFO:tools.evaluation_results_class:Current Best Return = -53.65423583984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.42541636495293
INFO:tools.evaluation_results_class:Counted Episodes = 2762
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.497108459472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.5028991699219
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.39495849609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2386.542724609375
INFO:tools.evaluation_results_class:Current Best Return = -53.497108459472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.1849710982659
INFO:tools.evaluation_results_class:Counted Episodes = 2768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.70122146606445
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.29876708984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.92556762695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2453.59326171875
INFO:tools.evaluation_results_class:Current Best Return = -53.70122146606445
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.5326352530541
INFO:tools.evaluation_results_class:Counted Episodes = 2865
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.40574645996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.5942687988281
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.23300170898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2489.1015625
INFO:tools.evaluation_results_class:Current Best Return = -54.40574645996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.651016117729505
INFO:tools.evaluation_results_class:Counted Episodes = 2854
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.09926223754883
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.9007263183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.77127075195312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2469.57177734375
INFO:tools.evaluation_results_class:Current Best Return = -53.09926223754883
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.824272185198176
INFO:tools.evaluation_results_class:Counted Episodes = 2851
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.645591735839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.3544006347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.3319549560547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2521.655517578125
INFO:tools.evaluation_results_class:Current Best Return = -54.645591735839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.88795223041799
INFO:tools.evaluation_results_class:Counted Episodes = 2847
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -33.762123107910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 286.2378845214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.38783264160156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1624.8917236328125
INFO:tools.evaluation_results_class:Current Best Return = -33.762123107910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.812719606465215
INFO:tools.evaluation_results_class:Counted Episodes = 2846
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.42675018310547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.5732421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.1685028076172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2542.9990234375
INFO:tools.evaluation_results_class:Current Best Return = -54.42675018310547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.210898796886056
INFO:tools.evaluation_results_class:Counted Episodes = 2826
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.84471130371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.1552734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.6015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2501.287109375
INFO:tools.evaluation_results_class:Current Best Return = -52.84471130371094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.17297488503714
INFO:tools.evaluation_results_class:Counted Episodes = 2827
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -35.111148834228516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.88885498046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.09506225585938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1688.0418701171875
INFO:tools.evaluation_results_class:Current Best Return = -35.111148834228516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.904326415758
INFO:tools.evaluation_results_class:Counted Episodes = 2843
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.52939224243164
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.4706115722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.94195556640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2518.40869140625
INFO:tools.evaluation_results_class:Current Best Return = -53.52939224243164
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.202903682719544
INFO:tools.evaluation_results_class:Counted Episodes = 2824
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.605525970458984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.39447021484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.02914428710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2543.445068359375
INFO:tools.evaluation_results_class:Current Best Return = -53.605525970458984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.11260623229462
INFO:tools.evaluation_results_class:Counted Episodes = 2824
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.3484992980957
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.6514892578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.35568237304688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2608.808349609375
INFO:tools.evaluation_results_class:Current Best Return = -54.3484992980957
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.11393298059965
INFO:tools.evaluation_results_class:Counted Episodes = 2835
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.01063537597656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.9893798828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.86749267578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2620.384765625
INFO:tools.evaluation_results_class:Current Best Return = -54.01063537597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.937256292095
INFO:tools.evaluation_results_class:Counted Episodes = 2821
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.22719192504883
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.7727966308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.53250122070312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2572.208251953125
INFO:tools.evaluation_results_class:Current Best Return = -52.22719192504883
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.822472701655514
INFO:tools.evaluation_results_class:Counted Episodes = 2839
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.88081741333008
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.1191711425781
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.78265380859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2479.271484375
INFO:tools.evaluation_results_class:Current Best Return = -52.88081741333008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.98095909732017
INFO:tools.evaluation_results_class:Counted Episodes = 2836
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.88173294067383
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.1182556152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.8130645751953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2524.402099609375
INFO:tools.evaluation_results_class:Current Best Return = -52.88173294067383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.958465329109465
INFO:tools.evaluation_results_class:Counted Episodes = 2841
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.042240142822266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.957763671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.98631286621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2504.33837890625
INFO:tools.evaluation_results_class:Current Best Return = -53.042240142822266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.700809574093626
INFO:tools.evaluation_results_class:Counted Episodes = 2841
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.581199645996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.4187927246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.49191284179688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2559.531982421875
INFO:tools.evaluation_results_class:Current Best Return = -53.581199645996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.691687127323746
INFO:tools.evaluation_results_class:Counted Episodes = 2851
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.59494400024414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.4050598144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.5560302734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2615.381591796875
INFO:tools.evaluation_results_class:Current Best Return = -55.59494400024414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.86556686556686
INFO:tools.evaluation_results_class:Counted Episodes = 2849
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.316741943359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.6832580566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.21463012695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2541.14306640625
INFO:tools.evaluation_results_class:Current Best Return = -53.316741943359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.30804037591368
INFO:tools.evaluation_results_class:Counted Episodes = 2873
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.48530960083008
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.5146789550781
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.1148223876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2588.681884765625
INFO:tools.evaluation_results_class:Current Best Return = -53.48530960083008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.10867256637168
INFO:tools.evaluation_results_class:Counted Episodes = 2825
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.78781509399414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.2121887207031
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.16567993164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2550.231201171875
INFO:tools.evaluation_results_class:Current Best Return = -54.78781509399414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.92927170868347
INFO:tools.evaluation_results_class:Counted Episodes = 2856
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.70763397216797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.2923583984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.09283447265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2540.842041015625
INFO:tools.evaluation_results_class:Current Best Return = -51.70763397216797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.72233893557423
INFO:tools.evaluation_results_class:Counted Episodes = 2856
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.117008209228516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.88299560546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.6959991455078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2489.613525390625
INFO:tools.evaluation_results_class:Current Best Return = -52.117008209228516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.74736472241743
INFO:tools.evaluation_results_class:Counted Episodes = 2846
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.645816802978516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.35418701171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.38938903808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2508.5849609375
INFO:tools.evaluation_results_class:Current Best Return = -54.645816802978516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.84961349262122
INFO:tools.evaluation_results_class:Counted Episodes = 2846
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -36.41487503051758
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.5851135253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.1345672607422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1584.220947265625
INFO:tools.evaluation_results_class:Current Best Return = -36.41487503051758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.79475027342326
INFO:tools.evaluation_results_class:Counted Episodes = 2743
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.93370056152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.0662841796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.0697784423828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1931.8004150390625
INFO:tools.evaluation_results_class:Current Best Return = -43.93370056152344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.242725598526704
INFO:tools.evaluation_results_class:Counted Episodes = 2715
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -61.764976501464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.2350158691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.50123596191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3808.74072265625
INFO:tools.evaluation_results_class:Current Best Return = -61.764976501464844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.06941508104299
INFO:tools.evaluation_results_class:Counted Episodes = 2838
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.2966423034668
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.703369140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.18113708496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3710.003662109375
INFO:tools.evaluation_results_class:Current Best Return = -59.2966423034668
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.63831308077198
INFO:tools.evaluation_results_class:Counted Episodes = 2798
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -41.829322814941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.1706848144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.53504943847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1794.0845947265625
INFO:tools.evaluation_results_class:Current Best Return = -41.829322814941406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.606856309263314
INFO:tools.evaluation_results_class:Counted Episodes = 2742
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -41.46477127075195
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.53521728515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.29107666015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1758.3714599609375
INFO:tools.evaluation_results_class:Current Best Return = -41.46477127075195
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.358539284396905
INFO:tools.evaluation_results_class:Counted Episodes = 2711
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -41.449031829833984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.55096435546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 206.8336181640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1742.16015625
INFO:tools.evaluation_results_class:Current Best Return = -41.449031829833984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.83668154761905
INFO:tools.evaluation_results_class:Counted Episodes = 2688
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.87266540527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.12733459472656
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.99789428710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3787.9130859375
INFO:tools.evaluation_results_class:Current Best Return = -72.87266540527344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.99964131994261
INFO:tools.evaluation_results_class:Counted Episodes = 2788
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.38159942626953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.61839294433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.7255401611328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3878.953369140625
INFO:tools.evaluation_results_class:Current Best Return = -72.38159942626953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.847646424721525
INFO:tools.evaluation_results_class:Counted Episodes = 2783
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.20270538330078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.7972869873047
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.8956756591797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4344.4697265625
INFO:tools.evaluation_results_class:Current Best Return = -74.20270538330078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.76024225151407
INFO:tools.evaluation_results_class:Counted Episodes = 2807
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.72051239013672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.27947998046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.4717254638672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4459.02978515625
INFO:tools.evaluation_results_class:Current Best Return = -77.72051239013672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.013581129378125
INFO:tools.evaluation_results_class:Counted Episodes = 2798
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.01068115234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.98931884765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.92396545410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4819.19775390625
INFO:tools.evaluation_results_class:Current Best Return = -71.01068115234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.382342470630114
INFO:tools.evaluation_results_class:Counted Episodes = 2809
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.50853729248047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.49147033691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.6051483154297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4236.44873046875
INFO:tools.evaluation_results_class:Current Best Return = -72.50853729248047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.56543385490754
INFO:tools.evaluation_results_class:Counted Episodes = 2812
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.4989242553711
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.50106811523438
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.7206573486328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4389.9931640625
INFO:tools.evaluation_results_class:Current Best Return = -75.4989242553711
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.84061604584527
INFO:tools.evaluation_results_class:Counted Episodes = 2792
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.73995971679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.26004028320312
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.3065643310547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4719.91748046875
INFO:tools.evaluation_results_class:Current Best Return = -68.73995971679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.97596843615495
INFO:tools.evaluation_results_class:Counted Episodes = 2788
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.1676254272461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.83238220214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.06150817871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4249.8623046875
INFO:tools.evaluation_results_class:Current Best Return = -75.1676254272461
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.70630372492837
INFO:tools.evaluation_results_class:Counted Episodes = 2792
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.3003921508789
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.69960021972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.04161071777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4397.16650390625
INFO:tools.evaluation_results_class:Current Best Return = -77.3003921508789
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.82885785893305
INFO:tools.evaluation_results_class:Counted Episodes = 2793
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.85082244873047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.14918518066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.2805633544922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4216.9052734375
INFO:tools.evaluation_results_class:Current Best Return = -73.85082244873047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.68807994289793
INFO:tools.evaluation_results_class:Counted Episodes = 2802
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.01502227783203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.9849853515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.1175537109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4402.9306640625
INFO:tools.evaluation_results_class:Current Best Return = -74.01502227783203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.71316165951359
INFO:tools.evaluation_results_class:Counted Episodes = 2796
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.9942855834961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.00572204589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.12721252441406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4197.517578125
INFO:tools.evaluation_results_class:Current Best Return = -73.9942855834961
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.724187209717755
INFO:tools.evaluation_results_class:Counted Episodes = 2799
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.3885269165039
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.61146545410156
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.45423889160156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4110.32568359375
INFO:tools.evaluation_results_class:Current Best Return = -72.3885269165039
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.848028673835124
INFO:tools.evaluation_results_class:Counted Episodes = 2790
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.31800079345703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.68199157714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.99014282226562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4189.3662109375
INFO:tools.evaluation_results_class:Current Best Return = -75.31800079345703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.67023172905526
INFO:tools.evaluation_results_class:Counted Episodes = 2805
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.71174621582031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.2882537841797
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.7667694091797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4278.17431640625
INFO:tools.evaluation_results_class:Current Best Return = -73.71174621582031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.32170818505338
INFO:tools.evaluation_results_class:Counted Episodes = 2810
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.60276794433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.39723205566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.89166259765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4238.79541015625
INFO:tools.evaluation_results_class:Current Best Return = -73.60276794433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.41178558750444
INFO:tools.evaluation_results_class:Counted Episodes = 2817
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.3801040649414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.61988830566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.18475341796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4311.00732421875
INFO:tools.evaluation_results_class:Current Best Return = -73.3801040649414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.26429840142096
INFO:tools.evaluation_results_class:Counted Episodes = 2815
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.50289154052734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.4971160888672
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.23272705078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4372.28271484375
INFO:tools.evaluation_results_class:Current Best Return = -74.50289154052734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.1242774566474
INFO:tools.evaluation_results_class:Counted Episodes = 2768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.36781311035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.63218688964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.7770233154297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4451.70751953125
INFO:tools.evaluation_results_class:Current Best Return = -76.36781311035156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.01149425287356
INFO:tools.evaluation_results_class:Counted Episodes = 2784
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.80049896240234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.1995086669922
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.50665283203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4383.265625
INFO:tools.evaluation_results_class:Current Best Return = -75.80049896240234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.744210901318134
INFO:tools.evaluation_results_class:Counted Episodes = 2807
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.22534942626953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.774658203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.4032440185547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4278.63427734375
INFO:tools.evaluation_results_class:Current Best Return = -74.22534942626953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.30758276966892
INFO:tools.evaluation_results_class:Counted Episodes = 2809
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.14392852783203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.85606384277344
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.26942443847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4483.291015625
INFO:tools.evaluation_results_class:Current Best Return = -76.14392852783203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.689223057644114
INFO:tools.evaluation_results_class:Counted Episodes = 2793
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.93535614013672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.06463623046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.29043579101562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4254.51123046875
INFO:tools.evaluation_results_class:Current Best Return = -73.93535614013672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.6075
INFO:tools.evaluation_results_class:Counted Episodes = 2800
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 27 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.97246551513672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.02752685546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.30584716796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3425.964111328125
INFO:tools.evaluation_results_class:Current Best Return = -37.576419830322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.06947730694773
INFO:tools.evaluation_results_class:Counted Episodes = 4649
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.8864547610282898
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 1.1936427354812622
INFO:agents.father_agent:Step: 10, Training loss: 1.097455620765686
INFO:agents.father_agent:Step: 15, Training loss: 0.9813317656517029
INFO:agents.father_agent:Step: 20, Training loss: 0.9769643545150757
INFO:agents.father_agent:Step: 25, Training loss: 0.8808803558349609
INFO:agents.father_agent:Step: 30, Training loss: 0.7565428018569946
INFO:agents.father_agent:Step: 35, Training loss: 0.6308713555335999
INFO:agents.father_agent:Step: 40, Training loss: 0.6279100179672241
INFO:agents.father_agent:Step: 45, Training loss: 0.5431492328643799
INFO:agents.father_agent:Step: 50, Training loss: 0.38278061151504517
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 55, Training loss: 0.31922754645347595
INFO:agents.father_agent:Step: 60, Training loss: 0.3221977651119232
INFO:agents.father_agent:Step: 65, Training loss: 0.29911983013153076
INFO:agents.father_agent:Step: 70, Training loss: 0.2914137840270996
INFO:agents.father_agent:Step: 75, Training loss: 0.30876654386520386
INFO:agents.father_agent:Step: 80, Training loss: 0.27458930015563965
INFO:agents.father_agent:Step: 85, Training loss: 0.33339715003967285
INFO:agents.father_agent:Step: 90, Training loss: 0.2135029435157776
INFO:agents.father_agent:Step: 95, Training loss: 0.2186858355998993
INFO:agents.father_agent:Step: 100, Training loss: 0.24907515943050385
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.7717170715332
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.228271484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.40567016601562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2415.589599609375
INFO:tools.evaluation_results_class:Current Best Return = -37.576419830322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.38425063505504
INFO:tools.evaluation_results_class:Counted Episodes = 5905
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -50.930442810058594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.0695495605469
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.69882202148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2384.65234375
INFO:tools.evaluation_results_class:Current Best Return = -37.576419830322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.81274627377077
INFO:tools.evaluation_results_class:Counted Episodes = 5837
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.911766052246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.0882263183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.8053436279297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2554.642333984375
INFO:tools.evaluation_results_class:Current Best Return = -51.911766052246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.82672268907563
INFO:tools.evaluation_results_class:Counted Episodes = 5950
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.64201354980469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.3579864501953
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.2800750732422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4372.130859375
INFO:tools.evaluation_results_class:Current Best Return = -72.64201354980469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.02668017561635
INFO:tools.evaluation_results_class:Counted Episodes = 5922
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 76.61244190809339
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 28 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.85956573486328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.14044189453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.14491271972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4097.11181640625
INFO:tools.evaluation_results_class:Current Best Return = -37.576419830322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.46175587352058
INFO:tools.evaluation_results_class:Counted Episodes = 5661
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.606123685836792
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 0.6907529234886169
INFO:agents.father_agent:Step: 10, Training loss: 0.6897429823875427
INFO:agents.father_agent:Step: 15, Training loss: 0.618886411190033
INFO:agents.father_agent:Step: 20, Training loss: 0.6425232887268066
INFO:agents.father_agent:Step: 25, Training loss: 0.6555944681167603
INFO:agents.father_agent:Step: 30, Training loss: 0.6980053782463074
INFO:agents.father_agent:Step: 35, Training loss: 0.5251961350440979
INFO:agents.father_agent:Step: 40, Training loss: 0.3749381899833679
INFO:agents.father_agent:Step: 45, Training loss: 0.3863420784473419
INFO:agents.father_agent:Step: 50, Training loss: 0.36111772060394287
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Step: 55, Training loss: 0.2827029526233673
INFO:agents.father_agent:Step: 60, Training loss: 0.26897817850112915
INFO:agents.father_agent:Step: 65, Training loss: 0.25684577226638794
INFO:agents.father_agent:Step: 70, Training loss: 0.22280961275100708
INFO:agents.father_agent:Step: 75, Training loss: 0.2197137176990509
INFO:agents.father_agent:Step: 80, Training loss: 0.20030513405799866
INFO:agents.father_agent:Step: 85, Training loss: 0.14821426570415497
INFO:agents.father_agent:Step: 90, Training loss: 0.2304438352584839
INFO:agents.father_agent:Step: 95, Training loss: 0.21644127368927002
INFO:agents.father_agent:Step: 100, Training loss: 0.2848653793334961
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.8633918762207
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.1365966796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.127685546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2378.68798828125
INFO:tools.evaluation_results_class:Current Best Return = -37.576419830322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.82017857142857
INFO:tools.evaluation_results_class:Counted Episodes = 5600
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -52.69215774536133
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.3078308105469
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.90603637695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2345.4296875
INFO:tools.evaluation_results_class:Current Best Return = -37.576419830322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.10519823788546
INFO:tools.evaluation_results_class:Counted Episodes = 5675
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.865257263183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.1347351074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.72116088867188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2419.653076171875
INFO:tools.evaluation_results_class:Current Best Return = -53.865257263183594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.19611992945326
INFO:tools.evaluation_results_class:Counted Episodes = 5670
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.81228637695312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.18771362304688
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.6214141845703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4227.57373046875
INFO:tools.evaluation_results_class:Current Best Return = -73.81228637695312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.3743450767841
INFO:tools.evaluation_results_class:Counted Episodes = 5535
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 77.96958968620257
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 29 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -32.588890075683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.4111022949219
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.95480346679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1298.4154052734375
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.747969015681086
INFO:tools.evaluation_results_class:Counted Episodes = 5293
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.5166385769844055
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 0.673029363155365
INFO:agents.father_agent:Step: 10, Training loss: 0.775932788848877
INFO:agents.father_agent:Step: 15, Training loss: 0.8581385612487793
INFO:agents.father_agent:Step: 20, Training loss: 0.6914725303649902
INFO:agents.father_agent:Step: 25, Training loss: 0.6742022037506104
INFO:agents.father_agent:Step: 30, Training loss: 0.6318590641021729
INFO:agents.father_agent:Step: 35, Training loss: 0.48310714960098267
INFO:agents.father_agent:Step: 40, Training loss: 0.4249015152454376
INFO:agents.father_agent:Step: 45, Training loss: 0.2987145781517029
INFO:agents.father_agent:Step: 50, Training loss: 0.2713804244995117
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 55, Training loss: 0.17600877583026886
INFO:agents.father_agent:Step: 60, Training loss: 0.2553321123123169
INFO:agents.father_agent:Step: 65, Training loss: 0.29703760147094727
INFO:agents.father_agent:Step: 70, Training loss: 0.32207950949668884
INFO:agents.father_agent:Step: 75, Training loss: 0.3310856521129608
INFO:agents.father_agent:Step: 80, Training loss: 0.32388177514076233
INFO:agents.father_agent:Step: 85, Training loss: 0.281442254781723
INFO:agents.father_agent:Step: 90, Training loss: 0.3958897888660431
INFO:agents.father_agent:Step: 95, Training loss: 0.3740795850753784
INFO:agents.father_agent:Step: 100, Training loss: 0.4576321244239807
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.92658996582031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.0733947753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.367919921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2409.72802734375
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.88107460643395
INFO:tools.evaluation_results_class:Counted Episodes = 5844
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -56.012596130371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.9873962402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.88601684570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2453.884521484375
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.371914893617024
INFO:tools.evaluation_results_class:Counted Episodes = 5875
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.311737060546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.6882629394531
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.87022399902344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2553.1708984375
INFO:tools.evaluation_results_class:Current Best Return = -57.311737060546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.135487144790254
INFO:tools.evaluation_results_class:Counted Episodes = 5912
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.6644058227539
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.33558654785156
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.6780242919922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4440.99072265625
INFO:tools.evaluation_results_class:Current Best Return = -80.6644058227539
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.308841410456836
INFO:tools.evaluation_results_class:Counted Episodes = 5757
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 84.0158381817758
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 30 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -33.98585510253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 286.0141296386719
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.39942932128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 304.3731384277344
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.68388157894736
INFO:tools.evaluation_results_class:Counted Episodes = 3040
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.27961039543151855
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 0.34860390424728394
INFO:agents.father_agent:Step: 10, Training loss: 0.48092323541641235
INFO:agents.father_agent:Step: 15, Training loss: 0.5157514810562134
INFO:agents.father_agent:Step: 20, Training loss: 0.6630391478538513
INFO:agents.father_agent:Step: 25, Training loss: 0.6370297074317932
INFO:agents.father_agent:Step: 30, Training loss: 0.577450156211853
INFO:agents.father_agent:Step: 35, Training loss: 0.516926109790802
INFO:agents.father_agent:Step: 40, Training loss: 0.4996449947357178
INFO:agents.father_agent:Step: 45, Training loss: 0.4713183343410492
INFO:agents.father_agent:Step: 50, Training loss: 0.47552192211151123
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 55, Training loss: 0.451158344745636
INFO:agents.father_agent:Step: 60, Training loss: 0.42097651958465576
INFO:agents.father_agent:Step: 65, Training loss: 0.4320752024650574
INFO:agents.father_agent:Step: 70, Training loss: 0.3609253168106079
INFO:agents.father_agent:Step: 75, Training loss: 0.32054680585861206
INFO:agents.father_agent:Step: 80, Training loss: 0.2894473075866699
INFO:agents.father_agent:Step: 85, Training loss: 0.15971563756465912
INFO:agents.father_agent:Step: 90, Training loss: 0.2323838174343109
INFO:agents.father_agent:Step: 95, Training loss: 0.1918725222349167
INFO:agents.father_agent:Step: 100, Training loss: 0.17391273379325867
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.34754943847656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.6524353027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.11634826660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2295.21142578125
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.73809523809524
INFO:tools.evaluation_results_class:Counted Episodes = 5838
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 6.25%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -46.46955871582031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.53045654296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.05259704589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2177.396240234375
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.51547619047619
INFO:tools.evaluation_results_class:Counted Episodes = 5880
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.654170989990234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.3458251953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.8070068359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2386.31982421875
INFO:tools.evaluation_results_class:Current Best Return = -49.654170989990234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.80498710232158
INFO:tools.evaluation_results_class:Counted Episodes = 5815
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.35506439208984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.6449432373047
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.91094970703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4175.33447265625
INFO:tools.evaluation_results_class:Current Best Return = -69.35506439208984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.23578363384188
INFO:tools.evaluation_results_class:Counted Episodes = 5768
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 72.5776753902226
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 31 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.3267822265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.6732177734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.626953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2970.567138671875
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.98919237571232
INFO:tools.evaluation_results_class:Counted Episodes = 5089
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.47036242485046387
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 0.5925025939941406
INFO:agents.father_agent:Step: 10, Training loss: 0.5798836946487427
INFO:agents.father_agent:Step: 15, Training loss: 0.5023476481437683
INFO:agents.father_agent:Step: 20, Training loss: 0.5078539848327637
INFO:agents.father_agent:Step: 25, Training loss: 0.49481308460235596
INFO:agents.father_agent:Step: 30, Training loss: 0.4505474269390106
INFO:agents.father_agent:Step: 35, Training loss: 0.5329273343086243
INFO:agents.father_agent:Step: 40, Training loss: 0.4419364035129547
INFO:agents.father_agent:Step: 45, Training loss: 0.5028197765350342
INFO:agents.father_agent:Step: 50, Training loss: 0.38124117255210876
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 55, Training loss: 0.35071027278900146
INFO:agents.father_agent:Step: 60, Training loss: 0.30277711153030396
INFO:agents.father_agent:Step: 65, Training loss: 0.30581843852996826
INFO:agents.father_agent:Step: 70, Training loss: 0.26872551441192627
INFO:agents.father_agent:Step: 75, Training loss: 0.28797411918640137
INFO:agents.father_agent:Step: 80, Training loss: 0.30248749256134033
INFO:agents.father_agent:Step: 85, Training loss: 0.26109927892684937
INFO:agents.father_agent:Step: 90, Training loss: 0.2016354501247406
INFO:agents.father_agent:Step: 95, Training loss: 0.22708962857723236
INFO:agents.father_agent:Step: 100, Training loss: 0.24066877365112305
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.29729080200195
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.70269775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.28543090820312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2486.02294921875
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.587093478631026
INFO:tools.evaluation_results_class:Counted Episodes = 5873
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -53.3646354675293
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.6353759765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.0718536376953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2412.2080078125
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.47935702199661
INFO:tools.evaluation_results_class:Counted Episodes = 5910
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.37017059326172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.62982177734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.07644653320312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2563.474609375
INFO:tools.evaluation_results_class:Current Best Return = -55.37017059326172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.690628149143436
INFO:tools.evaluation_results_class:Counted Episodes = 5954
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.92815399169922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.07183837890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.1908721923828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4562.453125
INFO:tools.evaluation_results_class:Current Best Return = -80.92815399169922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.67514609831557
INFO:tools.evaluation_results_class:Counted Episodes = 5818
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 82.74852494655488
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -31.146997451782227
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 288.8529968261719
INFO:tools.evaluation_results_class:Average Discounted Reward = 219.9211883544922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1260.0032958984375
INFO:tools.evaluation_results_class:Current Best Return = -31.146997451782227
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.04830917874396
INFO:tools.evaluation_results_class:Counted Episodes = 2898
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.23125457763672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.76873779296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.0865478515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2420.0244140625
INFO:tools.evaluation_results_class:Current Best Return = -56.23125457763672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.871758934828314
INFO:tools.evaluation_results_class:Counted Episodes = 2854
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.692359924316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.3076477050781
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.27366638183594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2378.28564453125
INFO:tools.evaluation_results_class:Current Best Return = -49.692359924316406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.84214069336924
INFO:tools.evaluation_results_class:Counted Episodes = 2971
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -41.44844055175781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.5515441894531
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.12986755371094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2050.072509765625
INFO:tools.evaluation_results_class:Current Best Return = -41.44844055175781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.390202124015076
INFO:tools.evaluation_results_class:Counted Episodes = 2919
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.938350677490234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.0616455078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.98062133789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2365.735107421875
INFO:tools.evaluation_results_class:Current Best Return = -54.938350677490234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.20606267029973
INFO:tools.evaluation_results_class:Counted Episodes = 2936
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.34773254394531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.6522521972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.7867431640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2183.13525390625
INFO:tools.evaluation_results_class:Current Best Return = -48.34773254394531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.274079320113316
INFO:tools.evaluation_results_class:Counted Episodes = 2824
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.119956970214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.8800354003906
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.3247528076172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2191.194580078125
INFO:tools.evaluation_results_class:Current Best Return = -50.119956970214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.24168435951876
INFO:tools.evaluation_results_class:Counted Episodes = 2826
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.82253646850586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.1774597167969
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.0524139404297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2302.259521484375
INFO:tools.evaluation_results_class:Current Best Return = -50.82253646850586
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.89696760854583
INFO:tools.evaluation_results_class:Counted Episodes = 2902
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.10470199584961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.8952941894531
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.04835510253906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2374.3203125
INFO:tools.evaluation_results_class:Current Best Return = -52.10470199584961
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.79780295228287
INFO:tools.evaluation_results_class:Counted Episodes = 2913
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.75526428222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.2447509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.3922882080078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2492.780029296875
INFO:tools.evaluation_results_class:Current Best Return = -55.75526428222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.92026234035209
INFO:tools.evaluation_results_class:Counted Episodes = 2897
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.240684509277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.7593078613281
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.45106506347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2543.996826171875
INFO:tools.evaluation_results_class:Current Best Return = -56.240684509277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.50632478632479
INFO:tools.evaluation_results_class:Counted Episodes = 2925
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.37569046020508
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.6242980957031
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.76803588867188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2386.254638671875
INFO:tools.evaluation_results_class:Current Best Return = -48.37569046020508
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.753787878787875
INFO:tools.evaluation_results_class:Counted Episodes = 2904
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.41128158569336
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.5887145996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.3275146484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2517.232177734375
INFO:tools.evaluation_results_class:Current Best Return = -56.41128158569336
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.50529914529915
INFO:tools.evaluation_results_class:Counted Episodes = 2925
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.80508041381836
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.1949157714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.62559509277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2545.531005859375
INFO:tools.evaluation_results_class:Current Best Return = -55.80508041381836
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.71310912834591
INFO:tools.evaluation_results_class:Counted Episodes = 2914
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.6654052734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.3345947265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.44638061523438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2433.71044921875
INFO:tools.evaluation_results_class:Current Best Return = -48.6654052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.82290233837689
INFO:tools.evaluation_results_class:Counted Episodes = 2908
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.37693786621094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.6230773925781
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.2185821533203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2585.206298828125
INFO:tools.evaluation_results_class:Current Best Return = -57.37693786621094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.732874354561105
INFO:tools.evaluation_results_class:Counted Episodes = 2905
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.100379943847656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.8996276855469
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.11831665039062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2476.31103515625
INFO:tools.evaluation_results_class:Current Best Return = -56.100379943847656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.00034494653329
INFO:tools.evaluation_results_class:Counted Episodes = 2899
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.752132415771484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.24786376953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.7506866455078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2561.49755859375
INFO:tools.evaluation_results_class:Current Best Return = -55.752132415771484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.634687606691706
INFO:tools.evaluation_results_class:Counted Episodes = 2929
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.811397552490234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.1885986328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.31295776367188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2606.05419921875
INFO:tools.evaluation_results_class:Current Best Return = -56.811397552490234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.10742659758204
INFO:tools.evaluation_results_class:Counted Episodes = 2895
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.67487335205078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.32513427734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.14208984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2600.0732421875
INFO:tools.evaluation_results_class:Current Best Return = -56.67487335205078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.47555555555556
INFO:tools.evaluation_results_class:Counted Episodes = 2925
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.335514068603516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.66448974609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.06065368652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2524.745849609375
INFO:tools.evaluation_results_class:Current Best Return = -56.335514068603516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.83258697898725
INFO:tools.evaluation_results_class:Counted Episodes = 2903
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.731075286865234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.2689208984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.32427978515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2596.9833984375
INFO:tools.evaluation_results_class:Current Best Return = -55.731075286865234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.99930867611476
INFO:tools.evaluation_results_class:Counted Episodes = 2893
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.24880599975586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.7511901855469
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.83465576171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2522.780517578125
INFO:tools.evaluation_results_class:Current Best Return = -57.24880599975586
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.31152010906612
INFO:tools.evaluation_results_class:Counted Episodes = 2934
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.873321533203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.1266784667969
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.52035522460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2544.978759765625
INFO:tools.evaluation_results_class:Current Best Return = -56.873321533203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.87435456110155
INFO:tools.evaluation_results_class:Counted Episodes = 2905
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.78871154785156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.2112731933594
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.6934814453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2587.174072265625
INFO:tools.evaluation_results_class:Current Best Return = -57.78871154785156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.874053682037164
INFO:tools.evaluation_results_class:Counted Episodes = 2906
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.112674713134766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.8873291015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.60882568359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2509.623046875
INFO:tools.evaluation_results_class:Current Best Return = -57.112674713134766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.60048093438681
INFO:tools.evaluation_results_class:Counted Episodes = 2911
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.857975006103516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.14202880859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.67739868164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2559.993896484375
INFO:tools.evaluation_results_class:Current Best Return = -55.857975006103516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.572553045859
INFO:tools.evaluation_results_class:Counted Episodes = 2922
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.53461837768555
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.46539306640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.7935791015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2533.24072265625
INFO:tools.evaluation_results_class:Current Best Return = -56.53461837768555
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.91319324836376
INFO:tools.evaluation_results_class:Counted Episodes = 2903
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.86163330078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.13836669921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.788818359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2529.14892578125
INFO:tools.evaluation_results_class:Current Best Return = -56.86163330078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.6043730782371
INFO:tools.evaluation_results_class:Counted Episodes = 2927
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.49571228027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.5043029785156
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.9427947998047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2530.088134765625
INFO:tools.evaluation_results_class:Current Best Return = -56.49571228027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.7663807890223
INFO:tools.evaluation_results_class:Counted Episodes = 2915
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.718719482421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.2812805175781
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.77606201171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2607.841552734375
INFO:tools.evaluation_results_class:Current Best Return = -56.718719482421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.76387452602551
INFO:tools.evaluation_results_class:Counted Episodes = 2901
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.48845291137695
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.51153564453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.70213317871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2370.21142578125
INFO:tools.evaluation_results_class:Current Best Return = -51.48845291137695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.6883833160979
INFO:tools.evaluation_results_class:Counted Episodes = 2901
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.664039611816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.3359680175781
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.0554962158203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2550.639404296875
INFO:tools.evaluation_results_class:Current Best Return = -56.664039611816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.529310935893044
INFO:tools.evaluation_results_class:Counted Episodes = 2917
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.23987579345703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.7601318359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.81529235839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2594.9736328125
INFO:tools.evaluation_results_class:Current Best Return = -56.23987579345703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.120110764970576
INFO:tools.evaluation_results_class:Counted Episodes = 2889
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.62200164794922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.37799072265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.28932189941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2574.2509765625
INFO:tools.evaluation_results_class:Current Best Return = -57.62200164794922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.462302947224124
INFO:tools.evaluation_results_class:Counted Episodes = 2918
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.019126892089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.9808654785156
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.7286376953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2560.837646484375
INFO:tools.evaluation_results_class:Current Best Return = -57.019126892089844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.6051912568306
INFO:tools.evaluation_results_class:Counted Episodes = 2928
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -35.727882385253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.2721252441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.5858154296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1534.7889404296875
INFO:tools.evaluation_results_class:Current Best Return = -35.727882385253906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.16525123849964
INFO:tools.evaluation_results_class:Counted Episodes = 2826
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -46.84132766723633
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.1586608886719
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.93313598632812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2075.96923828125
INFO:tools.evaluation_results_class:Current Best Return = -46.84132766723633
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.16290550070522
INFO:tools.evaluation_results_class:Counted Episodes = 2836
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.90713500976562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.09286499023438
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.26197814941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4213.005859375
INFO:tools.evaluation_results_class:Current Best Return = -69.90713500976562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.14830214830215
INFO:tools.evaluation_results_class:Counted Episodes = 2886
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.27855682373047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.721435546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.18637084960938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4286.04443359375
INFO:tools.evaluation_results_class:Current Best Return = -66.27855682373047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.680960548885075
INFO:tools.evaluation_results_class:Counted Episodes = 2915
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -46.76495361328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.23504638671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.46279907226562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2068.15185546875
INFO:tools.evaluation_results_class:Current Best Return = -46.76495361328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.69220006995453
INFO:tools.evaluation_results_class:Counted Episodes = 2859
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.28849792480469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.7115173339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.25189208984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1857.390869140625
INFO:tools.evaluation_results_class:Current Best Return = -43.28849792480469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.04291381175622
INFO:tools.evaluation_results_class:Counted Episodes = 2773
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.08101272583008
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.9189758300781
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.14682006835938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1866.54541015625
INFO:tools.evaluation_results_class:Current Best Return = -43.08101272583008
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.28607594936709
INFO:tools.evaluation_results_class:Counted Episodes = 2765
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.60238647460938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.39761352539062
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.4510498046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4039.583740234375
INFO:tools.evaluation_results_class:Current Best Return = -79.60238647460938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.637447405329596
INFO:tools.evaluation_results_class:Counted Episodes = 2852
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.73314666748047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.266845703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.5734405517578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3954.289794921875
INFO:tools.evaluation_results_class:Current Best Return = -77.73314666748047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.25573314801946
INFO:tools.evaluation_results_class:Counted Episodes = 2878
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.60327911376953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.39671325683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.7026824951172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4620.99365234375
INFO:tools.evaluation_results_class:Current Best Return = -83.60327911376953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.56978367062108
INFO:tools.evaluation_results_class:Counted Episodes = 2866
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.1468276977539
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.85317993164062
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.5150146484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4632.08935546875
INFO:tools.evaluation_results_class:Current Best Return = -83.1468276977539
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.23255813953488
INFO:tools.evaluation_results_class:Counted Episodes = 2881
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.71051788330078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.28948974609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.5780792236328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5300.5009765625
INFO:tools.evaluation_results_class:Current Best Return = -81.71051788330078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.19298854564387
INFO:tools.evaluation_results_class:Counted Episodes = 2881
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.2778549194336
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.72213745117188
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.04335021972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4555.96044921875
INFO:tools.evaluation_results_class:Current Best Return = -82.2778549194336
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.45473537604457
INFO:tools.evaluation_results_class:Counted Episodes = 2872
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.49183654785156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.50816345214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.21421813964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4553.7470703125
INFO:tools.evaluation_results_class:Current Best Return = -83.49183654785156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.29697811740188
INFO:tools.evaluation_results_class:Counted Episodes = 2879
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.30233001708984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.69766235351562
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.0634307861328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4976.5986328125
INFO:tools.evaluation_results_class:Current Best Return = -77.30233001708984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.61198188784396
INFO:tools.evaluation_results_class:Counted Episodes = 2871
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.811279296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.188720703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.66380310058594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4638.3564453125
INFO:tools.evaluation_results_class:Current Best Return = -83.811279296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.45891364902507
INFO:tools.evaluation_results_class:Counted Episodes = 2872
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.82102966308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.17897033691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.26560974121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4528.59228515625
INFO:tools.evaluation_results_class:Current Best Return = -82.82102966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.80344585091421
INFO:tools.evaluation_results_class:Counted Episodes = 2844
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.16397857666016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.8360137939453
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.5999298095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4434.60595703125
INFO:tools.evaluation_results_class:Current Best Return = -80.16397857666016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.770847932726
INFO:tools.evaluation_results_class:Counted Episodes = 2854
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.78543853759766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.2145538330078
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.73101806640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4686.04443359375
INFO:tools.evaluation_results_class:Current Best Return = -82.78543853759766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.34238941135493
INFO:tools.evaluation_results_class:Counted Episodes = 2871
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.84153747558594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.15846252441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.56932067871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4620.630859375
INFO:tools.evaluation_results_class:Current Best Return = -83.84153747558594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.58708551483421
INFO:tools.evaluation_results_class:Counted Episodes = 2865
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.46517181396484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.53482055664062
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.78184509277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4746.71630859375
INFO:tools.evaluation_results_class:Current Best Return = -83.46517181396484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.68603430171508
INFO:tools.evaluation_results_class:Counted Episodes = 2857
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.02296447753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.97703552246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.28990173339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4521.4267578125
INFO:tools.evaluation_results_class:Current Best Return = -82.02296447753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.45163535142658
INFO:tools.evaluation_results_class:Counted Episodes = 2874
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.22830200195312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.77169799804688
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.07359313964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4545.755859375
INFO:tools.evaluation_results_class:Current Best Return = -82.22830200195312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.52108748692925
INFO:tools.evaluation_results_class:Counted Episodes = 2869
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.32670593261719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.6732940673828
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.9883575439453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4443.6279296875
INFO:tools.evaluation_results_class:Current Best Return = -83.32670593261719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.610529986052995
INFO:tools.evaluation_results_class:Counted Episodes = 2868
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.35120391845703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.6488037109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.0486602783203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4462.0673828125
INFO:tools.evaluation_results_class:Current Best Return = -82.35120391845703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.42916811695092
INFO:tools.evaluation_results_class:Counted Episodes = 2873
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -85.30747985839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.69252014160156
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.31692504882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4507.03662109375
INFO:tools.evaluation_results_class:Current Best Return = -85.30747985839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.45947826086957
INFO:tools.evaluation_results_class:Counted Episodes = 2875
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.39036560058594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.60963439941406
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.81336975097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4405.98193359375
INFO:tools.evaluation_results_class:Current Best Return = -81.39036560058594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.49650837988827
INFO:tools.evaluation_results_class:Counted Episodes = 2864
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.86203002929688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.13796997070312
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.33697509765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4510.0048828125
INFO:tools.evaluation_results_class:Current Best Return = -82.86203002929688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.653161019909184
INFO:tools.evaluation_results_class:Counted Episodes = 2863
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.62310028076172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.3769073486328
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.1280517578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4718.21484375
INFO:tools.evaluation_results_class:Current Best Return = -83.62310028076172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.23305670816044
INFO:tools.evaluation_results_class:Counted Episodes = 2892
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.30274963378906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.69725036621094
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.1989288330078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4529.8642578125
INFO:tools.evaluation_results_class:Current Best Return = -82.30274963378906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.34584636774418
INFO:tools.evaluation_results_class:Counted Episodes = 2877
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.22684478759766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.77316284179688
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.1413116455078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4568.466796875
INFO:tools.evaluation_results_class:Current Best Return = -83.22684478759766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.56204124432017
INFO:tools.evaluation_results_class:Counted Episodes = 2861
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -78.6689682006836
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.33103942871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.0104522705078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3928.10888671875
INFO:tools.evaluation_results_class:Current Best Return = -78.6689682006836
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.98758620689655
INFO:tools.evaluation_results_class:Counted Episodes = 2900
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.87883758544922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.12115478515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.4026336669922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4437.11572265625
INFO:tools.evaluation_results_class:Current Best Return = -81.87883758544922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.49546089385475
INFO:tools.evaluation_results_class:Counted Episodes = 2864
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.53894805908203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.46104431152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.75047302246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4581.0908203125
INFO:tools.evaluation_results_class:Current Best Return = -84.53894805908203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.82164258019034
INFO:tools.evaluation_results_class:Counted Episodes = 2837
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.06019592285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.93980407714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.3339080810547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4416.81201171875
INFO:tools.evaluation_results_class:Current Best Return = -82.06019592285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.35455810716771
INFO:tools.evaluation_results_class:Counted Episodes = 2874
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.73597717285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.26402282714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.71795654296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4518.25341796875
INFO:tools.evaluation_results_class:Current Best Return = -83.73597717285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.47718564959944
INFO:tools.evaluation_results_class:Counted Episodes = 2871
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 32 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -44.02368927001953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.976318359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.30465698242188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2156.609619140625
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.73510058281632
INFO:tools.evaluation_results_class:Counted Episodes = 5319
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.4209175705909729
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 0.4268043637275696
INFO:agents.father_agent:Step: 10, Training loss: 0.6502352356910706
INFO:agents.father_agent:Step: 15, Training loss: 0.6960126161575317
INFO:agents.father_agent:Step: 20, Training loss: 0.6504647135734558
INFO:agents.father_agent:Step: 25, Training loss: 0.7057684063911438
INFO:agents.father_agent:Step: 30, Training loss: 0.6352810859680176
INFO:agents.father_agent:Step: 35, Training loss: 0.5251085162162781
INFO:agents.father_agent:Step: 40, Training loss: 0.46547648310661316
INFO:agents.father_agent:Step: 45, Training loss: 0.36607277393341064
INFO:agents.father_agent:Step: 50, Training loss: 0.28917646408081055
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 55, Training loss: 0.26581087708473206
INFO:agents.father_agent:Step: 60, Training loss: 0.263841837644577
INFO:agents.father_agent:Step: 65, Training loss: 0.36031338572502136
INFO:agents.father_agent:Step: 70, Training loss: 0.3009156584739685
INFO:agents.father_agent:Step: 75, Training loss: 0.48508769273757935
INFO:agents.father_agent:Step: 80, Training loss: 0.42499059438705444
INFO:agents.father_agent:Step: 85, Training loss: 0.39498066902160645
INFO:agents.father_agent:Step: 90, Training loss: 0.3418002724647522
INFO:agents.father_agent:Step: 95, Training loss: 0.40442079305648804
INFO:agents.father_agent:Step: 100, Training loss: 0.2664010226726532
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.22565841674805
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.77435302734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.99478149414062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2487.5234375
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.85596923614753
INFO:tools.evaluation_results_class:Counted Episodes = 5721
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -56.347267150878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.6527404785156
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.50697326660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2495.25341796875
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.29012345679013
INFO:tools.evaluation_results_class:Counted Episodes = 5670
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.21902084350586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.7809753417969
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.0213623046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2593.45556640625
INFO:tools.evaluation_results_class:Current Best Return = -57.21902084350586
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.04193993786676
INFO:tools.evaluation_results_class:Counted Episodes = 5794
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.50184631347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.49815368652344
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.2056884765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4430.05029296875
INFO:tools.evaluation_results_class:Current Best Return = -79.50184631347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.78444795506407
INFO:tools.evaluation_results_class:Counted Episodes = 5697
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 83.20754655302838
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 33 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.13539123535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.8646240234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 162.9036102294922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2993.073486328125
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.9163096258212
INFO:tools.evaluation_results_class:Counted Episodes = 3501
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.47016066312789917
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 0.5383610725402832
INFO:agents.father_agent:Step: 10, Training loss: 0.6220541000366211
INFO:agents.father_agent:Step: 15, Training loss: 0.7681906223297119
INFO:agents.father_agent:Step: 20, Training loss: 0.9357885122299194
INFO:agents.father_agent:Step: 25, Training loss: 1.119051456451416
INFO:agents.father_agent:Step: 30, Training loss: 1.1020801067352295
INFO:agents.father_agent:Step: 35, Training loss: 0.9453644752502441
INFO:agents.father_agent:Step: 40, Training loss: 1.0698516368865967
INFO:agents.father_agent:Step: 45, Training loss: 0.9286393523216248
INFO:agents.father_agent:Step: 50, Training loss: 0.8692823052406311
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 55, Training loss: 0.8786543607711792
INFO:agents.father_agent:Step: 60, Training loss: 0.6600186824798584
INFO:agents.father_agent:Step: 65, Training loss: 0.5650569796562195
INFO:agents.father_agent:Step: 70, Training loss: 0.41814884543418884
INFO:agents.father_agent:Step: 75, Training loss: 0.3064906895160675
INFO:agents.father_agent:Step: 80, Training loss: 0.24981117248535156
INFO:agents.father_agent:Step: 85, Training loss: 0.2526857852935791
INFO:agents.father_agent:Step: 90, Training loss: 0.2867465615272522
INFO:agents.father_agent:Step: 95, Training loss: 0.34060758352279663
INFO:agents.father_agent:Step: 100, Training loss: 0.3678921163082123
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.2447395324707
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.7552490234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.6797332763672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2635.692626953125
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.07296051312888
INFO:tools.evaluation_results_class:Counted Episodes = 4989
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -58.53755187988281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.46246337890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.24395751953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2613.134033203125
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.08815261044177
INFO:tools.evaluation_results_class:Counted Episodes = 4980
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -61.120975494384766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.8790283203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.9320068359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2753.044921875
INFO:tools.evaluation_results_class:Current Best Return = -61.120975494384766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.05618876224755
INFO:tools.evaluation_results_class:Counted Episodes = 5001
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.91082000732422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.0891876220703
INFO:tools.evaluation_results_class:Average Discounted Reward = 163.74630737304688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4316.1181640625
INFO:tools.evaluation_results_class:Current Best Return = -81.91082000732422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.67664307381193
INFO:tools.evaluation_results_class:Counted Episodes = 4945
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 83.11021625476202
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 34 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.888362884521484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.11163330078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.58372497558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3569.306884765625
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 71.7834410740925
INFO:tools.evaluation_results_class:Counted Episodes = 4022
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.6342245936393738
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Step: 5, Training loss: 0.7102841138839722
INFO:agents.father_agent:Step: 10, Training loss: 0.6283330917358398
INFO:agents.father_agent:Step: 15, Training loss: 0.6464403867721558
INFO:agents.father_agent:Step: 20, Training loss: 0.5446749329566956
INFO:agents.father_agent:Step: 25, Training loss: 0.45085787773132324
INFO:agents.father_agent:Step: 30, Training loss: 0.5472208857536316
INFO:agents.father_agent:Step: 35, Training loss: 0.5043196678161621
INFO:agents.father_agent:Step: 40, Training loss: 0.4172704219818115
INFO:agents.father_agent:Step: 45, Training loss: 0.4568755030632019
INFO:agents.father_agent:Step: 50, Training loss: 0.4284183979034424
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Step: 55, Training loss: 0.3660011291503906
INFO:agents.father_agent:Step: 60, Training loss: 0.2873428463935852
INFO:agents.father_agent:Step: 65, Training loss: 0.2838593125343323
INFO:agents.father_agent:Step: 70, Training loss: 0.2865418493747711
INFO:agents.father_agent:Step: 75, Training loss: 0.2041270136833191
INFO:agents.father_agent:Step: 80, Training loss: 0.23225197196006775
INFO:agents.father_agent:Step: 85, Training loss: 0.22793327271938324
INFO:agents.father_agent:Step: 90, Training loss: 0.14883048832416534
INFO:agents.father_agent:Step: 95, Training loss: 0.25733283162117004
INFO:agents.father_agent:Step: 100, Training loss: 0.3036041259765625
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.089996337890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.9100036621094
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.4828643798828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2479.849853515625
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.35687088958117
INFO:tools.evaluation_results_class:Counted Episodes = 5778
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -54.07840347290039
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.9216003417969
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.34996032714844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2497.030517578125
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.39791847354727
INFO:tools.evaluation_results_class:Counted Episodes = 5765
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.734336853027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.2656555175781
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.20123291015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2647.547119140625
INFO:tools.evaluation_results_class:Current Best Return = -56.734336853027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.12859120803046
INFO:tools.evaluation_results_class:Counted Episodes = 5778
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.15193939208984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.84805297851562
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.19143676757812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4242.6640625
INFO:tools.evaluation_results_class:Current Best Return = -76.15193939208984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.73726398289989
INFO:tools.evaluation_results_class:Counted Episodes = 5614
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 78.97218778361446
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 35 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -37.483333587646484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.51666259765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.71640014648438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1481.02001953125
INFO:tools.evaluation_results_class:Current Best Return = -32.588890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.58148148148148
INFO:tools.evaluation_results_class:Counted Episodes = 4320
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.1698179394006729
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Step: 5, Training loss: 0.2984699606895447
INFO:agents.father_agent:Step: 10, Training loss: 0.36036092042922974
INFO:agents.father_agent:Step: 15, Training loss: 0.4857760965824127
INFO:agents.father_agent:Step: 20, Training loss: 0.5936458110809326
INFO:agents.father_agent:Step: 25, Training loss: 0.4900677502155304
INFO:agents.father_agent:Step: 30, Training loss: 0.38645032048225403
INFO:agents.father_agent:Step: 35, Training loss: 0.4418864846229553
INFO:agents.father_agent:Step: 40, Training loss: 0.3975329101085663
INFO:agents.father_agent:Step: 45, Training loss: 0.32000067830085754
INFO:agents.father_agent:Step: 50, Training loss: 0.2802448868751526
2025-08-20 19:49:08.117273: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-20 19:49:08.119175: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 19:49:08.149632: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-20 19:49:08.149673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-20 19:49:08.150926: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-20 19:49:08.156543: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 19:49:08.156761: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-20 19:49:08.687117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/obstacles-8-5/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/obstacles-8-5/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"penalty"}min=? [F ((x = 8) & (y = 8))] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 468 states and 975 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"rewardmodel_penalty"}min=? [F ((x = 8) & (y = 8))] 
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1067.79150390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -989.3359375
INFO:tools.evaluation_results_class:Average Discounted Reward = -420.59771728515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.24517374517374518
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 357563.1875
INFO:tools.evaluation_results_class:Current Best Return = -1067.79150390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.24517374517374518
INFO:tools.evaluation_results_class:Average Episode Length = 546.7065637065637
INFO:tools.evaluation_results_class:Counted Episodes = 518
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 372.5179138183594
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 39.06%
INFO:agents.father_agent:Step: 5, Training loss: 9.984848976135254
INFO:agents.father_agent:Step: 10, Training loss: 8.107396125793457
INFO:agents.father_agent:Step: 15, Training loss: 6.232834815979004
INFO:agents.father_agent:Step: 20, Training loss: 6.929882526397705
INFO:agents.father_agent:Step: 25, Training loss: 12.201957702636719
INFO:agents.father_agent:Step: 30, Training loss: 8.20509147644043
INFO:agents.father_agent:Step: 35, Training loss: 5.472034454345703
INFO:agents.father_agent:Step: 40, Training loss: 8.397902488708496
INFO:agents.father_agent:Step: 45, Training loss: 6.29939079284668
INFO:agents.father_agent:Step: 50, Training loss: 8.041828155517578
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 55, Training loss: 9.014557838439941
INFO:agents.father_agent:Step: 60, Training loss: 6.925239086151123
INFO:agents.father_agent:Step: 65, Training loss: 7.571094036102295
INFO:agents.father_agent:Step: 70, Training loss: 5.971747398376465
INFO:agents.father_agent:Step: 75, Training loss: 6.247548580169678
INFO:agents.father_agent:Step: 80, Training loss: 7.733261585235596
INFO:agents.father_agent:Step: 85, Training loss: 6.199911594390869
INFO:agents.father_agent:Step: 90, Training loss: 5.502264022827148
INFO:agents.father_agent:Step: 95, Training loss: 8.559863090515137
INFO:agents.father_agent:Step: 100, Training loss: 8.343574523925781
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -369.9813232421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -52.96918869018555
INFO:tools.evaluation_results_class:Average Discounted Reward = -113.79466247558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9906629318394025
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 83796.5078125
INFO:tools.evaluation_results_class:Current Best Return = -369.9813232421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9906629318394025
INFO:tools.evaluation_results_class:Average Episode Length = 222.81325863678805
INFO:tools.evaluation_results_class:Counted Episodes = 1071
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 105, Training loss: 8.029244422912598
INFO:agents.father_agent:Step: 110, Training loss: 7.8260498046875
INFO:agents.father_agent:Step: 115, Training loss: 5.981713771820068
INFO:agents.father_agent:Step: 120, Training loss: 6.497208118438721
INFO:agents.father_agent:Step: 125, Training loss: 7.9272027015686035
INFO:agents.father_agent:Step: 130, Training loss: 6.205228328704834
INFO:agents.father_agent:Step: 135, Training loss: 7.396341800689697
INFO:agents.father_agent:Step: 140, Training loss: 7.400185585021973
INFO:agents.father_agent:Step: 145, Training loss: 9.502691268920898
INFO:agents.father_agent:Step: 150, Training loss: 8.981596946716309
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 155, Training loss: 8.95715045928955
INFO:agents.father_agent:Step: 160, Training loss: 8.343029022216797
INFO:agents.father_agent:Step: 165, Training loss: 9.272794723510742
INFO:agents.father_agent:Step: 170, Training loss: 11.549430847167969
INFO:agents.father_agent:Step: 175, Training loss: 10.75984001159668
INFO:agents.father_agent:Step: 180, Training loss: 13.012031555175781
INFO:agents.father_agent:Step: 185, Training loss: 12.154562950134277
INFO:agents.father_agent:Step: 190, Training loss: 13.348993301391602
INFO:agents.father_agent:Step: 195, Training loss: 12.90593147277832
INFO:agents.father_agent:Step: 200, Training loss: 12.607544898986816
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -168.74554443359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 151.25445556640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 54.92344284057617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20739.41015625
INFO:tools.evaluation_results_class:Current Best Return = -168.74554443359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.71147292692162
INFO:tools.evaluation_results_class:Counted Episodes = 2641
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -162.51348876953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 157.48651123046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 61.1431770324707
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21106.240234375
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.87106446776612
INFO:tools.evaluation_results_class:Counted Episodes = 2668
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -253.54641723632812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 66.45358276367188
INFO:tools.evaluation_results_class:Average Discounted Reward = -29.086484909057617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41382.546875
INFO:tools.evaluation_results_class:Current Best Return = -253.54641723632812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 152.2194092827004
INFO:tools.evaluation_results_class:Counted Episodes = 1896
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.8835906982422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 157.1164093017578
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.19287872314453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21378.111328125
INFO:tools.evaluation_results_class:Current Best Return = -162.8835906982422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.26536514822848
INFO:tools.evaluation_results_class:Counted Episodes = 2766
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 273.2564568703719
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -303.5748596191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 16.425142288208008
INFO:tools.evaluation_results_class:Average Discounted Reward = -56.50392150878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 60727.6796875
INFO:tools.evaluation_results_class:Current Best Return = -303.5748596191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 150.55657142857143
INFO:tools.evaluation_results_class:Counted Episodes = 875
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -267.8799133300781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 52.12009048461914
INFO:tools.evaluation_results_class:Average Discounted Reward = -32.63421630859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40244.01171875
INFO:tools.evaluation_results_class:Current Best Return = -267.8799133300781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 148.12008978675647
INFO:tools.evaluation_results_class:Counted Episodes = 891
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -313.92449951171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 6.075514793395996
INFO:tools.evaluation_results_class:Average Discounted Reward = -69.05181884765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 56394.12890625
INFO:tools.evaluation_results_class:Current Best Return = -313.92449951171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 150.33638443935928
INFO:tools.evaluation_results_class:Counted Episodes = 874
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -243.3881072998047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 76.61188507080078
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.503032684326172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38509.90625
INFO:tools.evaluation_results_class:Current Best Return = -243.3881072998047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 152.93240093240092
INFO:tools.evaluation_results_class:Counted Episodes = 858
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -231.220947265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 88.779052734375
INFO:tools.evaluation_results_class:Average Discounted Reward = -8.1400728225708
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37381.0234375
INFO:tools.evaluation_results_class:Current Best Return = -231.220947265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 151.6513233601841
INFO:tools.evaluation_results_class:Counted Episodes = 869
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -360.0182189941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -40.01822280883789
INFO:tools.evaluation_results_class:Average Discounted Reward = -101.79496002197266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 58641.55078125
INFO:tools.evaluation_results_class:Current Best Return = -360.0182189941406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 150.48974943052391
INFO:tools.evaluation_results_class:Counted Episodes = 878
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -187.973388671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 132.026611328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 48.679901123046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27667.169921875
INFO:tools.evaluation_results_class:Current Best Return = -187.973388671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.06768060836502
INFO:tools.evaluation_results_class:Counted Episodes = 1315
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -199.06822204589844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 120.9317855834961
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.057281494140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25148.16015625
INFO:tools.evaluation_results_class:Current Best Return = -199.06822204589844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.25937031484258
INFO:tools.evaluation_results_class:Counted Episodes = 1334
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.06617736816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 128.93382263183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.28429412841797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26752.853515625
INFO:tools.evaluation_results_class:Current Best Return = -191.06617736816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.47806691449814
INFO:tools.evaluation_results_class:Counted Episodes = 1345
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.59515380859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 177.40484619140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.15525817871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20070.66796875
INFO:tools.evaluation_results_class:Current Best Return = -142.59515380859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.17664897649735
INFO:tools.evaluation_results_class:Counted Episodes = 1319
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.4126434326172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 161.5873565673828
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.84324645996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18180.220703125
INFO:tools.evaluation_results_class:Current Best Return = -158.4126434326172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.37349397590361
INFO:tools.evaluation_results_class:Counted Episodes = 1328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -267.364990234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 52.63500213623047
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.420568466186523
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31840.259765625
INFO:tools.evaluation_results_class:Current Best Return = -267.364990234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.05684367988033
INFO:tools.evaluation_results_class:Counted Episodes = 1337
INFO:robust_rl.robust_rl_trainer:Iteration 2 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -185.5595245361328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 134.4404754638672
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.30723190307617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24425.826171875
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.76154712730003
INFO:tools.evaluation_results_class:Counted Episodes = 2663
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.307280540466309
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 12.487383842468262
INFO:agents.father_agent:Step: 10, Training loss: 10.297041893005371
INFO:agents.father_agent:Step: 15, Training loss: 11.25335693359375
INFO:agents.father_agent:Step: 20, Training loss: 11.262011528015137
INFO:agents.father_agent:Step: 25, Training loss: 12.355854988098145
INFO:agents.father_agent:Step: 30, Training loss: 9.085597038269043
INFO:agents.father_agent:Step: 35, Training loss: 10.29206371307373
INFO:agents.father_agent:Step: 40, Training loss: 8.865274429321289
INFO:agents.father_agent:Step: 45, Training loss: 8.737801551818848
INFO:agents.father_agent:Step: 50, Training loss: 9.882903099060059
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -165.6277618408203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 154.3722381591797
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.48819732666016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21401.076171875
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.63488293179505
INFO:tools.evaluation_results_class:Counted Episodes = 2947
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -231.5250244140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 88.47496795654297
INFO:tools.evaluation_results_class:Average Discounted Reward = -0.08648555725812912
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35796.8671875
INFO:tools.evaluation_results_class:Current Best Return = -231.5250244140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 123.20950778291964
INFO:tools.evaluation_results_class:Counted Episodes = 2377
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.46286010742188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 150.53713989257812
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.062644958496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23549.755859375
INFO:tools.evaluation_results_class:Current Best Return = -169.46286010742188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.49178172255095
INFO:tools.evaluation_results_class:Counted Episodes = 3042
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 250.24129473963677
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 3 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -177.08053588867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 142.91946411132812
INFO:tools.evaluation_results_class:Average Discounted Reward = 56.20132064819336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23689.947265625
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.29475465313028
INFO:tools.evaluation_results_class:Counted Episodes = 2955
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.825081825256348
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 9.548704147338867
INFO:agents.father_agent:Step: 10, Training loss: 8.307293891906738
INFO:agents.father_agent:Step: 15, Training loss: 8.622859001159668
INFO:agents.father_agent:Step: 20, Training loss: 7.979302883148193
INFO:agents.father_agent:Step: 25, Training loss: 8.445713996887207
INFO:agents.father_agent:Step: 30, Training loss: 9.225677490234375
INFO:agents.father_agent:Step: 35, Training loss: 6.856122970581055
INFO:agents.father_agent:Step: 40, Training loss: 7.334072113037109
INFO:agents.father_agent:Step: 45, Training loss: 8.05557632446289
INFO:agents.father_agent:Step: 50, Training loss: 7.8063459396362305
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -183.16561889648438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 136.83438110351562
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.87958908081055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23055.005859375
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.04532775453278
INFO:tools.evaluation_results_class:Counted Episodes = 2868
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -248.19854736328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.80146026611328
INFO:tools.evaluation_results_class:Average Discounted Reward = -16.15925407409668
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38738.52734375
INFO:tools.evaluation_results_class:Current Best Return = -248.19854736328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 131.61566484517303
INFO:tools.evaluation_results_class:Counted Episodes = 2196
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -182.31434631347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 137.68565368652344
INFO:tools.evaluation_results_class:Average Discounted Reward = 50.81105422973633
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25100.8984375
INFO:tools.evaluation_results_class:Current Best Return = -182.31434631347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.12198391420911
INFO:tools.evaluation_results_class:Counted Episodes = 2984
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 255.83293449062165
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 4 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.63136291503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 129.36863708496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.823909759521484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24724.197265625
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.16724738675958
INFO:tools.evaluation_results_class:Counted Episodes = 2870
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.401846408843994
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Step: 5, Training loss: 7.209687232971191
INFO:agents.father_agent:Step: 10, Training loss: 7.855710506439209
INFO:agents.father_agent:Step: 15, Training loss: 7.336864948272705
INFO:agents.father_agent:Step: 20, Training loss: 6.8556623458862305
INFO:agents.father_agent:Step: 25, Training loss: 7.5519633293151855
INFO:agents.father_agent:Step: 30, Training loss: 6.777915000915527
INFO:agents.father_agent:Step: 35, Training loss: 7.364762783050537
INFO:agents.father_agent:Step: 40, Training loss: 7.7478132247924805
INFO:agents.father_agent:Step: 45, Training loss: 6.419628143310547
INFO:agents.father_agent:Step: 50, Training loss: 7.162811279296875
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -180.18565368652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 139.81434631347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 56.70150375366211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23639.720703125
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.58854509545753
INFO:tools.evaluation_results_class:Counted Episodes = 3038
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -222.81527709960938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 97.1847152709961
INFO:tools.evaluation_results_class:Average Discounted Reward = 9.955211639404297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30703.3984375
INFO:tools.evaluation_results_class:Current Best Return = -222.81527709960938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.2489168964159
INFO:tools.evaluation_results_class:Counted Episodes = 2539
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.05262756347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 141.94737243652344
INFO:tools.evaluation_results_class:Average Discounted Reward = 57.19902801513672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23122.6953125
INFO:tools.evaluation_results_class:Current Best Return = -178.05262756347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.7989854153456
INFO:tools.evaluation_results_class:Counted Episodes = 3154
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 241.2231741174575
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 5 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.29281616210938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 128.70718383789062
INFO:tools.evaluation_results_class:Average Discounted Reward = 47.042972564697266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25768.986328125
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.366930171278
INFO:tools.evaluation_results_class:Counted Episodes = 3036
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.5122575759887695
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 6.952659606933594
INFO:agents.father_agent:Step: 10, Training loss: 7.585500240325928
INFO:agents.father_agent:Step: 15, Training loss: 6.979020595550537
INFO:agents.father_agent:Step: 20, Training loss: 6.530651569366455
INFO:agents.father_agent:Step: 25, Training loss: 5.640620231628418
INFO:agents.father_agent:Step: 30, Training loss: 8.215229034423828
INFO:agents.father_agent:Step: 35, Training loss: 5.388389587402344
INFO:agents.father_agent:Step: 40, Training loss: 6.194917678833008
INFO:agents.father_agent:Step: 45, Training loss: 6.150757789611816
INFO:agents.father_agent:Step: 50, Training loss: 6.643926620483398
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -191.3328857421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 128.6671142578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 46.53196716308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25215.609375
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.27136681077486
INFO:tools.evaluation_results_class:Counted Episodes = 3007
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -226.29644775390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 93.70355987548828
INFO:tools.evaluation_results_class:Average Discounted Reward = 9.276324272155762
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33059.81640625
INFO:tools.evaluation_results_class:Current Best Return = -226.29644775390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.91857707509881
INFO:tools.evaluation_results_class:Counted Episodes = 2530
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -189.30426025390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 130.69573974609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.09512710571289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26056.732421875
INFO:tools.evaluation_results_class:Current Best Return = -189.30426025390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.20687460216422
INFO:tools.evaluation_results_class:Counted Episodes = 3142
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 239.69653063652703
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 6 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -200.8592987060547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 119.14070129394531
INFO:tools.evaluation_results_class:Average Discounted Reward = 39.42443084716797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26935.619140625
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.26063651591289
INFO:tools.evaluation_results_class:Counted Episodes = 2985
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.933847904205322
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 5.132339954376221
INFO:agents.father_agent:Step: 10, Training loss: 5.334178924560547
INFO:agents.father_agent:Step: 15, Training loss: 5.860126495361328
INFO:agents.father_agent:Step: 20, Training loss: 5.543957233428955
INFO:agents.father_agent:Step: 25, Training loss: 5.254729747772217
INFO:agents.father_agent:Step: 30, Training loss: 4.0657548904418945
INFO:agents.father_agent:Step: 35, Training loss: 6.042176246643066
INFO:agents.father_agent:Step: 40, Training loss: 5.108897686004639
INFO:agents.father_agent:Step: 45, Training loss: 5.839520454406738
INFO:agents.father_agent:Step: 50, Training loss: 4.694223880767822
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -188.53811645507812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 131.46188354492188
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.81608963012695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26572.1640625
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.96742671009773
INFO:tools.evaluation_results_class:Counted Episodes = 3070
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -218.980224609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 101.019775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 17.16022300720215
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33043.296875
INFO:tools.evaluation_results_class:Current Best Return = -218.980224609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.45746268656717
INFO:tools.evaluation_results_class:Counted Episodes = 2680
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -183.37191772460938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 136.62808227539062
INFO:tools.evaluation_results_class:Average Discounted Reward = 53.66569137573242
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22661.740234375
INFO:tools.evaluation_results_class:Current Best Return = -183.37191772460938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.09391575663027
INFO:tools.evaluation_results_class:Counted Episodes = 3205
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 234.77318320088727
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -174.0234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 145.9765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.21452713012695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30769.13671875
INFO:tools.evaluation_results_class:Current Best Return = -174.0234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.471875
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -184.36912536621094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 135.63087463378906
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.253440856933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18182.091796875
INFO:tools.evaluation_results_class:Current Best Return = -184.36912536621094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.38087774294671
INFO:tools.evaluation_results_class:Counted Episodes = 1276
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.40579223632812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 154.59420776367188
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.29033279418945
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30393.3515625
INFO:tools.evaluation_results_class:Current Best Return = -165.40579223632812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.5793588741204
INFO:tools.evaluation_results_class:Counted Episodes = 1279
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.70196533203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 183.29803466796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 84.33616638183594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23705.708984375
INFO:tools.evaluation_results_class:Current Best Return = -136.70196533203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.12862745098039
INFO:tools.evaluation_results_class:Counted Episodes = 1275
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -172.98526000976562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 147.01473999023438
INFO:tools.evaluation_results_class:Average Discounted Reward = 58.96976852416992
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28656.1171875
INFO:tools.evaluation_results_class:Current Best Return = -172.98526000976562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.344453064391
INFO:tools.evaluation_results_class:Counted Episodes = 1289
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -248.2848358154297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.71516418457031
INFO:tools.evaluation_results_class:Average Discounted Reward = -4.461765766143799
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32606.1015625
INFO:tools.evaluation_results_class:Current Best Return = -248.2848358154297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.17764165390506
INFO:tools.evaluation_results_class:Counted Episodes = 1306
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -256.1029968261719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.897003173828125
INFO:tools.evaluation_results_class:Average Discounted Reward = -12.768312454223633
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31111.375
INFO:tools.evaluation_results_class:Current Best Return = -256.1029968261719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.28593389700231
INFO:tools.evaluation_results_class:Counted Episodes = 1301
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -257.18011474609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 62.81987762451172
INFO:tools.evaluation_results_class:Average Discounted Reward = -12.887099266052246
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33819.43359375
INFO:tools.evaluation_results_class:Current Best Return = -257.18011474609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.75776397515529
INFO:tools.evaluation_results_class:Counted Episodes = 1288
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -257.15264892578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 62.84733963012695
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.958059310913086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30328.484375
INFO:tools.evaluation_results_class:Current Best Return = -257.15264892578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.87818041634542
INFO:tools.evaluation_results_class:Counted Episodes = 1297
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -266.8999328613281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 53.100074768066406
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.753210067749023
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36415.22265625
INFO:tools.evaluation_results_class:Current Best Return = -266.8999328613281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.43391902215431
INFO:tools.evaluation_results_class:Counted Episodes = 1309
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -262.428466796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.57154083251953
INFO:tools.evaluation_results_class:Average Discounted Reward = -17.48662757873535
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33967.5234375
INFO:tools.evaluation_results_class:Current Best Return = -262.428466796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.58391337973704
INFO:tools.evaluation_results_class:Counted Episodes = 1293
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.6202850341797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 185.3797149658203
INFO:tools.evaluation_results_class:Average Discounted Reward = 98.78001403808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23430.703125
INFO:tools.evaluation_results_class:Current Best Return = -134.6202850341797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.1521456436931
INFO:tools.evaluation_results_class:Counted Episodes = 1538
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.10052490234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 154.89947509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.87654113769531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16117.0771484375
INFO:tools.evaluation_results_class:Current Best Return = -165.10052490234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.48563968668407
INFO:tools.evaluation_results_class:Counted Episodes = 1532
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -133.69908142089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 186.30091857910156
INFO:tools.evaluation_results_class:Average Discounted Reward = 98.18904113769531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22473.171875
INFO:tools.evaluation_results_class:Current Best Return = -133.69908142089844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.55847568988173
INFO:tools.evaluation_results_class:Counted Episodes = 1522
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -103.70341491699219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 216.2965850830078
INFO:tools.evaluation_results_class:Average Discounted Reward = 122.05654907226562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15223.3935546875
INFO:tools.evaluation_results_class:Current Best Return = -103.70341491699219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.33010960670535
INFO:tools.evaluation_results_class:Counted Episodes = 1551
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -146.31561279296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 173.68438720703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 86.63945770263672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18231.400390625
INFO:tools.evaluation_results_class:Current Best Return = -146.31561279296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.91121192482177
INFO:tools.evaluation_results_class:Counted Episodes = 1543
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -226.093505859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 93.906494140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.07914924621582
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24456.244140625
INFO:tools.evaluation_results_class:Current Best Return = -226.093505859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.17402597402598
INFO:tools.evaluation_results_class:Counted Episodes = 1540
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -233.2040252685547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 86.79597473144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.5220947265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25527.119140625
INFO:tools.evaluation_results_class:Current Best Return = -233.2040252685547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.30344379467186
INFO:tools.evaluation_results_class:Counted Episodes = 1539
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -231.90968322753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 88.09031677246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.392903327941895
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26341.751953125
INFO:tools.evaluation_results_class:Current Best Return = -231.90968322753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.94476933073425
INFO:tools.evaluation_results_class:Counted Episodes = 1539
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -230.5750274658203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 89.42496490478516
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.105321884155273
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22607.880859375
INFO:tools.evaluation_results_class:Current Best Return = -230.5750274658203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.35058214747735
INFO:tools.evaluation_results_class:Counted Episodes = 1546
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -232.8230743408203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 87.17693328857422
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.827309608459473
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24230.15234375
INFO:tools.evaluation_results_class:Current Best Return = -232.8230743408203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.68807339449542
INFO:tools.evaluation_results_class:Counted Episodes = 1526
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -233.84539794921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 86.15460205078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.134387016296387
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25538.302734375
INFO:tools.evaluation_results_class:Current Best Return = -233.84539794921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.90394736842106
INFO:tools.evaluation_results_class:Counted Episodes = 1520
INFO:robust_rl.robust_rl_trainer:Iteration 7 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -189.7952423095703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 130.2047576904297
INFO:tools.evaluation_results_class:Average Discounted Reward = 48.553470611572266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25372.171875
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.39600772698005
INFO:tools.evaluation_results_class:Counted Episodes = 3106
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.563465118408203
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 5.041143417358398
INFO:agents.father_agent:Step: 10, Training loss: 5.660519123077393
INFO:agents.father_agent:Step: 15, Training loss: 4.86531925201416
INFO:agents.father_agent:Step: 20, Training loss: 4.713255405426025
INFO:agents.father_agent:Step: 25, Training loss: 4.7931718826293945
INFO:agents.father_agent:Step: 30, Training loss: 4.546111106872559
INFO:agents.father_agent:Step: 35, Training loss: 4.1730780601501465
INFO:agents.father_agent:Step: 40, Training loss: 4.3544020652771
INFO:agents.father_agent:Step: 45, Training loss: 3.7090165615081787
INFO:agents.father_agent:Step: 50, Training loss: 3.1405155658721924
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -174.92962646484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 145.07037353515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.36652755737305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22850.830078125
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.8641975308642
INFO:tools.evaluation_results_class:Counted Episodes = 3240
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -199.58432006835938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 120.41567993164062
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.68583297729492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28488.138671875
INFO:tools.evaluation_results_class:Current Best Return = -199.58432006835938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.88813788442042
INFO:tools.evaluation_results_class:Counted Episodes = 2959
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -179.17930603027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 140.82069396972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 58.69574737548828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25253.6171875
INFO:tools.evaluation_results_class:Current Best Return = -179.17930603027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.71905339805825
INFO:tools.evaluation_results_class:Counted Episodes = 3296
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 227.0889742086222
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 8 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -180.54713439941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 139.45286560058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 57.67407989501953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22464.865234375
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.6611213801602
INFO:tools.evaluation_results_class:Counted Episodes = 3246
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.08177375793457
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 3.782294988632202
INFO:agents.father_agent:Step: 10, Training loss: 3.045717716217041
INFO:agents.father_agent:Step: 15, Training loss: 3.7960045337677
INFO:agents.father_agent:Step: 20, Training loss: 2.761409282684326
INFO:agents.father_agent:Step: 25, Training loss: 2.650730848312378
INFO:agents.father_agent:Step: 30, Training loss: 2.7295806407928467
INFO:agents.father_agent:Step: 35, Training loss: 2.808152675628662
INFO:agents.father_agent:Step: 40, Training loss: 3.8562824726104736
INFO:agents.father_agent:Step: 45, Training loss: 3.0160071849823
INFO:agents.father_agent:Step: 50, Training loss: 2.4394350051879883
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -162.742919921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 157.257080078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 73.19537353515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19523.13671875
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.22907357759905
INFO:tools.evaluation_results_class:Counted Episodes = 3357
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -177.21139526367188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 142.78860473632812
INFO:tools.evaluation_results_class:Average Discounted Reward = 58.235897064208984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24926.001953125
INFO:tools.evaluation_results_class:Current Best Return = -177.21139526367188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.06164383561644
INFO:tools.evaluation_results_class:Counted Episodes = 3212
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.6822052001953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 146.3177947998047
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.83005142211914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22623.4375
INFO:tools.evaluation_results_class:Current Best Return = -173.6822052001953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.71112422543523
INFO:tools.evaluation_results_class:Counted Episodes = 3389
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 224.12917180191585
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 9 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.46217346191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 150.53782653808594
INFO:tools.evaluation_results_class:Average Discounted Reward = 68.30653381347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21245.91796875
INFO:tools.evaluation_results_class:Current Best Return = -162.51348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.89100655151876
INFO:tools.evaluation_results_class:Counted Episodes = 3358
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.3737409114837646
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 3.2166264057159424
INFO:agents.father_agent:Step: 10, Training loss: 2.5132226943969727
INFO:agents.father_agent:Step: 15, Training loss: 2.694221258163452
INFO:agents.father_agent:Step: 20, Training loss: 3.000533103942871
INFO:agents.father_agent:Step: 25, Training loss: 2.831984758377075
INFO:agents.father_agent:Step: 30, Training loss: 3.2764859199523926
INFO:agents.father_agent:Step: 35, Training loss: 3.1255767345428467
INFO:agents.father_agent:Step: 40, Training loss: 3.00946044921875
INFO:agents.father_agent:Step: 45, Training loss: 2.7630441188812256
INFO:agents.father_agent:Step: 50, Training loss: 3.281996250152588
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -162.33462524414062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 157.66537475585938
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.02489471435547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21320.4453125
INFO:tools.evaluation_results_class:Current Best Return = -162.33462524414062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.52719167904904
INFO:tools.evaluation_results_class:Counted Episodes = 3365
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -171.08392333984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 148.91607666015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.58379364013672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22982.212890625
INFO:tools.evaluation_results_class:Current Best Return = -171.08392333984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.16905615292713
INFO:tools.evaluation_results_class:Counted Episodes = 3348
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.1416473388672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 155.8583526611328
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.5035400390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21115.830078125
INFO:tools.evaluation_results_class:Current Best Return = -164.1416473388672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.68289128533955
INFO:tools.evaluation_results_class:Counted Episodes = 3431
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 234.104337114116
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 10 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.54107666015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 154.45892333984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.46504974365234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22076.46875
INFO:tools.evaluation_results_class:Current Best Return = -162.33462524414062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.25827423167848
INFO:tools.evaluation_results_class:Counted Episodes = 3384
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.6088061332702637
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 2.8378090858459473
INFO:agents.father_agent:Step: 10, Training loss: 3.2964589595794678
INFO:agents.father_agent:Step: 15, Training loss: 2.6682605743408203
INFO:agents.father_agent:Step: 20, Training loss: 2.259079933166504
INFO:agents.father_agent:Step: 25, Training loss: 2.900303602218628
INFO:agents.father_agent:Step: 30, Training loss: 3.511563777923584
INFO:agents.father_agent:Step: 35, Training loss: 3.309831380844116
INFO:agents.father_agent:Step: 40, Training loss: 2.7446696758270264
INFO:agents.father_agent:Step: 45, Training loss: 2.554236888885498
INFO:agents.father_agent:Step: 50, Training loss: 2.431459903717041
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -156.3477325439453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 163.6522674560547
INFO:tools.evaluation_results_class:Average Discounted Reward = 80.820068359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22690.91015625
INFO:tools.evaluation_results_class:Current Best Return = -156.3477325439453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.4060372891388
INFO:tools.evaluation_results_class:Counted Episodes = 3379
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -152.45298767089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 167.54701232910156
INFO:tools.evaluation_results_class:Average Discounted Reward = 82.74920654296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20746.96484375
INFO:tools.evaluation_results_class:Current Best Return = -152.45298767089844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.2372634643377
INFO:tools.evaluation_results_class:Counted Episodes = 3435
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.75433349609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 163.24566650390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 79.98003387451172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22431.330078125
INFO:tools.evaluation_results_class:Current Best Return = -156.75433349609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.97280092592592
INFO:tools.evaluation_results_class:Counted Episodes = 3456
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 247.24619151798032
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 11 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -161.4403839111328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 158.5596160888672
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.82501983642578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20256.4140625
INFO:tools.evaluation_results_class:Current Best Return = -156.3477325439453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.95172210774213
INFO:tools.evaluation_results_class:Counted Episodes = 3397
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.378009557723999
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Step: 5, Training loss: 3.3242173194885254
INFO:agents.father_agent:Step: 10, Training loss: 2.4073052406311035
INFO:agents.father_agent:Step: 15, Training loss: 2.204735279083252
INFO:agents.father_agent:Step: 20, Training loss: 2.782541275024414
INFO:agents.father_agent:Step: 25, Training loss: 2.678586483001709
INFO:agents.father_agent:Step: 30, Training loss: 2.736860513687134
INFO:agents.father_agent:Step: 35, Training loss: 2.206939458847046
INFO:agents.father_agent:Step: 40, Training loss: 2.8158247470855713
INFO:agents.father_agent:Step: 45, Training loss: 2.6797962188720703
INFO:agents.father_agent:Step: 50, Training loss: 3.494093894958496
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -156.04922485351562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 163.95077514648438
INFO:tools.evaluation_results_class:Average Discounted Reward = 79.9908447265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20913.927734375
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.34423813734159
INFO:tools.evaluation_results_class:Counted Episodes = 3393
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.4306640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 154.5693359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 71.51724243164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24383.82421875
INFO:tools.evaluation_results_class:Current Best Return = -165.4306640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.8069046225863
INFO:tools.evaluation_results_class:Counted Episodes = 3418
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.70855712890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 161.29144287109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 77.6353530883789
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21440.76171875
INFO:tools.evaluation_results_class:Current Best Return = -158.70855712890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.83424618045547
INFO:tools.evaluation_results_class:Counted Episodes = 3469
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 267.01084805204295
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -109.86338806152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 210.13661193847656
INFO:tools.evaluation_results_class:Average Discounted Reward = 122.82982635498047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20258.13671875
INFO:tools.evaluation_results_class:Current Best Return = -109.86338806152344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.81177899210687
INFO:tools.evaluation_results_class:Counted Episodes = 1647
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.08363342285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 150.91636657714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 67.8544921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12715.4140625
INFO:tools.evaluation_results_class:Current Best Return = -169.08363342285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.8820697954272
INFO:tools.evaluation_results_class:Counted Episodes = 1662
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -104.54540252685547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 215.45460510253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 126.82992553710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17404.92578125
INFO:tools.evaluation_results_class:Current Best Return = -104.54540252685547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.9789536981359
INFO:tools.evaluation_results_class:Counted Episodes = 1663
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.78453826904297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.2154541015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 142.69552612304688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7012.2841796875
INFO:tools.evaluation_results_class:Current Best Return = -79.78453826904297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.81984175289105
INFO:tools.evaluation_results_class:Counted Episodes = 1643
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -92.16402435302734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 227.83596801757812
INFO:tools.evaluation_results_class:Average Discounted Reward = 131.73045349121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9689.6904296875
INFO:tools.evaluation_results_class:Current Best Return = -92.16402435302734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.85243902439025
INFO:tools.evaluation_results_class:Counted Episodes = 1640
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.38804626464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 129.61195373535156
INFO:tools.evaluation_results_class:Average Discounted Reward = 47.97698974609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18846.15234375
INFO:tools.evaluation_results_class:Current Best Return = -190.38804626464844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.35908267954134
INFO:tools.evaluation_results_class:Counted Episodes = 1657
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.34037780761719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 200.6596221923828
INFO:tools.evaluation_results_class:Average Discounted Reward = 117.37616729736328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21396.76953125
INFO:tools.evaluation_results_class:Current Best Return = -119.34037780761719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.90283645141822
INFO:tools.evaluation_results_class:Counted Episodes = 1657
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -113.26978302001953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 206.73020935058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 121.68245697021484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20538.22265625
INFO:tools.evaluation_results_class:Current Best Return = -113.26978302001953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.97601918465227
INFO:tools.evaluation_results_class:Counted Episodes = 1668
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -115.34767150878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 204.65232849121094
INFO:tools.evaluation_results_class:Average Discounted Reward = 120.54447937011719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22095.927734375
INFO:tools.evaluation_results_class:Current Best Return = -115.34767150878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.4994026284349
INFO:tools.evaluation_results_class:Counted Episodes = 1674
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -116.35675048828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 203.64324951171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 118.566650390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21219.80859375
INFO:tools.evaluation_results_class:Current Best Return = -116.35675048828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.11973121563837
INFO:tools.evaluation_results_class:Counted Episodes = 1637
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.18281555175781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 200.8171844482422
INFO:tools.evaluation_results_class:Average Discounted Reward = 116.99820709228516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22605.650390625
INFO:tools.evaluation_results_class:Current Best Return = -119.18281555175781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.5405240706886
INFO:tools.evaluation_results_class:Counted Episodes = 1641
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.3124237060547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 128.6875762939453
INFO:tools.evaluation_results_class:Average Discounted Reward = 47.136314392089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19263.037109375
INFO:tools.evaluation_results_class:Current Best Return = -191.3124237060547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.68940316686967
INFO:tools.evaluation_results_class:Counted Episodes = 1642
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -245.8323211669922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 74.16767883300781
INFO:tools.evaluation_results_class:Average Discounted Reward = -0.10799513012170792
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20469.4921875
INFO:tools.evaluation_results_class:Current Best Return = -245.8323211669922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.42493946731234
INFO:tools.evaluation_results_class:Counted Episodes = 1652
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -247.7306365966797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.26937103271484
INFO:tools.evaluation_results_class:Average Discounted Reward = -1.3921504020690918
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23281.8671875
INFO:tools.evaluation_results_class:Current Best Return = -247.7306365966797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.19249394673123
INFO:tools.evaluation_results_class:Counted Episodes = 1652
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -259.13287353515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 60.86711120605469
INFO:tools.evaluation_results_class:Average Discounted Reward = -16.21082305908203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23543.556640625
INFO:tools.evaluation_results_class:Current Best Return = -259.13287353515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.77791262135922
INFO:tools.evaluation_results_class:Counted Episodes = 1648
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -263.77496337890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.22504425048828
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.878082275390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21843.18359375
INFO:tools.evaluation_results_class:Current Best Return = -263.77496337890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.38777979431337
INFO:tools.evaluation_results_class:Counted Episodes = 1653
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -103.93353271484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 216.06646728515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 129.9511260986328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19530.751953125
INFO:tools.evaluation_results_class:Current Best Return = -103.93353271484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.03679525222552
INFO:tools.evaluation_results_class:Counted Episodes = 1685
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.2135467529297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 141.7864532470703
INFO:tools.evaluation_results_class:Average Discounted Reward = 61.7246208190918
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15665.650390625
INFO:tools.evaluation_results_class:Current Best Return = -178.2135467529297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.90502117362371
INFO:tools.evaluation_results_class:Counted Episodes = 1653
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -104.52980041503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 215.47019958496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 128.8986053466797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21134.92578125
INFO:tools.evaluation_results_class:Current Best Return = -104.52980041503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.96207104154124
INFO:tools.evaluation_results_class:Counted Episodes = 1661
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.70189666748047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.298095703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 151.17044067382812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6464.5
INFO:tools.evaluation_results_class:Current Best Return = -71.70189666748047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.15439429928742
INFO:tools.evaluation_results_class:Counted Episodes = 1684
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.62134552001953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.37864685058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 141.07907104492188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9581.8935546875
INFO:tools.evaluation_results_class:Current Best Return = -82.62134552001953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.56648777579011
INFO:tools.evaluation_results_class:Counted Episodes = 1677
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -197.59716796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 122.40282440185547
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.807125091552734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20074.8828125
INFO:tools.evaluation_results_class:Current Best Return = -197.59716796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.31919905771495
INFO:tools.evaluation_results_class:Counted Episodes = 1698
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -115.32975769042969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 204.6702423095703
INFO:tools.evaluation_results_class:Average Discounted Reward = 121.09425354003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20219.521484375
INFO:tools.evaluation_results_class:Current Best Return = -115.32975769042969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.39475253428742
INFO:tools.evaluation_results_class:Counted Episodes = 1677
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -117.4854736328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 202.5145263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 119.23825073242188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21482.900390625
INFO:tools.evaluation_results_class:Current Best Return = -117.4854736328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.95641646489103
INFO:tools.evaluation_results_class:Counted Episodes = 1652
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -127.49668884277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 192.50331115722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 111.95945739746094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23767.232421875
INFO:tools.evaluation_results_class:Current Best Return = -127.49668884277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.13546056592415
INFO:tools.evaluation_results_class:Counted Episodes = 1661
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -116.20130157470703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 203.79869079589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 120.87640380859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19653.806640625
INFO:tools.evaluation_results_class:Current Best Return = -116.20130157470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.30669034931913
INFO:tools.evaluation_results_class:Counted Episodes = 1689
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.404296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 200.595703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 117.95940399169922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21354.50390625
INFO:tools.evaluation_results_class:Current Best Return = -119.404296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.8706022659511
INFO:tools.evaluation_results_class:Counted Episodes = 1677
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -197.70516967773438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 122.2948226928711
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.26469421386719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22376.19921875
INFO:tools.evaluation_results_class:Current Best Return = -197.70516967773438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.13116726835139
INFO:tools.evaluation_results_class:Counted Episodes = 1662
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -249.85610961914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 70.14388275146484
INFO:tools.evaluation_results_class:Average Discounted Reward = -3.748122215270996
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19998.376953125
INFO:tools.evaluation_results_class:Current Best Return = -249.85610961914062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.70983213429257
INFO:tools.evaluation_results_class:Counted Episodes = 1668
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -248.09982299804688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.90017700195312
INFO:tools.evaluation_results_class:Average Discounted Reward = -2.3094124794006348
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20098.833984375
INFO:tools.evaluation_results_class:Current Best Return = -248.09982299804688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.09079975947084
INFO:tools.evaluation_results_class:Counted Episodes = 1663
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -261.50958251953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 58.49040603637695
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.16628074645996
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20962.603515625
INFO:tools.evaluation_results_class:Current Best Return = -261.50958251953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.48681055155875
INFO:tools.evaluation_results_class:Counted Episodes = 1668
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -271.5024108886719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.49760055541992
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.045875549316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24452.111328125
INFO:tools.evaluation_results_class:Current Best Return = -271.5024108886719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.04561824729892
INFO:tools.evaluation_results_class:Counted Episodes = 1666
INFO:robust_rl.robust_rl_trainer:Iteration 12 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.01889038085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 149.98110961914062
INFO:tools.evaluation_results_class:Average Discounted Reward = 67.83100891113281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23205.259765625
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.66155208025967
INFO:tools.evaluation_results_class:Counted Episodes = 3389
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.3774027824401855
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Step: 5, Training loss: 3.2584688663482666
INFO:agents.father_agent:Step: 10, Training loss: 2.934138774871826
INFO:agents.father_agent:Step: 15, Training loss: 2.2140603065490723
INFO:agents.father_agent:Step: 20, Training loss: 3.1877365112304688
INFO:agents.father_agent:Step: 25, Training loss: 2.657866954803467
INFO:agents.father_agent:Step: 30, Training loss: 2.6486849784851074
INFO:agents.father_agent:Step: 35, Training loss: 2.099083185195923
INFO:agents.father_agent:Step: 40, Training loss: 2.369929552078247
INFO:agents.father_agent:Step: 45, Training loss: 2.8654286861419678
INFO:agents.father_agent:Step: 50, Training loss: 2.8274707794189453
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -166.77711486816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 153.22288513183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 69.79360961914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23046.6796875
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.27756199581715
INFO:tools.evaluation_results_class:Counted Episodes = 3347
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -167.56314086914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 152.43685913085938
INFO:tools.evaluation_results_class:Average Discounted Reward = 68.55619049072266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24028.94921875
INFO:tools.evaluation_results_class:Current Best Return = -167.56314086914062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.91834354039078
INFO:tools.evaluation_results_class:Counted Episodes = 3429
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -163.8125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 156.1875
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.86708068847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21920.142578125
INFO:tools.evaluation_results_class:Current Best Return = -163.8125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.19839449541284
INFO:tools.evaluation_results_class:Counted Episodes = 3488
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 266.91711929648363
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 13 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -172.1122589111328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 147.8877410888672
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.44381713867188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24097.6875
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.09487330583383
INFO:tools.evaluation_results_class:Counted Episodes = 3394
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.2341036796569824
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Step: 5, Training loss: 2.6739890575408936
INFO:agents.father_agent:Step: 10, Training loss: 3.1207473278045654
INFO:agents.father_agent:Step: 15, Training loss: 2.365830659866333
INFO:agents.father_agent:Step: 20, Training loss: 2.9294538497924805
INFO:agents.father_agent:Step: 25, Training loss: 2.8162925243377686
INFO:agents.father_agent:Step: 30, Training loss: 2.647217035293579
INFO:agents.father_agent:Step: 35, Training loss: 1.9059429168701172
INFO:agents.father_agent:Step: 40, Training loss: 2.9988553524017334
INFO:agents.father_agent:Step: 45, Training loss: 2.4172089099884033
INFO:agents.father_agent:Step: 50, Training loss: 2.7617762088775635
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -170.63734436035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 149.36265563964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 66.42268371582031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22535.080078125
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.81295807681032
INFO:tools.evaluation_results_class:Counted Episodes = 3411
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.7551727294922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 149.2448272705078
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.77655792236328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23979.384765625
INFO:tools.evaluation_results_class:Current Best Return = -170.7551727294922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.49195402298851
INFO:tools.evaluation_results_class:Counted Episodes = 3480
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.40977478027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 146.59022521972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.952674865722656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23113.126953125
INFO:tools.evaluation_results_class:Current Best Return = -173.40977478027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.21264367816092
INFO:tools.evaluation_results_class:Counted Episodes = 3480
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 264.25444285418126
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 14 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -177.604736328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 142.395263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.75693893432617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23895.08203125
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.03313609467456
INFO:tools.evaluation_results_class:Counted Episodes = 3380
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.3803415298461914
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Step: 5, Training loss: 2.462268352508545
INFO:agents.father_agent:Step: 10, Training loss: 2.5181822776794434
INFO:agents.father_agent:Step: 15, Training loss: 2.630019426345825
INFO:agents.father_agent:Step: 20, Training loss: 2.7435059547424316
INFO:agents.father_agent:Step: 25, Training loss: 2.5560028553009033
INFO:agents.father_agent:Step: 30, Training loss: 2.0392684936523438
INFO:agents.father_agent:Step: 35, Training loss: 2.6012020111083984
INFO:agents.father_agent:Step: 40, Training loss: 2.4072742462158203
INFO:agents.father_agent:Step: 45, Training loss: 2.0935263633728027
INFO:agents.father_agent:Step: 50, Training loss: 2.270294189453125
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -176.56546020507812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 143.43453979492188
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.01076126098633
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23258.3046875
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.31009120329509
INFO:tools.evaluation_results_class:Counted Episodes = 3399
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -179.20225524902344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 140.79774475097656
INFO:tools.evaluation_results_class:Average Discounted Reward = 57.35588455200195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23764.68359375
INFO:tools.evaluation_results_class:Current Best Return = -179.20225524902344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.90292668791655
INFO:tools.evaluation_results_class:Counted Episodes = 3451
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -179.3809356689453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 140.6190643310547
INFO:tools.evaluation_results_class:Average Discounted Reward = 57.96416473388672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22930.376953125
INFO:tools.evaluation_results_class:Current Best Return = -179.3809356689453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.6086956521739
INFO:tools.evaluation_results_class:Counted Episodes = 3473
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 260.32610686515784
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 15 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.57534790039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 131.42465209960938
INFO:tools.evaluation_results_class:Average Discounted Reward = 50.45814514160156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25918.728515625
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.3433806146572
INFO:tools.evaluation_results_class:Counted Episodes = 3384
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.0398685932159424
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Step: 5, Training loss: 2.4094324111938477
INFO:agents.father_agent:Step: 10, Training loss: 2.7638304233551025
INFO:agents.father_agent:Step: 15, Training loss: 2.3516640663146973
INFO:agents.father_agent:Step: 20, Training loss: 2.3693690299987793
INFO:agents.father_agent:Step: 25, Training loss: 2.711428642272949
INFO:agents.father_agent:Step: 30, Training loss: 2.3575594425201416
INFO:agents.father_agent:Step: 35, Training loss: 2.7397115230560303
INFO:agents.father_agent:Step: 40, Training loss: 2.911350727081299
INFO:agents.father_agent:Step: 45, Training loss: 2.531499147415161
INFO:agents.father_agent:Step: 50, Training loss: 2.521111011505127
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -187.65907287597656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 132.34092712402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 51.14567947387695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26129.515625
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.2302456348032
INFO:tools.evaluation_results_class:Counted Episodes = 3379
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -193.3153076171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 126.68468475341797
INFO:tools.evaluation_results_class:Average Discounted Reward = 45.92765426635742
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27314.23046875
INFO:tools.evaluation_results_class:Current Best Return = -193.3153076171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.84511981297487
INFO:tools.evaluation_results_class:Counted Episodes = 3422
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -186.2515106201172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 133.7484893798828
INFO:tools.evaluation_results_class:Average Discounted Reward = 52.41990280151367
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26432.958984375
INFO:tools.evaluation_results_class:Current Best Return = -186.2515106201172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.66858955869628
INFO:tools.evaluation_results_class:Counted Episodes = 3467
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=6, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 261.3697355427542
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=6, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 16 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -198.90286254882812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 121.0971450805664
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.60521697998047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26578.541015625
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.60902255639098
INFO:tools.evaluation_results_class:Counted Episodes = 3325
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.492063045501709
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 7.81%
INFO:agents.father_agent:Step: 5, Training loss: 2.1361279487609863
INFO:agents.father_agent:Step: 10, Training loss: 2.703460454940796
INFO:agents.father_agent:Step: 15, Training loss: 2.485830307006836
INFO:agents.father_agent:Step: 20, Training loss: 2.5306601524353027
INFO:agents.father_agent:Step: 25, Training loss: 2.711073875427246
INFO:agents.father_agent:Step: 30, Training loss: 2.748992681503296
INFO:agents.father_agent:Step: 35, Training loss: 2.075909376144409
INFO:agents.father_agent:Step: 40, Training loss: 2.6705899238586426
INFO:agents.father_agent:Step: 45, Training loss: 2.5670828819274902
INFO:agents.father_agent:Step: 50, Training loss: 3.122056245803833
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 9.38%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -200.97174072265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 119.02825927734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 39.5191535949707
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30883.685546875
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.6393193558189
INFO:tools.evaluation_results_class:Counted Episodes = 3291
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -204.24903869628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 115.75096130371094
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.65204620361328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30306.572265625
INFO:tools.evaluation_results_class:Current Best Return = -204.24903869628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.77459379615952
INFO:tools.evaluation_results_class:Counted Episodes = 3385
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -197.7948455810547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 122.20515441894531
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.609867095947266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29842.92578125
INFO:tools.evaluation_results_class:Current Best Return = -197.7948455810547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.04975124378109
INFO:tools.evaluation_results_class:Counted Episodes = 3417
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=6, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 258.1725601083618
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=6, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.50336456298828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.49664306640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 147.85476684570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12289.572265625
INFO:tools.evaluation_results_class:Current Best Return = -76.50336456298828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.89113149847095
INFO:tools.evaluation_results_class:Counted Episodes = 1635
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -151.661376953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 168.338623046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.84368896484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6634.91455078125
INFO:tools.evaluation_results_class:Current Best Return = -151.661376953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.63026235509457
INFO:tools.evaluation_results_class:Counted Episodes = 1639
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.77864837646484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.22134399414062
INFO:tools.evaluation_results_class:Average Discounted Reward = 147.3423614501953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12225.7705078125
INFO:tools.evaluation_results_class:Current Best Return = -77.77864837646484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.88419782870929
INFO:tools.evaluation_results_class:Counted Episodes = 1658
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.90689086914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.09310913085938
INFO:tools.evaluation_results_class:Average Discounted Reward = 151.7523193359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10178.5107421875
INFO:tools.evaluation_results_class:Current Best Return = -70.90689086914062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.55018137847642
INFO:tools.evaluation_results_class:Counted Episodes = 1654
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -116.94132232666016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 203.05868530273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 116.84290313720703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25067.970703125
INFO:tools.evaluation_results_class:Current Best Return = -116.94132232666016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.32273838630807
INFO:tools.evaluation_results_class:Counted Episodes = 1636
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -198.74484252929688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 121.2551498413086
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.04784393310547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23041.908203125
INFO:tools.evaluation_results_class:Current Best Return = -198.74484252929688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.95030303030303
INFO:tools.evaluation_results_class:Counted Episodes = 1650
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.3266143798828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 178.6733856201172
INFO:tools.evaluation_results_class:Average Discounted Reward = 97.9556655883789
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32018.373046875
INFO:tools.evaluation_results_class:Current Best Return = -141.3266143798828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.12576312576313
INFO:tools.evaluation_results_class:Counted Episodes = 1638
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.60025024414062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 185.39974975585938
INFO:tools.evaluation_results_class:Average Discounted Reward = 103.5085220336914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27003.09375
INFO:tools.evaluation_results_class:Current Best Return = -134.60025024414062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.91931540342298
INFO:tools.evaluation_results_class:Counted Episodes = 1636
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.350830078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 181.649169921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 99.7269515991211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24843.888671875
INFO:tools.evaluation_results_class:Current Best Return = -138.350830078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.93776693105552
INFO:tools.evaluation_results_class:Counted Episodes = 1639
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.3290252685547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 181.6709747314453
INFO:tools.evaluation_results_class:Average Discounted Reward = 100.16542053222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25055.298828125
INFO:tools.evaluation_results_class:Current Best Return = -138.3290252685547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.54735547355473
INFO:tools.evaluation_results_class:Counted Episodes = 1626
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.8290557861328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 181.1709442138672
INFO:tools.evaluation_results_class:Average Discounted Reward = 99.11702728271484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25644.23046875
INFO:tools.evaluation_results_class:Current Best Return = -138.8290557861328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.15873015873017
INFO:tools.evaluation_results_class:Counted Episodes = 1638
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -198.8804473876953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 121.11956024169922
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.848384857177734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23035.3671875
INFO:tools.evaluation_results_class:Current Best Return = -198.8804473876953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.45125689760883
INFO:tools.evaluation_results_class:Counted Episodes = 1631
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -252.50669860839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.49330139160156
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.003448963165283
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23768.6171875
INFO:tools.evaluation_results_class:Current Best Return = -252.50669860839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.98538367844093
INFO:tools.evaluation_results_class:Counted Episodes = 1642
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -252.69187927246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.30811309814453
INFO:tools.evaluation_results_class:Average Discounted Reward = -5.784890651702881
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23993.240234375
INFO:tools.evaluation_results_class:Current Best Return = -252.69187927246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.58999389871873
INFO:tools.evaluation_results_class:Counted Episodes = 1639
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -250.75888061523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.24112701416016
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.622526168823242
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26507.05078125
INFO:tools.evaluation_results_class:Current Best Return = -250.75888061523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.15667074663402
INFO:tools.evaluation_results_class:Counted Episodes = 1634
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -254.368896484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 65.63109588623047
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.334090232849121
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27657.169921875
INFO:tools.evaluation_results_class:Current Best Return = -254.368896484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.9109756097561
INFO:tools.evaluation_results_class:Counted Episodes = 1640
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -251.4215087890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 68.57849884033203
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.963252067565918
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26548.78515625
INFO:tools.evaluation_results_class:Current Best Return = -251.4215087890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.75687232742823
INFO:tools.evaluation_results_class:Counted Episodes = 1637
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -255.1750030517578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 64.82499694824219
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.485228538513184
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27710.716796875
INFO:tools.evaluation_results_class:Current Best Return = -255.1750030517578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.99146341463414
INFO:tools.evaluation_results_class:Counted Episodes = 1640
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -256.9865417480469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.01347351074219
INFO:tools.evaluation_results_class:Average Discounted Reward = -12.50546932220459
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28614.3125
INFO:tools.evaluation_results_class:Current Best Return = -256.9865417480469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.65033680342927
INFO:tools.evaluation_results_class:Counted Episodes = 1633
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -261.5101013183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 58.48991012573242
INFO:tools.evaluation_results_class:Average Discounted Reward = -16.943357467651367
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24712.025390625
INFO:tools.evaluation_results_class:Current Best Return = -261.5101013183594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.20672782874618
INFO:tools.evaluation_results_class:Counted Episodes = 1635
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -261.2272033691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 58.77280807495117
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.283367156982422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20226.732421875
INFO:tools.evaluation_results_class:Current Best Return = -261.2272033691406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.6797583081571
INFO:tools.evaluation_results_class:Counted Episodes = 1655
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.00302124023438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.99697875976562
INFO:tools.evaluation_results_class:Average Discounted Reward = 147.60107421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12280.0673828125
INFO:tools.evaluation_results_class:Current Best Return = -77.00302124023438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.85126964933495
INFO:tools.evaluation_results_class:Counted Episodes = 1654
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -147.9921112060547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 172.0078887939453
INFO:tools.evaluation_results_class:Average Discounted Reward = 81.78527069091797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6873.58935546875
INFO:tools.evaluation_results_class:Current Best Return = -147.9921112060547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.80449301760777
INFO:tools.evaluation_results_class:Counted Episodes = 1647
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.19670104980469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.8032989501953
INFO:tools.evaluation_results_class:Average Discounted Reward = 142.8151092529297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14057.0703125
INFO:tools.evaluation_results_class:Current Best Return = -83.19670104980469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.44899205864387
INFO:tools.evaluation_results_class:Counted Episodes = 1637
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.29652404785156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.70347595214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 152.9744415283203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9988.2919921875
INFO:tools.evaluation_results_class:Current Best Return = -69.29652404785156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.64490543014033
INFO:tools.evaluation_results_class:Counted Episodes = 1639
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -111.95709991455078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 208.04290771484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 120.76261138916016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20534.228515625
INFO:tools.evaluation_results_class:Current Best Return = -111.95709991455078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.16495468277945
INFO:tools.evaluation_results_class:Counted Episodes = 1655
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -194.19488525390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 125.80511474609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.973785400390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20828.365234375
INFO:tools.evaluation_results_class:Current Best Return = -194.19488525390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.80998781973203
INFO:tools.evaluation_results_class:Counted Episodes = 1642
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -137.17945861816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 182.82054138183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 100.28433990478516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23860.796875
INFO:tools.evaluation_results_class:Current Best Return = -137.17945861816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.32084592145016
INFO:tools.evaluation_results_class:Counted Episodes = 1655
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -132.8667449951172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 187.1332550048828
INFO:tools.evaluation_results_class:Average Discounted Reward = 103.99284362792969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24750.943359375
INFO:tools.evaluation_results_class:Current Best Return = -132.8667449951172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.29133858267717
INFO:tools.evaluation_results_class:Counted Episodes = 1651
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -144.3941192626953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 175.6058807373047
INFO:tools.evaluation_results_class:Average Discounted Reward = 94.29895782470703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28806.65625
INFO:tools.evaluation_results_class:Current Best Return = -144.3941192626953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.52386780905752
INFO:tools.evaluation_results_class:Counted Episodes = 1634
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -146.4832763671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 173.5167236328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 93.88809967041016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29540.013671875
INFO:tools.evaluation_results_class:Current Best Return = -146.4832763671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.63161094224924
INFO:tools.evaluation_results_class:Counted Episodes = 1645
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -139.75772094726562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 180.24227905273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 98.99771881103516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28164.435546875
INFO:tools.evaluation_results_class:Current Best Return = -139.75772094726562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.57116898849182
INFO:tools.evaluation_results_class:Counted Episodes = 1651
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -194.0943603515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 125.9056396484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 45.010501861572266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21767.021484375
INFO:tools.evaluation_results_class:Current Best Return = -194.0943603515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.35049019607843
INFO:tools.evaluation_results_class:Counted Episodes = 1632
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -251.49391174316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 68.5060806274414
INFO:tools.evaluation_results_class:Average Discounted Reward = -3.808932304382324
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26194.93359375
INFO:tools.evaluation_results_class:Current Best Return = -251.49391174316406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.01946472019465
INFO:tools.evaluation_results_class:Counted Episodes = 1644
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -249.78428649902344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 70.21571350097656
INFO:tools.evaluation_results_class:Average Discounted Reward = -2.5537145137786865
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23767.94140625
INFO:tools.evaluation_results_class:Current Best Return = -249.78428649902344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.54199395770392
INFO:tools.evaluation_results_class:Counted Episodes = 1655
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -250.64553833007812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.3544692993164
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.543031692504883
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29194.51953125
INFO:tools.evaluation_results_class:Current Best Return = -250.64553833007812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.39975845410628
INFO:tools.evaluation_results_class:Counted Episodes = 1656
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -254.30868530273438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 65.69132232666016
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.265271186828613
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29796.38671875
INFO:tools.evaluation_results_class:Current Best Return = -254.30868530273438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.4437652811736
INFO:tools.evaluation_results_class:Counted Episodes = 1636
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -246.2405548095703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 73.75944519042969
INFO:tools.evaluation_results_class:Average Discounted Reward = -1.8438183069229126
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31075.482421875
INFO:tools.evaluation_results_class:Current Best Return = -246.2405548095703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.84703059388123
INFO:tools.evaluation_results_class:Counted Episodes = 1667
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -242.2971649169922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 77.70283508300781
INFO:tools.evaluation_results_class:Average Discounted Reward = -0.4649113714694977
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23053.435546875
INFO:tools.evaluation_results_class:Current Best Return = -242.2971649169922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.88004822182037
INFO:tools.evaluation_results_class:Counted Episodes = 1659
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -250.76161193847656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.23838806152344
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.751913070678711
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28881.798828125
INFO:tools.evaluation_results_class:Current Best Return = -250.76161193847656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.69926650366749
INFO:tools.evaluation_results_class:Counted Episodes = 1636
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -253.243896484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 66.75609588623047
INFO:tools.evaluation_results_class:Average Discounted Reward = -9.437850952148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24076.728515625
INFO:tools.evaluation_results_class:Current Best Return = -253.243896484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.11219512195122
INFO:tools.evaluation_results_class:Counted Episodes = 1640
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -251.6375732421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 68.3624267578125
INFO:tools.evaluation_results_class:Average Discounted Reward = -5.819769859313965
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18973.33984375
INFO:tools.evaluation_results_class:Current Best Return = -251.6375732421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.48848484848484
INFO:tools.evaluation_results_class:Counted Episodes = 1650
INFO:robust_rl.robust_rl_trainer:Iteration 17 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -200.60665893554688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 119.39334106445312
INFO:tools.evaluation_results_class:Average Discounted Reward = 39.33213424682617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27536.201171875
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.8965204236006
INFO:tools.evaluation_results_class:Counted Episodes = 3305
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.5367813110351562
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 10.94%
INFO:agents.father_agent:Step: 5, Training loss: 2.6425840854644775
INFO:agents.father_agent:Step: 10, Training loss: 2.2403101921081543
INFO:agents.father_agent:Step: 15, Training loss: 2.822394847869873
INFO:agents.father_agent:Step: 20, Training loss: 1.9293214082717896
INFO:agents.father_agent:Step: 25, Training loss: 2.8031015396118164
INFO:agents.father_agent:Step: 30, Training loss: 2.8707761764526367
INFO:agents.father_agent:Step: 35, Training loss: 2.188732862472534
INFO:agents.father_agent:Step: 40, Training loss: 2.6486380100250244
INFO:agents.father_agent:Step: 45, Training loss: 2.870680809020996
INFO:agents.father_agent:Step: 50, Training loss: 2.5808513164520264
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -193.7083740234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 126.2916259765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 46.25761413574219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26539.54296875
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.78403336312184
INFO:tools.evaluation_results_class:Counted Episodes = 3357
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -197.1220703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 122.87793731689453
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.896968841552734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27102.033203125
INFO:tools.evaluation_results_class:Current Best Return = -197.1220703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.16431924882629
INFO:tools.evaluation_results_class:Counted Episodes = 3408
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.6408233642578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 129.3591766357422
INFO:tools.evaluation_results_class:Average Discounted Reward = 48.67910385131836
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24185.490234375
INFO:tools.evaluation_results_class:Current Best Return = -190.6408233642578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.78754684347074
INFO:tools.evaluation_results_class:Counted Episodes = 3469
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=6, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 240.91968696167174
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=6, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 18 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -200.60218811035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 119.3978042602539
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.730987548828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25572.65625
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.36907582938389
INFO:tools.evaluation_results_class:Counted Episodes = 3376
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.116671085357666
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 2.9501824378967285
INFO:agents.father_agent:Step: 10, Training loss: 2.1355979442596436
INFO:agents.father_agent:Step: 15, Training loss: 2.3876872062683105
INFO:agents.father_agent:Step: 20, Training loss: 2.0795323848724365
INFO:agents.father_agent:Step: 25, Training loss: 2.5979714393615723
INFO:agents.father_agent:Step: 30, Training loss: 2.7033793926239014
INFO:agents.father_agent:Step: 35, Training loss: 2.894608974456787
INFO:agents.father_agent:Step: 40, Training loss: 3.143350839614868
INFO:agents.father_agent:Step: 45, Training loss: 2.880321979522705
INFO:agents.father_agent:Step: 50, Training loss: 3.1553168296813965
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -186.27247619628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 133.72752380371094
INFO:tools.evaluation_results_class:Average Discounted Reward = 53.410884857177734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24441.66796875
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.79988088147707
INFO:tools.evaluation_results_class:Counted Episodes = 3358
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.82533264160156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 128.17466735839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 48.783241271972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26903.75390625
INFO:tools.evaluation_results_class:Current Best Return = -191.82533264160156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.0122878876536
INFO:tools.evaluation_results_class:Counted Episodes = 3418
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -189.4888916015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 130.5111083984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 50.69633102416992
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25381.65234375
INFO:tools.evaluation_results_class:Current Best Return = -189.4888916015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.7988455988456
INFO:tools.evaluation_results_class:Counted Episodes = 3465
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=6, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 224.65093819476164
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=6, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 19 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.35546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 131.64453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 51.45295333862305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23928.798828125
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.78742958790394
INFO:tools.evaluation_results_class:Counted Episodes = 3373
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.0810508728027344
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 3.315354585647583
INFO:agents.father_agent:Step: 10, Training loss: 2.6735410690307617
INFO:agents.father_agent:Step: 15, Training loss: 2.5899107456207275
INFO:agents.father_agent:Step: 20, Training loss: 2.9729580879211426
INFO:agents.father_agent:Step: 25, Training loss: 2.7191390991210938
INFO:agents.father_agent:Step: 30, Training loss: 2.5564069747924805
INFO:agents.father_agent:Step: 35, Training loss: 3.027346611022949
INFO:agents.father_agent:Step: 40, Training loss: 2.8549699783325195
INFO:agents.father_agent:Step: 45, Training loss: 3.058964252471924
INFO:agents.father_agent:Step: 50, Training loss: 2.919614791870117
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -186.23239135742188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 133.76760864257812
INFO:tools.evaluation_results_class:Average Discounted Reward = 54.196380615234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23317.255859375
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.4063245823389
INFO:tools.evaluation_results_class:Counted Episodes = 3352
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -186.38778686523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 133.61221313476562
INFO:tools.evaluation_results_class:Average Discounted Reward = 54.038543701171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24321.138671875
INFO:tools.evaluation_results_class:Current Best Return = -186.38778686523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.27558139534884
INFO:tools.evaluation_results_class:Counted Episodes = 3440
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -183.0992431640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 136.9007568359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 56.76298522949219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23949.798828125
INFO:tools.evaluation_results_class:Current Best Return = -183.0992431640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.26639582124201
INFO:tools.evaluation_results_class:Counted Episodes = 3446
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=6, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 241.68871788334414
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=6, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 20 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.99880981445312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 128.00119018554688
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.13918685913086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25431.9921875
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.23769758425291
INFO:tools.evaluation_results_class:Counted Episodes = 3353
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.812567710876465
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 2.6611392498016357
INFO:agents.father_agent:Step: 10, Training loss: 2.3971948623657227
INFO:agents.father_agent:Step: 15, Training loss: 2.2369933128356934
INFO:agents.father_agent:Step: 20, Training loss: 2.662274122238159
INFO:agents.father_agent:Step: 25, Training loss: 2.509211540222168
INFO:agents.father_agent:Step: 30, Training loss: 2.360527276992798
INFO:agents.father_agent:Step: 35, Training loss: 1.8511523008346558
INFO:agents.father_agent:Step: 40, Training loss: 2.529764413833618
INFO:agents.father_agent:Step: 45, Training loss: 2.3615267276763916
INFO:agents.father_agent:Step: 50, Training loss: 1.964677333831787
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -170.3988800048828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 149.6011199951172
INFO:tools.evaluation_results_class:Average Discounted Reward = 67.48434448242188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20848.232421875
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.44405078240331
INFO:tools.evaluation_results_class:Counted Episodes = 3387
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -172.8763427734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 147.1236572265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.609130859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19919.791015625
INFO:tools.evaluation_results_class:Current Best Return = -172.8763427734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.39762525340284
INFO:tools.evaluation_results_class:Counted Episodes = 3453
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -171.08963012695312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 148.91036987304688
INFO:tools.evaluation_results_class:Average Discounted Reward = 66.5194320678711
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19147.923828125
INFO:tools.evaluation_results_class:Current Best Return = -171.08963012695312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.4985590778098
INFO:tools.evaluation_results_class:Counted Episodes = 3470
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 242.2512641850783
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 21 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -177.57298278808594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 142.42701721191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.916404724121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22280.66796875
INFO:tools.evaluation_results_class:Current Best Return = -156.04922485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.35328811560012
INFO:tools.evaluation_results_class:Counted Episodes = 3391
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.9596470594406128
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 2.050548791885376
INFO:agents.father_agent:Step: 10, Training loss: 2.913689613342285
INFO:agents.father_agent:Step: 15, Training loss: 1.9107952117919922
INFO:agents.father_agent:Step: 20, Training loss: 2.106895923614502
INFO:agents.father_agent:Step: 25, Training loss: 1.8751990795135498
INFO:agents.father_agent:Step: 30, Training loss: 2.039367914199829
INFO:agents.father_agent:Step: 35, Training loss: 1.8348357677459717
INFO:agents.father_agent:Step: 40, Training loss: 2.4535837173461914
INFO:agents.father_agent:Step: 45, Training loss: 1.901678204536438
INFO:agents.father_agent:Step: 50, Training loss: 1.7369170188903809
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -135.89154052734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 184.10845947265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 97.99192810058594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15382.0625
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.80188124632569
INFO:tools.evaluation_results_class:Counted Episodes = 3402
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.37112426757812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 178.62887573242188
INFO:tools.evaluation_results_class:Average Discounted Reward = 93.69139099121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17353.54296875
INFO:tools.evaluation_results_class:Current Best Return = -141.37112426757812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.40355912743972
INFO:tools.evaluation_results_class:Counted Episodes = 3484
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.21658325195312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 165.78341674804688
INFO:tools.evaluation_results_class:Average Discounted Reward = 82.77833557128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20189.490234375
INFO:tools.evaluation_results_class:Current Best Return = -154.21658325195312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.13784907902554
INFO:tools.evaluation_results_class:Counted Episodes = 3366
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 282.896438144759
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -115.48798370361328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 204.51202392578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 117.60079193115234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16387.814453125
INFO:tools.evaluation_results_class:Current Best Return = -115.48798370361328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.25
INFO:tools.evaluation_results_class:Counted Episodes = 1664
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -115.54815673828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 204.45184326171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 115.16985321044922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14498.99609375
INFO:tools.evaluation_results_class:Current Best Return = -115.54815673828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.28299643281808
INFO:tools.evaluation_results_class:Counted Episodes = 1682
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.57296752929688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 162.42703247070312
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.447021484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14234.12890625
INFO:tools.evaluation_results_class:Current Best Return = -157.57296752929688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.50930930930932
INFO:tools.evaluation_results_class:Counted Episodes = 1665
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -140.53562927246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 179.46437072753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 89.65713500976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6865.08251953125
INFO:tools.evaluation_results_class:Current Best Return = -140.53562927246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.21140142517815
INFO:tools.evaluation_results_class:Counted Episodes = 1684
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.4505386352539
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 200.54946899414062
INFO:tools.evaluation_results_class:Average Discounted Reward = 111.64891052246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11458.927734375
INFO:tools.evaluation_results_class:Current Best Return = -119.4505386352539
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.44815256257449
INFO:tools.evaluation_results_class:Counted Episodes = 1678
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -100.86053466796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 219.13946533203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 132.30960083007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14214.0751953125
INFO:tools.evaluation_results_class:Current Best Return = -100.86053466796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.11513353115727
INFO:tools.evaluation_results_class:Counted Episodes = 1685
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.78268432617188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 131.21731567382812
INFO:tools.evaluation_results_class:Average Discounted Reward = 48.324703216552734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13953.103515625
INFO:tools.evaluation_results_class:Current Best Return = -188.78268432617188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.15522388059702
INFO:tools.evaluation_results_class:Counted Episodes = 1675
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -185.99880981445312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 134.00119018554688
INFO:tools.evaluation_results_class:Average Discounted Reward = 51.63173294067383
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17835.677734375
INFO:tools.evaluation_results_class:Current Best Return = -185.99880981445312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.11499703615887
INFO:tools.evaluation_results_class:Counted Episodes = 1687
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.363525390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 131.636474609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.019126892089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15490.7060546875
INFO:tools.evaluation_results_class:Current Best Return = -188.363525390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.21930870083433
INFO:tools.evaluation_results_class:Counted Episodes = 1678
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -187.6680450439453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 132.3319549560547
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.4913215637207
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15577.4794921875
INFO:tools.evaluation_results_class:Current Best Return = -187.6680450439453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.52756372258447
INFO:tools.evaluation_results_class:Counted Episodes = 1687
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.2128448486328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 128.7871551513672
INFO:tools.evaluation_results_class:Average Discounted Reward = 47.553749084472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18236.61328125
INFO:tools.evaluation_results_class:Current Best Return = -191.2128448486328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.16884661117717
INFO:tools.evaluation_results_class:Counted Episodes = 1682
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -108.26061248779297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 211.73939514160156
INFO:tools.evaluation_results_class:Average Discounted Reward = 127.05513763427734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17891.166015625
INFO:tools.evaluation_results_class:Current Best Return = -108.26061248779297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.93664076509265
INFO:tools.evaluation_results_class:Counted Episodes = 1673
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -109.62147521972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 210.37852478027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 126.23762512207031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17945.302734375
INFO:tools.evaluation_results_class:Current Best Return = -109.62147521972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.32873425314936
INFO:tools.evaluation_results_class:Counted Episodes = 1667
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -103.32939147949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 216.6706085205078
INFO:tools.evaluation_results_class:Average Discounted Reward = 131.1608123779297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15510.4462890625
INFO:tools.evaluation_results_class:Current Best Return = -103.32939147949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.7244234180958
INFO:tools.evaluation_results_class:Counted Episodes = 1691
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -106.10089874267578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 213.89910888671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 128.60321044921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17001.134765625
INFO:tools.evaluation_results_class:Current Best Return = -106.10089874267578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.5420895522388
INFO:tools.evaluation_results_class:Counted Episodes = 1675
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -104.54637145996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 215.45362854003906
INFO:tools.evaluation_results_class:Average Discounted Reward = 129.25747680664062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15587.0537109375
INFO:tools.evaluation_results_class:Current Best Return = -104.54637145996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.26516052318668
INFO:tools.evaluation_results_class:Counted Episodes = 1682
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -105.02710723876953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 214.97288513183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 128.98379516601562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18535.75390625
INFO:tools.evaluation_results_class:Current Best Return = -105.02710723876953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.73132530120482
INFO:tools.evaluation_results_class:Counted Episodes = 1660
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -105.2222900390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 214.7777099609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 128.69972229003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16280.287109375
INFO:tools.evaluation_results_class:Current Best Return = -105.2222900390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.85680047932894
INFO:tools.evaluation_results_class:Counted Episodes = 1669
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -109.42203521728516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 210.57797241210938
INFO:tools.evaluation_results_class:Average Discounted Reward = 125.59880828857422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17926.435546875
INFO:tools.evaluation_results_class:Current Best Return = -109.42203521728516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.69476219145093
INFO:tools.evaluation_results_class:Counted Episodes = 1661
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -95.54925537109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 224.45074462890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.3784637451172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9232.3291015625
INFO:tools.evaluation_results_class:Current Best Return = -95.54925537109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.63522388059701
INFO:tools.evaluation_results_class:Counted Episodes = 1675
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -95.28246307373047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 224.717529296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.38455200195312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9148.998046875
INFO:tools.evaluation_results_class:Current Best Return = -95.28246307373047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.75164572112507
INFO:tools.evaluation_results_class:Counted Episodes = 1671
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -94.9368667602539
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 225.06312561035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 135.17782592773438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9767.00390625
INFO:tools.evaluation_results_class:Current Best Return = -94.9368667602539
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.25491363907088
INFO:tools.evaluation_results_class:Counted Episodes = 1679
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.13157653808594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 141.86842346191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 55.05403518676758
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9202.287109375
INFO:tools.evaluation_results_class:Current Best Return = -178.13157653808594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.4066985645933
INFO:tools.evaluation_results_class:Counted Episodes = 1672
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -176.0694580078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 143.9305419921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 56.96019744873047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9112.3046875
INFO:tools.evaluation_results_class:Current Best Return = -176.0694580078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.05061801059446
INFO:tools.evaluation_results_class:Counted Episodes = 1699
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -261.0011901855469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 58.998802185058594
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.690223693847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18844.203125
INFO:tools.evaluation_results_class:Current Best Return = -261.0011901855469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.83882564409826
INFO:tools.evaluation_results_class:Counted Episodes = 1669
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -255.42916870117188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 64.57083129882812
INFO:tools.evaluation_results_class:Average Discounted Reward = -14.194754600524902
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17396.046875
INFO:tools.evaluation_results_class:Current Best Return = -255.42916870117188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.34190077704721
INFO:tools.evaluation_results_class:Counted Episodes = 1673
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -147.66810607910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 172.33189392089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 91.08280181884766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22334.978515625
INFO:tools.evaluation_results_class:Current Best Return = -147.66810607910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.63664404688464
INFO:tools.evaluation_results_class:Counted Episodes = 1621
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.9814910888672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 185.0185089111328
INFO:tools.evaluation_results_class:Average Discounted Reward = 97.590087890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19791.447265625
INFO:tools.evaluation_results_class:Current Best Return = -134.9814910888672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.7661937075879
INFO:tools.evaluation_results_class:Counted Episodes = 1621
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -182.49197387695312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 137.50802612304688
INFO:tools.evaluation_results_class:Average Discounted Reward = 56.82624435424805
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21556.11328125
INFO:tools.evaluation_results_class:Current Best Return = -182.49197387695312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.94197530864197
INFO:tools.evaluation_results_class:Counted Episodes = 1620
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -152.87136840820312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 167.12863159179688
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.38976287841797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9045.4853515625
INFO:tools.evaluation_results_class:Current Best Return = -152.87136840820312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.3222016079159
INFO:tools.evaluation_results_class:Counted Episodes = 1617
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -116.39556121826172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 203.60443115234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 114.20112609863281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13877.140625
INFO:tools.evaluation_results_class:Current Best Return = -116.39556121826172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.04004929143561
INFO:tools.evaluation_results_class:Counted Episodes = 1623
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.5766372680664
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 200.42335510253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 117.50064086914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19365.13671875
INFO:tools.evaluation_results_class:Current Best Return = -119.5766372680664
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.19777503090235
INFO:tools.evaluation_results_class:Counted Episodes = 1618
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -214.14002990722656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 105.8599624633789
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.2155818939209
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22141.7890625
INFO:tools.evaluation_results_class:Current Best Return = -214.14002990722656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.19555829734732
INFO:tools.evaluation_results_class:Counted Episodes = 1621
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -204.79176330566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 115.2082290649414
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.09800720214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22594.125
INFO:tools.evaluation_results_class:Current Best Return = -204.79176330566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.35257985257985
INFO:tools.evaluation_results_class:Counted Episodes = 1628
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -212.72755432128906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 107.27244567871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.78772735595703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26969.169921875
INFO:tools.evaluation_results_class:Current Best Return = -212.72755432128906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.92507739938081
INFO:tools.evaluation_results_class:Counted Episodes = 1615
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -208.5192108154297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 111.48079681396484
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.324703216552734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23744.6015625
INFO:tools.evaluation_results_class:Current Best Return = -208.5192108154297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.3358116480793
INFO:tools.evaluation_results_class:Counted Episodes = 1614
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -210.576904296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.42310333251953
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.107086181640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22942.546875
INFO:tools.evaluation_results_class:Current Best Return = -210.576904296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.95120444718962
INFO:tools.evaluation_results_class:Counted Episodes = 1619
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -129.0043182373047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 190.9956817626953
INFO:tools.evaluation_results_class:Average Discounted Reward = 110.54399108886719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23915.794921875
INFO:tools.evaluation_results_class:Current Best Return = -129.0043182373047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.70660901791229
INFO:tools.evaluation_results_class:Counted Episodes = 1619
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -126.35555267333984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 193.64443969726562
INFO:tools.evaluation_results_class:Average Discounted Reward = 113.08688354492188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21995.12890625
INFO:tools.evaluation_results_class:Current Best Return = -126.35555267333984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.79012345679013
INFO:tools.evaluation_results_class:Counted Episodes = 1620
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -125.82411193847656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 194.17588806152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 113.39544677734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22874.74609375
INFO:tools.evaluation_results_class:Current Best Return = -125.82411193847656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.79458794587946
INFO:tools.evaluation_results_class:Counted Episodes = 1626
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -118.42681121826172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 201.57318115234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 118.60189819335938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19512.515625
INFO:tools.evaluation_results_class:Current Best Return = -118.42681121826172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.69126691266912
INFO:tools.evaluation_results_class:Counted Episodes = 1626
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -128.2895050048828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 191.7104949951172
INFO:tools.evaluation_results_class:Average Discounted Reward = 111.21868133544922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24568.330078125
INFO:tools.evaluation_results_class:Current Best Return = -128.2895050048828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.28271604938271
INFO:tools.evaluation_results_class:Counted Episodes = 1620
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -126.88235473632812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 193.11764526367188
INFO:tools.evaluation_results_class:Average Discounted Reward = 113.1220932006836
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23886.126953125
INFO:tools.evaluation_results_class:Current Best Return = -126.88235473632812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.75490196078431
INFO:tools.evaluation_results_class:Counted Episodes = 1632
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -127.19583892822266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 192.80416870117188
INFO:tools.evaluation_results_class:Average Discounted Reward = 112.13406372070312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23601.365234375
INFO:tools.evaluation_results_class:Current Best Return = -127.19583892822266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.51652386780906
INFO:tools.evaluation_results_class:Counted Episodes = 1634
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -130.25555419921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 189.74444580078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 109.7451400756836
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26834.84765625
INFO:tools.evaluation_results_class:Current Best Return = -130.25555419921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.22716049382716
INFO:tools.evaluation_results_class:Counted Episodes = 1620
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -107.33622741699219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 212.6637725830078
INFO:tools.evaluation_results_class:Average Discounted Reward = 124.296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9738.5615234375
INFO:tools.evaluation_results_class:Current Best Return = -107.33622741699219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.99007444168734
INFO:tools.evaluation_results_class:Counted Episodes = 1612
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -109.5125961303711
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 210.48739624023438
INFO:tools.evaluation_results_class:Average Discounted Reward = 123.40814971923828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9798.1982421875
INFO:tools.evaluation_results_class:Current Best Return = -109.5125961303711
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.32083589428396
INFO:tools.evaluation_results_class:Counted Episodes = 1627
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -107.18226623535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 212.81773376464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 124.41136932373047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10239.8369140625
INFO:tools.evaluation_results_class:Current Best Return = -107.18226623535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.18226600985221
INFO:tools.evaluation_results_class:Counted Episodes = 1624
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -185.59567260742188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 134.40432739257812
INFO:tools.evaluation_results_class:Average Discounted Reward = 48.219482421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10742.068359375
INFO:tools.evaluation_results_class:Current Best Return = -185.59567260742188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.04567901234567
INFO:tools.evaluation_results_class:Counted Episodes = 1620
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.50733947753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 131.49266052246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 45.78396987915039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9134.4072265625
INFO:tools.evaluation_results_class:Current Best Return = -188.50733947753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.45410036719706
INFO:tools.evaluation_results_class:Counted Episodes = 1634
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -280.1136474609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.8863639831543
INFO:tools.evaluation_results_class:Average Discounted Reward = -33.75102996826172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23572.0703125
INFO:tools.evaluation_results_class:Current Best Return = -280.1136474609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.80958230958231
INFO:tools.evaluation_results_class:Counted Episodes = 1628
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -278.69659423828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.30341720581055
INFO:tools.evaluation_results_class:Average Discounted Reward = -32.20563888549805
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24189.646484375
INFO:tools.evaluation_results_class:Current Best Return = -278.69659423828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.53601953601954
INFO:tools.evaluation_results_class:Counted Episodes = 1638
INFO:robust_rl.robust_rl_trainer:Iteration 22 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -144.79811096191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 175.20188903808594
INFO:tools.evaluation_results_class:Average Discounted Reward = 89.27037811279297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16205.49609375
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.03842743127402
INFO:tools.evaluation_results_class:Counted Episodes = 3383
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.8120110034942627
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 2.035001516342163
INFO:agents.father_agent:Step: 10, Training loss: 2.127439022064209
INFO:agents.father_agent:Step: 15, Training loss: 1.7761820554733276
INFO:agents.father_agent:Step: 20, Training loss: 1.9181711673736572
INFO:agents.father_agent:Step: 25, Training loss: 1.5327351093292236
INFO:agents.father_agent:Step: 30, Training loss: 1.646757960319519
INFO:agents.father_agent:Step: 35, Training loss: 1.7848697900772095
INFO:agents.father_agent:Step: 40, Training loss: 2.613203287124634
INFO:agents.father_agent:Step: 45, Training loss: 1.6114369630813599
INFO:agents.father_agent:Step: 50, Training loss: 1.8406176567077637
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -140.72933959960938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 179.27066040039062
INFO:tools.evaluation_results_class:Average Discounted Reward = 92.71223449707031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18863.470703125
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.1517119244392
INFO:tools.evaluation_results_class:Counted Episodes = 3388
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.7497100830078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 174.2502899169922
INFO:tools.evaluation_results_class:Average Discounted Reward = 88.97322082519531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18779.0390625
INFO:tools.evaluation_results_class:Current Best Return = -145.7497100830078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.6316091954023
INFO:tools.evaluation_results_class:Counted Episodes = 3480
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -146.71551513671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 173.28448486328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 87.08145904541016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21032.583984375
INFO:tools.evaluation_results_class:Current Best Return = -146.71551513671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.50694239290989
INFO:tools.evaluation_results_class:Counted Episodes = 3385
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 282.01141500654734
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 23 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -152.5422821044922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 167.4577178955078
INFO:tools.evaluation_results_class:Average Discounted Reward = 82.93685913085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20671.70703125
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.14665878178593
INFO:tools.evaluation_results_class:Counted Episodes = 3382
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.9668790102005005
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 1.6836352348327637
INFO:agents.father_agent:Step: 10, Training loss: 1.9552733898162842
INFO:agents.father_agent:Step: 15, Training loss: 2.138712167739868
INFO:agents.father_agent:Step: 20, Training loss: 1.76729154586792
INFO:agents.father_agent:Step: 25, Training loss: 2.104405641555786
INFO:agents.father_agent:Step: 30, Training loss: 2.4076788425445557
INFO:agents.father_agent:Step: 35, Training loss: 1.835347056388855
INFO:agents.father_agent:Step: 40, Training loss: 2.258347749710083
INFO:agents.father_agent:Step: 45, Training loss: 1.8660544157028198
INFO:agents.father_agent:Step: 50, Training loss: 2.036482572555542
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -192.6345977783203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 127.36539459228516
INFO:tools.evaluation_results_class:Average Discounted Reward = 47.639122009277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38081.90625
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.12558450233801
INFO:tools.evaluation_results_class:Counted Episodes = 2994
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -202.0888671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 117.91114044189453
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.85129928588867
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42392.49609375
INFO:tools.evaluation_results_class:Current Best Return = -202.0888671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.3451384417257
INFO:tools.evaluation_results_class:Counted Episodes = 3106
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.07254028320312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 129.92745971679688
INFO:tools.evaluation_results_class:Average Discounted Reward = 47.84287643432617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44479.13671875
INFO:tools.evaluation_results_class:Current Best Return = -190.07254028320312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 98.07886855241264
INFO:tools.evaluation_results_class:Counted Episodes = 3005
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 346.545390961753
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 24 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -212.4365234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 107.5634765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.26348304748535
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42864.0546875
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.63978673775408
INFO:tools.evaluation_results_class:Counted Episodes = 3001
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.3508212566375732
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 1.6997082233428955
INFO:agents.father_agent:Step: 10, Training loss: 2.277564764022827
INFO:agents.father_agent:Step: 15, Training loss: 2.4256691932678223
INFO:agents.father_agent:Step: 20, Training loss: 2.2163209915161133
INFO:agents.father_agent:Step: 25, Training loss: 2.328336000442505
INFO:agents.father_agent:Step: 30, Training loss: 2.4592666625976562
INFO:agents.father_agent:Step: 35, Training loss: 2.2413277626037598
INFO:agents.father_agent:Step: 40, Training loss: 2.284205913543701
INFO:agents.father_agent:Step: 45, Training loss: 2.024838924407959
INFO:agents.father_agent:Step: 50, Training loss: 2.025758743286133
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -169.38253784179688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 150.61746215820312
INFO:tools.evaluation_results_class:Average Discounted Reward = 69.07291412353516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21930.72265625
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.22662721893491
INFO:tools.evaluation_results_class:Counted Episodes = 3380
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -168.36184692382812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 151.63815307617188
INFO:tools.evaluation_results_class:Average Discounted Reward = 69.38317108154297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22250.716796875
INFO:tools.evaluation_results_class:Current Best Return = -168.36184692382812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.04783995360974
INFO:tools.evaluation_results_class:Counted Episodes = 3449
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -163.80992126464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 156.19007873535156
INFO:tools.evaluation_results_class:Average Discounted Reward = 71.47643280029297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24357.048828125
INFO:tools.evaluation_results_class:Current Best Return = -163.80992126464844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.52420306965762
INFO:tools.evaluation_results_class:Counted Episodes = 3388
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 287.4328087601145
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 25 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -179.64505004882812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 140.35494995117188
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.37725067138672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22332.2265625
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.86878905087771
INFO:tools.evaluation_results_class:Counted Episodes = 3361
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.309211254119873
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 2.69046688079834
INFO:agents.father_agent:Step: 10, Training loss: 2.0837976932525635
INFO:agents.father_agent:Step: 15, Training loss: 2.1241276264190674
INFO:agents.father_agent:Step: 20, Training loss: 2.2729992866516113
INFO:agents.father_agent:Step: 25, Training loss: 2.3610353469848633
INFO:agents.father_agent:Step: 30, Training loss: 2.1540768146514893
INFO:agents.father_agent:Step: 35, Training loss: 2.409681558609009
INFO:agents.father_agent:Step: 40, Training loss: 2.4289603233337402
INFO:agents.father_agent:Step: 45, Training loss: 2.0781750679016113
INFO:agents.father_agent:Step: 50, Training loss: 2.198228597640991
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -166.26895141601562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 153.73104858398438
INFO:tools.evaluation_results_class:Average Discounted Reward = 70.67603302001953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20615.3359375
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.29880011706175
INFO:tools.evaluation_results_class:Counted Episodes = 3417
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.53451538085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 146.46548461914062
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.33743286132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21221.169921875
INFO:tools.evaluation_results_class:Current Best Return = -173.53451538085938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.385500575374
INFO:tools.evaluation_results_class:Counted Episodes = 3476
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.27793884277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 149.72206115722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.60713195800781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22284.498046875
INFO:tools.evaluation_results_class:Current Best Return = -170.27793884277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.38358556461002
INFO:tools.evaluation_results_class:Counted Episodes = 3436
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 281.71065369187886
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 26 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -179.0303192138672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 140.9696807861328
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.40965270996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22734.3984375
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.98175397292525
INFO:tools.evaluation_results_class:Counted Episodes = 3398
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.7176845073699951
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 1.8537265062332153
INFO:agents.father_agent:Step: 10, Training loss: 2.105168104171753
INFO:agents.father_agent:Step: 15, Training loss: 2.1101229190826416
INFO:agents.father_agent:Step: 20, Training loss: 2.5330986976623535
INFO:agents.father_agent:Step: 25, Training loss: 2.3727991580963135
INFO:agents.father_agent:Step: 30, Training loss: 2.3204808235168457
INFO:agents.father_agent:Step: 35, Training loss: 2.460681915283203
INFO:agents.father_agent:Step: 40, Training loss: 2.6795589923858643
INFO:agents.father_agent:Step: 45, Training loss: 2.578211545944214
INFO:agents.father_agent:Step: 50, Training loss: 2.7587785720825195
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -179.07728576660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 140.92271423339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.149845123291016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20503.537109375
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.06489675516224
INFO:tools.evaluation_results_class:Counted Episodes = 3390
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.0645294189453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 141.9354705810547
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.00812911987305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21323.439453125
INFO:tools.evaluation_results_class:Current Best Return = -178.0645294189453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.67332952598515
INFO:tools.evaluation_results_class:Counted Episodes = 3502
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.40017700195312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 141.59982299804688
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.3642578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24786.626953125
INFO:tools.evaluation_results_class:Current Best Return = -178.40017700195312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.81307870370371
INFO:tools.evaluation_results_class:Counted Episodes = 3456
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 278.327114516961
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -172.5826416015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 147.4173583984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.0488052368164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18702.048828125
INFO:tools.evaluation_results_class:Current Best Return = -172.5826416015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.00826446280992
INFO:tools.evaluation_results_class:Counted Episodes = 1694
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -171.4014129638672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 148.5985870361328
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.47852325439453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16180.1416015625
INFO:tools.evaluation_results_class:Current Best Return = -171.4014129638672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.94114184814597
INFO:tools.evaluation_results_class:Counted Episodes = 1699
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.97979736328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 129.02020263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 50.861854553222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16262.2900390625
INFO:tools.evaluation_results_class:Current Best Return = -190.97979736328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.11170528817587
INFO:tools.evaluation_results_class:Counted Episodes = 1683
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -140.348388671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 179.651611328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 88.1134033203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4065.886962890625
INFO:tools.evaluation_results_class:Current Best Return = -140.348388671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.97621878715815
INFO:tools.evaluation_results_class:Counted Episodes = 1682
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.101585388183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.8984069824219
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.85299682617188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5786.88623046875
INFO:tools.evaluation_results_class:Current Best Return = -49.101585388183594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.4092777451556
INFO:tools.evaluation_results_class:Counted Episodes = 1703
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -106.35035705566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 213.64964294433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 131.8268585205078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16302.78125
INFO:tools.evaluation_results_class:Current Best Return = -106.35035705566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.61282660332542
INFO:tools.evaluation_results_class:Counted Episodes = 1684
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -199.2841796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 120.7158203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.679439544677734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17514.00390625
INFO:tools.evaluation_results_class:Current Best Return = -199.2841796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.4776119402985
INFO:tools.evaluation_results_class:Counted Episodes = 1675
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -199.02920532226562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 120.9708023071289
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.097652435302734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18176.501953125
INFO:tools.evaluation_results_class:Current Best Return = -199.02920532226562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.20858164481525
INFO:tools.evaluation_results_class:Counted Episodes = 1678
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -195.09405517578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 124.90595245361328
INFO:tools.evaluation_results_class:Average Discounted Reward = 46.08909225463867
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16628.669921875
INFO:tools.evaluation_results_class:Current Best Return = -195.09405517578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.0547619047619
INFO:tools.evaluation_results_class:Counted Episodes = 1680
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -198.33969116210938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 121.66030883789062
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.26935577392578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17915.384765625
INFO:tools.evaluation_results_class:Current Best Return = -198.33969116210938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.00476758045292
INFO:tools.evaluation_results_class:Counted Episodes = 1678
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -200.86776733398438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 119.13223266601562
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.42842483520508
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17984.470703125
INFO:tools.evaluation_results_class:Current Best Return = -200.86776733398438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.70720188902007
INFO:tools.evaluation_results_class:Counted Episodes = 1694
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -109.48014068603516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 210.5198516845703
INFO:tools.evaluation_results_class:Average Discounted Reward = 129.12608337402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16369.13671875
INFO:tools.evaluation_results_class:Current Best Return = -109.48014068603516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.16123295791346
INFO:tools.evaluation_results_class:Counted Episodes = 1687
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -108.23213195800781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 211.7678680419922
INFO:tools.evaluation_results_class:Average Discounted Reward = 130.86282348632812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16438.775390625
INFO:tools.evaluation_results_class:Current Best Return = -108.23213195800781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.28883638511518
INFO:tools.evaluation_results_class:Counted Episodes = 1693
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -110.8769302368164
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 209.12306213378906
INFO:tools.evaluation_results_class:Average Discounted Reward = 128.43629455566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18723.1875
INFO:tools.evaluation_results_class:Current Best Return = -110.8769302368164
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.07966706302021
INFO:tools.evaluation_results_class:Counted Episodes = 1682
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -108.02420043945312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 211.97579956054688
INFO:tools.evaluation_results_class:Average Discounted Reward = 130.66436767578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17960.44921875
INFO:tools.evaluation_results_class:Current Best Return = -108.02420043945312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.78866587957496
INFO:tools.evaluation_results_class:Counted Episodes = 1694
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -112.52412414550781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 207.4758758544922
INFO:tools.evaluation_results_class:Average Discounted Reward = 126.74969482421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20079.8515625
INFO:tools.evaluation_results_class:Current Best Return = -112.52412414550781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.35854675402025
INFO:tools.evaluation_results_class:Counted Episodes = 1679
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -104.19281005859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 215.80718994140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 133.2527313232422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16430.646484375
INFO:tools.evaluation_results_class:Current Best Return = -104.19281005859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.74646226415095
INFO:tools.evaluation_results_class:Counted Episodes = 1696
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -104.06331634521484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 215.9366912841797
INFO:tools.evaluation_results_class:Average Discounted Reward = 133.3980712890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15484.380859375
INFO:tools.evaluation_results_class:Current Best Return = -104.06331634521484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.55384615384615
INFO:tools.evaluation_results_class:Counted Episodes = 1690
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -110.6352310180664
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 209.36476135253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 128.40936279296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17287.3828125
INFO:tools.evaluation_results_class:Current Best Return = -110.6352310180664
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.99406880189798
INFO:tools.evaluation_results_class:Counted Episodes = 1686
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -65.75456237792969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.2454376220703
INFO:tools.evaluation_results_class:Average Discounted Reward = 157.8953399658203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5197.40185546875
INFO:tools.evaluation_results_class:Current Best Return = -65.75456237792969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.60918187168923
INFO:tools.evaluation_results_class:Counted Episodes = 1699
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -65.64727020263672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.3527374267578
INFO:tools.evaluation_results_class:Average Discounted Reward = 157.9038543701172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7563.1328125
INFO:tools.evaluation_results_class:Current Best Return = -65.64727020263672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.1686460807601
INFO:tools.evaluation_results_class:Counted Episodes = 1684
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.24002075195312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.75997924804688
INFO:tools.evaluation_results_class:Average Discounted Reward = 155.47975158691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5111.7451171875
INFO:tools.evaluation_results_class:Current Best Return = -68.24002075195312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.16914830256106
INFO:tools.evaluation_results_class:Counted Episodes = 1679
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.18601989746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 166.81398010253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 73.04347229003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5602.0673828125
INFO:tools.evaluation_results_class:Current Best Return = -153.18601989746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.05450236966824
INFO:tools.evaluation_results_class:Counted Episodes = 1688
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.80755615234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 164.19244384765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 71.468505859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6578.51806640625
INFO:tools.evaluation_results_class:Current Best Return = -155.80755615234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.94805194805195
INFO:tools.evaluation_results_class:Counted Episodes = 1694
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -289.399658203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 30.60035514831543
INFO:tools.evaluation_results_class:Average Discounted Reward = -40.91033935546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17752.048828125
INFO:tools.evaluation_results_class:Current Best Return = -289.399658203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.06512729425695
INFO:tools.evaluation_results_class:Counted Episodes = 1689
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -288.0539245605469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 31.946075439453125
INFO:tools.evaluation_results_class:Average Discounted Reward = -40.72966766357422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18008.42578125
INFO:tools.evaluation_results_class:Current Best Return = -288.0539245605469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.66267225883763
INFO:tools.evaluation_results_class:Counted Episodes = 1669
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -281.0141296386719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.98585891723633
INFO:tools.evaluation_results_class:Average Discounted Reward = -34.13357925415039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18184.810546875
INFO:tools.evaluation_results_class:Current Best Return = -281.0141296386719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.5875073659399
INFO:tools.evaluation_results_class:Counted Episodes = 1697
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -296.96258544921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 23.037410736083984
INFO:tools.evaluation_results_class:Average Discounted Reward = -47.4755973815918
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15516.8955078125
INFO:tools.evaluation_results_class:Current Best Return = -296.96258544921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.93705463182899
INFO:tools.evaluation_results_class:Counted Episodes = 1684
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -293.25
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 26.75
INFO:tools.evaluation_results_class:Average Discounted Reward = -44.79249572753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14158.4345703125
INFO:tools.evaluation_results_class:Current Best Return = -293.25
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.10807600950119
INFO:tools.evaluation_results_class:Counted Episodes = 1684
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -290.6839294433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 29.316070556640625
INFO:tools.evaluation_results_class:Average Discounted Reward = -42.826194763183594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14452.982421875
INFO:tools.evaluation_results_class:Current Best Return = -290.6839294433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.02261904761905
INFO:tools.evaluation_results_class:Counted Episodes = 1680
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -295.3502197265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 24.649791717529297
INFO:tools.evaluation_results_class:Average Discounted Reward = -46.12041091918945
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15222.07421875
INFO:tools.evaluation_results_class:Current Best Return = -295.3502197265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.87611673615247
INFO:tools.evaluation_results_class:Counted Episodes = 1679
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.49464416503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 164.50535583496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 83.48098754882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19647.57421875
INFO:tools.evaluation_results_class:Current Best Return = -155.49464416503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.31547619047619
INFO:tools.evaluation_results_class:Counted Episodes = 1680
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.6907958984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 160.3092041015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.80259704589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15106.119140625
INFO:tools.evaluation_results_class:Current Best Return = -159.6907958984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.00598086124401
INFO:tools.evaluation_results_class:Counted Episodes = 1672
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -168.93626403808594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 151.06373596191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 66.05575561523438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17375.728515625
INFO:tools.evaluation_results_class:Current Best Return = -168.93626403808594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.91401082381239
INFO:tools.evaluation_results_class:Counted Episodes = 1663
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.17347717285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 181.82652282714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 89.69683074951172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5293.203125
INFO:tools.evaluation_results_class:Current Best Return = -138.17347717285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.99052693901717
INFO:tools.evaluation_results_class:Counted Episodes = 1689
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.20071792602539
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.7992858886719
INFO:tools.evaluation_results_class:Average Discounted Reward = 160.7326202392578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9115.9873046875
INFO:tools.evaluation_results_class:Current Best Return = -60.20071792602539
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.70940683043739
INFO:tools.evaluation_results_class:Counted Episodes = 1669
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -98.60785675048828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 221.39215087890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 136.0406494140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20550.54296875
INFO:tools.evaluation_results_class:Current Best Return = -98.60785675048828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.190332326284
INFO:tools.evaluation_results_class:Counted Episodes = 1655
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.9939422607422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 129.0060577392578
INFO:tools.evaluation_results_class:Average Discounted Reward = 48.143253326416016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21935.408203125
INFO:tools.evaluation_results_class:Current Best Return = -190.9939422607422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.62666666666667
INFO:tools.evaluation_results_class:Counted Episodes = 1650
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -196.3953857421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 123.60460662841797
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.49300765991211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23056.751953125
INFO:tools.evaluation_results_class:Current Best Return = -196.3953857421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.37659187386295
INFO:tools.evaluation_results_class:Counted Episodes = 1649
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -189.3931121826172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 130.6068878173828
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.569820404052734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21029.458984375
INFO:tools.evaluation_results_class:Current Best Return = -189.3931121826172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.91425120772946
INFO:tools.evaluation_results_class:Counted Episodes = 1656
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -180.08084106445312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 139.91915893554688
INFO:tools.evaluation_results_class:Average Discounted Reward = 56.64847183227539
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18692.669921875
INFO:tools.evaluation_results_class:Current Best Return = -180.08084106445312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.51137724550898
INFO:tools.evaluation_results_class:Counted Episodes = 1670
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.18138122558594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 128.81861877441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 48.705718994140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22098.9453125
INFO:tools.evaluation_results_class:Current Best Return = -191.18138122558594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.71831831831832
INFO:tools.evaluation_results_class:Counted Episodes = 1665
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -98.3842544555664
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 221.61575317382812
INFO:tools.evaluation_results_class:Average Discounted Reward = 136.7257843017578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17003.791015625
INFO:tools.evaluation_results_class:Current Best Return = -98.3842544555664
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.97631734754293
INFO:tools.evaluation_results_class:Counted Episodes = 1689
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -106.30829620361328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 213.6916961669922
INFO:tools.evaluation_results_class:Average Discounted Reward = 130.48876953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24155.150390625
INFO:tools.evaluation_results_class:Current Best Return = -106.30829620361328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.48031496062993
INFO:tools.evaluation_results_class:Counted Episodes = 1651
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -96.11992645263672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 223.88006591796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 137.68609619140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18630.298828125
INFO:tools.evaluation_results_class:Current Best Return = -96.11992645263672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.40399757722592
INFO:tools.evaluation_results_class:Counted Episodes = 1651
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -99.79314422607422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 220.20684814453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 135.40159606933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18753.31640625
INFO:tools.evaluation_results_class:Current Best Return = -99.79314422607422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.96812988574865
INFO:tools.evaluation_results_class:Counted Episodes = 1663
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -99.60977172851562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 220.39022827148438
INFO:tools.evaluation_results_class:Average Discounted Reward = 135.61378479003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20192.2890625
INFO:tools.evaluation_results_class:Current Best Return = -99.60977172851562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.86972255729795
INFO:tools.evaluation_results_class:Counted Episodes = 1658
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -95.61998748779297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 224.3800048828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 138.5889892578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17452.142578125
INFO:tools.evaluation_results_class:Current Best Return = -95.61998748779297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.62238180730101
INFO:tools.evaluation_results_class:Counted Episodes = 1671
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -96.747314453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 223.252685546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 138.31512451171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17550.359375
INFO:tools.evaluation_results_class:Current Best Return = -96.747314453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.10488676996424
INFO:tools.evaluation_results_class:Counted Episodes = 1678
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -104.38172149658203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 215.6182861328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 132.26934814453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21335.841796875
INFO:tools.evaluation_results_class:Current Best Return = -104.38172149658203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.65352449223417
INFO:tools.evaluation_results_class:Counted Episodes = 1674
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.07108306884766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.9289093017578
INFO:tools.evaluation_results_class:Average Discounted Reward = 147.05784606933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8385.642578125
INFO:tools.evaluation_results_class:Current Best Return = -80.07108306884766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.63373493975904
INFO:tools.evaluation_results_class:Counted Episodes = 1660
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.48419952392578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.51580810546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 144.4969482421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7716.64501953125
INFO:tools.evaluation_results_class:Current Best Return = -83.48419952392578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.37686344663089
INFO:tools.evaluation_results_class:Counted Episodes = 1677
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.36090850830078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.6390838623047
INFO:tools.evaluation_results_class:Average Discounted Reward = 147.18072509765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7345.81396484375
INFO:tools.evaluation_results_class:Current Best Return = -80.36090850830078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.64508393285372
INFO:tools.evaluation_results_class:Counted Episodes = 1668
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.63746643066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 153.36253356933594
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.52131271362305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7835.33251953125
INFO:tools.evaluation_results_class:Current Best Return = -166.63746643066406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.87552870090634
INFO:tools.evaluation_results_class:Counted Episodes = 1655
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.80287170410156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 154.19712829589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.75718688964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8754.1171875
INFO:tools.evaluation_results_class:Current Best Return = -165.80287170410156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.52965847813061
INFO:tools.evaluation_results_class:Counted Episodes = 1669
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -275.6178283691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.38217544555664
INFO:tools.evaluation_results_class:Average Discounted Reward = -30.926624298095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20623.16796875
INFO:tools.evaluation_results_class:Current Best Return = -275.6178283691406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.39593301435407
INFO:tools.evaluation_results_class:Counted Episodes = 1672
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -269.0218811035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.97810745239258
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.068805694580078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18074.728515625
INFO:tools.evaluation_results_class:Current Best Return = -269.0218811035156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.19289940828402
INFO:tools.evaluation_results_class:Counted Episodes = 1690
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -278.3030700683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.69693374633789
INFO:tools.evaluation_results_class:Average Discounted Reward = -33.622276306152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21583.2109375
INFO:tools.evaluation_results_class:Current Best Return = -278.3030700683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.19182200841853
INFO:tools.evaluation_results_class:Counted Episodes = 1663
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -279.0263671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.9736213684082
INFO:tools.evaluation_results_class:Average Discounted Reward = -35.56986618041992
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17108.375
INFO:tools.evaluation_results_class:Current Best Return = -279.0263671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.55155875299761
INFO:tools.evaluation_results_class:Counted Episodes = 1668
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -271.68505859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.314937591552734
INFO:tools.evaluation_results_class:Average Discounted Reward = -30.209653854370117
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15484.736328125
INFO:tools.evaluation_results_class:Current Best Return = -271.68505859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.54469106178765
INFO:tools.evaluation_results_class:Counted Episodes = 1667
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -274.7384948730469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.261512756347656
INFO:tools.evaluation_results_class:Average Discounted Reward = -31.80625343322754
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15581.146484375
INFO:tools.evaluation_results_class:Current Best Return = -274.7384948730469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.83825265643448
INFO:tools.evaluation_results_class:Counted Episodes = 1694
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -281.4612731933594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.53874206542969
INFO:tools.evaluation_results_class:Average Discounted Reward = -38.19217300415039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16837.826171875
INFO:tools.evaluation_results_class:Current Best Return = -281.4612731933594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.27602905569007
INFO:tools.evaluation_results_class:Counted Episodes = 1652
INFO:robust_rl.robust_rl_trainer:Iteration 27 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -189.26002502441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 130.73997497558594
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.679134368896484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21306.962890625
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.3620283018868
INFO:tools.evaluation_results_class:Counted Episodes = 3392
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.5227115154266357
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 2.2930045127868652
INFO:agents.father_agent:Step: 10, Training loss: 2.466953754425049
INFO:agents.father_agent:Step: 15, Training loss: 2.417773962020874
INFO:agents.father_agent:Step: 20, Training loss: 2.227749824523926
INFO:agents.father_agent:Step: 25, Training loss: 2.313715696334839
INFO:agents.father_agent:Step: 30, Training loss: 2.3174307346343994
INFO:agents.father_agent:Step: 35, Training loss: 2.5949156284332275
INFO:agents.father_agent:Step: 40, Training loss: 2.6327455043792725
INFO:agents.father_agent:Step: 45, Training loss: 2.214574098587036
INFO:agents.father_agent:Step: 50, Training loss: 2.015223264694214
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -208.86642456054688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 111.13358306884766
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.9815559387207
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30774.017578125
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.66882117574639
INFO:tools.evaluation_results_class:Counted Episodes = 3249
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -206.88294982910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 113.11705017089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.1265869140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30585.9296875
INFO:tools.evaluation_results_class:Current Best Return = -206.88294982910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.89369961182443
INFO:tools.evaluation_results_class:Counted Episodes = 3349
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -197.4872589111328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 122.51274108886719
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.42189407348633
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30601.708984375
INFO:tools.evaluation_results_class:Current Best Return = -197.4872589111328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.79733009708738
INFO:tools.evaluation_results_class:Counted Episodes = 3296
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 304.5252481720008
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 28 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -216.1749267578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 103.8250732421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 27.80169677734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31870.529296875
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.22324159021407
INFO:tools.evaluation_results_class:Counted Episodes = 3270
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.777883291244507
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 2.047037124633789
INFO:agents.father_agent:Step: 10, Training loss: 2.260866165161133
INFO:agents.father_agent:Step: 15, Training loss: 1.873937964439392
INFO:agents.father_agent:Step: 20, Training loss: 2.39713454246521
INFO:agents.father_agent:Step: 25, Training loss: 2.1474661827087402
INFO:agents.father_agent:Step: 30, Training loss: 1.9874496459960938
INFO:agents.father_agent:Step: 35, Training loss: 1.9131311178207397
INFO:agents.father_agent:Step: 40, Training loss: 2.317918062210083
INFO:agents.father_agent:Step: 45, Training loss: 1.6871713399887085
INFO:agents.father_agent:Step: 50, Training loss: 1.7168693542480469
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -241.4188232421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 78.5811767578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.893561363220215
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 48781.390625
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.78779927845196
INFO:tools.evaluation_results_class:Counted Episodes = 3049
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -237.66912841796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 82.33087158203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.936892032623291
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45323.27734375
INFO:tools.evaluation_results_class:Current Best Return = -237.66912841796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.33762057877813
INFO:tools.evaluation_results_class:Counted Episodes = 3110
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -229.8697052001953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 90.13029479980469
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.073781967163086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 48326.6796875
INFO:tools.evaluation_results_class:Current Best Return = -229.8697052001953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.14046603216278
INFO:tools.evaluation_results_class:Counted Episodes = 3047
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 338.62659117792197
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 29 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -244.75338745117188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 75.2466049194336
INFO:tools.evaluation_results_class:Average Discounted Reward = 2.011828660964966
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45879.20703125
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.73121482952665
INFO:tools.evaluation_results_class:Counted Episodes = 3021
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.4800925254821777
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 1.8581383228302002
INFO:agents.father_agent:Step: 10, Training loss: 1.9301544427871704
INFO:agents.father_agent:Step: 15, Training loss: 1.6642249822616577
INFO:agents.father_agent:Step: 20, Training loss: 1.8949379920959473
INFO:agents.father_agent:Step: 25, Training loss: 1.869723916053772
INFO:agents.father_agent:Step: 30, Training loss: 1.726921796798706
INFO:agents.father_agent:Step: 35, Training loss: 1.8408703804016113
INFO:agents.father_agent:Step: 40, Training loss: 2.0926380157470703
INFO:agents.father_agent:Step: 45, Training loss: 1.8571298122406006
INFO:agents.father_agent:Step: 50, Training loss: 2.287968635559082
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -216.81712341308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 103.18287658691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 25.50432014465332
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28713.7421875
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.19729978521019
INFO:tools.evaluation_results_class:Counted Episodes = 3259
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -220.32810974121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 99.67189025878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.300378799438477
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29961.080078125
INFO:tools.evaluation_results_class:Current Best Return = -220.32810974121094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.95798816568048
INFO:tools.evaluation_results_class:Counted Episodes = 3380
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -204.83819580078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 115.16181182861328
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.350433349609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30720.775390625
INFO:tools.evaluation_results_class:Current Best Return = -204.83819580078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.70734669095324
INFO:tools.evaluation_results_class:Counted Episodes = 3294
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 302.0103628821574
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 30 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -230.87681579589844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 89.12317657470703
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.631064414978027
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31804.8359375
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.06887992553521
INFO:tools.evaluation_results_class:Counted Episodes = 3223
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.694602608680725
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 2.363278388977051
INFO:agents.father_agent:Step: 10, Training loss: 1.6346598863601685
INFO:agents.father_agent:Step: 15, Training loss: 1.6187915802001953
INFO:agents.father_agent:Step: 20, Training loss: 1.5933016538619995
INFO:agents.father_agent:Step: 25, Training loss: 1.8547550439834595
INFO:agents.father_agent:Step: 30, Training loss: 1.6483001708984375
INFO:agents.father_agent:Step: 35, Training loss: 1.9867053031921387
INFO:agents.father_agent:Step: 40, Training loss: 1.6284865140914917
INFO:agents.father_agent:Step: 45, Training loss: 1.9013488292694092
INFO:agents.father_agent:Step: 50, Training loss: 1.8376103639602661
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -250.95545959472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.04454040527344
INFO:tools.evaluation_results_class:Average Discounted Reward = -4.089633464813232
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 43908.81640625
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.49323655559222
INFO:tools.evaluation_results_class:Counted Episodes = 3031
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -257.81634521484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 62.18366241455078
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.8126606941223145
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 47070.97265625
INFO:tools.evaluation_results_class:Current Best Return = -257.81634521484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.8752374920836
INFO:tools.evaluation_results_class:Counted Episodes = 3158
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -241.71206665039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 78.2879409790039
INFO:tools.evaluation_results_class:Average Discounted Reward = 2.159456968307495
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 50072.64453125
INFO:tools.evaluation_results_class:Current Best Return = -241.71206665039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.96757457846952
INFO:tools.evaluation_results_class:Counted Episodes = 3084
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 332.50140173170416
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 31 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -257.5907897949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 62.40922546386719
INFO:tools.evaluation_results_class:Average Discounted Reward = -8.893372535705566
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44206.7890625
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.66470395812888
INFO:tools.evaluation_results_class:Counted Episodes = 3057
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.6496285200119019
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 1.350277066230774
INFO:agents.father_agent:Step: 10, Training loss: 1.6603041887283325
INFO:agents.father_agent:Step: 15, Training loss: 2.044538736343384
INFO:agents.father_agent:Step: 20, Training loss: 1.750848412513733
INFO:agents.father_agent:Step: 25, Training loss: 2.0381579399108887
INFO:agents.father_agent:Step: 30, Training loss: 1.4566566944122314
INFO:agents.father_agent:Step: 35, Training loss: 2.1794519424438477
INFO:agents.father_agent:Step: 40, Training loss: 1.7270803451538086
INFO:agents.father_agent:Step: 45, Training loss: 2.2767786979675293
INFO:agents.father_agent:Step: 50, Training loss: 1.6463029384613037
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -251.17210388183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 68.8279037475586
INFO:tools.evaluation_results_class:Average Discounted Reward = -4.078611373901367
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 46205.6015625
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.39113428943938
INFO:tools.evaluation_results_class:Counted Episodes = 3068
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -250.1338348388672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.86616516113281
INFO:tools.evaluation_results_class:Average Discounted Reward = -3.170409679412842
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45393.87109375
INFO:tools.evaluation_results_class:Current Best Return = -250.1338348388672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.08356896010054
INFO:tools.evaluation_results_class:Counted Episodes = 3183
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -248.73216247558594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.26783752441406
INFO:tools.evaluation_results_class:Average Discounted Reward = -3.8755850791931152
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 50424.61328125
INFO:tools.evaluation_results_class:Current Best Return = -248.73216247558594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.87029831387808
INFO:tools.evaluation_results_class:Counted Episodes = 3084
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 326.9594682064859
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -204.01316833496094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 115.98682403564453
INFO:tools.evaluation_results_class:Average Discounted Reward = 47.42461395263672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 51803.73046875
INFO:tools.evaluation_results_class:Current Best Return = -204.01316833496094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.21870882740448
INFO:tools.evaluation_results_class:Counted Episodes = 1518
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -204.70314025878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 115.2968521118164
INFO:tools.evaluation_results_class:Average Discounted Reward = 46.55781173706055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44216.87890625
INFO:tools.evaluation_results_class:Current Best Return = -204.70314025878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.64482306684141
INFO:tools.evaluation_results_class:Counted Episodes = 1526
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -231.7984161376953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 88.20158386230469
INFO:tools.evaluation_results_class:Average Discounted Reward = 17.28561019897461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40598.57421875
INFO:tools.evaluation_results_class:Current Best Return = -231.7984161376953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.01520158625247
INFO:tools.evaluation_results_class:Counted Episodes = 1513
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.8787078857422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 178.1212921142578
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.73847198486328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3940.96875
INFO:tools.evaluation_results_class:Current Best Return = -141.8787078857422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.43375082399473
INFO:tools.evaluation_results_class:Counted Episodes = 1517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.0033073425293
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.9967041015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 151.04489135742188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3814.133056640625
INFO:tools.evaluation_results_class:Current Best Return = -59.0033073425293
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.29497354497354
INFO:tools.evaluation_results_class:Counted Episodes = 1512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.44561767578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 164.55438232421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 91.48687744140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41838.0078125
INFO:tools.evaluation_results_class:Current Best Return = -155.44561767578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.75411997363217
INFO:tools.evaluation_results_class:Counted Episodes = 1517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -237.3412628173828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 82.65872955322266
INFO:tools.evaluation_results_class:Average Discounted Reward = 11.2741117477417
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45906.17578125
INFO:tools.evaluation_results_class:Current Best Return = -237.3412628173828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.87830687830687
INFO:tools.evaluation_results_class:Counted Episodes = 1512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -240.54098510742188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.45901489257812
INFO:tools.evaluation_results_class:Average Discounted Reward = 9.080955505371094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41079.515625
INFO:tools.evaluation_results_class:Current Best Return = -240.54098510742188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.2911475409836
INFO:tools.evaluation_results_class:Counted Episodes = 1525
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -240.9921417236328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.00785827636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 9.320716857910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42887.5234375
INFO:tools.evaluation_results_class:Current Best Return = -240.9921417236328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.65160445317616
INFO:tools.evaluation_results_class:Counted Episodes = 1527
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -234.937255859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 85.062744140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.670207977294922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41556.390625
INFO:tools.evaluation_results_class:Current Best Return = -234.937255859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.69150326797386
INFO:tools.evaluation_results_class:Counted Episodes = 1530
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -237.83444213867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 82.16555786132812
INFO:tools.evaluation_results_class:Average Discounted Reward = 11.257654190063477
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44136.17578125
INFO:tools.evaluation_results_class:Current Best Return = -237.83444213867188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.37898936170212
INFO:tools.evaluation_results_class:Counted Episodes = 1504
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -147.6815185546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 172.3184814453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 97.0638198852539
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38687.96875
INFO:tools.evaluation_results_class:Current Best Return = -147.6815185546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.97903014416777
INFO:tools.evaluation_results_class:Counted Episodes = 1526
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -148.20379638671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 171.79620361328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 96.82524108886719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 39724.3203125
INFO:tools.evaluation_results_class:Current Best Return = -148.20379638671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.96723460026212
INFO:tools.evaluation_results_class:Counted Episodes = 1526
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -151.7159881591797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 168.2840118408203
INFO:tools.evaluation_results_class:Average Discounted Reward = 93.96910095214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45257.73046875
INFO:tools.evaluation_results_class:Current Best Return = -151.7159881591797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.71267418712674
INFO:tools.evaluation_results_class:Counted Episodes = 1507
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.42405700683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 176.57594299316406
INFO:tools.evaluation_results_class:Average Discounted Reward = 100.3697280883789
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 39256.953125
INFO:tools.evaluation_results_class:Current Best Return = -143.42405700683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.85141354372124
INFO:tools.evaluation_results_class:Counted Episodes = 1521
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -151.1116180419922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 168.8883819580078
INFO:tools.evaluation_results_class:Average Discounted Reward = 95.42743682861328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42988.96875
INFO:tools.evaluation_results_class:Current Best Return = -151.1116180419922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.7032173342088
INFO:tools.evaluation_results_class:Counted Episodes = 1523
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.7989501953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 176.2010498046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 99.95708465576172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42432.359375
INFO:tools.evaluation_results_class:Current Best Return = -143.7989501953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.84231274638633
INFO:tools.evaluation_results_class:Counted Episodes = 1522
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -152.56748962402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 167.43251037597656
INFO:tools.evaluation_results_class:Average Discounted Reward = 93.52969360351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 43879.4453125
INFO:tools.evaluation_results_class:Current Best Return = -152.56748962402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.19921363040629
INFO:tools.evaluation_results_class:Counted Episodes = 1526
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.8494110107422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 178.1505889892578
INFO:tools.evaluation_results_class:Average Discounted Reward = 101.25437927246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38923.390625
INFO:tools.evaluation_results_class:Current Best Return = -141.8494110107422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.60495436766624
INFO:tools.evaluation_results_class:Counted Episodes = 1534
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.7975082397461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.20248413085938
INFO:tools.evaluation_results_class:Average Discounted Reward = 150.39328002929688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3982.58740234375
INFO:tools.evaluation_results_class:Current Best Return = -64.7975082397461
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.05373525557012
INFO:tools.evaluation_results_class:Counted Episodes = 1526
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.43860626220703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.56138610839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 147.88153076171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4886.82421875
INFO:tools.evaluation_results_class:Current Best Return = -68.43860626220703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.05384110308601
INFO:tools.evaluation_results_class:Counted Episodes = 1523
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.01451110839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.98548889160156
INFO:tools.evaluation_results_class:Average Discounted Reward = 148.02505493164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4452.2138671875
INFO:tools.evaluation_results_class:Current Best Return = -68.01451110839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.41952506596306
INFO:tools.evaluation_results_class:Counted Episodes = 1516
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.38775634765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 166.61224365234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.58839416503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5222.4443359375
INFO:tools.evaluation_results_class:Current Best Return = -153.38775634765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.16984858459513
INFO:tools.evaluation_results_class:Counted Episodes = 1519
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.53192138671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 166.46807861328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.44892120361328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4777.80126953125
INFO:tools.evaluation_results_class:Current Best Return = -153.53192138671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.2127659574468
INFO:tools.evaluation_results_class:Counted Episodes = 1504
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -327.3948974609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -7.394891738891602
INFO:tools.evaluation_results_class:Average Discounted Reward = -71.77623748779297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45608.58203125
INFO:tools.evaluation_results_class:Current Best Return = -327.3948974609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.7969875573019
INFO:tools.evaluation_results_class:Counted Episodes = 1527
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -329.86163330078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -9.861639022827148
INFO:tools.evaluation_results_class:Average Discounted Reward = -74.3241195678711
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41688.8515625
INFO:tools.evaluation_results_class:Current Best Return = -329.86163330078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.9527868852459
INFO:tools.evaluation_results_class:Counted Episodes = 1525
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -323.79644775390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -3.796442747116089
INFO:tools.evaluation_results_class:Average Discounted Reward = -70.47660064697266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40651.68359375
INFO:tools.evaluation_results_class:Current Best Return = -323.79644775390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.7364953886693
INFO:tools.evaluation_results_class:Counted Episodes = 1518
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -336.638916015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -16.638925552368164
INFO:tools.evaluation_results_class:Average Discounted Reward = -79.7031021118164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40618.33203125
INFO:tools.evaluation_results_class:Current Best Return = -336.638916015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.77850589777195
INFO:tools.evaluation_results_class:Counted Episodes = 1526
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -347.9197998046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -27.919815063476562
INFO:tools.evaluation_results_class:Average Discounted Reward = -88.63679504394531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42688.25
INFO:tools.evaluation_results_class:Current Best Return = -347.9197998046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.88005301524188
INFO:tools.evaluation_results_class:Counted Episodes = 1509
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -336.6622619628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -16.66225242614746
INFO:tools.evaluation_results_class:Average Discounted Reward = -80.8112564086914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37761.953125
INFO:tools.evaluation_results_class:Current Best Return = -336.6622619628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.61192052980132
INFO:tools.evaluation_results_class:Counted Episodes = 1510
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -338.60760498046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -18.60761070251465
INFO:tools.evaluation_results_class:Average Discounted Reward = -82.05381774902344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36992.72265625
INFO:tools.evaluation_results_class:Current Best Return = -338.60760498046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.40682414698163
INFO:tools.evaluation_results_class:Counted Episodes = 1524
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -333.9862365722656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -13.98622989654541
INFO:tools.evaluation_results_class:Average Discounted Reward = -78.89495086669922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35494.671875
INFO:tools.evaluation_results_class:Current Best Return = -333.9862365722656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.30688524590164
INFO:tools.evaluation_results_class:Counted Episodes = 1525
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -332.81439208984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -12.81437873840332
INFO:tools.evaluation_results_class:Average Discounted Reward = -77.47164154052734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33575.44140625
INFO:tools.evaluation_results_class:Current Best Return = -332.81439208984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.89542483660131
INFO:tools.evaluation_results_class:Counted Episodes = 1530
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -344.66973876953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -24.66973114013672
INFO:tools.evaluation_results_class:Average Discounted Reward = -85.65692901611328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45171.48828125
INFO:tools.evaluation_results_class:Current Best Return = -344.66973876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.44780039395928
INFO:tools.evaluation_results_class:Counted Episodes = 1523
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -332.1020202636719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -12.102027893066406
INFO:tools.evaluation_results_class:Average Discounted Reward = -77.67192840576172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34810.16015625
INFO:tools.evaluation_results_class:Current Best Return = -332.1020202636719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.87311968606933
INFO:tools.evaluation_results_class:Counted Episodes = 1529
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -333.16064453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -13.160632133483887
INFO:tools.evaluation_results_class:Average Discounted Reward = -77.70805358886719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37305.38671875
INFO:tools.evaluation_results_class:Current Best Return = -333.16064453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.4660961158657
INFO:tools.evaluation_results_class:Counted Episodes = 1519
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.16555786132812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 128.83444213867188
INFO:tools.evaluation_results_class:Average Discounted Reward = 54.52859115600586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42231.53515625
INFO:tools.evaluation_results_class:Current Best Return = -191.16555786132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.61192052980132
INFO:tools.evaluation_results_class:Counted Episodes = 1510
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -206.09681701660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 113.90318298339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.4280891418457
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45054.7890625
INFO:tools.evaluation_results_class:Current Best Return = -206.09681701660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.48010610079575
INFO:tools.evaluation_results_class:Counted Episodes = 1508
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -213.4346923828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 106.5653076171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.42469024658203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 39497.421875
INFO:tools.evaluation_results_class:Current Best Return = -213.4346923828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.72672471533825
INFO:tools.evaluation_results_class:Counted Episodes = 1493
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -144.24534606933594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 175.75465393066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 77.30569458007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3979.7236328125
INFO:tools.evaluation_results_class:Current Best Return = -144.24534606933594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.54654255319149
INFO:tools.evaluation_results_class:Counted Episodes = 1504
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.13937759399414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.8606262207031
INFO:tools.evaluation_results_class:Average Discounted Reward = 155.2286834716797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5242.18896484375
INFO:tools.evaluation_results_class:Current Best Return = -54.13937759399414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.5764546684709
INFO:tools.evaluation_results_class:Counted Episodes = 1478
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -139.78897094726562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 180.21102905273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 101.9576187133789
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 43644.796875
INFO:tools.evaluation_results_class:Current Best Return = -139.78897094726562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.25537634408602
INFO:tools.evaluation_results_class:Counted Episodes = 1488
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -219.1511688232422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 100.84883117675781
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.727373123168945
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42714.234375
INFO:tools.evaluation_results_class:Current Best Return = -219.1511688232422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.48494983277592
INFO:tools.evaluation_results_class:Counted Episodes = 1495
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -227.44100952148438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 92.55899810791016
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.22730255126953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45868.78125
INFO:tools.evaluation_results_class:Current Best Return = -227.44100952148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.03032300593276
INFO:tools.evaluation_results_class:Counted Episodes = 1517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -220.0630340576172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 99.93695831298828
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.141071319580078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40542.40625
INFO:tools.evaluation_results_class:Current Best Return = -220.0630340576172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.70869276708693
INFO:tools.evaluation_results_class:Counted Episodes = 1507
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -229.0421142578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 90.9578857421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 17.48480796813965
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 49103.359375
INFO:tools.evaluation_results_class:Current Best Return = -229.0421142578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.48262032085562
INFO:tools.evaluation_results_class:Counted Episodes = 1496
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -224.93972778320312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 95.06026458740234
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.831666946411133
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 43686.1953125
INFO:tools.evaluation_results_class:Current Best Return = -224.93972778320312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.60794701986755
INFO:tools.evaluation_results_class:Counted Episodes = 1510
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -135.2460174560547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 184.7539825439453
INFO:tools.evaluation_results_class:Average Discounted Reward = 105.94727325439453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38605.953125
INFO:tools.evaluation_results_class:Current Best Return = -135.2460174560547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.30901856763926
INFO:tools.evaluation_results_class:Counted Episodes = 1508
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -149.22984313964844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 170.77015686035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 95.16635131835938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 46722.484375
INFO:tools.evaluation_results_class:Current Best Return = -149.22984313964844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.64247311827957
INFO:tools.evaluation_results_class:Counted Episodes = 1488
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.10369873046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 185.89630126953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 106.45438385009766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36884.54296875
INFO:tools.evaluation_results_class:Current Best Return = -134.10369873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.03698811096433
INFO:tools.evaluation_results_class:Counted Episodes = 1514
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.96112060546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 174.03887939453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 97.22905731201172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40285.80078125
INFO:tools.evaluation_results_class:Current Best Return = -145.96112060546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.85790884718499
INFO:tools.evaluation_results_class:Counted Episodes = 1492
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -137.17503356933594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 182.82496643066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 104.91402435302734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42912.1015625
INFO:tools.evaluation_results_class:Current Best Return = -137.17503356933594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.44253632760898
INFO:tools.evaluation_results_class:Counted Episodes = 1514
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -147.2029266357422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 172.7970733642578
INFO:tools.evaluation_results_class:Average Discounted Reward = 96.79936218261719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45880.6171875
INFO:tools.evaluation_results_class:Current Best Return = -147.2029266357422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.41450432468396
INFO:tools.evaluation_results_class:Counted Episodes = 1503
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -139.54437255859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 180.45562744140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 103.81307220458984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45342.3515625
INFO:tools.evaluation_results_class:Current Best Return = -139.54437255859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.42119205298013
INFO:tools.evaluation_results_class:Counted Episodes = 1510
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -140.71485900878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 179.28514099121094
INFO:tools.evaluation_results_class:Average Discounted Reward = 102.64952850341797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42277.25
INFO:tools.evaluation_results_class:Current Best Return = -140.71485900878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.98151815181518
INFO:tools.evaluation_results_class:Counted Episodes = 1515
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.58858489990234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.41140747070312
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.89268493652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4640.32861328125
INFO:tools.evaluation_results_class:Current Best Return = -70.58858489990234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.87591240875912
INFO:tools.evaluation_results_class:Counted Episodes = 1507
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.66978454589844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.33021545410156
INFO:tools.evaluation_results_class:Average Discounted Reward = 147.5887908935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4352.48974609375
INFO:tools.evaluation_results_class:Current Best Return = -67.66978454589844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.43048128342247
INFO:tools.evaluation_results_class:Counted Episodes = 1496
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.5650405883789
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.43495178222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.00218200683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4576.0166015625
INFO:tools.evaluation_results_class:Current Best Return = -71.5650405883789
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.39959973315544
INFO:tools.evaluation_results_class:Counted Episodes = 1499
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.2589111328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 164.7410888671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.24259948730469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5177.8779296875
INFO:tools.evaluation_results_class:Current Best Return = -155.2589111328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.96301188903567
INFO:tools.evaluation_results_class:Counted Episodes = 1514
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.8302459716797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 160.1697540283203
INFO:tools.evaluation_results_class:Average Discounted Reward = 61.03817367553711
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5244.40380859375
INFO:tools.evaluation_results_class:Current Best Return = -159.8302459716797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.44253632760898
INFO:tools.evaluation_results_class:Counted Episodes = 1514
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -315.252685546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 4.747319221496582
INFO:tools.evaluation_results_class:Average Discounted Reward = -64.19358825683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40843.01953125
INFO:tools.evaluation_results_class:Current Best Return = -315.252685546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.38739946380697
INFO:tools.evaluation_results_class:Counted Episodes = 1492
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -308.86541748046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 11.134576797485352
INFO:tools.evaluation_results_class:Average Discounted Reward = -59.664310455322266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 39122.73828125
INFO:tools.evaluation_results_class:Current Best Return = -308.86541748046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.99133910726182
INFO:tools.evaluation_results_class:Counted Episodes = 1501
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -312.53619384765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 7.463815689086914
INFO:tools.evaluation_results_class:Average Discounted Reward = -61.149330139160156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 43724.3671875
INFO:tools.evaluation_results_class:Current Best Return = -312.53619384765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.33815789473684
INFO:tools.evaluation_results_class:Counted Episodes = 1520
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -318.0441589355469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 1.955833911895752
INFO:tools.evaluation_results_class:Average Discounted Reward = -66.91730499267578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35150.640625
INFO:tools.evaluation_results_class:Current Best Return = -318.0441589355469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.20830586684245
INFO:tools.evaluation_results_class:Counted Episodes = 1517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -324.5404968261719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -4.540485858917236
INFO:tools.evaluation_results_class:Average Discounted Reward = -72.59428405761719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40479.99609375
INFO:tools.evaluation_results_class:Current Best Return = -324.5404968261719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.70175438596492
INFO:tools.evaluation_results_class:Counted Episodes = 1482
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -315.8531188964844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 4.146866798400879
INFO:tools.evaluation_results_class:Average Discounted Reward = -65.27049255371094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36995.4140625
INFO:tools.evaluation_results_class:Current Best Return = -315.8531188964844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.69321148825065
INFO:tools.evaluation_results_class:Counted Episodes = 1532
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -321.54931640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1.5493050813674927
INFO:tools.evaluation_results_class:Average Discounted Reward = -69.15892028808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40789.0390625
INFO:tools.evaluation_results_class:Current Best Return = -321.54931640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.15354070152218
INFO:tools.evaluation_results_class:Counted Episodes = 1511
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -316.07672119140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 3.9232804775238037
INFO:tools.evaluation_results_class:Average Discounted Reward = -67.54766082763672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32656.826171875
INFO:tools.evaluation_results_class:Current Best Return = -316.07672119140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.67195767195767
INFO:tools.evaluation_results_class:Counted Episodes = 1512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -326.72662353515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -6.726613998413086
INFO:tools.evaluation_results_class:Average Discounted Reward = -73.77716064453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41086.49609375
INFO:tools.evaluation_results_class:Current Best Return = -326.72662353515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.53096179183136
INFO:tools.evaluation_results_class:Counted Episodes = 1518
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -320.437255859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -0.43725231289863586
INFO:tools.evaluation_results_class:Average Discounted Reward = -68.76193237304688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 39034.04296875
INFO:tools.evaluation_results_class:Current Best Return = -320.437255859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.28005284015852
INFO:tools.evaluation_results_class:Counted Episodes = 1514
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -313.8826599121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 6.117336750030518
INFO:tools.evaluation_results_class:Average Discounted Reward = -65.36502075195312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32114.310546875
INFO:tools.evaluation_results_class:Current Best Return = -313.8826599121094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.55240606460119
INFO:tools.evaluation_results_class:Counted Episodes = 1517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -324.2550048828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -4.255006790161133
INFO:tools.evaluation_results_class:Average Discounted Reward = -72.56790161132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38256.53515625
INFO:tools.evaluation_results_class:Current Best Return = -324.2550048828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.29773030707611
INFO:tools.evaluation_results_class:Counted Episodes = 1498
INFO:robust_rl.robust_rl_trainer:Iteration 32 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -252.80397033691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.19602966308594
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.026373386383057
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 46580.5625
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.4220123738196
INFO:tools.evaluation_results_class:Counted Episodes = 3071
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.6957849264144897
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 1.9317420721054077
INFO:agents.father_agent:Step: 10, Training loss: 1.5805928707122803
INFO:agents.father_agent:Step: 15, Training loss: 1.4922767877578735
INFO:agents.father_agent:Step: 20, Training loss: 1.8926714658737183
INFO:agents.father_agent:Step: 25, Training loss: 1.9118218421936035
INFO:agents.father_agent:Step: 30, Training loss: 1.658814787864685
INFO:agents.father_agent:Step: 35, Training loss: 1.913496971130371
INFO:agents.father_agent:Step: 40, Training loss: 1.7878611087799072
INFO:agents.father_agent:Step: 45, Training loss: 1.990331768989563
INFO:agents.father_agent:Step: 50, Training loss: 1.484777569770813
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -282.38134765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.618656158447266
INFO:tools.evaluation_results_class:Average Discounted Reward = -29.428569793701172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 61327.22265625
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 98.37174211248285
INFO:tools.evaluation_results_class:Counted Episodes = 2916
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -283.0184631347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.981544494628906
INFO:tools.evaluation_results_class:Average Discounted Reward = -30.65870475769043
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 59194.5078125
INFO:tools.evaluation_results_class:Current Best Return = -283.0184631347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 98.99798657718121
INFO:tools.evaluation_results_class:Counted Episodes = 2980
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -260.21148681640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.78850555419922
INFO:tools.evaluation_results_class:Average Discounted Reward = -14.438413619995117
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 51968.5078125
INFO:tools.evaluation_results_class:Current Best Return = -260.21148681640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 98.38189107918477
INFO:tools.evaluation_results_class:Counted Episodes = 2993
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 349.21034165018574
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 33 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -281.5917663574219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.40824890136719
INFO:tools.evaluation_results_class:Average Discounted Reward = -29.547889709472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 55985.84765625
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.7766323024055
INFO:tools.evaluation_results_class:Counted Episodes = 2910
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.9716683626174927
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 1.4835422039031982
INFO:agents.father_agent:Step: 10, Training loss: 2.1227853298187256
INFO:agents.father_agent:Step: 15, Training loss: 1.5649802684783936
INFO:agents.father_agent:Step: 20, Training loss: 1.439791202545166
INFO:agents.father_agent:Step: 25, Training loss: 1.5440701246261597
INFO:agents.father_agent:Step: 30, Training loss: 1.745266079902649
INFO:agents.father_agent:Step: 35, Training loss: 2.4751954078674316
INFO:agents.father_agent:Step: 40, Training loss: 2.363288640975952
INFO:agents.father_agent:Step: 45, Training loss: 1.6587013006210327
INFO:agents.father_agent:Step: 50, Training loss: 2.110213041305542
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -282.3369140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.6630973815918
INFO:tools.evaluation_results_class:Average Discounted Reward = -27.95501136779785
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 52846.6875
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.40749414519907
INFO:tools.evaluation_results_class:Counted Episodes = 2989
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -279.3949890136719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.60501480102539
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.16164779663086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 52019.359375
INFO:tools.evaluation_results_class:Current Best Return = -279.3949890136719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.96906545099316
INFO:tools.evaluation_results_class:Counted Episodes = 3071
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -253.875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 66.125
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.425429821014404
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 49094.16015625
INFO:tools.evaluation_results_class:Current Best Return = -253.875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.90295629820051
INFO:tools.evaluation_results_class:Counted Episodes = 3112
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 327.4619960890096
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 34 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -284.8662414550781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.133758544921875
INFO:tools.evaluation_results_class:Average Discounted Reward = -31.516698837280273
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 48818.30078125
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.71203486423065
INFO:tools.evaluation_results_class:Counted Episodes = 2983
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.4722315073013306
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 1.682621955871582
INFO:agents.father_agent:Step: 10, Training loss: 1.7057198286056519
INFO:agents.father_agent:Step: 15, Training loss: 1.753953456878662
INFO:agents.father_agent:Step: 20, Training loss: 1.5199462175369263
INFO:agents.father_agent:Step: 25, Training loss: 1.4933170080184937
INFO:agents.father_agent:Step: 30, Training loss: 1.7119464874267578
INFO:agents.father_agent:Step: 35, Training loss: 1.9134466648101807
INFO:agents.father_agent:Step: 40, Training loss: 1.7689132690429688
INFO:agents.father_agent:Step: 45, Training loss: 1.6192171573638916
INFO:agents.father_agent:Step: 50, Training loss: 1.7275985479354858
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -316.0172424316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 3.98274564743042
INFO:tools.evaluation_results_class:Average Discounted Reward = -55.446571350097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 76752.578125
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.76519129782446
INFO:tools.evaluation_results_class:Counted Episodes = 2666
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -321.08380126953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1.0837887525558472
INFO:tools.evaluation_results_class:Average Discounted Reward = -58.877159118652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 82334.171875
INFO:tools.evaluation_results_class:Current Best Return = -321.08380126953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.42040072859746
INFO:tools.evaluation_results_class:Counted Episodes = 2745
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -282.8613586425781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.138641357421875
INFO:tools.evaluation_results_class:Average Discounted Reward = -32.61927795410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 65572.375
INFO:tools.evaluation_results_class:Current Best Return = -282.8613586425781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.73266974760043
INFO:tools.evaluation_results_class:Counted Episodes = 2813
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 359.2000314877216
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 35 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -325.88726806640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -5.88725471496582
INFO:tools.evaluation_results_class:Average Discounted Reward = -63.872459411621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 74534.53125
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.23906485671192
INFO:tools.evaluation_results_class:Counted Episodes = 2652
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.955028772354126
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 1.812714695930481
INFO:agents.father_agent:Step: 10, Training loss: 1.9307541847229004
INFO:agents.father_agent:Step: 15, Training loss: 2.168776750564575
INFO:agents.father_agent:Step: 20, Training loss: 1.5128737688064575
INFO:agents.father_agent:Step: 25, Training loss: 1.8093229532241821
INFO:agents.father_agent:Step: 30, Training loss: 1.9595367908477783
INFO:agents.father_agent:Step: 35, Training loss: 1.7409991025924683
INFO:agents.father_agent:Step: 40, Training loss: 1.9857763051986694
INFO:agents.father_agent:Step: 45, Training loss: 1.7262228727340698
INFO:agents.father_agent:Step: 50, Training loss: 1.6866596937179565
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -406.4671936035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -86.46719360351562
INFO:tools.evaluation_results_class:Average Discounted Reward = -121.08716583251953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 183371.0625
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 133.4493041749503
INFO:tools.evaluation_results_class:Counted Episodes = 2012
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -416.8614501953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -96.86144256591797
INFO:tools.evaluation_results_class:Average Discounted Reward = -130.90516662597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 174983.09375
INFO:tools.evaluation_results_class:Current Best Return = -416.8614501953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 137.32680363115145
INFO:tools.evaluation_results_class:Counted Episodes = 2093
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -373.8427734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -53.84276580810547
INFO:tools.evaluation_results_class:Average Discounted Reward = -100.6942138671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 147302.84375
INFO:tools.evaluation_results_class:Current Best Return = -373.8427734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 129.85534591194968
INFO:tools.evaluation_results_class:Counted Episodes = 2226
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=4, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 472.7958739286374
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=4, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 36 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -410.5882568359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -90.58826446533203
INFO:tools.evaluation_results_class:Average Discounted Reward = -126.68064880371094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 166360.171875
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 135.30391173520562
INFO:tools.evaluation_results_class:Counted Episodes = 1994
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.7704641819000244
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 1.750569462776184
INFO:agents.father_agent:Step: 10, Training loss: 2.0477232933044434
INFO:agents.father_agent:Step: 15, Training loss: 2.2316195964813232
INFO:agents.father_agent:Step: 20, Training loss: 1.8237254619598389
INFO:agents.father_agent:Step: 25, Training loss: 1.9462170600891113
INFO:agents.father_agent:Step: 30, Training loss: 1.8594309091567993
INFO:agents.father_agent:Step: 35, Training loss: 2.309462070465088
INFO:agents.father_agent:Step: 40, Training loss: 2.301274299621582
INFO:agents.father_agent:Step: 45, Training loss: 2.147066354751587
INFO:agents.father_agent:Step: 50, Training loss: 1.9676038026809692
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -384.3918762207031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -64.39188385009766
INFO:tools.evaluation_results_class:Average Discounted Reward = -103.04478454589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 138303.328125
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 122.6121266161391
INFO:tools.evaluation_results_class:Counted Episodes = 2243
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -389.3399658203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -69.33997344970703
INFO:tools.evaluation_results_class:Average Discounted Reward = -107.718994140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 138081.953125
INFO:tools.evaluation_results_class:Current Best Return = -389.3399658203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 126.3382416630576
INFO:tools.evaluation_results_class:Counted Episodes = 2309
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -335.54840087890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -15.548399925231934
INFO:tools.evaluation_results_class:Average Discounted Reward = -71.13709259033203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 100545.28125
INFO:tools.evaluation_results_class:Current Best Return = -335.54840087890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 118.27176994734711
INFO:tools.evaluation_results_class:Counted Episodes = 2469
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=4, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 399.5613195578159
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=4, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -429.9513854980469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -109.95137786865234
INFO:tools.evaluation_results_class:Average Discounted Reward = -131.023193359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 157097.40625
INFO:tools.evaluation_results_class:Current Best Return = -429.9513854980469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.93211009174311
INFO:tools.evaluation_results_class:Counted Episodes = 1090
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -410.2732238769531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -90.27322387695312
INFO:tools.evaluation_results_class:Average Discounted Reward = -110.9321060180664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 145103.671875
INFO:tools.evaluation_results_class:Current Best Return = -410.2732238769531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.16575591985428
INFO:tools.evaluation_results_class:Counted Episodes = 1098
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -434.2550354003906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -114.25504302978516
INFO:tools.evaluation_results_class:Average Discounted Reward = -135.0499725341797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 157765.953125
INFO:tools.evaluation_results_class:Current Best Return = -434.2550354003906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.43302752293577
INFO:tools.evaluation_results_class:Counted Episodes = 1090
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.83030700683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 153.16969299316406
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.13920783996582
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9504.314453125
INFO:tools.evaluation_results_class:Current Best Return = -166.83030700683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.60617059891108
INFO:tools.evaluation_results_class:Counted Episodes = 1102
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.64897918701172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.3510284423828
INFO:tools.evaluation_results_class:Average Discounted Reward = 113.65545654296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8508.4208984375
INFO:tools.evaluation_results_class:Current Best Return = -71.64897918701172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.77281191806331
INFO:tools.evaluation_results_class:Counted Episodes = 1074
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -391.937255859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -71.93726348876953
INFO:tools.evaluation_results_class:Average Discounted Reward = -92.54816436767578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 147141.8125
INFO:tools.evaluation_results_class:Current Best Return = -391.937255859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 127.84456928838951
INFO:tools.evaluation_results_class:Counted Episodes = 1068
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -411.568359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -91.56834411621094
INFO:tools.evaluation_results_class:Average Discounted Reward = -114.97844696044922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 145649.203125
INFO:tools.evaluation_results_class:Current Best Return = -411.568359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.07194244604317
INFO:tools.evaluation_results_class:Counted Episodes = 1112
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -412.4912109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -92.4912109375
INFO:tools.evaluation_results_class:Average Discounted Reward = -116.5451889038086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 148467.96875
INFO:tools.evaluation_results_class:Current Best Return = -412.4912109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.76965772432932
INFO:tools.evaluation_results_class:Counted Episodes = 1081
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -424.7837219238281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -104.78373718261719
INFO:tools.evaluation_results_class:Average Discounted Reward = -122.85185241699219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 154407.03125
INFO:tools.evaluation_results_class:Current Best Return = -424.7837219238281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 126.82994454713493
INFO:tools.evaluation_results_class:Counted Episodes = 1082
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -417.7074279785156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -97.70742797851562
INFO:tools.evaluation_results_class:Average Discounted Reward = -118.17886352539062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 154113.015625
INFO:tools.evaluation_results_class:Current Best Return = -417.7074279785156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 123.83876811594203
INFO:tools.evaluation_results_class:Counted Episodes = 1104
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -402.8720397949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -82.8720474243164
INFO:tools.evaluation_results_class:Average Discounted Reward = -110.06330871582031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 140720.46875
INFO:tools.evaluation_results_class:Current Best Return = -402.8720397949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.70417422867514
INFO:tools.evaluation_results_class:Counted Episodes = 1102
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -356.76397705078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -36.76397705078125
INFO:tools.evaluation_results_class:Average Discounted Reward = -67.89815521240234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 139011.640625
INFO:tools.evaluation_results_class:Current Best Return = -356.76397705078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 121.8695652173913
INFO:tools.evaluation_results_class:Counted Episodes = 1127
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -370.0250549316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -50.02506637573242
INFO:tools.evaluation_results_class:Average Discounted Reward = -75.77734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 144067.59375
INFO:tools.evaluation_results_class:Current Best Return = -370.0250549316406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 122.70993733213966
INFO:tools.evaluation_results_class:Counted Episodes = 1117
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -358.3605041503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -38.36051940917969
INFO:tools.evaluation_results_class:Average Discounted Reward = -70.25817108154297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 136401.0625
INFO:tools.evaluation_results_class:Current Best Return = -358.3605041503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.83410565338276
INFO:tools.evaluation_results_class:Counted Episodes = 1079
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -385.1329650878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -65.13296508789062
INFO:tools.evaluation_results_class:Average Discounted Reward = -88.89387512207031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 141152.03125
INFO:tools.evaluation_results_class:Current Best Return = -385.1329650878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.50600184672207
INFO:tools.evaluation_results_class:Counted Episodes = 1083
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -389.03509521484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -69.03508758544922
INFO:tools.evaluation_results_class:Average Discounted Reward = -89.9928207397461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 152531.0625
INFO:tools.evaluation_results_class:Current Best Return = -389.03509521484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 126.20406278855032
INFO:tools.evaluation_results_class:Counted Episodes = 1083
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -392.2113952636719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -72.21139526367188
INFO:tools.evaluation_results_class:Average Discounted Reward = -92.68107604980469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 155302.640625
INFO:tools.evaluation_results_class:Current Best Return = -392.2113952636719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.60661764705883
INFO:tools.evaluation_results_class:Counted Episodes = 1088
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -375.45782470703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -55.45783233642578
INFO:tools.evaluation_results_class:Average Discounted Reward = -81.16655731201172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 141371.484375
INFO:tools.evaluation_results_class:Current Best Return = -375.45782470703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.54494902687674
INFO:tools.evaluation_results_class:Counted Episodes = 1079
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -378.8517761230469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -58.851783752441406
INFO:tools.evaluation_results_class:Average Discounted Reward = -82.87471771240234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 139157.90625
INFO:tools.evaluation_results_class:Current Best Return = -378.8517761230469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.23513266239708
INFO:tools.evaluation_results_class:Counted Episodes = 1093
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -129.67428588867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 190.32571411132812
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.56208801269531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13118.408203125
INFO:tools.evaluation_results_class:Current Best Return = -129.67428588867188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.50411710887465
INFO:tools.evaluation_results_class:Counted Episodes = 1093
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -120.98516082763672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 199.0148468017578
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.28704071044922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13042.2177734375
INFO:tools.evaluation_results_class:Current Best Return = -120.98516082763672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.68274582560296
INFO:tools.evaluation_results_class:Counted Episodes = 1078
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -122.70211029052734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 197.2978973388672
INFO:tools.evaluation_results_class:Average Discounted Reward = 71.1425552368164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15080.896484375
INFO:tools.evaluation_results_class:Current Best Return = -122.70211029052734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.47846012832264
INFO:tools.evaluation_results_class:Counted Episodes = 1091
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -152.17738342285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 167.82261657714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.498722076416016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6176.009765625
INFO:tools.evaluation_results_class:Current Best Return = -152.17738342285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.84375
INFO:tools.evaluation_results_class:Counted Episodes = 1088
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.00450134277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 161.99549865722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.49024200439453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8478.8974609375
INFO:tools.evaluation_results_class:Current Best Return = -158.00450134277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 123.13886384129847
INFO:tools.evaluation_results_class:Counted Episodes = 1109
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -447.10491943359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -127.10492706298828
INFO:tools.evaluation_results_class:Average Discounted Reward = -147.24656677246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 145008.171875
INFO:tools.evaluation_results_class:Current Best Return = -447.10491943359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.6587591240876
INFO:tools.evaluation_results_class:Counted Episodes = 1096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -474.586181640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -154.586181640625
INFO:tools.evaluation_results_class:Average Discounted Reward = -163.82083129882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 185649.71875
INFO:tools.evaluation_results_class:Current Best Return = -474.586181640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 126.52073732718894
INFO:tools.evaluation_results_class:Counted Episodes = 1085
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -437.2864990234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -117.28649139404297
INFO:tools.evaluation_results_class:Average Discounted Reward = -143.34329223632812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 138795.140625
INFO:tools.evaluation_results_class:Current Best Return = -437.2864990234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.37987307343609
INFO:tools.evaluation_results_class:Counted Episodes = 1103
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -450.3272705078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -130.32725524902344
INFO:tools.evaluation_results_class:Average Discounted Reward = -152.56161499023438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 143935.65625
INFO:tools.evaluation_results_class:Current Best Return = -450.3272705078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.9890610756609
INFO:tools.evaluation_results_class:Counted Episodes = 1097
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -489.17364501953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -169.1736297607422
INFO:tools.evaluation_results_class:Average Discounted Reward = -177.504638671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 163107.4375
INFO:tools.evaluation_results_class:Current Best Return = -489.17364501953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.08181818181818
INFO:tools.evaluation_results_class:Counted Episodes = 1100
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -444.1752624511719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -124.1752700805664
INFO:tools.evaluation_results_class:Average Discounted Reward = -146.33367919921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 146374.0
INFO:tools.evaluation_results_class:Current Best Return = -444.1752624511719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 122.63523131672598
INFO:tools.evaluation_results_class:Counted Episodes = 1124
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -482.0826416015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -162.0826416015625
INFO:tools.evaluation_results_class:Average Discounted Reward = -172.54193115234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 160906.484375
INFO:tools.evaluation_results_class:Current Best Return = -482.0826416015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.69823584029712
INFO:tools.evaluation_results_class:Counted Episodes = 1077
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -457.47674560546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -137.4767608642578
INFO:tools.evaluation_results_class:Average Discounted Reward = -154.63792419433594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 161586.5
INFO:tools.evaluation_results_class:Current Best Return = -457.47674560546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.67730173199635
INFO:tools.evaluation_results_class:Counted Episodes = 1097
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -443.91241455078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -123.91240692138672
INFO:tools.evaluation_results_class:Average Discounted Reward = -145.5995635986328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 147272.359375
INFO:tools.evaluation_results_class:Current Best Return = -443.91241455078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.11678832116789
INFO:tools.evaluation_results_class:Counted Episodes = 1096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -443.56561279296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -123.56561279296875
INFO:tools.evaluation_results_class:Average Discounted Reward = -146.9752960205078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 139806.21875
INFO:tools.evaluation_results_class:Current Best Return = -443.56561279296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.21628959276018
INFO:tools.evaluation_results_class:Counted Episodes = 1105
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -444.77398681640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -124.77398681640625
INFO:tools.evaluation_results_class:Average Discounted Reward = -148.20278930664062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 138848.765625
INFO:tools.evaluation_results_class:Current Best Return = -444.77398681640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.58487084870849
INFO:tools.evaluation_results_class:Counted Episodes = 1084
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -459.01171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -139.01171875
INFO:tools.evaluation_results_class:Average Discounted Reward = -155.7398223876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 170355.46875
INFO:tools.evaluation_results_class:Current Best Return = -459.01171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.85392245266006
INFO:tools.evaluation_results_class:Counted Episodes = 1109
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -460.4562683105469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -140.45626831054688
INFO:tools.evaluation_results_class:Average Discounted Reward = -157.1637725830078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 159111.21875
INFO:tools.evaluation_results_class:Current Best Return = -460.4562683105469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 123.78268710550046
INFO:tools.evaluation_results_class:Counted Episodes = 1109
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -455.8709716796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -135.8709716796875
INFO:tools.evaluation_results_class:Average Discounted Reward = -154.17152404785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 155693.015625
INFO:tools.evaluation_results_class:Current Best Return = -455.8709716796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.3815668202765
INFO:tools.evaluation_results_class:Counted Episodes = 1085
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -456.87841796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -136.87843322753906
INFO:tools.evaluation_results_class:Average Discounted Reward = -153.9002227783203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 166207.5
INFO:tools.evaluation_results_class:Current Best Return = -456.87841796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.43327239488117
INFO:tools.evaluation_results_class:Counted Episodes = 1094
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -476.3391418457031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -156.33914184570312
INFO:tools.evaluation_results_class:Average Discounted Reward = -167.82847595214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 162829.625
INFO:tools.evaluation_results_class:Current Best Return = -476.3391418457031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.57745187901008
INFO:tools.evaluation_results_class:Counted Episodes = 1091
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -472.8125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -152.8125
INFO:tools.evaluation_results_class:Average Discounted Reward = -165.46405029296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 148692.984375
INFO:tools.evaluation_results_class:Current Best Return = -472.8125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.03442028985508
INFO:tools.evaluation_results_class:Counted Episodes = 1104
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -336.8641052246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -16.864093780517578
INFO:tools.evaluation_results_class:Average Discounted Reward = -67.87163543701172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 97901.0703125
INFO:tools.evaluation_results_class:Current Best Return = -336.8641052246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.29194630872483
INFO:tools.evaluation_results_class:Counted Episodes = 1192
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -343.4588623046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -23.458864212036133
INFO:tools.evaluation_results_class:Average Discounted Reward = -64.92842864990234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 127609.2421875
INFO:tools.evaluation_results_class:Current Best Return = -343.4588623046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.0958439355386
INFO:tools.evaluation_results_class:Counted Episodes = 1179
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -342.3103332519531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -22.310344696044922
INFO:tools.evaluation_results_class:Average Discounted Reward = -72.38823699951172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 105490.3671875
INFO:tools.evaluation_results_class:Current Best Return = -342.3103332519531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.93019343986543
INFO:tools.evaluation_results_class:Counted Episodes = 1189
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.8351593017578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 159.1648406982422
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.92674255371094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9115.4296875
INFO:tools.evaluation_results_class:Current Best Return = -160.8351593017578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.8385460693153
INFO:tools.evaluation_results_class:Counted Episodes = 1183
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.17557144165039
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.8244323730469
INFO:tools.evaluation_results_class:Average Discounted Reward = 130.2376708984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2964.180419921875
INFO:tools.evaluation_results_class:Current Best Return = -60.17557144165039
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.65055131467345
INFO:tools.evaluation_results_class:Counted Episodes = 1179
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -291.7852478027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 28.214765548706055
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.21218490600586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 112748.2421875
INFO:tools.evaluation_results_class:Current Best Return = -291.7852478027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.12751677852349
INFO:tools.evaluation_results_class:Counted Episodes = 1192
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -334.89190673828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -14.891891479492188
INFO:tools.evaluation_results_class:Average Discounted Reward = -63.1992073059082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 112136.296875
INFO:tools.evaluation_results_class:Current Best Return = -334.89190673828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.46621621621621
INFO:tools.evaluation_results_class:Counted Episodes = 1184
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -331.7062683105469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -11.706281661987305
INFO:tools.evaluation_results_class:Average Discounted Reward = -61.266754150390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 103649.6875
INFO:tools.evaluation_results_class:Current Best Return = -331.7062683105469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.2597623089983
INFO:tools.evaluation_results_class:Counted Episodes = 1178
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -314.3304748535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 5.669520378112793
INFO:tools.evaluation_results_class:Average Discounted Reward = -50.67099380493164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 91509.9609375
INFO:tools.evaluation_results_class:Current Best Return = -314.3304748535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.20890410958904
INFO:tools.evaluation_results_class:Counted Episodes = 1168
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -330.96099853515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -10.960984230041504
INFO:tools.evaluation_results_class:Average Discounted Reward = -61.29361343383789
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 102411.7109375
INFO:tools.evaluation_results_class:Current Best Return = -330.96099853515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.93553859202714
INFO:tools.evaluation_results_class:Counted Episodes = 1179
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -319.0301208496094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 0.9698745012283325
INFO:tools.evaluation_results_class:Average Discounted Reward = -53.05205535888672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 93109.5234375
INFO:tools.evaluation_results_class:Current Best Return = -319.0301208496094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.48953974895397
INFO:tools.evaluation_results_class:Counted Episodes = 1195
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -275.54705810546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.45295715332031
INFO:tools.evaluation_results_class:Average Discounted Reward = -15.251646995544434
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 86084.421875
INFO:tools.evaluation_results_class:Current Best Return = -275.54705810546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.84679433805162
INFO:tools.evaluation_results_class:Counted Episodes = 1201
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -278.9664306640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.0335578918457
INFO:tools.evaluation_results_class:Average Discounted Reward = -14.533974647521973
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 105294.609375
INFO:tools.evaluation_results_class:Current Best Return = -278.9664306640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.7248322147651
INFO:tools.evaluation_results_class:Counted Episodes = 1192
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -277.3142395019531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.68576431274414
INFO:tools.evaluation_results_class:Average Discounted Reward = -12.435447692871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 110843.28125
INFO:tools.evaluation_results_class:Current Best Return = -277.3142395019531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.44060657118787
INFO:tools.evaluation_results_class:Counted Episodes = 1187
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -280.989990234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.01001739501953
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.188276290893555
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 91597.3046875
INFO:tools.evaluation_results_class:Current Best Return = -280.989990234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.61769616026712
INFO:tools.evaluation_results_class:Counted Episodes = 1198
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -293.455322265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 26.544681549072266
INFO:tools.evaluation_results_class:Average Discounted Reward = -27.019535064697266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 102967.109375
INFO:tools.evaluation_results_class:Current Best Return = -293.455322265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.85446808510638
INFO:tools.evaluation_results_class:Counted Episodes = 1175
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -285.6035461425781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.39646530151367
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.768171310424805
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 115626.3046875
INFO:tools.evaluation_results_class:Current Best Return = -285.6035461425781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.48653198653199
INFO:tools.evaluation_results_class:Counted Episodes = 1188
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -281.0838317871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.91617202758789
INFO:tools.evaluation_results_class:Average Discounted Reward = -17.161468505859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 100330.015625
INFO:tools.evaluation_results_class:Current Best Return = -281.0838317871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.49449618966977
INFO:tools.evaluation_results_class:Counted Episodes = 1181
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -284.11737060546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.88264465332031
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.643775939941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 96953.7265625
INFO:tools.evaluation_results_class:Current Best Return = -284.11737060546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.699173553719
INFO:tools.evaluation_results_class:Counted Episodes = 1210
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -116.36544799804688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 203.63455200195312
INFO:tools.evaluation_results_class:Average Discounted Reward = 82.64359283447266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9677.16796875
INFO:tools.evaluation_results_class:Current Best Return = -116.36544799804688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 117.44965277777777
INFO:tools.evaluation_results_class:Counted Episodes = 1152
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -118.03101348876953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 201.96897888183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 82.93321990966797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10297.1279296875
INFO:tools.evaluation_results_class:Current Best Return = -118.03101348876953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.04191114836547
INFO:tools.evaluation_results_class:Counted Episodes = 1193
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -116.21987915039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 203.78012084960938
INFO:tools.evaluation_results_class:Average Discounted Reward = 84.2382583618164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9272.8310546875
INFO:tools.evaluation_results_class:Current Best Return = -116.21987915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.86099410278011
INFO:tools.evaluation_results_class:Counted Episodes = 1187
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.8485107421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 166.1514892578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 50.892059326171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5129.29443359375
INFO:tools.evaluation_results_class:Current Best Return = -153.8485107421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 113.74006622516556
INFO:tools.evaluation_results_class:Counted Episodes = 1208
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.33192443847656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 164.66807556152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 47.42178726196289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4559.76953125
INFO:tools.evaluation_results_class:Current Best Return = -155.33192443847656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.93649449618967
INFO:tools.evaluation_results_class:Counted Episodes = 1181
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -357.5352783203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -37.53527069091797
INFO:tools.evaluation_results_class:Average Discounted Reward = -87.48037719726562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 102483.5859375
INFO:tools.evaluation_results_class:Current Best Return = -357.5352783203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.4390041493776
INFO:tools.evaluation_results_class:Counted Episodes = 1205
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -353.3280944824219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -33.328086853027344
INFO:tools.evaluation_results_class:Average Discounted Reward = -83.12107849121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 105357.34375
INFO:tools.evaluation_results_class:Current Best Return = -353.3280944824219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.93371996685998
INFO:tools.evaluation_results_class:Counted Episodes = 1207
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -358.2574768066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -38.257476806640625
INFO:tools.evaluation_results_class:Average Discounted Reward = -85.8946304321289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 103537.765625
INFO:tools.evaluation_results_class:Current Best Return = -358.2574768066406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.09966777408638
INFO:tools.evaluation_results_class:Counted Episodes = 1204
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -367.10955810546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -47.10954284667969
INFO:tools.evaluation_results_class:Average Discounted Reward = -93.77070617675781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 105044.6171875
INFO:tools.evaluation_results_class:Current Best Return = -367.10955810546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.1253112033195
INFO:tools.evaluation_results_class:Counted Episodes = 1205
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -373.5321350097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -53.53215026855469
INFO:tools.evaluation_results_class:Average Discounted Reward = -101.04452514648438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 97573.4375
INFO:tools.evaluation_results_class:Current Best Return = -373.5321350097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.16412859560067
INFO:tools.evaluation_results_class:Counted Episodes = 1182
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -362.5012512207031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -42.50125503540039
INFO:tools.evaluation_results_class:Average Discounted Reward = -92.37042999267578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 89401.875
INFO:tools.evaluation_results_class:Current Best Return = -362.5012512207031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.65580618212198
INFO:tools.evaluation_results_class:Counted Episodes = 1197
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -372.9544982910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -52.954505920410156
INFO:tools.evaluation_results_class:Average Discounted Reward = -100.92918395996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 97866.1875
INFO:tools.evaluation_results_class:Current Best Return = -372.9544982910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.08340353833194
INFO:tools.evaluation_results_class:Counted Episodes = 1187
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -381.1557312011719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -61.155723571777344
INFO:tools.evaluation_results_class:Average Discounted Reward = -103.24207305908203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 107598.421875
INFO:tools.evaluation_results_class:Current Best Return = -381.1557312011719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.55723905723906
INFO:tools.evaluation_results_class:Counted Episodes = 1188
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -378.9825134277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -58.98249816894531
INFO:tools.evaluation_results_class:Average Discounted Reward = -101.05359649658203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 110349.3359375
INFO:tools.evaluation_results_class:Current Best Return = -378.9825134277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.18166666666667
INFO:tools.evaluation_results_class:Counted Episodes = 1200
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -381.2704772949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -61.270477294921875
INFO:tools.evaluation_results_class:Average Discounted Reward = -104.90694427490234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 104313.71875
INFO:tools.evaluation_results_class:Current Best Return = -381.2704772949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.87542662116041
INFO:tools.evaluation_results_class:Counted Episodes = 1172
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -375.4561462402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -55.456153869628906
INFO:tools.evaluation_results_class:Average Discounted Reward = -100.75782775878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 100980.46875
INFO:tools.evaluation_results_class:Current Best Return = -375.4561462402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.15514333895447
INFO:tools.evaluation_results_class:Counted Episodes = 1186
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -379.26373291015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -59.26374435424805
INFO:tools.evaluation_results_class:Average Discounted Reward = -104.25006866455078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 94181.5703125
INFO:tools.evaluation_results_class:Current Best Return = -379.26373291015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.75429553264605
INFO:tools.evaluation_results_class:Counted Episodes = 1164
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -395.4122009277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -75.41221618652344
INFO:tools.evaluation_results_class:Average Discounted Reward = -115.2867202758789
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 116419.5859375
INFO:tools.evaluation_results_class:Current Best Return = -395.4122009277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 117.38337574215437
INFO:tools.evaluation_results_class:Counted Episodes = 1179
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -374.64202880859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -54.642024993896484
INFO:tools.evaluation_results_class:Average Discounted Reward = -98.89790344238281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 99586.21875
INFO:tools.evaluation_results_class:Current Best Return = -374.64202880859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.234219269103
INFO:tools.evaluation_results_class:Counted Episodes = 1204
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -356.6433410644531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -36.64333724975586
INFO:tools.evaluation_results_class:Average Discounted Reward = -87.9474105834961
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 89044.046875
INFO:tools.evaluation_results_class:Current Best Return = -356.6433410644531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.5885328836425
INFO:tools.evaluation_results_class:Counted Episodes = 1186
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -381.95648193359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -61.956485748291016
INFO:tools.evaluation_results_class:Average Discounted Reward = -104.27730560302734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 100457.375
INFO:tools.evaluation_results_class:Current Best Return = -381.95648193359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.87201365187714
INFO:tools.evaluation_results_class:Counted Episodes = 1172
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -383.701416015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -63.7014045715332
INFO:tools.evaluation_results_class:Average Discounted Reward = -106.95966339111328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 92027.1796875
INFO:tools.evaluation_results_class:Current Best Return = -383.701416015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.8626964433416
INFO:tools.evaluation_results_class:Counted Episodes = 1209
INFO:robust_rl.robust_rl_trainer:Iteration 37 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -403.5096130371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -83.5096206665039
INFO:tools.evaluation_results_class:Average Discounted Reward = -117.2953109741211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 142914.90625
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.26397800183318
INFO:tools.evaluation_results_class:Counted Episodes = 2182
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.12385630607605
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 1.6520110368728638
INFO:agents.father_agent:Step: 10, Training loss: 1.7540640830993652
INFO:agents.father_agent:Step: 15, Training loss: 2.165797710418701
INFO:agents.father_agent:Step: 20, Training loss: 2.0010759830474854
INFO:agents.father_agent:Step: 25, Training loss: 1.681831955909729
INFO:agents.father_agent:Step: 30, Training loss: 1.4276659488677979
INFO:agents.father_agent:Step: 35, Training loss: 1.4330973625183105
INFO:agents.father_agent:Step: 40, Training loss: 1.4613046646118164
INFO:agents.father_agent:Step: 45, Training loss: 1.5502238273620605
INFO:agents.father_agent:Step: 50, Training loss: 1.90682053565979
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -451.2982177734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -131.2982177734375
INFO:tools.evaluation_results_class:Average Discounted Reward = -138.210693359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 282369.90625
INFO:tools.evaluation_results_class:Current Best Return = -135.89154052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 148.32329988851728
INFO:tools.evaluation_results_class:Counted Episodes = 1794
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -464.09478759765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -144.09478759765625
INFO:tools.evaluation_results_class:Average Discounted Reward = -148.39198303222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 307200.21875
INFO:tools.evaluation_results_class:Current Best Return = -464.09478759765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 151.9563365282215
INFO:tools.evaluation_results_class:Counted Episodes = 1878
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -359.0390930175781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -39.03910446166992
INFO:tools.evaluation_results_class:Average Discounted Reward = -83.2196044921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 133346.796875
INFO:tools.evaluation_results_class:Current Best Return = -359.0390930175781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.05930382466696
INFO:tools.evaluation_results_class:Counted Episodes = 2327
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 426.61747154961733
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=2, o4x=5, o4y=6, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 38 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
2025-08-20 22:49:08.680173: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-20 22:49:08.682123: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 22:49:08.712816: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-20 22:49:08.712858: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-20 22:49:08.714123: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-20 22:49:08.719852: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 22:49:08.720031: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-20 22:49:09.248496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/rover/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/rover/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"value"}max=? [F "empty"] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 86 states and 19241 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"value"}max=? [F "label_empty"] 
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 378.5566101074219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 379.5566101074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 297.28045654296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 48955.7109375
INFO:tools.evaluation_results_class:Current Best Return = 378.5566101074219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.59953770586536
INFO:tools.evaluation_results_class:Counted Episodes = 3461
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 611.9122314453125
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 17.20334243774414
INFO:agents.father_agent:Step: 10, Training loss: 18.419536590576172
INFO:agents.father_agent:Step: 15, Training loss: 19.614107131958008
INFO:agents.father_agent:Step: 20, Training loss: 17.479185104370117
INFO:agents.father_agent:Step: 25, Training loss: 23.006818771362305
INFO:agents.father_agent:Step: 30, Training loss: 20.574438095092773
INFO:agents.father_agent:Step: 35, Training loss: 23.126203536987305
INFO:agents.father_agent:Step: 40, Training loss: 21.946096420288086
INFO:agents.father_agent:Step: 45, Training loss: 21.159271240234375
INFO:agents.father_agent:Step: 50, Training loss: 24.82217025756836
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 55, Training loss: 26.550697326660156
INFO:agents.father_agent:Step: 60, Training loss: 28.861696243286133
INFO:agents.father_agent:Step: 65, Training loss: 27.90277099609375
INFO:agents.father_agent:Step: 70, Training loss: 26.207637786865234
INFO:agents.father_agent:Step: 75, Training loss: 27.016254425048828
INFO:agents.father_agent:Step: 80, Training loss: 32.05872344970703
INFO:agents.father_agent:Step: 85, Training loss: 27.836503982543945
INFO:agents.father_agent:Step: 90, Training loss: 25.483858108520508
INFO:agents.father_agent:Step: 95, Training loss: 27.582521438598633
INFO:agents.father_agent:Step: 100, Training loss: 28.990388870239258
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 501.52294921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 502.52294921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 416.1825256347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 100749.9765625
INFO:tools.evaluation_results_class:Current Best Return = 501.52294921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.81128074639525
INFO:tools.evaluation_results_class:Counted Episodes = 4716
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 40.62%
INFO:agents.father_agent:Step: 105, Training loss: 28.764480590820312
INFO:agents.father_agent:Step: 110, Training loss: 27.580059051513672
INFO:agents.father_agent:Step: 115, Training loss: 31.773012161254883
INFO:agents.father_agent:Step: 120, Training loss: 28.004032135009766
INFO:agents.father_agent:Step: 125, Training loss: 24.47117805480957
INFO:agents.father_agent:Step: 130, Training loss: 26.115779876708984
INFO:agents.father_agent:Step: 135, Training loss: 25.19332504272461
INFO:agents.father_agent:Step: 140, Training loss: 26.382183074951172
INFO:agents.father_agent:Step: 145, Training loss: 22.181045532226562
INFO:agents.father_agent:Step: 150, Training loss: 21.929563522338867
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 39.06%
INFO:agents.father_agent:Step: 155, Training loss: 23.562423706054688
INFO:agents.father_agent:Step: 160, Training loss: 25.289812088012695
INFO:agents.father_agent:Step: 165, Training loss: 22.171710968017578
INFO:agents.father_agent:Step: 170, Training loss: 23.290515899658203
INFO:agents.father_agent:Step: 175, Training loss: 21.730623245239258
INFO:agents.father_agent:Step: 180, Training loss: 21.99101448059082
INFO:agents.father_agent:Step: 185, Training loss: 21.064146041870117
INFO:agents.father_agent:Step: 190, Training loss: 22.462827682495117
INFO:agents.father_agent:Step: 195, Training loss: 25.201492309570312
INFO:agents.father_agent:Step: 200, Training loss: 24.9079647064209
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 510.28057861328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 511.28057861328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 422.5054626464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 98142.3203125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.95648427401982
INFO:tools.evaluation_results_class:Counted Episodes = 4642
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 43.75%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 510.2729187011719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 511.2729187011719
INFO:tools.evaluation_results_class:Average Discounted Reward = 420.3475036621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 108045.015625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.38760366652117
INFO:tools.evaluation_results_class:Counted Episodes = 4582
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 515.6456298828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 516.6456298828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 421.45306396484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 108297.5546875
INFO:tools.evaluation_results_class:Current Best Return = 515.6456298828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.80689350419797
INFO:tools.evaluation_results_class:Counted Episodes = 4526
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 517.7833251953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 518.7833251953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 426.4723815917969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 110032.40625
INFO:tools.evaluation_results_class:Current Best Return = 517.7833251953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.84292745559598
INFO:tools.evaluation_results_class:Counted Episodes = 4673
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 235.4196929769409
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 877.9317626953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 878.9317626953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 680.6679077148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 194252.578125
INFO:tools.evaluation_results_class:Current Best Return = 877.9317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.75731176104543
INFO:tools.evaluation_results_class:Counted Episodes = 1607
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 309.99908447265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 310.99908447265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 256.476806640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37364.71875
INFO:tools.evaluation_results_class:Current Best Return = 309.99908447265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.13484162895928
INFO:tools.evaluation_results_class:Counted Episodes = 2210
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 659.9668579101562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 660.9668579101562
INFO:tools.evaluation_results_class:Average Discounted Reward = 537.9827270507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 100585.203125
INFO:tools.evaluation_results_class:Current Best Return = 659.9668579101562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 70.55169034786869
INFO:tools.evaluation_results_class:Counted Episodes = 2041
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 479.1803283691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 480.1803283691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 407.5883483886719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 58767.11328125
INFO:tools.evaluation_results_class:Current Best Return = 479.1803283691406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.37289404717335
INFO:tools.evaluation_results_class:Counted Episodes = 2671
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 678.0952758789062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 679.0952758789062
INFO:tools.evaluation_results_class:Average Discounted Reward = 545.6399536132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 112646.9375
INFO:tools.evaluation_results_class:Current Best Return = 678.0952758789062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 75.3646934460888
INFO:tools.evaluation_results_class:Counted Episodes = 1892
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 226.8424835205078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 227.8424835205078
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.63101196289062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21887.7734375
INFO:tools.evaluation_results_class:Current Best Return = 226.8424835205078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.72326589595376
INFO:tools.evaluation_results_class:Counted Episodes = 2768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 856.7199096679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 857.7199096679688
INFO:tools.evaluation_results_class:Average Discounted Reward = 675.6942749023438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 160312.59375
INFO:tools.evaluation_results_class:Current Best Return = 856.7199096679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.51482799525505
INFO:tools.evaluation_results_class:Counted Episodes = 1686
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 303.0588684082031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 304.0588684082031
INFO:tools.evaluation_results_class:Average Discounted Reward = 252.19219970703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34475.5
INFO:tools.evaluation_results_class:Current Best Return = 303.0588684082031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.762157382847036
INFO:tools.evaluation_results_class:Counted Episodes = 2262
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 648.6636352539062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 649.6636352539062
INFO:tools.evaluation_results_class:Average Discounted Reward = 533.439697265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 99208.8984375
INFO:tools.evaluation_results_class:Current Best Return = 648.6636352539062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.13526119402985
INFO:tools.evaluation_results_class:Counted Episodes = 2144
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 478.074951171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 479.074951171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 409.0489501953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 56493.78515625
INFO:tools.evaluation_results_class:Current Best Return = 478.074951171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.992748368382884
INFO:tools.evaluation_results_class:Counted Episodes = 2758
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 671.9365234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 672.9365234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 544.8330078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 106955.484375
INFO:tools.evaluation_results_class:Current Best Return = 671.9365234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.4030534351145
INFO:tools.evaluation_results_class:Counted Episodes = 1965
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 226.32785034179688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 227.32785034179688
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.9022979736328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22391.248046875
INFO:tools.evaluation_results_class:Current Best Return = 226.32785034179688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.402379286214135
INFO:tools.evaluation_results_class:Counted Episodes = 2858
INFO:robust_rl.robust_rl_trainer:Iteration 2 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 454.0937805175781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 455.0937805175781
INFO:tools.evaluation_results_class:Average Discounted Reward = 376.20166015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 96694.28125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.209680784477364
INFO:tools.evaluation_results_class:Counted Episodes = 4793
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 23.310333251953125
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 43.75%
INFO:agents.father_agent:Step: 5, Training loss: 22.372539520263672
INFO:agents.father_agent:Step: 10, Training loss: 25.867422103881836
INFO:agents.father_agent:Step: 15, Training loss: 25.712148666381836
INFO:agents.father_agent:Step: 20, Training loss: 24.319074630737305
INFO:agents.father_agent:Step: 25, Training loss: 27.910173416137695
INFO:agents.father_agent:Step: 30, Training loss: 30.611074447631836
INFO:agents.father_agent:Step: 35, Training loss: 28.340225219726562
INFO:agents.father_agent:Step: 40, Training loss: 25.27974510192871
INFO:agents.father_agent:Step: 45, Training loss: 26.99579620361328
INFO:agents.father_agent:Step: 50, Training loss: 26.03573989868164
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 40.62%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 458.6699523925781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 459.6699523925781
INFO:tools.evaluation_results_class:Average Discounted Reward = 376.29486083984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 97684.53125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.10236220472441
INFO:tools.evaluation_results_class:Counted Episodes = 4445
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 444.52691650390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 445.52691650390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 362.0485534667969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 88005.75
INFO:tools.evaluation_results_class:Current Best Return = 444.52691650390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.18508535489667
INFO:tools.evaluation_results_class:Counted Episodes = 4452
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 447.125732421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 448.125732421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 367.4594421386719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 89672.03125
INFO:tools.evaluation_results_class:Current Best Return = 447.125732421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.52350150927124
INFO:tools.evaluation_results_class:Counted Episodes = 4638
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 237.92600987605402
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 3 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 400.9330749511719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 401.9330749511719
INFO:tools.evaluation_results_class:Average Discounted Reward = 332.44384765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 79140.1875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.31478770131772
INFO:tools.evaluation_results_class:Counted Episodes = 4781
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 25.443754196166992
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 40.62%
INFO:agents.father_agent:Step: 5, Training loss: 28.952430725097656
INFO:agents.father_agent:Step: 10, Training loss: 24.39068603515625
INFO:agents.father_agent:Step: 15, Training loss: 21.09014129638672
INFO:agents.father_agent:Step: 20, Training loss: 24.08838653564453
INFO:agents.father_agent:Step: 25, Training loss: 22.02062225341797
INFO:agents.father_agent:Step: 30, Training loss: 20.896106719970703
INFO:agents.father_agent:Step: 35, Training loss: 22.79878807067871
INFO:agents.father_agent:Step: 40, Training loss: 20.916168212890625
INFO:agents.father_agent:Step: 45, Training loss: 18.66716766357422
INFO:agents.father_agent:Step: 50, Training loss: 18.864463806152344
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 396.89166259765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 397.89166259765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 326.6304931640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 81040.3984375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.03780964797914
INFO:tools.evaluation_results_class:Counted Episodes = 4602
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 409.06842041015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 410.06842041015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 333.92559814453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 84892.40625
INFO:tools.evaluation_results_class:Current Best Return = 409.06842041015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.20456054903697
INFO:tools.evaluation_results_class:Counted Episodes = 4517
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 396.2708740234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 397.2708740234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 327.1016540527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 77986.5390625
INFO:tools.evaluation_results_class:Current Best Return = 396.2708740234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.34169803429528
INFO:tools.evaluation_results_class:Counted Episodes = 4782
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 239.31695723794135
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 4 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 375.7159423828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 376.7159423828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 309.39569091796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 81200.0234375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.088379009985125
INFO:tools.evaluation_results_class:Counted Episodes = 4707
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 20.844968795776367
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 16.416479110717773
INFO:agents.father_agent:Step: 10, Training loss: 17.208972930908203
INFO:agents.father_agent:Step: 15, Training loss: 15.26427173614502
INFO:agents.father_agent:Step: 20, Training loss: 15.657825469970703
INFO:agents.father_agent:Step: 25, Training loss: 16.90349578857422
INFO:agents.father_agent:Step: 30, Training loss: 13.725518226623535
INFO:agents.father_agent:Step: 35, Training loss: 16.886539459228516
INFO:agents.father_agent:Step: 40, Training loss: 15.28302001953125
INFO:agents.father_agent:Step: 45, Training loss: 16.12324333190918
INFO:agents.father_agent:Step: 50, Training loss: 16.29818344116211
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 370.33837890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 371.33837890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 307.8490905761719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 71228.3359375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.04326328800989
INFO:tools.evaluation_results_class:Counted Episodes = 4854
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 392.9106750488281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 393.9106750488281
INFO:tools.evaluation_results_class:Average Discounted Reward = 322.51849365234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 79099.9375
INFO:tools.evaluation_results_class:Current Best Return = 392.9106750488281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.93625325238509
INFO:tools.evaluation_results_class:Counted Episodes = 4612
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 379.439697265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 380.439697265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 314.64190673828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 76429.671875
INFO:tools.evaluation_results_class:Current Best Return = 379.439697265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.41835274882485
INFO:tools.evaluation_results_class:Counted Episodes = 4893
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 239.6447212367744
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 5 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 372.4979553222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 373.4979553222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 308.6195983886719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 74302.078125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.412360261548194
INFO:tools.evaluation_results_class:Counted Episodes = 4741
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.488673210144043
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 16.216514587402344
INFO:agents.father_agent:Step: 10, Training loss: 16.943897247314453
INFO:agents.father_agent:Step: 15, Training loss: 19.107772827148438
INFO:agents.father_agent:Step: 20, Training loss: 19.110576629638672
INFO:agents.father_agent:Step: 25, Training loss: 15.259685516357422
INFO:agents.father_agent:Step: 30, Training loss: 16.803728103637695
INFO:agents.father_agent:Step: 35, Training loss: 15.606579780578613
INFO:agents.father_agent:Step: 40, Training loss: 15.023377418518066
INFO:agents.father_agent:Step: 45, Training loss: 18.415266036987305
INFO:agents.father_agent:Step: 50, Training loss: 17.96782875061035
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 358.72601318359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 359.72601318359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 298.90460205078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 69898.4140625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.806918744971846
INFO:tools.evaluation_results_class:Counted Episodes = 4972
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 356.7017822265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 357.7017822265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 295.46185302734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 73236.7890625
INFO:tools.evaluation_results_class:Current Best Return = 356.7017822265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.48668280871671
INFO:tools.evaluation_results_class:Counted Episodes = 4956
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 367.3362731933594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 368.3362731933594
INFO:tools.evaluation_results_class:Average Discounted Reward = 304.7217102050781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 76981.7265625
INFO:tools.evaluation_results_class:Current Best Return = 367.3362731933594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.07896857373086
INFO:tools.evaluation_results_class:Counted Episodes = 4964
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 242.25082770935015
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 6 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 335.57550048828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 336.57550048828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 281.8253479003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 60462.54296875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.60841360330254
INFO:tools.evaluation_results_class:Counted Episodes = 5087
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.207805633544922
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 17.40771484375
INFO:agents.father_agent:Step: 10, Training loss: 17.260759353637695
INFO:agents.father_agent:Step: 15, Training loss: 18.58917236328125
INFO:agents.father_agent:Step: 20, Training loss: 16.60091781616211
INFO:agents.father_agent:Step: 25, Training loss: 18.401514053344727
INFO:agents.father_agent:Step: 30, Training loss: 17.737009048461914
INFO:agents.father_agent:Step: 35, Training loss: 18.88764190673828
INFO:agents.father_agent:Step: 40, Training loss: 18.805280685424805
INFO:agents.father_agent:Step: 45, Training loss: 16.851966857910156
INFO:agents.father_agent:Step: 50, Training loss: 17.3662109375
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 344.6169738769531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 345.6169738769531
INFO:tools.evaluation_results_class:Average Discounted Reward = 286.3600158691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 60668.7734375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.22175906627761
INFO:tools.evaluation_results_class:Counted Episodes = 4798
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 347.9461364746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 348.9461364746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 286.4672546386719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 64925.55859375
INFO:tools.evaluation_results_class:Current Best Return = 347.9461364746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.04351499788762
INFO:tools.evaluation_results_class:Counted Episodes = 4734
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 344.4715270996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 345.4715270996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 286.3480224609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 59875.7421875
INFO:tools.evaluation_results_class:Current Best Return = 344.4715270996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.127964022894524
INFO:tools.evaluation_results_class:Counted Episodes = 4892
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 246.3219058740301
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 837.5489501953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 838.5489501953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 642.9052124023438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 157415.34375
INFO:tools.evaluation_results_class:Current Best Return = 837.5489501953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.53861386138614
INFO:tools.evaluation_results_class:Counted Episodes = 1515
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 317.9383544921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 318.9383544921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 261.5342712402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31576.267578125
INFO:tools.evaluation_results_class:Current Best Return = 317.9383544921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.06032045240339
INFO:tools.evaluation_results_class:Counted Episodes = 2122
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 644.4447021484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 645.4447021484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 515.4276733398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 97972.046875
INFO:tools.evaluation_results_class:Current Best Return = 644.4447021484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.39804772234274
INFO:tools.evaluation_results_class:Counted Episodes = 1844
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 443.78912353515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 444.78912353515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 373.72979736328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 46099.01953125
INFO:tools.evaluation_results_class:Current Best Return = 443.78912353515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.657280772325024
INFO:tools.evaluation_results_class:Counted Episodes = 2486
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 610.5236206054688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 611.5236206054688
INFO:tools.evaluation_results_class:Average Discounted Reward = 484.6740417480469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 91457.890625
INFO:tools.evaluation_results_class:Current Best Return = 610.5236206054688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.69758291174817
INFO:tools.evaluation_results_class:Counted Episodes = 1779
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 244.93002319335938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.93002319335938
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.479248046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21277.935546875
INFO:tools.evaluation_results_class:Current Best Return = 244.93002319335938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.2571860816944
INFO:tools.evaluation_results_class:Counted Episodes = 2644
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.03741455078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.03741455078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.3692626953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20000.19921875
INFO:tools.evaluation_results_class:Current Best Return = 241.03741455078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.02040816326531
INFO:tools.evaluation_results_class:Counted Episodes = 2646
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 239.69772338867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.69772338867188
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.11117553710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20396.115234375
INFO:tools.evaluation_results_class:Current Best Return = 239.69772338867188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.18631178707224
INFO:tools.evaluation_results_class:Counted Episodes = 2630
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.60693359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.60693359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.3441925048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21708.330078125
INFO:tools.evaluation_results_class:Current Best Return = 241.60693359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.49675943576058
INFO:tools.evaluation_results_class:Counted Episodes = 2623
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 240.37640380859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.37640380859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.02944946289062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20484.212890625
INFO:tools.evaluation_results_class:Current Best Return = 240.37640380859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.562546816479404
INFO:tools.evaluation_results_class:Counted Episodes = 2670
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.1819610595703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.1819610595703
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.23324584960938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20484.73828125
INFO:tools.evaluation_results_class:Current Best Return = 241.1819610595703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.37342976779596
INFO:tools.evaluation_results_class:Counted Episodes = 2627
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 838.8994140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 839.8994140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 648.846923828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 151978.84375
INFO:tools.evaluation_results_class:Current Best Return = 838.8994140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.69833119383826
INFO:tools.evaluation_results_class:Counted Episodes = 1558
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 320.2964782714844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 321.2964782714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 263.6463623046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32337.3203125
INFO:tools.evaluation_results_class:Current Best Return = 320.2964782714844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.4249649368864
INFO:tools.evaluation_results_class:Counted Episodes = 2139
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 641.0203857421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 642.0203857421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 515.92138671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 94205.25
INFO:tools.evaluation_results_class:Current Best Return = 641.0203857421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 75.36421052631579
INFO:tools.evaluation_results_class:Counted Episodes = 1900
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 446.7347106933594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 447.7347106933594
INFO:tools.evaluation_results_class:Average Discounted Reward = 377.142333984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 48760.22265625
INFO:tools.evaluation_results_class:Current Best Return = 446.7347106933594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.71870047543582
INFO:tools.evaluation_results_class:Counted Episodes = 2524
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 617.4221801757812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 618.4221801757812
INFO:tools.evaluation_results_class:Average Discounted Reward = 493.3727111816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 89564.75
INFO:tools.evaluation_results_class:Current Best Return = 617.4221801757812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.83397050791918
INFO:tools.evaluation_results_class:Counted Episodes = 1831
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.24253845214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.24253845214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.64971923828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20712.896484375
INFO:tools.evaluation_results_class:Current Best Return = 241.24253845214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.69701492537313
INFO:tools.evaluation_results_class:Counted Episodes = 2680
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 235.45689392089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.45689392089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.18325805664062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19785.826171875
INFO:tools.evaluation_results_class:Current Best Return = 235.45689392089844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.77523949889462
INFO:tools.evaluation_results_class:Counted Episodes = 2714
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 235.52163696289062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.52163696289062
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.5095672607422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19377.640625
INFO:tools.evaluation_results_class:Current Best Return = 235.52163696289062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.01183869774325
INFO:tools.evaluation_results_class:Counted Episodes = 2703
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 233.6172637939453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.6172637939453
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.6329803466797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19702.6796875
INFO:tools.evaluation_results_class:Current Best Return = 233.6172637939453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.88495575221239
INFO:tools.evaluation_results_class:Counted Episodes = 2712
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 238.69175720214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.69175720214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.90696716308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19930.453125
INFO:tools.evaluation_results_class:Current Best Return = 238.69175720214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.43309727916511
INFO:tools.evaluation_results_class:Counted Episodes = 2683
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 237.20840454101562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.20840454101562
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.86578369140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19645.525390625
INFO:tools.evaluation_results_class:Current Best Return = 237.20840454101562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.3345521023766
INFO:tools.evaluation_results_class:Counted Episodes = 2735
INFO:robust_rl.robust_rl_trainer:Iteration 7 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 318.4522399902344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 319.4522399902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 268.1346740722656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 50416.5390625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.73653622626889
INFO:tools.evaluation_results_class:Counted Episodes = 5162
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.19211769104004
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 15.656631469726562
INFO:agents.father_agent:Step: 10, Training loss: 14.695960998535156
INFO:agents.father_agent:Step: 15, Training loss: 13.958027839660645
INFO:agents.father_agent:Step: 20, Training loss: 12.675812721252441
INFO:agents.father_agent:Step: 25, Training loss: 13.291383743286133
INFO:agents.father_agent:Step: 30, Training loss: 13.67341136932373
INFO:agents.father_agent:Step: 35, Training loss: 13.785794258117676
INFO:agents.father_agent:Step: 40, Training loss: 14.718557357788086
INFO:agents.father_agent:Step: 45, Training loss: 12.145915031433105
INFO:agents.father_agent:Step: 50, Training loss: 13.965500831604004
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 339.6054382324219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 340.6054382324219
INFO:tools.evaluation_results_class:Average Discounted Reward = 282.3617858886719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 51395.60546875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.59256913658434
INFO:tools.evaluation_results_class:Counted Episodes = 4737
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 324.77642822265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 325.77642822265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 268.13055419921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 54664.70703125
INFO:tools.evaluation_results_class:Current Best Return = 324.77642822265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.82767950052029
INFO:tools.evaluation_results_class:Counted Episodes = 4805
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 327.1155700683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 328.1155700683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 272.80267333984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 55078.94140625
INFO:tools.evaluation_results_class:Current Best Return = 327.1155700683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.61590457256461
INFO:tools.evaluation_results_class:Counted Episodes = 5030
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 249.36438226203867
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 8 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 320.5979919433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 321.5979919433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 267.276611328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 50178.15234375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.319441634634835
INFO:tools.evaluation_results_class:Counted Episodes = 4943
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.24120807647705
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 13.400006294250488
INFO:agents.father_agent:Step: 10, Training loss: 12.945758819580078
INFO:agents.father_agent:Step: 15, Training loss: 12.281530380249023
INFO:agents.father_agent:Step: 20, Training loss: 13.665298461914062
INFO:agents.father_agent:Step: 25, Training loss: 15.021145820617676
INFO:agents.father_agent:Step: 30, Training loss: 12.31183910369873
INFO:agents.father_agent:Step: 35, Training loss: 11.512941360473633
INFO:agents.father_agent:Step: 40, Training loss: 12.415789604187012
INFO:agents.father_agent:Step: 45, Training loss: 12.392522811889648
INFO:agents.father_agent:Step: 50, Training loss: 12.559267044067383
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 322.5916748046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 323.5916748046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 268.1187438964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44318.875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.49577928762611
INFO:tools.evaluation_results_class:Counted Episodes = 4857
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 314.1826171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 315.1826171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 260.8048400878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42663.01953125
INFO:tools.evaluation_results_class:Current Best Return = 314.1826171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.53646477132262
INFO:tools.evaluation_results_class:Counted Episodes = 4854
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 324.78594970703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 325.78594970703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 270.6335754394531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44385.08203125
INFO:tools.evaluation_results_class:Current Best Return = 324.78594970703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.63134479271992
INFO:tools.evaluation_results_class:Counted Episodes = 4945
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 257.1756038804131
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 9 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 309.1700439453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 310.1700439453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 258.5957946777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42738.21875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.48875502008032
INFO:tools.evaluation_results_class:Counted Episodes = 4980
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.753922462463379
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 12.242021560668945
INFO:agents.father_agent:Step: 10, Training loss: 11.990370750427246
INFO:agents.father_agent:Step: 15, Training loss: 12.046056747436523
INFO:agents.father_agent:Step: 20, Training loss: 12.815533638000488
INFO:agents.father_agent:Step: 25, Training loss: 11.292020797729492
INFO:agents.father_agent:Step: 30, Training loss: 11.456900596618652
INFO:agents.father_agent:Step: 35, Training loss: 11.2924165725708
INFO:agents.father_agent:Step: 40, Training loss: 10.500714302062988
INFO:agents.father_agent:Step: 45, Training loss: 11.31714916229248
INFO:agents.father_agent:Step: 50, Training loss: 12.266883850097656
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 308.8531188964844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 309.8531188964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 259.2643737792969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33755.97265625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.323482428115014
INFO:tools.evaluation_results_class:Counted Episodes = 5008
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 312.36297607421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 313.36297607421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 259.07965087890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37059.4921875
INFO:tools.evaluation_results_class:Current Best Return = 312.36297607421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.65610859728507
INFO:tools.evaluation_results_class:Counted Episodes = 4862
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 330.355224609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 331.355224609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 275.03692626953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38711.671875
INFO:tools.evaluation_results_class:Current Best Return = 330.355224609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.48609107768391
INFO:tools.evaluation_results_class:Counted Episodes = 4853
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 267.2873173921198
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 10 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 307.8520812988281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 308.8520812988281
INFO:tools.evaluation_results_class:Average Discounted Reward = 258.3066101074219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34554.94140625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.45236188951161
INFO:tools.evaluation_results_class:Counted Episodes = 4996
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.958444595336914
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 12.868608474731445
INFO:agents.father_agent:Step: 10, Training loss: 12.055086135864258
INFO:agents.father_agent:Step: 15, Training loss: 12.792920112609863
INFO:agents.father_agent:Step: 20, Training loss: 13.365442276000977
INFO:agents.father_agent:Step: 25, Training loss: 12.437784194946289
INFO:agents.father_agent:Step: 30, Training loss: 10.917379379272461
INFO:agents.father_agent:Step: 35, Training loss: 12.607183456420898
INFO:agents.father_agent:Step: 40, Training loss: 12.612719535827637
INFO:agents.father_agent:Step: 45, Training loss: 13.178431510925293
INFO:agents.father_agent:Step: 50, Training loss: 15.08705997467041
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 308.69183349609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 309.69183349609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 259.62811279296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31913.28125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.51305561092286
INFO:tools.evaluation_results_class:Counted Episodes = 5017
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 302.8409118652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 303.8409118652344
INFO:tools.evaluation_results_class:Average Discounted Reward = 253.13502502441406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34170.2578125
INFO:tools.evaluation_results_class:Current Best Return = 302.8409118652344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.28794775380962
INFO:tools.evaluation_results_class:Counted Episodes = 5053
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 312.2115173339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 313.2115173339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 259.9197692871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36591.58984375
INFO:tools.evaluation_results_class:Current Best Return = 312.2115173339844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.24738955823293
INFO:tools.evaluation_results_class:Counted Episodes = 4980
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 268.88616085346695
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 11 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 318.09283447265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 319.09283447265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 264.60302734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37505.02734375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.57922824302135
INFO:tools.evaluation_results_class:Counted Episodes = 4872
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.98661994934082
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 13.395498275756836
INFO:agents.father_agent:Step: 10, Training loss: 13.484220504760742
INFO:agents.father_agent:Step: 15, Training loss: 14.282700538635254
INFO:agents.father_agent:Step: 20, Training loss: 10.994314193725586
INFO:agents.father_agent:Step: 25, Training loss: 13.635688781738281
INFO:agents.father_agent:Step: 30, Training loss: 13.4104642868042
INFO:agents.father_agent:Step: 35, Training loss: 13.88330078125
INFO:agents.father_agent:Step: 40, Training loss: 13.722646713256836
INFO:agents.father_agent:Step: 45, Training loss: 10.611711502075195
INFO:agents.father_agent:Step: 50, Training loss: 13.686604499816895
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 311.6304626464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 312.6304626464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 262.1953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32408.150390625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.87063077541181
INFO:tools.evaluation_results_class:Counted Episodes = 4978
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 304.49505615234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 305.49505615234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 254.6603240966797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34850.23828125
INFO:tools.evaluation_results_class:Current Best Return = 304.49505615234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.954617205998424
INFO:tools.evaluation_results_class:Counted Episodes = 5068
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 294.34808349609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 295.34808349609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.82020568847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29589.615234375
INFO:tools.evaluation_results_class:Current Best Return = 294.34808349609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.366170653607085
INFO:tools.evaluation_results_class:Counted Episodes = 5309
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 270.0477136032518
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 765.5399780273438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 766.5399780273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 584.9869995117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 126991.5390625
INFO:tools.evaluation_results_class:Current Best Return = 765.5399780273438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.07482993197279
INFO:tools.evaluation_results_class:Counted Episodes = 1470
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 349.6322937011719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 350.6322937011719
INFO:tools.evaluation_results_class:Average Discounted Reward = 285.7241516113281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29732.669921875
INFO:tools.evaluation_results_class:Current Best Return = 349.6322937011719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 69.79136341581757
INFO:tools.evaluation_results_class:Counted Episodes = 2061
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 614.08447265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 615.08447265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 486.055908203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 81040.203125
INFO:tools.evaluation_results_class:Current Best Return = 614.08447265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 82.16898148148148
INFO:tools.evaluation_results_class:Counted Episodes = 1728
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 402.1014099121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 403.1014099121094
INFO:tools.evaluation_results_class:Average Discounted Reward = 336.8523864746094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35390.82421875
INFO:tools.evaluation_results_class:Current Best Return = 402.1014099121094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.73548113607461
INFO:tools.evaluation_results_class:Counted Episodes = 2359
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 520.8638916015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 521.8638916015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 416.0154724121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 57674.0625
INFO:tools.evaluation_results_class:Current Best Return = 520.8638916015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.656691134952
INFO:tools.evaluation_results_class:Counted Episodes = 1771
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.4192199707031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.4192199707031
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.716796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17917.08984375
INFO:tools.evaluation_results_class:Current Best Return = 262.4192199707031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.285384615384615
INFO:tools.evaluation_results_class:Counted Episodes = 2600
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 260.811279296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.811279296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.89837646484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17263.1796875
INFO:tools.evaluation_results_class:Current Best Return = 260.811279296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.032988108937474
INFO:tools.evaluation_results_class:Counted Episodes = 2607
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 260.6831359863281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.6831359863281
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.52989196777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17808.05859375
INFO:tools.evaluation_results_class:Current Best Return = 260.6831359863281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.946421737466515
INFO:tools.evaluation_results_class:Counted Episodes = 2613
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.3993835449219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.3993835449219
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.4796905517578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18446.5625
INFO:tools.evaluation_results_class:Current Best Return = 262.3993835449219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.00689919509391
INFO:tools.evaluation_results_class:Counted Episodes = 2609
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 261.8197326660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.8197326660156
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.35528564453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17353.490234375
INFO:tools.evaluation_results_class:Current Best Return = 261.8197326660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.79257558362036
INFO:tools.evaluation_results_class:Counted Episodes = 2613
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 261.7591857910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.7591857910156
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.1320037841797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18481.58203125
INFO:tools.evaluation_results_class:Current Best Return = 261.7591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.07120980091884
INFO:tools.evaluation_results_class:Counted Episodes = 2612
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 260.61712646484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.61712646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.72125244140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17758.666015625
INFO:tools.evaluation_results_class:Current Best Return = 260.61712646484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.58731371799771
INFO:tools.evaluation_results_class:Counted Episodes = 2617
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 263.7605895996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.7605895996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.8020477294922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18617.421875
INFO:tools.evaluation_results_class:Current Best Return = 263.7605895996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.203546646106396
INFO:tools.evaluation_results_class:Counted Episodes = 2594
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 258.9413146972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.9413146972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 219.7647247314453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18498.765625
INFO:tools.evaluation_results_class:Current Best Return = 258.9413146972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.801304181051016
INFO:tools.evaluation_results_class:Counted Episodes = 2607
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 263.3974304199219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.3974304199219
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.41964721679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18344.955078125
INFO:tools.evaluation_results_class:Current Best Return = 263.3974304199219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.44755244755245
INFO:tools.evaluation_results_class:Counted Episodes = 2574
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 261.4891662597656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.4891662597656
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.26693725585938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18012.005859375
INFO:tools.evaluation_results_class:Current Best Return = 261.4891662597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.216111541440746
INFO:tools.evaluation_results_class:Counted Episodes = 2582
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 772.2798461914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 773.2798461914062
INFO:tools.evaluation_results_class:Average Discounted Reward = 592.1912841796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 129305.859375
INFO:tools.evaluation_results_class:Current Best Return = 772.2798461914062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.84408602150538
INFO:tools.evaluation_results_class:Counted Episodes = 1488
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 359.6555480957031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 360.6555480957031
INFO:tools.evaluation_results_class:Average Discounted Reward = 293.9112854003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32930.66796875
INFO:tools.evaluation_results_class:Current Best Return = 359.6555480957031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 69.38269230769231
INFO:tools.evaluation_results_class:Counted Episodes = 2080
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 614.4030151367188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 615.4030151367188
INFO:tools.evaluation_results_class:Average Discounted Reward = 487.9723815917969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 87917.703125
INFO:tools.evaluation_results_class:Current Best Return = 614.4030151367188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.71493212669684
INFO:tools.evaluation_results_class:Counted Episodes = 1768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 412.2773132324219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 413.2773132324219
INFO:tools.evaluation_results_class:Average Discounted Reward = 344.5898132324219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38712.69921875
INFO:tools.evaluation_results_class:Current Best Return = 412.2773132324219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.360337552742614
INFO:tools.evaluation_results_class:Counted Episodes = 2370
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 538.20947265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 539.20947265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 428.5697326660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 65523.1171875
INFO:tools.evaluation_results_class:Current Best Return = 538.20947265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.0932439977666
INFO:tools.evaluation_results_class:Counted Episodes = 1791
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.8317565917969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.8317565917969
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.00608825683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18172.939453125
INFO:tools.evaluation_results_class:Current Best Return = 265.8317565917969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.47087933003426
INFO:tools.evaluation_results_class:Counted Episodes = 2627
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.668701171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.668701171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.36289978027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18556.853515625
INFO:tools.evaluation_results_class:Current Best Return = 268.668701171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.90512624330528
INFO:tools.evaluation_results_class:Counted Episodes = 2614
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.0333557128906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.0333557128906
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.93023681640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18376.865234375
INFO:tools.evaluation_results_class:Current Best Return = 267.0333557128906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.973169796857036
INFO:tools.evaluation_results_class:Counted Episodes = 2609
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.9977111816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.9977111816406
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.67137145996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18662.96484375
INFO:tools.evaluation_results_class:Current Best Return = 267.9977111816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.90257000383583
INFO:tools.evaluation_results_class:Counted Episodes = 2607
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 266.3259582519531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.3259582519531
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.5140380859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18378.466796875
INFO:tools.evaluation_results_class:Current Best Return = 266.3259582519531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.76461597248758
INFO:tools.evaluation_results_class:Counted Episodes = 2617
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.3731384277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.3731384277344
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.36241149902344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18007.435546875
INFO:tools.evaluation_results_class:Current Best Return = 267.3731384277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.866463181991605
INFO:tools.evaluation_results_class:Counted Episodes = 2621
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 257.79107666015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.79107666015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 219.7545928955078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17142.162109375
INFO:tools.evaluation_results_class:Current Best Return = 257.79107666015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.730063646574315
INFO:tools.evaluation_results_class:Counted Episodes = 2671
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 261.08154296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.08154296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.4331817626953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16777.455078125
INFO:tools.evaluation_results_class:Current Best Return = 261.08154296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.11362778406946
INFO:tools.evaluation_results_class:Counted Episodes = 2649
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 260.237548828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.237548828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.9638214111328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16672.79296875
INFO:tools.evaluation_results_class:Current Best Return = 260.237548828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.13876319758673
INFO:tools.evaluation_results_class:Counted Episodes = 2652
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 258.8918151855469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.8918151855469
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.85302734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17186.697265625
INFO:tools.evaluation_results_class:Current Best Return = 258.8918151855469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.71359041557469
INFO:tools.evaluation_results_class:Counted Episodes = 2671
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 261.6528625488281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.6528625488281
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.9401397705078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17615.69921875
INFO:tools.evaluation_results_class:Current Best Return = 261.6528625488281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.09939759036145
INFO:tools.evaluation_results_class:Counted Episodes = 2656
INFO:robust_rl.robust_rl_trainer:Iteration 12 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 311.5963439941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 312.5963439941406
INFO:tools.evaluation_results_class:Average Discounted Reward = 260.58392333984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35537.81640625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.27541514783313
INFO:tools.evaluation_results_class:Counted Episodes = 4938
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.204585075378418
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Step: 5, Training loss: 12.813909530639648
INFO:agents.father_agent:Step: 10, Training loss: 12.443683624267578
INFO:agents.father_agent:Step: 15, Training loss: 13.401076316833496
INFO:agents.father_agent:Step: 20, Training loss: 12.961435317993164
INFO:agents.father_agent:Step: 25, Training loss: 13.707724571228027
INFO:agents.father_agent:Step: 30, Training loss: 11.692962646484375
INFO:agents.father_agent:Step: 35, Training loss: 14.603650093078613
INFO:agents.father_agent:Step: 40, Training loss: 11.684755325317383
INFO:agents.father_agent:Step: 45, Training loss: 12.184001922607422
INFO:agents.father_agent:Step: 50, Training loss: 13.640549659729004
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 299.68133544921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.68133544921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 252.31503295898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27264.87109375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.263137407551575
INFO:tools.evaluation_results_class:Counted Episodes = 5138
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 293.35601806640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 294.35601806640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.47865295410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27553.974609375
INFO:tools.evaluation_results_class:Current Best Return = 293.35601806640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.803769185933554
INFO:tools.evaluation_results_class:Counted Episodes = 5147
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 302.7742919921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 303.7742919921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 255.05931091308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27761.275390625
INFO:tools.evaluation_results_class:Current Best Return = 302.7742919921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.1195987654321
INFO:tools.evaluation_results_class:Counted Episodes = 5184
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=4/5, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 278.4338585696854
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=4/5, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 13 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 314.89453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 315.89453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 262.9606018066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32508.89453125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.73698291829594
INFO:tools.evaluation_results_class:Counted Episodes = 4859
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.327478408813477
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 12.452980041503906
INFO:agents.father_agent:Step: 10, Training loss: 11.405461311340332
INFO:agents.father_agent:Step: 15, Training loss: 13.647814750671387
INFO:agents.father_agent:Step: 20, Training loss: 13.254535675048828
INFO:agents.father_agent:Step: 25, Training loss: 11.41008186340332
INFO:agents.father_agent:Step: 30, Training loss: 12.733750343322754
INFO:agents.father_agent:Step: 35, Training loss: 11.767895698547363
INFO:agents.father_agent:Step: 40, Training loss: 14.914395332336426
INFO:agents.father_agent:Step: 45, Training loss: 13.127829551696777
INFO:agents.father_agent:Step: 50, Training loss: 14.34874153137207
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 309.8736572265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 310.8736572265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 260.2220458984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28056.103515625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.08214500903796
INFO:tools.evaluation_results_class:Counted Episodes = 4979
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 304.7837219238281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 305.7837219238281
INFO:tools.evaluation_results_class:Average Discounted Reward = 255.79095458984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26658.625
INFO:tools.evaluation_results_class:Current Best Return = 304.7837219238281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.444136291600636
INFO:tools.evaluation_results_class:Counted Episodes = 5048
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 293.8306579589844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 294.8306579589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.44435119628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24696.31640625
INFO:tools.evaluation_results_class:Current Best Return = 293.8306579589844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.56818611689049
INFO:tools.evaluation_results_class:Counted Episodes = 5287
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 279.3618980691116
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 14 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 297.0160827636719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 298.0160827636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 250.74264526367188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23507.373046875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.71507498026835
INFO:tools.evaluation_results_class:Counted Episodes = 5068
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.586941719055176
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 12.189885139465332
INFO:agents.father_agent:Step: 10, Training loss: 13.692689895629883
INFO:agents.father_agent:Step: 15, Training loss: 11.867262840270996
INFO:agents.father_agent:Step: 20, Training loss: 13.270211219787598
INFO:agents.father_agent:Step: 25, Training loss: 13.859148979187012
INFO:agents.father_agent:Step: 30, Training loss: 11.180938720703125
INFO:agents.father_agent:Step: 35, Training loss: 13.334757804870605
INFO:agents.father_agent:Step: 40, Training loss: 13.937026977539062
INFO:agents.father_agent:Step: 45, Training loss: 16.343883514404297
INFO:agents.father_agent:Step: 50, Training loss: 12.240339279174805
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 292.512451171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.512451171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 247.36441040039062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25719.658203125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.898595343467385
INFO:tools.evaluation_results_class:Counted Episodes = 5197
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 312.6166687011719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 313.6166687011719
INFO:tools.evaluation_results_class:Average Discounted Reward = 259.0255432128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31485.84375
INFO:tools.evaluation_results_class:Current Best Return = 312.6166687011719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.693355690630895
INFO:tools.evaluation_results_class:Counted Episodes = 4771
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 302.7347717285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 303.7347717285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 252.5413360595703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30845.453125
INFO:tools.evaluation_results_class:Current Best Return = 302.7347717285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.55528894221156
INFO:tools.evaluation_results_class:Counted Episodes = 5001
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 275.7663823974463
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 15 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 286.1217956542969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.1217956542969
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.5223846435547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24981.12890625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.07381507381508
INFO:tools.evaluation_results_class:Counted Episodes = 5148
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.217060089111328
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 13.813752174377441
INFO:agents.father_agent:Step: 10, Training loss: 13.35924243927002
INFO:agents.father_agent:Step: 15, Training loss: 13.093005180358887
INFO:agents.father_agent:Step: 20, Training loss: 12.528572082519531
INFO:agents.father_agent:Step: 25, Training loss: 12.075100898742676
INFO:agents.father_agent:Step: 30, Training loss: 13.354859352111816
INFO:agents.father_agent:Step: 35, Training loss: 13.102761268615723
INFO:agents.father_agent:Step: 40, Training loss: 14.946797370910645
INFO:agents.father_agent:Step: 45, Training loss: 12.3179349899292
INFO:agents.father_agent:Step: 50, Training loss: 12.925790786743164
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 293.93896484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 294.93896484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.33935546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27840.880859375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.624249699879954
INFO:tools.evaluation_results_class:Counted Episodes = 4998
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 299.900390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.900390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 251.27569580078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29389.234375
INFO:tools.evaluation_results_class:Current Best Return = 299.900390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.99055118110236
INFO:tools.evaluation_results_class:Counted Episodes = 5080
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 311.3197937011719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 312.3197937011719
INFO:tools.evaluation_results_class:Average Discounted Reward = 260.1762390136719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30396.6328125
INFO:tools.evaluation_results_class:Current Best Return = 311.3197937011719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.97073561836039
INFO:tools.evaluation_results_class:Counted Episodes = 4989
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 276.8251685366069
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 16 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 292.1146240234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.1146240234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.679443359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25464.861328125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.00787246604999
INFO:tools.evaluation_results_class:Counted Episodes = 5081
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.856037139892578
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 13.6297607421875
INFO:agents.father_agent:Step: 10, Training loss: 14.367470741271973
INFO:agents.father_agent:Step: 15, Training loss: 11.989999771118164
INFO:agents.father_agent:Step: 20, Training loss: 12.335750579833984
INFO:agents.father_agent:Step: 25, Training loss: 14.378426551818848
INFO:agents.father_agent:Step: 30, Training loss: 12.4938383102417
INFO:agents.father_agent:Step: 35, Training loss: 11.510456085205078
INFO:agents.father_agent:Step: 40, Training loss: 12.813408851623535
INFO:agents.father_agent:Step: 45, Training loss: 12.065652847290039
INFO:agents.father_agent:Step: 50, Training loss: 13.943490028381348
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 308.583251953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 309.583251953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 258.8988952636719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24287.259765625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.70253929866989
INFO:tools.evaluation_results_class:Counted Episodes = 4962
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 302.5464172363281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 303.5464172363281
INFO:tools.evaluation_results_class:Average Discounted Reward = 253.42767333984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25029.0625
INFO:tools.evaluation_results_class:Current Best Return = 302.5464172363281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.821825396825396
INFO:tools.evaluation_results_class:Counted Episodes = 5040
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 308.500732421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 309.500732421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 258.25982666015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26027.9140625
INFO:tools.evaluation_results_class:Current Best Return = 308.500732421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.51854775059195
INFO:tools.evaluation_results_class:Counted Episodes = 5068
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 283.5091034570237
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 688.0239868164062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 689.0239868164062
INFO:tools.evaluation_results_class:Average Discounted Reward = 530.1577758789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 101262.6328125
INFO:tools.evaluation_results_class:Current Best Return = 688.0239868164062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.59342105263158
INFO:tools.evaluation_results_class:Counted Episodes = 1520
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 355.64654541015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 356.64654541015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 291.9380798339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28257.50390625
INFO:tools.evaluation_results_class:Current Best Return = 355.64654541015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.1983863312767
INFO:tools.evaluation_results_class:Counted Episodes = 2107
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 582.220703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 583.2206420898438
INFO:tools.evaluation_results_class:Average Discounted Reward = 458.24334716796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 73080.6953125
INFO:tools.evaluation_results_class:Current Best Return = 582.220703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.73602853745541
INFO:tools.evaluation_results_class:Counted Episodes = 1682
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 368.16217041015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 369.16217041015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 307.0669860839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28555.095703125
INFO:tools.evaluation_results_class:Current Best Return = 368.16217041015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.604390873869995
INFO:tools.evaluation_results_class:Counted Episodes = 2323
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 470.4966735839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 471.4966735839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 375.408447265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 47065.0234375
INFO:tools.evaluation_results_class:Current Best Return = 470.4966735839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.72787610619469
INFO:tools.evaluation_results_class:Counted Episodes = 1808
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.9814147949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.9814147949219
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.71493530273438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16524.03515625
INFO:tools.evaluation_results_class:Current Best Return = 272.9814147949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.40449438202247
INFO:tools.evaluation_results_class:Counted Episodes = 2581
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.4951171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.4951171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.99716186523438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16746.47265625
INFO:tools.evaluation_results_class:Current Best Return = 278.4951171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.13972602739726
INFO:tools.evaluation_results_class:Counted Episodes = 2555
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.31793212890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.31793212890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.38601684570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15567.7998046875
INFO:tools.evaluation_results_class:Current Best Return = 276.31793212890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.80512422360248
INFO:tools.evaluation_results_class:Counted Episodes = 2576
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.5738220214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.5738220214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.8955078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15936.470703125
INFO:tools.evaluation_results_class:Current Best Return = 274.5738220214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.647810925997675
INFO:tools.evaluation_results_class:Counted Episodes = 2581
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.3849182128906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.3849182128906
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.55657958984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16178.314453125
INFO:tools.evaluation_results_class:Current Best Return = 275.3849182128906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.61353965183753
INFO:tools.evaluation_results_class:Counted Episodes = 2585
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.05078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.05078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.18093872070312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16160.4267578125
INFO:tools.evaluation_results_class:Current Best Return = 275.05078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.875
INFO:tools.evaluation_results_class:Counted Episodes = 2560
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.4976806640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.4976806640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.6709747314453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16781.9140625
INFO:tools.evaluation_results_class:Current Best Return = 274.4976806640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.497681607418855
INFO:tools.evaluation_results_class:Counted Episodes = 2588
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.55963134765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.55963134765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.51219177246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16426.6953125
INFO:tools.evaluation_results_class:Current Best Return = 271.55963134765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.95489296636086
INFO:tools.evaluation_results_class:Counted Episodes = 2616
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.1748352050781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.1748352050781
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.40374755859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16396.822265625
INFO:tools.evaluation_results_class:Current Best Return = 275.1748352050781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.60994560994561
INFO:tools.evaluation_results_class:Counted Episodes = 2574
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.1666564941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.1666564941406
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.5372772216797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17096.005859375
INFO:tools.evaluation_results_class:Current Best Return = 274.1666564941406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.226851851851855
INFO:tools.evaluation_results_class:Counted Episodes = 2592
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.9010925292969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.9010925292969
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.7458953857422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16520.845703125
INFO:tools.evaluation_results_class:Current Best Return = 271.9010925292969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.3516228748068
INFO:tools.evaluation_results_class:Counted Episodes = 2588
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.9231262207031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.9231262207031
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.7715301513672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15982.5810546875
INFO:tools.evaluation_results_class:Current Best Return = 273.9231262207031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.14980694980695
INFO:tools.evaluation_results_class:Counted Episodes = 2590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.1864929199219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.1864929199219
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.34474182128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17411.037109375
INFO:tools.evaluation_results_class:Current Best Return = 275.1864929199219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.61693861693862
INFO:tools.evaluation_results_class:Counted Episodes = 2574
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.2811279296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.2811279296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.41563415527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17327.90625
INFO:tools.evaluation_results_class:Current Best Return = 275.2811279296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.37921675067856
INFO:tools.evaluation_results_class:Counted Episodes = 2579
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.7779846191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.7779846191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.4556884765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16609.015625
INFO:tools.evaluation_results_class:Current Best Return = 274.7779846191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.215444015444014
INFO:tools.evaluation_results_class:Counted Episodes = 2590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.6814880371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.6814880371094
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.67337036132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16251.96484375
INFO:tools.evaluation_results_class:Current Best Return = 271.6814880371094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.09045226130653
INFO:tools.evaluation_results_class:Counted Episodes = 2587
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 708.0216064453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 709.0216064453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 542.823486328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 98979.046875
INFO:tools.evaluation_results_class:Current Best Return = 708.0216064453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.34800270819228
INFO:tools.evaluation_results_class:Counted Episodes = 1477
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 368.2352294921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 369.2352294921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 301.97784423828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27727.875
INFO:tools.evaluation_results_class:Current Best Return = 368.2352294921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.87839771101574
INFO:tools.evaluation_results_class:Counted Episodes = 2097
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 576.589111328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 577.589111328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 456.4036865234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 68824.609375
INFO:tools.evaluation_results_class:Current Best Return = 576.589111328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.93402777777777
INFO:tools.evaluation_results_class:Counted Episodes = 1728
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 373.7060241699219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 374.7060241699219
INFO:tools.evaluation_results_class:Average Discounted Reward = 311.4574890136719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29971.259765625
INFO:tools.evaluation_results_class:Current Best Return = 373.7060241699219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.155298155298155
INFO:tools.evaluation_results_class:Counted Episodes = 2331
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 483.0536193847656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 484.0536193847656
INFO:tools.evaluation_results_class:Average Discounted Reward = 385.10955810546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44375.21875
INFO:tools.evaluation_results_class:Current Best Return = 483.0536193847656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.60561797752808
INFO:tools.evaluation_results_class:Counted Episodes = 1780
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.4851989746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.4851989746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.88638305664062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17754.958984375
INFO:tools.evaluation_results_class:Current Best Return = 279.4851989746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.02305032654629
INFO:tools.evaluation_results_class:Counted Episodes = 2603
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.7007751464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.7007751464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.76351928710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18364.705078125
INFO:tools.evaluation_results_class:Current Best Return = 282.7007751464844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.403861003861
INFO:tools.evaluation_results_class:Counted Episodes = 2590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.6566162109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.6566162109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.7278594970703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16658.255859375
INFO:tools.evaluation_results_class:Current Best Return = 282.6566162109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.70997679814385
INFO:tools.evaluation_results_class:Counted Episodes = 2586
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.2373962402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.2373962402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.77879333496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17161.19140625
INFO:tools.evaluation_results_class:Current Best Return = 284.2373962402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.105983574501366
INFO:tools.evaluation_results_class:Counted Episodes = 2557
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.4140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.4140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.97500610351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17615.22265625
INFO:tools.evaluation_results_class:Current Best Return = 284.4140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.8953125
INFO:tools.evaluation_results_class:Counted Episodes = 2560
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.86871337890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.86871337890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.52749633789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16784.564453125
INFO:tools.evaluation_results_class:Current Best Return = 283.86871337890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.96756545525596
INFO:tools.evaluation_results_class:Counted Episodes = 2559
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.8610534667969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.8610534667969
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.22445678710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17058.576171875
INFO:tools.evaluation_results_class:Current Best Return = 280.8610534667969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.5046439628483
INFO:tools.evaluation_results_class:Counted Episodes = 2584
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.0332336425781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.0332336425781
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.9842987060547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16372.4638671875
INFO:tools.evaluation_results_class:Current Best Return = 278.0332336425781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.42735703245749
INFO:tools.evaluation_results_class:Counted Episodes = 2588
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.8455505371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.8455505371094
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.4994659423828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16417.369140625
INFO:tools.evaluation_results_class:Current Best Return = 279.8455505371094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.31119691119691
INFO:tools.evaluation_results_class:Counted Episodes = 2590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.99420166015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.99420166015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.67857360839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17518.8828125
INFO:tools.evaluation_results_class:Current Best Return = 278.99420166015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.44092664092664
INFO:tools.evaluation_results_class:Counted Episodes = 2590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.3438720703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.3438720703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.02120971679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16227.146484375
INFO:tools.evaluation_results_class:Current Best Return = 279.3438720703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.44384407564647
INFO:tools.evaluation_results_class:Counted Episodes = 2591
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.6307067871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.6307067871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.60047912597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16104.8984375
INFO:tools.evaluation_results_class:Current Best Return = 273.6307067871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.609885931558935
INFO:tools.evaluation_results_class:Counted Episodes = 2630
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.422119140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.422119140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.02854919433594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15299.1767578125
INFO:tools.evaluation_results_class:Current Best Return = 272.422119140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.390577507598785
INFO:tools.evaluation_results_class:Counted Episodes = 2632
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.96990966796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.96990966796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.12432861328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15734.154296875
INFO:tools.evaluation_results_class:Current Best Return = 273.96990966796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.72429550647372
INFO:tools.evaluation_results_class:Counted Episodes = 2626
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.02410888671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.02410888671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.47715759277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15287.7060546875
INFO:tools.evaluation_results_class:Current Best Return = 274.02410888671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.151469480030144
INFO:tools.evaluation_results_class:Counted Episodes = 2654
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.5216979980469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.5216979980469
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.85520935058594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15065.9365234375
INFO:tools.evaluation_results_class:Current Best Return = 273.5216979980469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.53348554033486
INFO:tools.evaluation_results_class:Counted Episodes = 2628
INFO:robust_rl.robust_rl_trainer:Iteration 17 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 296.8492126464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 297.8492126464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.90658569335938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23678.587890625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.52066115702479
INFO:tools.evaluation_results_class:Counted Episodes = 5082
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.44489860534668
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 15.438772201538086
INFO:agents.father_agent:Step: 10, Training loss: 15.057454109191895
INFO:agents.father_agent:Step: 15, Training loss: 11.287898063659668
INFO:agents.father_agent:Step: 20, Training loss: 13.038154602050781
INFO:agents.father_agent:Step: 25, Training loss: 14.209633827209473
INFO:agents.father_agent:Step: 30, Training loss: 12.209428787231445
INFO:agents.father_agent:Step: 35, Training loss: 13.758779525756836
INFO:agents.father_agent:Step: 40, Training loss: 13.024320602416992
INFO:agents.father_agent:Step: 45, Training loss: 17.053083419799805
INFO:agents.father_agent:Step: 50, Training loss: 12.800710678100586
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 299.3402404785156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.3402404785156
INFO:tools.evaluation_results_class:Average Discounted Reward = 250.89566040039062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28114.53515625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.41275571600482
INFO:tools.evaluation_results_class:Counted Episodes = 4986
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 285.3219299316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 286.3219299316406
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.71334838867188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22490.9453125
INFO:tools.evaluation_results_class:Current Best Return = 285.3219299316406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.928271652041204
INFO:tools.evaluation_results_class:Counted Episodes = 5242
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 301.7992858886719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 302.7992858886719
INFO:tools.evaluation_results_class:Average Discounted Reward = 253.60409545898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24686.146484375
INFO:tools.evaluation_results_class:Current Best Return = 301.7992858886719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.7039127895659
INFO:tools.evaluation_results_class:Counted Episodes = 5137
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 280.8868413808928
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 18 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 303.7438659667969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 304.7438659667969
INFO:tools.evaluation_results_class:Average Discounted Reward = 254.8311309814453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23762.8984375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.71172638436482
INFO:tools.evaluation_results_class:Counted Episodes = 4912
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.68418312072754
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 14.726214408874512
INFO:agents.father_agent:Step: 10, Training loss: 12.01435661315918
INFO:agents.father_agent:Step: 15, Training loss: 12.40920639038086
INFO:agents.father_agent:Step: 20, Training loss: 14.700052261352539
INFO:agents.father_agent:Step: 25, Training loss: 11.753952026367188
INFO:agents.father_agent:Step: 30, Training loss: 12.763384819030762
INFO:agents.father_agent:Step: 35, Training loss: 15.15455436706543
INFO:agents.father_agent:Step: 40, Training loss: 11.959211349487305
INFO:agents.father_agent:Step: 45, Training loss: 16.433582305908203
INFO:agents.father_agent:Step: 50, Training loss: 11.835731506347656
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 304.5538330078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 305.5538330078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 254.31553649902344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26761.771484375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.19421063436666
INFO:tools.evaluation_results_class:Counted Episodes = 4871
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 299.72540283203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.72540283203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 251.37185668945312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23855.091796875
INFO:tools.evaluation_results_class:Current Best Return = 299.72540283203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.68210862619808
INFO:tools.evaluation_results_class:Counted Episodes = 5008
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 295.8898010253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 296.8898010253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.0538330078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22927.732421875
INFO:tools.evaluation_results_class:Current Best Return = 295.8898010253906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.68930323846909
INFO:tools.evaluation_results_class:Counted Episodes = 5095
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 281.9213891057755
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 19 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 302.4903564453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 303.4903564453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 253.79690551757812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23875.1875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.91975185111067
INFO:tools.evaluation_results_class:Counted Episodes = 4997
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.207524299621582
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 14.452224731445312
INFO:agents.father_agent:Step: 10, Training loss: 14.07458782196045
INFO:agents.father_agent:Step: 15, Training loss: 12.636063575744629
INFO:agents.father_agent:Step: 20, Training loss: 12.299162864685059
INFO:agents.father_agent:Step: 25, Training loss: 11.789618492126465
INFO:agents.father_agent:Step: 30, Training loss: 13.378243446350098
INFO:agents.father_agent:Step: 35, Training loss: 15.603862762451172
INFO:agents.father_agent:Step: 40, Training loss: 12.942672729492188
INFO:agents.father_agent:Step: 45, Training loss: 13.898324012756348
INFO:agents.father_agent:Step: 50, Training loss: 13.086853981018066
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 297.5658874511719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 298.5658874511719
INFO:tools.evaluation_results_class:Average Discounted Reward = 250.8203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19936.998046875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.419335347432025
INFO:tools.evaluation_results_class:Counted Episodes = 4965
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 301.5273132324219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 302.5273132324219
INFO:tools.evaluation_results_class:Average Discounted Reward = 253.59402465820312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21560.162109375
INFO:tools.evaluation_results_class:Current Best Return = 301.5273132324219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.68345323741007
INFO:tools.evaluation_results_class:Counted Episodes = 5004
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 291.57061767578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.57061767578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.91323852539062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18287.091796875
INFO:tools.evaluation_results_class:Current Best Return = 291.57061767578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.804562737642584
INFO:tools.evaluation_results_class:Counted Episodes = 5260
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 285.1732448225797
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 20 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 298.7216796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 299.7216796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 251.44970703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22536.23828125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.281830901459124
INFO:tools.evaluation_results_class:Counted Episodes = 5003
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.744132995605469
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 13.056269645690918
INFO:agents.father_agent:Step: 10, Training loss: 12.780814170837402
INFO:agents.father_agent:Step: 15, Training loss: 13.472192764282227
INFO:agents.father_agent:Step: 20, Training loss: 14.708721160888672
INFO:agents.father_agent:Step: 25, Training loss: 15.177460670471191
INFO:agents.father_agent:Step: 30, Training loss: 13.635923385620117
INFO:agents.father_agent:Step: 35, Training loss: 12.114435195922852
INFO:agents.father_agent:Step: 40, Training loss: 12.327381134033203
INFO:agents.father_agent:Step: 45, Training loss: 16.40321159362793
INFO:agents.father_agent:Step: 50, Training loss: 13.856837272644043
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 293.9681396484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 294.9681396484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.50689697265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20317.984375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.71378910776362
INFO:tools.evaluation_results_class:Counted Episodes = 5178
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 295.468017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 296.468017578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.15203857421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21730.59375
INFO:tools.evaluation_results_class:Current Best Return = 295.468017578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.958446601941745
INFO:tools.evaluation_results_class:Counted Episodes = 5150
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 295.2657470703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 296.2657470703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.99710083007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22192.1875
INFO:tools.evaluation_results_class:Current Best Return = 295.2657470703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.89388385105151
INFO:tools.evaluation_results_class:Counted Episodes = 5183
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 283.58067200745785
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 21 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 303.8980712890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 304.8980712890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 254.24801635742188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24750.20703125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.71300448430493
INFO:tools.evaluation_results_class:Counted Episodes = 4906
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 16.450529098510742
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 13.73812484741211
INFO:agents.father_agent:Step: 10, Training loss: 18.138683319091797
INFO:agents.father_agent:Step: 15, Training loss: 15.271223068237305
INFO:agents.father_agent:Step: 20, Training loss: 12.59549617767334
INFO:agents.father_agent:Step: 25, Training loss: 16.959196090698242
INFO:agents.father_agent:Step: 30, Training loss: 15.184975624084473
INFO:agents.father_agent:Step: 35, Training loss: 15.7913179397583
INFO:agents.father_agent:Step: 40, Training loss: 13.287028312683105
INFO:agents.father_agent:Step: 45, Training loss: 14.21261215209961
INFO:agents.father_agent:Step: 50, Training loss: 14.332157135009766
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 293.3020935058594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 294.3020935058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.1356964111328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29954.1640625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.62757527733756
INFO:tools.evaluation_results_class:Counted Episodes = 5048
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.7262268066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.7262268066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.0225067138672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21156.2578125
INFO:tools.evaluation_results_class:Current Best Return = 284.7262268066406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.200688731586
INFO:tools.evaluation_results_class:Counted Episodes = 5227
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 294.423095703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 295.423095703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.0186004638672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22102.845703125
INFO:tools.evaluation_results_class:Current Best Return = 294.423095703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.49016169881161
INFO:tools.evaluation_results_class:Counted Episodes = 5133
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 277.40210545792735
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 727.3408203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 728.3408203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 557.7232055664062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 112076.5390625
INFO:tools.evaluation_results_class:Current Best Return = 727.3408203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.88948787061994
INFO:tools.evaluation_results_class:Counted Episodes = 1484
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 350.6258850097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 351.6258850097656
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.2427978515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29006.341796875
INFO:tools.evaluation_results_class:Current Best Return = 350.6258850097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.67462971810798
INFO:tools.evaluation_results_class:Counted Episodes = 2093
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 597.4725341796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 598.4725341796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 469.4465637207031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 78496.265625
INFO:tools.evaluation_results_class:Current Best Return = 597.4725341796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.34128878281624
INFO:tools.evaluation_results_class:Counted Episodes = 1676
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 376.7606201171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 377.7606201171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 314.54864501953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32128.564453125
INFO:tools.evaluation_results_class:Current Best Return = 376.7606201171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.6892294593444
INFO:tools.evaluation_results_class:Counted Episodes = 2349
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 497.5901794433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 498.5902099609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 395.24639892578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 54099.4765625
INFO:tools.evaluation_results_class:Current Best Return = 497.5901794433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 80.60090702947846
INFO:tools.evaluation_results_class:Counted Episodes = 1764
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.7459411621094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.7459411621094
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.87335205078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17444.533203125
INFO:tools.evaluation_results_class:Current Best Return = 267.7459411621094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.67002323780015
INFO:tools.evaluation_results_class:Counted Episodes = 2582
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.0529479980469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.0529479980469
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.21484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16787.748046875
INFO:tools.evaluation_results_class:Current Best Return = 267.0529479980469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.939370683039144
INFO:tools.evaluation_results_class:Counted Episodes = 2606
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.7514343261719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.7514343261719
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.98483276367188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17175.609375
INFO:tools.evaluation_results_class:Current Best Return = 268.7514343261719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.11946050096339
INFO:tools.evaluation_results_class:Counted Episodes = 2595
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 266.0666198730469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.0666198730469
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.27767944335938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16773.236328125
INFO:tools.evaluation_results_class:Current Best Return = 266.0666198730469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.98228725452445
INFO:tools.evaluation_results_class:Counted Episodes = 2597
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.2848815917969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.2848815917969
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.7933807373047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16491.84375
INFO:tools.evaluation_results_class:Current Best Return = 267.2848815917969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.424031007751935
INFO:tools.evaluation_results_class:Counted Episodes = 2580
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 264.8369140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.8369140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.0485382080078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16745.33984375
INFO:tools.evaluation_results_class:Current Best Return = 264.8369140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.798925556408285
INFO:tools.evaluation_results_class:Counted Episodes = 2606
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.1017761230469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.1017761230469
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.9484405517578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16639.41796875
INFO:tools.evaluation_results_class:Current Best Return = 269.1017761230469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.172602739726024
INFO:tools.evaluation_results_class:Counted Episodes = 2555
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.0855712890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.0855712890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.19076538085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17730.876953125
INFO:tools.evaluation_results_class:Current Best Return = 274.0855712890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.04650254005471
INFO:tools.evaluation_results_class:Counted Episodes = 2559
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.6400451660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.6400451660156
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.89743041992188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17225.146484375
INFO:tools.evaluation_results_class:Current Best Return = 268.6400451660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.40586419753087
INFO:tools.evaluation_results_class:Counted Episodes = 2592
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.5342712402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.5342712402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.32064819335938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18156.16015625
INFO:tools.evaluation_results_class:Current Best Return = 270.5342712402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.66434378629501
INFO:tools.evaluation_results_class:Counted Episodes = 2583
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.6986083984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.6986083984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.07363891601562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16704.85546875
INFO:tools.evaluation_results_class:Current Best Return = 269.6986083984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.4064914992272
INFO:tools.evaluation_results_class:Counted Episodes = 2588
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.2269592285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.2269592285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.3678436279297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17138.666015625
INFO:tools.evaluation_results_class:Current Best Return = 268.2269592285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.449864498644985
INFO:tools.evaluation_results_class:Counted Episodes = 2583
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.119384765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.119384765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.6381072998047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16690.998046875
INFO:tools.evaluation_results_class:Current Best Return = 267.119384765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.265842349304485
INFO:tools.evaluation_results_class:Counted Episodes = 2588
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.1765441894531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.1765441894531
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.445068359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17174.8984375
INFO:tools.evaluation_results_class:Current Best Return = 268.1765441894531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.49709639953542
INFO:tools.evaluation_results_class:Counted Episodes = 2583
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.5005798339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.5005798339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.77151489257812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17099.576171875
INFO:tools.evaluation_results_class:Current Best Return = 268.5005798339844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.31416441528368
INFO:tools.evaluation_results_class:Counted Episodes = 2591
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.6241760253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.6241760253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.87060546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16656.326171875
INFO:tools.evaluation_results_class:Current Best Return = 268.6241760253906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.4920326467159
INFO:tools.evaluation_results_class:Counted Episodes = 2573
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 266.463134765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.463134765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.2546844482422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17182.708984375
INFO:tools.evaluation_results_class:Current Best Return = 266.463134765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.95622119815668
INFO:tools.evaluation_results_class:Counted Episodes = 2604
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.39788818359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.39788818359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.53453063964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16522.931640625
INFO:tools.evaluation_results_class:Current Best Return = 269.39788818359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.51753702260327
INFO:tools.evaluation_results_class:Counted Episodes = 2566
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.73321533203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.73321533203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.875244140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16503.876953125
INFO:tools.evaluation_results_class:Current Best Return = 265.73321533203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.96621880998081
INFO:tools.evaluation_results_class:Counted Episodes = 2605
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.3042907714844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.3042907714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.96041870117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16852.525390625
INFO:tools.evaluation_results_class:Current Best Return = 268.3042907714844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.11691831204026
INFO:tools.evaluation_results_class:Counted Episodes = 2583
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.1048583984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.1048583984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.0447540283203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18024.279296875
INFO:tools.evaluation_results_class:Current Best Return = 270.1048583984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.47766990291262
INFO:tools.evaluation_results_class:Counted Episodes = 2575
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 721.2200927734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 722.2200927734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 554.342529296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 110928.984375
INFO:tools.evaluation_results_class:Current Best Return = 721.2200927734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.63362357669122
INFO:tools.evaluation_results_class:Counted Episodes = 1493
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 359.16680908203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 360.16680908203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 293.9435119628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30806.568359375
INFO:tools.evaluation_results_class:Current Best Return = 359.16680908203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.62226450999049
INFO:tools.evaluation_results_class:Counted Episodes = 2102
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 595.135498046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 596.135498046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 470.8338928222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 78788.125
INFO:tools.evaluation_results_class:Current Best Return = 595.135498046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.90404624277457
INFO:tools.evaluation_results_class:Counted Episodes = 1730
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 388.09375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 389.09375
INFO:tools.evaluation_results_class:Average Discounted Reward = 323.60302734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34032.671875
INFO:tools.evaluation_results_class:Current Best Return = 388.09375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.27777777777778
INFO:tools.evaluation_results_class:Counted Episodes = 2340
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 501.9698181152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 502.9698181152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 399.04510498046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 58433.21484375
INFO:tools.evaluation_results_class:Current Best Return = 501.9698181152344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.58516020236088
INFO:tools.evaluation_results_class:Counted Episodes = 1779
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.5717468261719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.5717468261719
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.07562255859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17503.92578125
INFO:tools.evaluation_results_class:Current Best Return = 274.5717468261719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.5054012345679
INFO:tools.evaluation_results_class:Counted Episodes = 2592
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.53167724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.53167724609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.7232666015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17113.720703125
INFO:tools.evaluation_results_class:Current Best Return = 271.53167724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.98618042226487
INFO:tools.evaluation_results_class:Counted Episodes = 2605
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.7697448730469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.7697448730469
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.7425994873047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17531.833984375
INFO:tools.evaluation_results_class:Current Best Return = 271.7697448730469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.82436044291715
INFO:tools.evaluation_results_class:Counted Episodes = 2619
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.16937255859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.16937255859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.7217254638672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17100.513671875
INFO:tools.evaluation_results_class:Current Best Return = 272.16937255859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.39520494972931
INFO:tools.evaluation_results_class:Counted Episodes = 2586
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.1004638671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.1004638671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.06394958496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18041.623046875
INFO:tools.evaluation_results_class:Current Best Return = 275.1004638671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.55023183925812
INFO:tools.evaluation_results_class:Counted Episodes = 2588
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.8810729980469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.8810729980469
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.63650512695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16998.771484375
INFO:tools.evaluation_results_class:Current Best Return = 272.8810729980469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.17167051578137
INFO:tools.evaluation_results_class:Counted Episodes = 2598
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.1167297363281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.1167297363281
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.95248413085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18419.06640625
INFO:tools.evaluation_results_class:Current Best Return = 276.1167297363281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.61556420233463
INFO:tools.evaluation_results_class:Counted Episodes = 2570
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.55010986328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.55010986328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.2635498046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18053.42578125
INFO:tools.evaluation_results_class:Current Best Return = 276.55010986328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.64102564102564
INFO:tools.evaluation_results_class:Counted Episodes = 2574
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.1033630371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.1033630371094
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.1204071044922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18557.609375
INFO:tools.evaluation_results_class:Current Best Return = 276.1033630371094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.383275261324044
INFO:tools.evaluation_results_class:Counted Episodes = 2583
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.1428527832031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.1428527832031
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.36453247070312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18503.546875
INFO:tools.evaluation_results_class:Current Best Return = 278.1428527832031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.16320939334638
INFO:tools.evaluation_results_class:Counted Episodes = 2555
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.9215087890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.9215087890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.91282653808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17883.318359375
INFO:tools.evaluation_results_class:Current Best Return = 275.9215087890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.86528699726669
INFO:tools.evaluation_results_class:Counted Episodes = 2561
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.9825439453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.9825439453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.2273712158203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17024.884765625
INFO:tools.evaluation_results_class:Current Best Return = 274.9825439453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.69980582524272
INFO:tools.evaluation_results_class:Counted Episodes = 2575
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.8063659667969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.8063659667969
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.53131103515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16890.462890625
INFO:tools.evaluation_results_class:Current Best Return = 272.8063659667969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.23050326546293
INFO:tools.evaluation_results_class:Counted Episodes = 2603
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.51849365234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.51849365234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.88125610351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17260.380859375
INFO:tools.evaluation_results_class:Current Best Return = 274.51849365234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.526194144838215
INFO:tools.evaluation_results_class:Counted Episodes = 2596
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.7296447753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.7296447753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.05911254882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18105.919921875
INFO:tools.evaluation_results_class:Current Best Return = 273.7296447753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.48099301784329
INFO:tools.evaluation_results_class:Counted Episodes = 2578
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.366455078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.366455078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.36895751953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17681.849609375
INFO:tools.evaluation_results_class:Current Best Return = 276.366455078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.47545419404716
INFO:tools.evaluation_results_class:Counted Episodes = 2587
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.0551452636719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.0551452636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.97215270996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16683.9453125
INFO:tools.evaluation_results_class:Current Best Return = 268.0551452636719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.38022813688213
INFO:tools.evaluation_results_class:Counted Episodes = 2630
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.73309326171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.73309326171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.98121643066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16514.41015625
INFO:tools.evaluation_results_class:Current Best Return = 270.73309326171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.872470408552886
INFO:tools.evaluation_results_class:Counted Episodes = 2619
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.8179016113281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.8179016113281
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.78404235839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15998.7373046875
INFO:tools.evaluation_results_class:Current Best Return = 268.8179016113281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.85539403213466
INFO:tools.evaluation_results_class:Counted Episodes = 2614
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.46917724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.46917724609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.20272827148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16684.22265625
INFO:tools.evaluation_results_class:Current Best Return = 269.46917724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.6324200913242
INFO:tools.evaluation_results_class:Counted Episodes = 2628
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.6750183105469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.6750183105469
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.58192443847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15988.75
INFO:tools.evaluation_results_class:Current Best Return = 269.6750183105469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.569745343975676
INFO:tools.evaluation_results_class:Counted Episodes = 2631
INFO:robust_rl.robust_rl_trainer:Iteration 22 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 288.0795593261719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.0795593261719
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.46600341796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25200.41015625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.72996446900908
INFO:tools.evaluation_results_class:Counted Episodes = 5066
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 16.136676788330078
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 17.674758911132812
INFO:agents.father_agent:Step: 10, Training loss: 17.689815521240234
INFO:agents.father_agent:Step: 15, Training loss: 14.927043914794922
INFO:agents.father_agent:Step: 20, Training loss: 13.855818748474121
INFO:agents.father_agent:Step: 25, Training loss: 17.057231903076172
INFO:agents.father_agent:Step: 30, Training loss: 12.380049705505371
INFO:agents.father_agent:Step: 35, Training loss: 11.139056205749512
INFO:agents.father_agent:Step: 40, Training loss: 13.024477005004883
INFO:agents.father_agent:Step: 45, Training loss: 13.281442642211914
INFO:agents.father_agent:Step: 50, Training loss: 14.461021423339844
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 289.422607421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 290.422607421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.525634765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21695.912109375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.046095767931654
INFO:tools.evaluation_results_class:Counted Episodes = 5033
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.28070068359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.28070068359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.7640380859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20384.642578125
INFO:tools.evaluation_results_class:Current Best Return = 283.28070068359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.10187150299055
INFO:tools.evaluation_results_class:Counted Episodes = 5183
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 286.9205322265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.9205322265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.03375244140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21453.458984375
INFO:tools.evaluation_results_class:Current Best Return = 286.9205322265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.14421613394216
INFO:tools.evaluation_results_class:Counted Episodes = 5256
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 281.1917589873696
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 23 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.1126708984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.1126708984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.15492248535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20423.134765625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.15551219512195
INFO:tools.evaluation_results_class:Counted Episodes = 5125
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.826202392578125
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 11.635557174682617
INFO:agents.father_agent:Step: 10, Training loss: 14.088448524475098
INFO:agents.father_agent:Step: 15, Training loss: 12.147577285766602
INFO:agents.father_agent:Step: 20, Training loss: 12.372285842895508
INFO:agents.father_agent:Step: 25, Training loss: 10.969831466674805
INFO:agents.father_agent:Step: 30, Training loss: 14.583492279052734
INFO:agents.father_agent:Step: 35, Training loss: 16.573469161987305
INFO:agents.father_agent:Step: 40, Training loss: 12.678780555725098
INFO:agents.father_agent:Step: 45, Training loss: 15.765119552612305
INFO:agents.father_agent:Step: 50, Training loss: 15.56794261932373
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 287.9676513671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 288.9676513671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.74058532714844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19745.171875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.293155893536124
INFO:tools.evaluation_results_class:Counted Episodes = 5260
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 307.052490234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 308.052490234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 256.5622253417969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21335.087890625
INFO:tools.evaluation_results_class:Current Best Return = 307.052490234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.903318312167144
INFO:tools.evaluation_results_class:Counted Episodes = 4882
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 303.8376770019531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 304.8376770019531
INFO:tools.evaluation_results_class:Average Discounted Reward = 254.78890991210938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23571.373046875
INFO:tools.evaluation_results_class:Current Best Return = 303.8376770019531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.30716117577431
INFO:tools.evaluation_results_class:Counted Episodes = 5069
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 286.5750490097918
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 24 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 285.3305358886719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 286.3305358886719
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.74276733398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20352.169921875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.08867745004758
INFO:tools.evaluation_results_class:Counted Episodes = 5255
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.314457893371582
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 16.731462478637695
INFO:agents.father_agent:Step: 10, Training loss: 16.092435836791992
INFO:agents.father_agent:Step: 15, Training loss: 12.247833251953125
INFO:agents.father_agent:Step: 20, Training loss: 14.869478225708008
INFO:agents.father_agent:Step: 25, Training loss: 12.864350318908691
INFO:agents.father_agent:Step: 30, Training loss: 17.304967880249023
INFO:agents.father_agent:Step: 35, Training loss: 15.957870483398438
INFO:agents.father_agent:Step: 40, Training loss: 14.40280818939209
INFO:agents.father_agent:Step: 45, Training loss: 16.783714294433594
INFO:agents.father_agent:Step: 50, Training loss: 17.22960662841797
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 282.1178283691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.1178283691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.76414489746094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20059.06640625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.403037599555475
INFO:tools.evaluation_results_class:Counted Episodes = 5399
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 292.510986328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.510986328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.9038543701172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19983.912109375
INFO:tools.evaluation_results_class:Current Best Return = 292.510986328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.49942682460833
INFO:tools.evaluation_results_class:Counted Episodes = 5234
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 300.668212890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 301.668212890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 253.65769958496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21109.0234375
INFO:tools.evaluation_results_class:Current Best Return = 300.668212890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.9014931161528
INFO:tools.evaluation_results_class:Counted Episodes = 5157
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 288.4003708808354
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 25 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 294.141357421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 295.141357421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.510498046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18330.8046875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.46837146702557
INFO:tools.evaluation_results_class:Counted Episodes = 5201
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 18.79627227783203
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 14.887271881103516
INFO:agents.father_agent:Step: 10, Training loss: 15.834815979003906
INFO:agents.father_agent:Step: 15, Training loss: 15.888923645019531
INFO:agents.father_agent:Step: 20, Training loss: 17.44313621520996
INFO:agents.father_agent:Step: 25, Training loss: 14.96393871307373
INFO:agents.father_agent:Step: 30, Training loss: 11.092272758483887
INFO:agents.father_agent:Step: 35, Training loss: 12.773919105529785
INFO:agents.father_agent:Step: 40, Training loss: 15.053557395935059
INFO:agents.father_agent:Step: 45, Training loss: 18.571537017822266
INFO:agents.father_agent:Step: 50, Training loss: 15.075936317443848
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 280.8516540527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.8516540527344
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.45709228515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19128.548828125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.326335520149954
INFO:tools.evaluation_results_class:Counted Episodes = 5335
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 300.04742431640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 301.04742431640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 251.66371154785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23375.087890625
INFO:tools.evaluation_results_class:Current Best Return = 300.04742431640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.257548845470694
INFO:tools.evaluation_results_class:Counted Episodes = 5067
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.02099609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.02099609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.09629821777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16381.0126953125
INFO:tools.evaluation_results_class:Current Best Return = 283.02099609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.45976799852698
INFO:tools.evaluation_results_class:Counted Episodes = 5431
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 286.39377084331096
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 26 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 302.1125183105469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 303.1125183105469
INFO:tools.evaluation_results_class:Average Discounted Reward = 254.42105102539062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21441.5703125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.379629629629626
INFO:tools.evaluation_results_class:Counted Episodes = 4968
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.543205261230469
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 16.403076171875
INFO:agents.father_agent:Step: 10, Training loss: 17.272981643676758
INFO:agents.father_agent:Step: 15, Training loss: 16.70449447631836
INFO:agents.father_agent:Step: 20, Training loss: 15.134188652038574
INFO:agents.father_agent:Step: 25, Training loss: 13.122218132019043
INFO:agents.father_agent:Step: 30, Training loss: 14.668320655822754
INFO:agents.father_agent:Step: 35, Training loss: 15.048233985900879
INFO:agents.father_agent:Step: 40, Training loss: 17.004728317260742
INFO:agents.father_agent:Step: 45, Training loss: 14.566672325134277
INFO:agents.father_agent:Step: 50, Training loss: 16.34687614440918
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 278.2365417480469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.2365417480469
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.30990600585938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20171.873046875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.69547401750791
INFO:tools.evaluation_results_class:Counted Episodes = 5369
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.243896484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.243896484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.65283203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18572.9921875
INFO:tools.evaluation_results_class:Current Best Return = 282.243896484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.38776276276276
INFO:tools.evaluation_results_class:Counted Episodes = 5328
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 295.852294921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 296.852294921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.81788635253906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19765.73828125
INFO:tools.evaluation_results_class:Current Best Return = 295.852294921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.463740458015266
INFO:tools.evaluation_results_class:Counted Episodes = 5240
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 286.61988322311964
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 695.3602905273438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 696.3602905273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 533.0175170898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 95307.6796875
INFO:tools.evaluation_results_class:Current Best Return = 695.3602905273438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.50236326806213
INFO:tools.evaluation_results_class:Counted Episodes = 1481
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 368.3208923339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 369.3208923339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 301.95208740234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27443.90625
INFO:tools.evaluation_results_class:Current Best Return = 368.3208923339844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 69.0450623202301
INFO:tools.evaluation_results_class:Counted Episodes = 2086
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 578.761474609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 579.761474609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 455.00567626953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 71733.9140625
INFO:tools.evaluation_results_class:Current Best Return = 578.761474609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.9929203539823
INFO:tools.evaluation_results_class:Counted Episodes = 1695
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 366.3582763671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 367.3582763671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 305.6528015136719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29327.404296875
INFO:tools.evaluation_results_class:Current Best Return = 366.3582763671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.51635111876076
INFO:tools.evaluation_results_class:Counted Episodes = 2324
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 470.1985168457031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 471.1985168457031
INFO:tools.evaluation_results_class:Average Discounted Reward = 375.5053405761719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45141.578125
INFO:tools.evaluation_results_class:Current Best Return = 470.1985168457031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.568281938326
INFO:tools.evaluation_results_class:Counted Episodes = 1816
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.11224365234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.11224365234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.6953887939453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15833.037109375
INFO:tools.evaluation_results_class:Current Best Return = 280.11224365234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.4891640866873
INFO:tools.evaluation_results_class:Counted Episodes = 2584
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.3846130371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.3846130371094
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.44749450683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16264.544921875
INFO:tools.evaluation_results_class:Current Best Return = 277.3846130371094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.23384615384615
INFO:tools.evaluation_results_class:Counted Episodes = 2600
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.76300048828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.76300048828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.82394409179688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16121.833984375
INFO:tools.evaluation_results_class:Current Best Return = 276.76300048828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.40077071290944
INFO:tools.evaluation_results_class:Counted Episodes = 2595
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.2467346191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.2467346191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.63075256347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17391.052734375
INFO:tools.evaluation_results_class:Current Best Return = 280.2467346191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.37933693138011
INFO:tools.evaluation_results_class:Counted Episodes = 2594
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.2471008300781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.2471008300781
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.88258361816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17221.888671875
INFO:tools.evaluation_results_class:Current Best Return = 278.2471008300781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.435521235521236
INFO:tools.evaluation_results_class:Counted Episodes = 2590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.2891845703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.2891845703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.13665771484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16452.28515625
INFO:tools.evaluation_results_class:Current Best Return = 278.2891845703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.16973886328725
INFO:tools.evaluation_results_class:Counted Episodes = 2604
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.5302429199219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.5302429199219
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.33241271972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16173.638671875
INFO:tools.evaluation_results_class:Current Best Return = 275.5302429199219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.64088820826952
INFO:tools.evaluation_results_class:Counted Episodes = 2612
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.9317626953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.9317626953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.54344177246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15607.0546875
INFO:tools.evaluation_results_class:Current Best Return = 277.9317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.67213114754098
INFO:tools.evaluation_results_class:Counted Episodes = 2623
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.1255798339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.1255798339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.27903747558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16464.001953125
INFO:tools.evaluation_results_class:Current Best Return = 277.1255798339844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.03149001536098
INFO:tools.evaluation_results_class:Counted Episodes = 2604
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.115966796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.115966796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.9661407470703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16136.611328125
INFO:tools.evaluation_results_class:Current Best Return = 275.115966796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.50570342205323
INFO:tools.evaluation_results_class:Counted Episodes = 2630
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.8006896972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.8006896972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.88363647460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15940.2509765625
INFO:tools.evaluation_results_class:Current Best Return = 274.8006896972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.33864844343204
INFO:tools.evaluation_results_class:Counted Episodes = 2634
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.8371276855469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.8371276855469
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.6077880859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16407.580078125
INFO:tools.evaluation_results_class:Current Best Return = 279.8371276855469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.29680891964629
INFO:tools.evaluation_results_class:Counted Episodes = 2601
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.851318359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.851318359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.33558654785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16602.626953125
INFO:tools.evaluation_results_class:Current Best Return = 280.851318359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.462249614791986
INFO:tools.evaluation_results_class:Counted Episodes = 2596
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.20269775390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.20269775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.91123962402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16583.98828125
INFO:tools.evaluation_results_class:Current Best Return = 279.20269775390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.30965250965251
INFO:tools.evaluation_results_class:Counted Episodes = 2590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.3309020996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.3309020996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.20974731445312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15847.32421875
INFO:tools.evaluation_results_class:Current Best Return = 279.3309020996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.509062861550326
INFO:tools.evaluation_results_class:Counted Episodes = 2593
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.8832702636719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.8832702636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17318.0546875
INFO:tools.evaluation_results_class:Current Best Return = 280.8832702636719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.46772323154233
INFO:tools.evaluation_results_class:Counted Episodes = 2587
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.85906982421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.85906982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.78692626953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16478.166015625
INFO:tools.evaluation_results_class:Current Best Return = 277.85906982421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.332818532818536
INFO:tools.evaluation_results_class:Counted Episodes = 2590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.4942321777344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.4942321777344
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.6601104736328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16212.7353515625
INFO:tools.evaluation_results_class:Current Best Return = 277.4942321777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.11128165771297
INFO:tools.evaluation_results_class:Counted Episodes = 2606
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.1061706542969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.1061706542969
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.2915802001953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16544.13671875
INFO:tools.evaluation_results_class:Current Best Return = 277.1061706542969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.00766577232656
INFO:tools.evaluation_results_class:Counted Episodes = 2609
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.520751953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.520751953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.4615936279297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17185.87890625
INFO:tools.evaluation_results_class:Current Best Return = 276.520751953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.08294930875576
INFO:tools.evaluation_results_class:Counted Episodes = 2604
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.62933349609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.62933349609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.3013916015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17293.748046875
INFO:tools.evaluation_results_class:Current Best Return = 279.62933349609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.949560565533055
INFO:tools.evaluation_results_class:Counted Episodes = 2617
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.30670166015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.30670166015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.73104858398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17133.908203125
INFO:tools.evaluation_results_class:Current Best Return = 281.30670166015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.25048468398604
INFO:tools.evaluation_results_class:Counted Episodes = 2579
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.4913330078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.4913330078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.7046661376953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16092.1572265625
INFO:tools.evaluation_results_class:Current Best Return = 277.4913330078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.1101270696958
INFO:tools.evaluation_results_class:Counted Episodes = 2597
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.30621337890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.30621337890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.40145874023438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16660.05078125
INFO:tools.evaluation_results_class:Current Best Return = 277.30621337890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.07173158503664
INFO:tools.evaluation_results_class:Counted Episodes = 2593
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.401611328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.401611328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.79100036621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16880.1015625
INFO:tools.evaluation_results_class:Current Best Return = 276.401611328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.522501906941265
INFO:tools.evaluation_results_class:Counted Episodes = 2622
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.3759460449219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.3759460449219
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.10305786132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16656.73828125
INFO:tools.evaluation_results_class:Current Best Return = 275.3759460449219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.60076335877863
INFO:tools.evaluation_results_class:Counted Episodes = 2620
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 685.6690063476562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 686.6690063476562
INFO:tools.evaluation_results_class:Average Discounted Reward = 526.673583984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 101735.484375
INFO:tools.evaluation_results_class:Current Best Return = 685.6690063476562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.49442622950819
INFO:tools.evaluation_results_class:Counted Episodes = 1525
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 365.18634033203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 366.18634033203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 300.2685546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28881.978515625
INFO:tools.evaluation_results_class:Current Best Return = 365.18634033203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.25139664804469
INFO:tools.evaluation_results_class:Counted Episodes = 2148
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 578.9434814453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 579.9434814453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 457.2353210449219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 64772.8515625
INFO:tools.evaluation_results_class:Current Best Return = 578.9434814453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 82.95611468695144
INFO:tools.evaluation_results_class:Counted Episodes = 1709
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 359.4609069824219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 360.4609069824219
INFO:tools.evaluation_results_class:Average Discounted Reward = 300.6830749511719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26702.11328125
INFO:tools.evaluation_results_class:Current Best Return = 359.4609069824219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.38735680950361
INFO:tools.evaluation_results_class:Counted Episodes = 2357
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 467.01702880859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 468.01702880859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 373.80926513671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45698.796875
INFO:tools.evaluation_results_class:Current Best Return = 467.01702880859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.61286804798254
INFO:tools.evaluation_results_class:Counted Episodes = 1834
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.9539489746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.9539489746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.53073120117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16007.3193359375
INFO:tools.evaluation_results_class:Current Best Return = 277.9539489746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.53635325466311
INFO:tools.evaluation_results_class:Counted Episodes = 2627
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 286.1128234863281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.1128234863281
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.71969604492188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17589.796875
INFO:tools.evaluation_results_class:Current Best Return = 286.1128234863281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.52936630602782
INFO:tools.evaluation_results_class:Counted Episodes = 2588
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.8604431152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.8604431152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.51620483398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16504.451171875
INFO:tools.evaluation_results_class:Current Best Return = 282.8604431152344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.125766871165645
INFO:tools.evaluation_results_class:Counted Episodes = 2608
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 288.15667724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.15667724609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 244.26976013183594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17404.123046875
INFO:tools.evaluation_results_class:Current Best Return = 288.15667724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.03585346843336
INFO:tools.evaluation_results_class:Counted Episodes = 2566
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 286.54913330078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.54913330078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.2232666015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17322.6875
INFO:tools.evaluation_results_class:Current Best Return = 286.54913330078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.50019267822736
INFO:tools.evaluation_results_class:Counted Episodes = 2595
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.9219970703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.9219970703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.0912322998047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17271.8671875
INFO:tools.evaluation_results_class:Current Best Return = 283.9219970703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.30514988470407
INFO:tools.evaluation_results_class:Counted Episodes = 2602
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.1025695800781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.1025695800781
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.1742706298828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17129.373046875
INFO:tools.evaluation_results_class:Current Best Return = 283.1025695800781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.20208252988816
INFO:tools.evaluation_results_class:Counted Episodes = 2593
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.2361145019531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.2361145019531
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.06703186035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16900.978515625
INFO:tools.evaluation_results_class:Current Best Return = 284.2361145019531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.451388888888886
INFO:tools.evaluation_results_class:Counted Episodes = 2592
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.7289123535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.7289123535156
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.97442626953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17369.5390625
INFO:tools.evaluation_results_class:Current Best Return = 283.7289123535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.93711656441718
INFO:tools.evaluation_results_class:Counted Episodes = 2608
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.1091613769531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.1091613769531
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.0043182373047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17153.841796875
INFO:tools.evaluation_results_class:Current Best Return = 283.1091613769531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.09069946195235
INFO:tools.evaluation_results_class:Counted Episodes = 2602
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.8381652832031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.8381652832031
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.7587890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16836.193359375
INFO:tools.evaluation_results_class:Current Best Return = 283.8381652832031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.097109826589595
INFO:tools.evaluation_results_class:Counted Episodes = 2595
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 286.0958251953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.0958251953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.91404724121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16608.703125
INFO:tools.evaluation_results_class:Current Best Return = 286.0958251953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.62248062015504
INFO:tools.evaluation_results_class:Counted Episodes = 2580
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 287.17364501953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 288.17364501953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.3109893798828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17722.810546875
INFO:tools.evaluation_results_class:Current Best Return = 287.17364501953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.77156177156177
INFO:tools.evaluation_results_class:Counted Episodes = 2574
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 286.11566162109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.11566162109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.46363830566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17132.6640625
INFO:tools.evaluation_results_class:Current Best Return = 286.11566162109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.72352024922118
INFO:tools.evaluation_results_class:Counted Episodes = 2568
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.9010925292969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.9010925292969
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.66122436523438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16721.818359375
INFO:tools.evaluation_results_class:Current Best Return = 284.9010925292969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.62684251357641
INFO:tools.evaluation_results_class:Counted Episodes = 2578
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 287.6277770996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 288.6277770996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.82086181640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17111.265625
INFO:tools.evaluation_results_class:Current Best Return = 287.6277770996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.97619976589934
INFO:tools.evaluation_results_class:Counted Episodes = 2563
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.9927062988281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.9927062988281
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.2633514404297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16394.83203125
INFO:tools.evaluation_results_class:Current Best Return = 282.9927062988281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.12806748466258
INFO:tools.evaluation_results_class:Counted Episodes = 2608
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.34783935546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.34783935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.5592498779297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16307.0107421875
INFO:tools.evaluation_results_class:Current Best Return = 284.34783935546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.360908041554445
INFO:tools.evaluation_results_class:Counted Episodes = 2599
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.708984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.708984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.756103515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16077.3662109375
INFO:tools.evaluation_results_class:Current Best Return = 283.708984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.48012350443844
INFO:tools.evaluation_results_class:Counted Episodes = 2591
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.8150329589844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.8150329589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.03121948242188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16536.671875
INFO:tools.evaluation_results_class:Current Best Return = 282.8150329589844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.20886319845857
INFO:tools.evaluation_results_class:Counted Episodes = 2595
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.9338073730469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.9338073730469
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.21243286132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16646.943359375
INFO:tools.evaluation_results_class:Current Best Return = 282.9338073730469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.15005771450558
INFO:tools.evaluation_results_class:Counted Episodes = 2599
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.5220947265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.5220947265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.8399658203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15673.8095703125
INFO:tools.evaluation_results_class:Current Best Return = 279.5220947265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.73495811119574
INFO:tools.evaluation_results_class:Counted Episodes = 2626
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.6081237792969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.6081237792969
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.44241333007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15575.60546875
INFO:tools.evaluation_results_class:Current Best Return = 277.6081237792969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.404400606980275
INFO:tools.evaluation_results_class:Counted Episodes = 2636
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.1166076660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.1166076660156
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.94557189941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15977.00390625
INFO:tools.evaluation_results_class:Current Best Return = 277.1166076660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.410071942446045
INFO:tools.evaluation_results_class:Counted Episodes = 2641
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.4018859863281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.4018859863281
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.38587951660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16046.85546875
INFO:tools.evaluation_results_class:Current Best Return = 276.4018859863281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.194716981132075
INFO:tools.evaluation_results_class:Counted Episodes = 2650
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.5278015136719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.5278015136719
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.622802734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15483.8017578125
INFO:tools.evaluation_results_class:Current Best Return = 275.5278015136719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.263715474839195
INFO:tools.evaluation_results_class:Counted Episodes = 2643
INFO:robust_rl.robust_rl_trainer:Iteration 27 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 286.1382751464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.1382751464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.01913452148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21460.193359375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.3327536231884
INFO:tools.evaluation_results_class:Counted Episodes = 5175
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.586542129516602
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 17.032123565673828
INFO:agents.father_agent:Step: 10, Training loss: 16.4112491607666
INFO:agents.father_agent:Step: 15, Training loss: 13.87472915649414
INFO:agents.father_agent:Step: 20, Training loss: 13.928976058959961
INFO:agents.father_agent:Step: 25, Training loss: 15.480660438537598
INFO:agents.father_agent:Step: 30, Training loss: 12.143769264221191
INFO:agents.father_agent:Step: 35, Training loss: 14.201119422912598
INFO:agents.father_agent:Step: 40, Training loss: 16.239925384521484
INFO:agents.father_agent:Step: 45, Training loss: 13.720882415771484
INFO:agents.father_agent:Step: 50, Training loss: 15.436410903930664
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 302.3580627441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 303.3580627441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 255.12403869628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18224.306640625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.80040322580645
INFO:tools.evaluation_results_class:Counted Episodes = 4960
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 295.53155517578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 296.53155517578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.22052001953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22125.71484375
INFO:tools.evaluation_results_class:Current Best Return = 295.53155517578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.000394477317556
INFO:tools.evaluation_results_class:Counted Episodes = 5070
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 303.52423095703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 304.52423095703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 256.2803649902344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17074.892578125
INFO:tools.evaluation_results_class:Current Best Return = 303.52423095703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.390977443609025
INFO:tools.evaluation_results_class:Counted Episodes = 5054
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 286.7687550476495
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 28 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 289.7379150390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 290.7379150390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 244.8387451171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21980.61328125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.21105722599418
INFO:tools.evaluation_results_class:Counted Episodes = 5155
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 18.230918884277344
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 13.569389343261719
INFO:agents.father_agent:Step: 10, Training loss: 15.380818367004395
INFO:agents.father_agent:Step: 15, Training loss: 14.425509452819824
INFO:agents.father_agent:Step: 20, Training loss: 17.49126434326172
INFO:agents.father_agent:Step: 25, Training loss: 16.20714569091797
INFO:agents.father_agent:Step: 30, Training loss: 15.321310997009277
INFO:agents.father_agent:Step: 35, Training loss: 15.335165977478027
INFO:agents.father_agent:Step: 40, Training loss: 15.120235443115234
INFO:agents.father_agent:Step: 45, Training loss: 20.363468170166016
INFO:agents.father_agent:Step: 50, Training loss: 18.38870620727539
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 299.8385925292969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.8385925292969
INFO:tools.evaluation_results_class:Average Discounted Reward = 253.68331909179688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18892.853515625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.63306372065496
INFO:tools.evaluation_results_class:Counted Episodes = 5069
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.9760437011719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.9760437011719
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.02435302734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19223.921875
INFO:tools.evaluation_results_class:Current Best Return = 283.9760437011719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.70246679316888
INFO:tools.evaluation_results_class:Counted Episodes = 5270
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 289.7730712890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 290.7730712890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.45928955078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18854.23046875
INFO:tools.evaluation_results_class:Current Best Return = 289.7730712890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.3230394909227
INFO:tools.evaluation_results_class:Counted Episodes = 5343
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 289.18046830926147
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 29 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.6308288574219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.6308288574219
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.61837768554688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16972.173828125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.260885885885884
INFO:tools.evaluation_results_class:Counted Episodes = 5328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.766599655151367
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 16.091753005981445
INFO:agents.father_agent:Step: 10, Training loss: 17.093523025512695
INFO:agents.father_agent:Step: 15, Training loss: 15.124523162841797
INFO:agents.father_agent:Step: 20, Training loss: 16.13646697998047
INFO:agents.father_agent:Step: 25, Training loss: 15.781485557556152
INFO:agents.father_agent:Step: 30, Training loss: 17.824819564819336
INFO:agents.father_agent:Step: 35, Training loss: 14.893389701843262
INFO:agents.father_agent:Step: 40, Training loss: 14.084000587463379
INFO:agents.father_agent:Step: 45, Training loss: 18.01624870300293
INFO:agents.father_agent:Step: 50, Training loss: 15.291948318481445
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 281.7084045410156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.7084045410156
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.2828369140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14690.3544921875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.33736305251228
INFO:tools.evaluation_results_class:Counted Episodes = 5294
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 288.06219482421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.06219482421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.15701293945312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21254.66015625
INFO:tools.evaluation_results_class:Current Best Return = 288.06219482421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.76018957345971
INFO:tools.evaluation_results_class:Counted Episodes = 5275
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 292.2530517578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.2530517578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 247.4164276123047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20109.4453125
INFO:tools.evaluation_results_class:Current Best Return = 292.2530517578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.247233164509474
INFO:tools.evaluation_results_class:Counted Episodes = 5331
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 289.104614442195
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 30 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 285.2073669433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 286.2073669433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.02342224121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18144.01171875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.60415094339623
INFO:tools.evaluation_results_class:Counted Episodes = 5300
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.801661491394043
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 15.446295738220215
INFO:agents.father_agent:Step: 10, Training loss: 17.38250160217285
INFO:agents.father_agent:Step: 15, Training loss: 14.867466926574707
INFO:agents.father_agent:Step: 20, Training loss: 10.546664237976074
INFO:agents.father_agent:Step: 25, Training loss: 15.391504287719727
INFO:agents.father_agent:Step: 30, Training loss: 15.262527465820312
INFO:agents.father_agent:Step: 35, Training loss: 13.837000846862793
INFO:agents.father_agent:Step: 40, Training loss: 17.568517684936523
INFO:agents.father_agent:Step: 45, Training loss: 12.486948013305664
INFO:agents.father_agent:Step: 50, Training loss: 15.926339149475098
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 279.1208801269531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.1208801269531
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.0331268310547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17448.67578125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.56890325033264
INFO:tools.evaluation_results_class:Counted Episodes = 5261
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 300.0459899902344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 301.0459899902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 252.5605926513672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21656.2734375
INFO:tools.evaluation_results_class:Current Best Return = 300.0459899902344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.6425215348473
INFO:tools.evaluation_results_class:Counted Episodes = 5108
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 301.1304626464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 302.1304626464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 252.68031311035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23683.888671875
INFO:tools.evaluation_results_class:Current Best Return = 301.1304626464844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.746588693957115
INFO:tools.evaluation_results_class:Counted Episodes = 5130
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 287.1702160632735
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 31 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.480224609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.480224609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.8500213623047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20333.591796875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.238651346769636
INFO:tools.evaluation_results_class:Counted Episodes = 5309
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.459132194519043
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 16.8582706451416
INFO:agents.father_agent:Step: 10, Training loss: 16.241832733154297
INFO:agents.father_agent:Step: 15, Training loss: 17.984485626220703
INFO:agents.father_agent:Step: 20, Training loss: 12.966450691223145
INFO:agents.father_agent:Step: 25, Training loss: 16.499771118164062
INFO:agents.father_agent:Step: 30, Training loss: 15.021357536315918
INFO:agents.father_agent:Step: 35, Training loss: 16.586122512817383
INFO:agents.father_agent:Step: 40, Training loss: 18.488378524780273
INFO:agents.father_agent:Step: 45, Training loss: 18.61818504333496
INFO:agents.father_agent:Step: 50, Training loss: 17.993141174316406
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 290.3714294433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 291.3714294433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.53306579589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19625.60546875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.27757736852098
INFO:tools.evaluation_results_class:Counted Episodes = 5267
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.2930908203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.2930908203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.9540557861328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20296.248046875
INFO:tools.evaluation_results_class:Current Best Return = 284.2930908203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.23435114503817
INFO:tools.evaluation_results_class:Counted Episodes = 5240
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.7494201660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.7494201660156
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.62661743164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19998.287109375
INFO:tools.evaluation_results_class:Current Best Return = 284.7494201660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.79163498098859
INFO:tools.evaluation_results_class:Counted Episodes = 5260
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 286.3182769098749
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 683.5401611328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 684.5401611328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 525.7755126953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 95853.9453125
INFO:tools.evaluation_results_class:Current Best Return = 683.5401611328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.31004657351963
INFO:tools.evaluation_results_class:Counted Episodes = 1503
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 363.54083251953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 364.54083251953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 298.7254333496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28028.11328125
INFO:tools.evaluation_results_class:Current Best Return = 363.54083251953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.67669172932331
INFO:tools.evaluation_results_class:Counted Episodes = 2128
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 579.6256103515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 580.6256103515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 457.6690979003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 70834.59375
INFO:tools.evaluation_results_class:Current Best Return = 579.6256103515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 82.72769679300292
INFO:tools.evaluation_results_class:Counted Episodes = 1715
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 366.134521484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 367.134521484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 305.0748291015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28909.677734375
INFO:tools.evaluation_results_class:Current Best Return = 366.134521484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.58961038961039
INFO:tools.evaluation_results_class:Counted Episodes = 2310
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 479.0144958496094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 480.0144958496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 382.1689758300781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 47446.56640625
INFO:tools.evaluation_results_class:Current Best Return = 479.0144958496094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.12325711098717
INFO:tools.evaluation_results_class:Counted Episodes = 1793
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.3063659667969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.3063659667969
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.5018768310547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16727.54296875
INFO:tools.evaluation_results_class:Current Best Return = 276.3063659667969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.085549132947975
INFO:tools.evaluation_results_class:Counted Episodes = 2595
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.740478515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.740478515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.97430419921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16447.4921875
INFO:tools.evaluation_results_class:Current Best Return = 276.740478515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.30111667308433
INFO:tools.evaluation_results_class:Counted Episodes = 2597
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.16485595703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.16485595703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.49432373046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15773.896484375
INFO:tools.evaluation_results_class:Current Best Return = 277.16485595703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.098613251155626
INFO:tools.evaluation_results_class:Counted Episodes = 2596
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.487060546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.487060546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.75082397460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15711.4873046875
INFO:tools.evaluation_results_class:Current Best Return = 278.487060546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.349671941335394
INFO:tools.evaluation_results_class:Counted Episodes = 2591
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.71923828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.71923828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.67132568359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16501.625
INFO:tools.evaluation_results_class:Current Best Return = 277.71923828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.278461538461535
INFO:tools.evaluation_results_class:Counted Episodes = 2600
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.92657470703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.92657470703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.34246826171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16092.41015625
INFO:tools.evaluation_results_class:Current Best Return = 280.92657470703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.62237762237762
INFO:tools.evaluation_results_class:Counted Episodes = 2574
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.48846435546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.48846435546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.5400390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16742.39453125
INFO:tools.evaluation_results_class:Current Best Return = 277.48846435546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.1967717140661
INFO:tools.evaluation_results_class:Counted Episodes = 2602
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.2509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.2509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.15628051757812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16481.708984375
INFO:tools.evaluation_results_class:Current Best Return = 277.2509765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.3003861003861
INFO:tools.evaluation_results_class:Counted Episodes = 2590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.62762451171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.62762451171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.0416717529297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17216.84765625
INFO:tools.evaluation_results_class:Current Best Return = 279.62762451171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.299424184261035
INFO:tools.evaluation_results_class:Counted Episodes = 2605
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.5992431640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.5992431640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.16802978515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16885.728515625
INFO:tools.evaluation_results_class:Current Best Return = 275.5992431640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.77328244274809
INFO:tools.evaluation_results_class:Counted Episodes = 2620
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.57208251953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.57208251953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.40789794921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16967.490234375
INFO:tools.evaluation_results_class:Current Best Return = 277.57208251953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.270665128796615
INFO:tools.evaluation_results_class:Counted Episodes = 2601
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.3241882324219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.3241882324219
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.6944580078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17316.70703125
INFO:tools.evaluation_results_class:Current Best Return = 277.3241882324219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.70019120458891
INFO:tools.evaluation_results_class:Counted Episodes = 2615
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.625
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.4221649169922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15926.2431640625
INFO:tools.evaluation_results_class:Current Best Return = 275.625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.65519877675841
INFO:tools.evaluation_results_class:Counted Episodes = 2616
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.0934753417969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.0934753417969
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.81443786621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16735.140625
INFO:tools.evaluation_results_class:Current Best Return = 275.0934753417969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.65890881342999
INFO:tools.evaluation_results_class:Counted Episodes = 2621
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.6583251953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.6583251953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.90733337402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16859.80859375
INFO:tools.evaluation_results_class:Current Best Return = 276.6583251953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.7847809377402
INFO:tools.evaluation_results_class:Counted Episodes = 2602
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.4817810058594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.4817810058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.6510467529297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15915.36328125
INFO:tools.evaluation_results_class:Current Best Return = 274.4817810058594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.27107061503417
INFO:tools.evaluation_results_class:Counted Episodes = 2634
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.3346252441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.3346252441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.56427001953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16142.966796875
INFO:tools.evaluation_results_class:Current Best Return = 278.3346252441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.1151189562548
INFO:tools.evaluation_results_class:Counted Episodes = 2606
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.3467102050781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.3467102050781
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.51889038085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16296.1416015625
INFO:tools.evaluation_results_class:Current Best Return = 277.3467102050781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.281527188584654
INFO:tools.evaluation_results_class:Counted Episodes = 2593
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.8674621582031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.8674621582031
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.1185760498047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16653.353515625
INFO:tools.evaluation_results_class:Current Best Return = 280.8674621582031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.58191653786708
INFO:tools.evaluation_results_class:Counted Episodes = 2588
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.8959655761719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.8959655761719
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.1034698486328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15967.1904296875
INFO:tools.evaluation_results_class:Current Best Return = 277.8959655761719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.2242774566474
INFO:tools.evaluation_results_class:Counted Episodes = 2595
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.75299072265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.75299072265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.9461669921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17251.40234375
INFO:tools.evaluation_results_class:Current Best Return = 281.75299072265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.56204097410127
INFO:tools.evaluation_results_class:Counted Episodes = 2587
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.0843200683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.0843200683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.83570861816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16942.421875
INFO:tools.evaluation_results_class:Current Best Return = 280.0843200683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.94020697585282
INFO:tools.evaluation_results_class:Counted Episodes = 2609
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.9061279296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.9061279296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.66954040527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16314.658203125
INFO:tools.evaluation_results_class:Current Best Return = 275.9061279296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.54368561617703
INFO:tools.evaluation_results_class:Counted Episodes = 2621
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.7225646972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.7225646972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.1012725830078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16745.7734375
INFO:tools.evaluation_results_class:Current Best Return = 276.7225646972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.5655487804878
INFO:tools.evaluation_results_class:Counted Episodes = 2624
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.3935546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.3935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.447021484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16650.486328125
INFO:tools.evaluation_results_class:Current Best Return = 277.3935546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.04602991944764
INFO:tools.evaluation_results_class:Counted Episodes = 2607
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.7035827636719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.7035827636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.37432861328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16325.7451171875
INFO:tools.evaluation_results_class:Current Best Return = 276.7035827636719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.66233766233766
INFO:tools.evaluation_results_class:Counted Episodes = 2618
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.5318908691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.5318908691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.4700927734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17523.3828125
INFO:tools.evaluation_results_class:Current Best Return = 278.5318908691406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.906994619523445
INFO:tools.evaluation_results_class:Counted Episodes = 2602
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.5145263671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.5145263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.0707550048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16571.208984375
INFO:tools.evaluation_results_class:Current Best Return = 276.5145263671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.611917494270436
INFO:tools.evaluation_results_class:Counted Episodes = 2618
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.4176025390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.4176025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.44259643554688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17168.43359375
INFO:tools.evaluation_results_class:Current Best Return = 278.4176025390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.95986105750676
INFO:tools.evaluation_results_class:Counted Episodes = 2591
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.76544189453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.76544189453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.7308349609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16042.306640625
INFO:tools.evaluation_results_class:Current Best Return = 274.76544189453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.51563691838292
INFO:tools.evaluation_results_class:Counted Episodes = 2622
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.5203552246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.5203552246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.56077575683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16668.544921875
INFO:tools.evaluation_results_class:Current Best Return = 278.5203552246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.999231360491926
INFO:tools.evaluation_results_class:Counted Episodes = 2602
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 706.9236450195312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 707.9236450195312
INFO:tools.evaluation_results_class:Average Discounted Reward = 541.5917358398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 98178.109375
INFO:tools.evaluation_results_class:Current Best Return = 706.9236450195312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.57104377104378
INFO:tools.evaluation_results_class:Counted Episodes = 1485
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 371.1488037109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 372.1488037109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 305.3329162597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27374.552734375
INFO:tools.evaluation_results_class:Current Best Return = 371.1488037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.83686940122584
INFO:tools.evaluation_results_class:Counted Episodes = 2121
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 587.4663696289062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 588.4663696289062
INFO:tools.evaluation_results_class:Average Discounted Reward = 465.0612487792969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 64762.67578125
INFO:tools.evaluation_results_class:Current Best Return = 587.4663696289062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 82.8785046728972
INFO:tools.evaluation_results_class:Counted Episodes = 1712
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 369.7134704589844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 370.7134704589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 309.1195373535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27184.693359375
INFO:tools.evaluation_results_class:Current Best Return = 369.7134704589844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.95470085470085
INFO:tools.evaluation_results_class:Counted Episodes = 2340
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 478.16693115234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 479.16693115234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 382.3882141113281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44417.82421875
INFO:tools.evaluation_results_class:Current Best Return = 478.16693115234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.61003861003861
INFO:tools.evaluation_results_class:Counted Episodes = 1813
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 286.3929443359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.3929443359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.3047332763672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16388.177734375
INFO:tools.evaluation_results_class:Current Best Return = 286.3929443359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.22640061396777
INFO:tools.evaluation_results_class:Counted Episodes = 2606
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.0296325683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.0296325683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.53964233398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16124.36328125
INFO:tools.evaluation_results_class:Current Best Return = 279.0296325683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.32320546904671
INFO:tools.evaluation_results_class:Counted Episodes = 2633
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.7314147949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.7314147949219
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.43397521972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17291.7734375
INFO:tools.evaluation_results_class:Current Best Return = 281.7314147949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.72495238095238
INFO:tools.evaluation_results_class:Counted Episodes = 2625
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.9483947753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.9483947753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.09530639648438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16709.02734375
INFO:tools.evaluation_results_class:Current Best Return = 279.9483947753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.6131498470948
INFO:tools.evaluation_results_class:Counted Episodes = 2616
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.01251220703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.01251220703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.3891143798828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16173.958984375
INFO:tools.evaluation_results_class:Current Best Return = 279.01251220703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.48066717210008
INFO:tools.evaluation_results_class:Counted Episodes = 2638
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.71466064453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.71466064453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.79971313476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16597.24609375
INFO:tools.evaluation_results_class:Current Best Return = 279.71466064453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.725775564917654
INFO:tools.evaluation_results_class:Counted Episodes = 2611
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.6416320800781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.6416320800781
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.40821838378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18003.166015625
INFO:tools.evaluation_results_class:Current Best Return = 284.6416320800781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.371483622350674
INFO:tools.evaluation_results_class:Counted Episodes = 2595
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.86529541015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.86529541015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.99110412597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16951.43359375
INFO:tools.evaluation_results_class:Current Best Return = 283.86529541015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.50250868390583
INFO:tools.evaluation_results_class:Counted Episodes = 2591
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.22650146484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.22650146484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.26475524902344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16929.212890625
INFO:tools.evaluation_results_class:Current Best Return = 280.22650146484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.8743814236772
INFO:tools.evaluation_results_class:Counted Episodes = 2627
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.9846496582031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.9846496582031
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.15049743652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17337.953125
INFO:tools.evaluation_results_class:Current Best Return = 282.9846496582031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.31708253358925
INFO:tools.evaluation_results_class:Counted Episodes = 2605
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.8036804199219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.8036804199219
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.1990509033203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17235.3984375
INFO:tools.evaluation_results_class:Current Best Return = 283.8036804199219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.22162576687116
INFO:tools.evaluation_results_class:Counted Episodes = 2608
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.1390380859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.1390380859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.09942626953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16436.404296875
INFO:tools.evaluation_results_class:Current Best Return = 281.1390380859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.56838905775076
INFO:tools.evaluation_results_class:Counted Episodes = 2632
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.2557373046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.2557373046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.87887573242188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17185.904296875
INFO:tools.evaluation_results_class:Current Best Return = 281.2557373046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.89050535987749
INFO:tools.evaluation_results_class:Counted Episodes = 2612
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.10479736328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.10479736328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.751708984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17116.091796875
INFO:tools.evaluation_results_class:Current Best Return = 281.10479736328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.66894457099468
INFO:tools.evaluation_results_class:Counted Episodes = 2634
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.28570556640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.28570556640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.47097778320312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16633.6328125
INFO:tools.evaluation_results_class:Current Best Return = 279.28570556640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.52838095238095
INFO:tools.evaluation_results_class:Counted Episodes = 2625
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.2305603027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.2305603027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.31625366210938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17255.255859375
INFO:tools.evaluation_results_class:Current Best Return = 283.2305603027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.96246648793566
INFO:tools.evaluation_results_class:Counted Episodes = 2611
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 287.90625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 288.90625
INFO:tools.evaluation_results_class:Average Discounted Reward = 244.05166625976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16997.51171875
INFO:tools.evaluation_results_class:Current Best Return = 287.90625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.96796875
INFO:tools.evaluation_results_class:Counted Episodes = 2560
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 288.8169250488281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.8169250488281
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.00440979003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17111.7890625
INFO:tools.evaluation_results_class:Current Best Return = 288.8169250488281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.659425911559346
INFO:tools.evaluation_results_class:Counted Episodes = 2578
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 288.0384826660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.0384826660156
INFO:tools.evaluation_results_class:Average Discounted Reward = 244.32550048828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17239.396484375
INFO:tools.evaluation_results_class:Current Best Return = 288.0384826660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.657853810264385
INFO:tools.evaluation_results_class:Counted Episodes = 2572
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 287.6436462402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 288.6436462402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.97763061523438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17703.72265625
INFO:tools.evaluation_results_class:Current Best Return = 287.6436462402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.659937888198755
INFO:tools.evaluation_results_class:Counted Episodes = 2576
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 287.1270751953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 288.1270751953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.4490509033203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17257.623046875
INFO:tools.evaluation_results_class:Current Best Return = 287.1270751953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.63696241766757
INFO:tools.evaluation_results_class:Counted Episodes = 2581
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.5263671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.5263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.71678161621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16533.16015625
INFO:tools.evaluation_results_class:Current Best Return = 284.5263671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.28879476318829
INFO:tools.evaluation_results_class:Counted Episodes = 2597
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 285.02313232421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 286.02313232421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.14938354492188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16553.546875
INFO:tools.evaluation_results_class:Current Best Return = 285.02313232421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.27602158828065
INFO:tools.evaluation_results_class:Counted Episodes = 2594
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.0572509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.0572509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.4365692138672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16477.865234375
INFO:tools.evaluation_results_class:Current Best Return = 283.0572509765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.196003074558035
INFO:tools.evaluation_results_class:Counted Episodes = 2602
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.6214294433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.6214294433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.48451232910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16606.1484375
INFO:tools.evaluation_results_class:Current Best Return = 281.6214294433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.92810707456979
INFO:tools.evaluation_results_class:Counted Episodes = 2615
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.6043701171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.6043701171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.96055603027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16982.837890625
INFO:tools.evaluation_results_class:Current Best Return = 282.6043701171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.03753351206434
INFO:tools.evaluation_results_class:Counted Episodes = 2611
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.3057861328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.3057861328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.00389099121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16151.6376953125
INFO:tools.evaluation_results_class:Current Best Return = 278.3057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.06836964688204
INFO:tools.evaluation_results_class:Counted Episodes = 2662
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.3841857910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.3841857910156
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.3999481201172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16562.677734375
INFO:tools.evaluation_results_class:Current Best Return = 277.3841857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.03050847457627
INFO:tools.evaluation_results_class:Counted Episodes = 2655
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.5713806152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.5713806152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.39114379882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15495.958984375
INFO:tools.evaluation_results_class:Current Best Return = 276.5713806152344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.25558500567967
INFO:tools.evaluation_results_class:Counted Episodes = 2641
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.54779052734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.54779052734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.97308349609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15405.310546875
INFO:tools.evaluation_results_class:Current Best Return = 275.54779052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.177937287495276
INFO:tools.evaluation_results_class:Counted Episodes = 2647
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.08734130859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.08734130859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.9134063720703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16171.869140625
INFO:tools.evaluation_results_class:Current Best Return = 278.08734130859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.1144578313253
INFO:tools.evaluation_results_class:Counted Episodes = 2656
INFO:robust_rl.robust_rl_trainer:Iteration 32 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 293.45196533203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 294.45196533203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.35301208496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23267.9453125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.84343036978757
INFO:tools.evaluation_results_class:Counted Episodes = 5084
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.393853187561035
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 20.215078353881836
INFO:agents.father_agent:Step: 10, Training loss: 17.981687545776367
INFO:agents.father_agent:Step: 15, Training loss: 17.169315338134766
INFO:agents.father_agent:Step: 20, Training loss: 13.55685806274414
INFO:agents.father_agent:Step: 25, Training loss: 15.43652629852295
INFO:agents.father_agent:Step: 30, Training loss: 20.178451538085938
INFO:agents.father_agent:Step: 35, Training loss: 12.47635555267334
INFO:agents.father_agent:Step: 40, Training loss: 16.69175148010254
INFO:agents.father_agent:Step: 45, Training loss: 16.134489059448242
INFO:agents.father_agent:Step: 50, Training loss: 17.28188705444336
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 289.0469665527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 290.0469665527344
INFO:tools.evaluation_results_class:Average Discounted Reward = 244.31866455078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19610.388671875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.0646843019888
INFO:tools.evaluation_results_class:Counted Episodes = 5179
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 290.998291015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 291.998291015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.80458068847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18789.978515625
INFO:tools.evaluation_results_class:Current Best Return = 290.998291015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.70089112746997
INFO:tools.evaluation_results_class:Counted Episodes = 5162
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 295.0033264160156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 296.0033264160156
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.4774932861328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18545.25390625
INFO:tools.evaluation_results_class:Current Best Return = 295.0033264160156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.65486725663717
INFO:tools.evaluation_results_class:Counted Episodes = 5198
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 287.9554833820107
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 33 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 287.9837341308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 288.9837341308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.4423828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19833.736328125
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.516129032258064
INFO:tools.evaluation_results_class:Counted Episodes = 5177
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.918863296508789
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 15.905468940734863
INFO:agents.father_agent:Step: 10, Training loss: 16.764673233032227
INFO:agents.father_agent:Step: 15, Training loss: 13.619110107421875
INFO:agents.father_agent:Step: 20, Training loss: 17.21923828125
INFO:agents.father_agent:Step: 25, Training loss: 18.68114471435547
INFO:agents.father_agent:Step: 30, Training loss: 13.847600936889648
INFO:agents.father_agent:Step: 35, Training loss: 14.172883033752441
INFO:agents.father_agent:Step: 40, Training loss: 16.90591049194336
INFO:agents.father_agent:Step: 45, Training loss: 19.91904067993164
INFO:agents.father_agent:Step: 50, Training loss: 18.4747314453125
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 303.449462890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 304.449462890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 254.82766723632812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20738.4609375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.290012033694346
INFO:tools.evaluation_results_class:Counted Episodes = 4986
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 299.234130859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.234130859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 251.75637817382812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17973.341796875
INFO:tools.evaluation_results_class:Current Best Return = 299.234130859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.2301603008114
INFO:tools.evaluation_results_class:Counted Episodes = 5053
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 299.8747863769531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.8747863769531
INFO:tools.evaluation_results_class:Average Discounted Reward = 252.7324981689453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19992.693359375
INFO:tools.evaluation_results_class:Current Best Return = 299.8747863769531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.231083446800234
INFO:tools.evaluation_results_class:Counted Episodes = 5141
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 292.4580701224593
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 34 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.6011047363281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.6011047363281
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.80963134765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15793.3623046875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.40496800903274
INFO:tools.evaluation_results_class:Counted Episodes = 5314
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.3962345123291
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 15.184513092041016
INFO:agents.father_agent:Step: 10, Training loss: 12.780008316040039
INFO:agents.father_agent:Step: 15, Training loss: 16.027603149414062
INFO:agents.father_agent:Step: 20, Training loss: 16.20111846923828
INFO:agents.father_agent:Step: 25, Training loss: 12.26414680480957
INFO:agents.father_agent:Step: 30, Training loss: 18.301984786987305
INFO:agents.father_agent:Step: 35, Training loss: 17.147729873657227
INFO:agents.father_agent:Step: 40, Training loss: 13.394688606262207
INFO:agents.father_agent:Step: 45, Training loss: 21.838298797607422
INFO:agents.father_agent:Step: 50, Training loss: 21.13249969482422
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 297.58587646484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 298.58587646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 251.3709259033203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18015.669921875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.47744213417026
INFO:tools.evaluation_results_class:Counted Episodes = 5098
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 295.11993408203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 296.11993408203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.60470581054688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18383.13671875
INFO:tools.evaluation_results_class:Current Best Return = 295.11993408203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.30807885581755
INFO:tools.evaluation_results_class:Counted Episodes = 5174
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 300.1290283203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 301.1290283203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 253.2030487060547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17876.79296875
INFO:tools.evaluation_results_class:Current Best Return = 300.1290283203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.09603979338053
INFO:tools.evaluation_results_class:Counted Episodes = 5227
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 292.96355857546337
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 35 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.786865234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.786865234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.0154571533203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16885.498046875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.81408665775911
INFO:tools.evaluation_results_class:Counted Episodes = 5239
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 20.56989288330078
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 19.62087059020996
INFO:agents.father_agent:Step: 10, Training loss: 15.188323020935059
INFO:agents.father_agent:Step: 15, Training loss: 20.922407150268555
INFO:agents.father_agent:Step: 20, Training loss: 20.960702896118164
INFO:agents.father_agent:Step: 25, Training loss: 15.109746932983398
INFO:agents.father_agent:Step: 30, Training loss: 16.454641342163086
INFO:agents.father_agent:Step: 35, Training loss: 17.81131362915039
INFO:agents.father_agent:Step: 40, Training loss: 16.621097564697266
INFO:agents.father_agent:Step: 45, Training loss: 18.549346923828125
INFO:agents.father_agent:Step: 50, Training loss: 18.12724494934082
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 293.11431884765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 294.11431884765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 247.56582641601562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18137.91796875
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.78231562252181
INFO:tools.evaluation_results_class:Counted Episodes = 5044
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.7640686035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.7640686035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.37652587890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16100.044921875
INFO:tools.evaluation_results_class:Current Best Return = 277.7640686035156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.06581899775617
INFO:tools.evaluation_results_class:Counted Episodes = 5348
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 292.44189453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.44189453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.67129516601562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20521.6015625
INFO:tools.evaluation_results_class:Current Best Return = 292.44189453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.16505955757232
INFO:tools.evaluation_results_class:Counted Episodes = 5289
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 289.5847446721347
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 36 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.82952880859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.82952880859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.67962646484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17337.962890625
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.25104761904762
INFO:tools.evaluation_results_class:Counted Episodes = 5250
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.228748321533203
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 16.587127685546875
INFO:agents.father_agent:Step: 10, Training loss: 12.705053329467773
INFO:agents.father_agent:Step: 15, Training loss: 17.034320831298828
INFO:agents.father_agent:Step: 20, Training loss: 18.948144912719727
INFO:agents.father_agent:Step: 25, Training loss: 17.016870498657227
INFO:agents.father_agent:Step: 30, Training loss: 17.37017250061035
INFO:agents.father_agent:Step: 35, Training loss: 12.196269989013672
INFO:agents.father_agent:Step: 40, Training loss: 19.164194107055664
INFO:agents.father_agent:Step: 45, Training loss: 16.78253746032715
INFO:agents.father_agent:Step: 50, Training loss: 18.256389617919922
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 268.4696960449219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.4696960449219
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.74588012695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18809.55859375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.85464358831193
INFO:tools.evaluation_results_class:Counted Episodes = 5373
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 288.7698669433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.7698669433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.28846740722656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19812.03515625
INFO:tools.evaluation_results_class:Current Best Return = 288.7698669433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.591512988300614
INFO:tools.evaluation_results_class:Counted Episodes = 5043
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 286.5877685546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.5877685546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.25526428222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18932.09765625
INFO:tools.evaluation_results_class:Current Best Return = 286.5877685546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.30739223717573
INFO:tools.evaluation_results_class:Counted Episodes = 5127
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 284.66696738750585
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 698.9431762695312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 699.9431762695312
INFO:tools.evaluation_results_class:Average Discounted Reward = 534.3663330078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 100041.375
INFO:tools.evaluation_results_class:Current Best Return = 698.9431762695312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.28882833787466
INFO:tools.evaluation_results_class:Counted Episodes = 1468
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 361.04217529296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 362.04217529296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 294.9983825683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27252.189453125
INFO:tools.evaluation_results_class:Current Best Return = 361.04217529296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 70.33056234718826
INFO:tools.evaluation_results_class:Counted Episodes = 2045
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 583.5948486328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 584.5948486328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 455.86175537109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 72532.9921875
INFO:tools.evaluation_results_class:Current Best Return = 583.5948486328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.04901960784314
INFO:tools.evaluation_results_class:Counted Episodes = 1632
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 368.9436950683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 369.9436950683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 306.2495422363281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28393.248046875
INFO:tools.evaluation_results_class:Current Best Return = 368.9436950683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.53006681514476
INFO:tools.evaluation_results_class:Counted Episodes = 2245
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 467.73065185546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 468.73065185546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 370.2061767578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45582.08203125
INFO:tools.evaluation_results_class:Current Best Return = 467.73065185546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.70281124497993
INFO:tools.evaluation_results_class:Counted Episodes = 1743
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.266357421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.266357421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.52749633789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16500.330078125
INFO:tools.evaluation_results_class:Current Best Return = 276.266357421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.67974633372969
INFO:tools.evaluation_results_class:Counted Episodes = 2523
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.0511474609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.0511474609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.4645233154297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16045.513671875
INFO:tools.evaluation_results_class:Current Best Return = 274.0511474609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.94326807830603
INFO:tools.evaluation_results_class:Counted Episodes = 2503
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.3492126464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.3492126464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.7582244873047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16662.919921875
INFO:tools.evaluation_results_class:Current Best Return = 273.3492126464844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.615415176797775
INFO:tools.evaluation_results_class:Counted Episodes = 2517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.8885803222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.8885803222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.99537658691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16906.369140625
INFO:tools.evaluation_results_class:Current Best Return = 275.8885803222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.91932907348243
INFO:tools.evaluation_results_class:Counted Episodes = 2504
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.5751647949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.5751647949219
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.76841735839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17681.2890625
INFO:tools.evaluation_results_class:Current Best Return = 276.5751647949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.58631662688942
INFO:tools.evaluation_results_class:Counted Episodes = 2514
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.9980163574219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.9980163574219
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.3863525390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17371.31640625
INFO:tools.evaluation_results_class:Current Best Return = 273.9980163574219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.6
INFO:tools.evaluation_results_class:Counted Episodes = 2510
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.297607421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.297607421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.99388122558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16258.3154296875
INFO:tools.evaluation_results_class:Current Best Return = 274.297607421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.945238095238096
INFO:tools.evaluation_results_class:Counted Episodes = 2520
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.1343994140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.1343994140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.329345703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15602.2626953125
INFO:tools.evaluation_results_class:Current Best Return = 272.1343994140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.569169960474305
INFO:tools.evaluation_results_class:Counted Episodes = 2530
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.1681823730469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.1681823730469
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.1623992919922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15822.484375
INFO:tools.evaluation_results_class:Current Best Return = 273.1681823730469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.537307540465854
INFO:tools.evaluation_results_class:Counted Episodes = 2533
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.30633544921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.30633544921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.3830108642578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15963.544921875
INFO:tools.evaluation_results_class:Current Best Return = 272.30633544921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.45770750988142
INFO:tools.evaluation_results_class:Counted Episodes = 2530
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.72314453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.72314453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.3665008544922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16027.1884765625
INFO:tools.evaluation_results_class:Current Best Return = 273.72314453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.83611774065235
INFO:tools.evaluation_results_class:Counted Episodes = 2514
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.2432556152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.2432556152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.9827880859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16230.05078125
INFO:tools.evaluation_results_class:Current Best Return = 272.2432556152344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.808426073131955
INFO:tools.evaluation_results_class:Counted Episodes = 2516
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.7872314453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.7872314453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.46719360351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15973.6083984375
INFO:tools.evaluation_results_class:Current Best Return = 273.7872314453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.685589519650655
INFO:tools.evaluation_results_class:Counted Episodes = 2519
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.764892578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.764892578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.24032592773438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16451.392578125
INFO:tools.evaluation_results_class:Current Best Return = 273.764892578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.9166004765687
INFO:tools.evaluation_results_class:Counted Episodes = 2518
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.4526672363281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.4526672363281
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.8352508544922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16581.36328125
INFO:tools.evaluation_results_class:Current Best Return = 274.4526672363281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.13823411905713
INFO:tools.evaluation_results_class:Counted Episodes = 2503
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.3544616699219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.3544616699219
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.88131713867188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16234.4140625
INFO:tools.evaluation_results_class:Current Best Return = 274.3544616699219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.695841584158416
INFO:tools.evaluation_results_class:Counted Episodes = 2525
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.77593994140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.77593994140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.55125427246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15225.4814453125
INFO:tools.evaluation_results_class:Current Best Return = 269.77593994140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.22641509433962
INFO:tools.evaluation_results_class:Counted Episodes = 2544
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.6658935546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.6658935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.389404296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15865.8603515625
INFO:tools.evaluation_results_class:Current Best Return = 269.6658935546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.7557639703009
INFO:tools.evaluation_results_class:Counted Episodes = 2559
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.55206298828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.55206298828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.09408569335938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16132.2548828125
INFO:tools.evaluation_results_class:Current Best Return = 269.55206298828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.161493123772104
INFO:tools.evaluation_results_class:Counted Episodes = 2545
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.71710205078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.71710205078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.92372131347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16265.0234375
INFO:tools.evaluation_results_class:Current Best Return = 272.71710205078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.172495088408645
INFO:tools.evaluation_results_class:Counted Episodes = 2545
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.8669128417969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.8669128417969
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.22055053710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15754.0224609375
INFO:tools.evaluation_results_class:Current Best Return = 271.8669128417969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.100117785630154
INFO:tools.evaluation_results_class:Counted Episodes = 2547
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.3039855957031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.3039855957031
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.74415588378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15763.9873046875
INFO:tools.evaluation_results_class:Current Best Return = 275.3039855957031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.144
INFO:tools.evaluation_results_class:Counted Episodes = 2500
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.6176452636719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.6176452636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.85633850097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15889.1767578125
INFO:tools.evaluation_results_class:Current Best Return = 276.6176452636719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.84419713831478
INFO:tools.evaluation_results_class:Counted Episodes = 2516
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.1396179199219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.1396179199219
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.3364715576172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15832.14453125
INFO:tools.evaluation_results_class:Current Best Return = 276.1396179199219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.106603023070804
INFO:tools.evaluation_results_class:Counted Episodes = 2514
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.9723205566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.9723205566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.81556701660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15934.8525390625
INFO:tools.evaluation_results_class:Current Best Return = 273.9723205566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.6300395256917
INFO:tools.evaluation_results_class:Counted Episodes = 2530
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.72344970703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.72344970703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.0578155517578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16144.87890625
INFO:tools.evaluation_results_class:Current Best Return = 275.72344970703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.16032064128257
INFO:tools.evaluation_results_class:Counted Episodes = 2495
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.7840576171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.7840576171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.33685302734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16391.671875
INFO:tools.evaluation_results_class:Current Best Return = 273.7840576171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.596525858665615
INFO:tools.evaluation_results_class:Counted Episodes = 2533
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.13018798828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.13018798828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.29388427734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15965.9365234375
INFO:tools.evaluation_results_class:Current Best Return = 272.13018798828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.34674556213018
INFO:tools.evaluation_results_class:Counted Episodes = 2535
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.1769714355469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.1769714355469
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.0091094970703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16460.779296875
INFO:tools.evaluation_results_class:Current Best Return = 273.1769714355469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.63341250989707
INFO:tools.evaluation_results_class:Counted Episodes = 2526
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.7633972167969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.7633972167969
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.47622680664062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15835.1630859375
INFO:tools.evaluation_results_class:Current Best Return = 273.7633972167969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.693529178245335
INFO:tools.evaluation_results_class:Counted Episodes = 2519
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.54632568359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.54632568359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.84622192382812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16375.888671875
INFO:tools.evaluation_results_class:Current Best Return = 275.54632568359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.6714172604909
INFO:tools.evaluation_results_class:Counted Episodes = 2526
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.7987976074219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.7987976074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.43899536132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16655.74609375
INFO:tools.evaluation_results_class:Current Best Return = 274.7987976074219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.71792828685259
INFO:tools.evaluation_results_class:Counted Episodes = 2510
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.6194763183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.6194763183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.847412109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15731.3798828125
INFO:tools.evaluation_results_class:Current Best Return = 273.6194763183594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.20253164556962
INFO:tools.evaluation_results_class:Counted Episodes = 2528
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.77301025390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.77301025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.19285583496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15776.3994140625
INFO:tools.evaluation_results_class:Current Best Return = 270.77301025390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.166797797010226
INFO:tools.evaluation_results_class:Counted Episodes = 2542
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.45526123046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.45526123046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.86451721191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16145.3486328125
INFO:tools.evaluation_results_class:Current Best Return = 274.45526123046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.791650099403576
INFO:tools.evaluation_results_class:Counted Episodes = 2515
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.55682373046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.55682373046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.823974609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16093.77734375
INFO:tools.evaluation_results_class:Current Best Return = 271.55682373046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.25887924230466
INFO:tools.evaluation_results_class:Counted Episodes = 2534
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 704.6087646484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 705.6087646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 538.0780639648438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 103698.6796875
INFO:tools.evaluation_results_class:Current Best Return = 704.6087646484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.8972972972973
INFO:tools.evaluation_results_class:Counted Episodes = 1480
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 367.20257568359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 368.20257568359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 300.7301330566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27673.8046875
INFO:tools.evaluation_results_class:Current Best Return = 367.20257568359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 69.4625
INFO:tools.evaluation_results_class:Counted Episodes = 2080
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 586.4381713867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 587.4381713867188
INFO:tools.evaluation_results_class:Average Discounted Reward = 458.6554260253906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 78289.703125
INFO:tools.evaluation_results_class:Current Best Return = 586.4381713867188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.25
INFO:tools.evaluation_results_class:Counted Episodes = 1656
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 362.4362487792969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 363.4362487792969
INFO:tools.evaluation_results_class:Average Discounted Reward = 301.5763854980469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27723.875
INFO:tools.evaluation_results_class:Current Best Return = 362.4362487792969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.01655052264808
INFO:tools.evaluation_results_class:Counted Episodes = 2296
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 464.1692199707031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 465.1692199707031
INFO:tools.evaluation_results_class:Average Discounted Reward = 368.8413391113281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 46593.4296875
INFO:tools.evaluation_results_class:Current Best Return = 464.1692199707031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.93700787401575
INFO:tools.evaluation_results_class:Counted Episodes = 1778
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.6496276855469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.6496276855469
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.1141357421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17474.5390625
INFO:tools.evaluation_results_class:Current Best Return = 281.6496276855469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.24262681871805
INFO:tools.evaluation_results_class:Counted Episodes = 2543
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.4066467285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.4066467285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.78944396972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16088.4853515625
INFO:tools.evaluation_results_class:Current Best Return = 283.4066467285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.64011097899326
INFO:tools.evaluation_results_class:Counted Episodes = 2523
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.07373046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.07373046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.126220703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16069.76171875
INFO:tools.evaluation_results_class:Current Best Return = 282.07373046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.970737417089346
INFO:tools.evaluation_results_class:Counted Episodes = 2563
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.3028869628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.3028869628906
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.2060089111328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16535.02734375
INFO:tools.evaluation_results_class:Current Best Return = 281.3028869628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.04937304075235
INFO:tools.evaluation_results_class:Counted Episodes = 2552
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.5035400390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.5035400390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.9407501220703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16319.5771484375
INFO:tools.evaluation_results_class:Current Best Return = 283.5035400390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.389589905362776
INFO:tools.evaluation_results_class:Counted Episodes = 2536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.0697021484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.0697021484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.9371795654297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16137.3583984375
INFO:tools.evaluation_results_class:Current Best Return = 282.0697021484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.157148483654986
INFO:tools.evaluation_results_class:Counted Episodes = 2539
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.8705139160156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.8705139160156
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.5519256591797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15944.6904296875
INFO:tools.evaluation_results_class:Current Best Return = 278.8705139160156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.63996889580093
INFO:tools.evaluation_results_class:Counted Episodes = 2572
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.05950927734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.05950927734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.82794189453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15842.3525390625
INFO:tools.evaluation_results_class:Current Best Return = 277.05950927734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.710258418167584
INFO:tools.evaluation_results_class:Counted Episodes = 2554
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.71624755859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.71624755859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.2450408935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16197.970703125
INFO:tools.evaluation_results_class:Current Best Return = 279.71624755859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.8133072407045
INFO:tools.evaluation_results_class:Counted Episodes = 2555
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.3172607421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.3172607421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.8295440673828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16278.8056640625
INFO:tools.evaluation_results_class:Current Best Return = 280.3172607421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.71758715236976
INFO:tools.evaluation_results_class:Counted Episodes = 2553
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.786865234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.786865234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.6143035888672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15928.44921875
INFO:tools.evaluation_results_class:Current Best Return = 278.786865234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.49689440993789
INFO:tools.evaluation_results_class:Counted Episodes = 2576
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.4663391113281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.4663391113281
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.9040985107422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16559.23828125
INFO:tools.evaluation_results_class:Current Best Return = 282.4663391113281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.654574132492115
INFO:tools.evaluation_results_class:Counted Episodes = 2536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.64501953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.64501953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.92677307128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17075.69921875
INFO:tools.evaluation_results_class:Current Best Return = 283.64501953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.78367670364501
INFO:tools.evaluation_results_class:Counted Episodes = 2524
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.51727294921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.51727294921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.61961364746094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17224.111328125
INFO:tools.evaluation_results_class:Current Best Return = 284.51727294921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.92371871275328
INFO:tools.evaluation_results_class:Counted Episodes = 2517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.89373779296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.89373779296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.9141845703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17439.755859375
INFO:tools.evaluation_results_class:Current Best Return = 283.89373779296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.95876288659794
INFO:tools.evaluation_results_class:Counted Episodes = 2522
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.3018798828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.3018798828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.40147399902344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15686.10546875
INFO:tools.evaluation_results_class:Current Best Return = 277.3018798828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.75757575757576
INFO:tools.evaluation_results_class:Counted Episodes = 2574
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.5605773925781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.5605773925781
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.43936157226562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16394.53125
INFO:tools.evaluation_results_class:Current Best Return = 280.5605773925781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.161113288906314
INFO:tools.evaluation_results_class:Counted Episodes = 2551
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.6896057128906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.6896057128906
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.14671325683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16487.6015625
INFO:tools.evaluation_results_class:Current Best Return = 277.6896057128906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.5562038117464
INFO:tools.evaluation_results_class:Counted Episodes = 2571
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.4510192871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.4510192871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.62782287597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16566.146484375
INFO:tools.evaluation_results_class:Current Best Return = 280.4510192871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.72317262830482
INFO:tools.evaluation_results_class:Counted Episodes = 2572
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.4142150878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.4142150878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.00624084472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16802.541015625
INFO:tools.evaluation_results_class:Current Best Return = 278.4142150878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.50776397515528
INFO:tools.evaluation_results_class:Counted Episodes = 2576
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.8829040527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.8829040527344
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.84085083007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16468.193359375
INFO:tools.evaluation_results_class:Current Best Return = 279.8829040527344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.94535519125683
INFO:tools.evaluation_results_class:Counted Episodes = 2562
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.88800048828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.88800048828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.86004638671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17625.9296875
INFO:tools.evaluation_results_class:Current Best Return = 284.88800048828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.18035363457761
INFO:tools.evaluation_results_class:Counted Episodes = 2545
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 285.0888366699219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 286.0888366699219
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.890869140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17528.23046875
INFO:tools.evaluation_results_class:Current Best Return = 285.0888366699219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.545203316225816
INFO:tools.evaluation_results_class:Counted Episodes = 2533
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.4079284667969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.4079284667969
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.62806701660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16615.919921875
INFO:tools.evaluation_results_class:Current Best Return = 283.4079284667969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.59207920792079
INFO:tools.evaluation_results_class:Counted Episodes = 2525
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.95263671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.95263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.93582153320312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17173.162109375
INFO:tools.evaluation_results_class:Current Best Return = 284.95263671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.493291239147595
INFO:tools.evaluation_results_class:Counted Episodes = 2534
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.7509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.7509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.9259033203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17297.779296875
INFO:tools.evaluation_results_class:Current Best Return = 283.7509765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.52091554853986
INFO:tools.evaluation_results_class:Counted Episodes = 2534
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.4263610839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.4263610839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.77499389648438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17139.30078125
INFO:tools.evaluation_results_class:Current Best Return = 283.4263610839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.5193982581156
INFO:tools.evaluation_results_class:Counted Episodes = 2526
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.9556579589844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.9556579589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.7616729736328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16582.337890625
INFO:tools.evaluation_results_class:Current Best Return = 280.9556579589844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.171899529042385
INFO:tools.evaluation_results_class:Counted Episodes = 2548
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.9432373046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.9432373046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.63375854492188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15496.2490234375
INFO:tools.evaluation_results_class:Current Best Return = 277.9432373046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.77025440313111
INFO:tools.evaluation_results_class:Counted Episodes = 2555
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.8638916015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.8638916015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.01498413085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15966.5908203125
INFO:tools.evaluation_results_class:Current Best Return = 280.8638916015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.261538461538464
INFO:tools.evaluation_results_class:Counted Episodes = 2535
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.9921569824219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.9921569824219
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.2491912841797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15808.140625
INFO:tools.evaluation_results_class:Current Best Return = 279.9921569824219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.893150684931506
INFO:tools.evaluation_results_class:Counted Episodes = 2555
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.4566345214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.4566345214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.73892211914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15907.5078125
INFO:tools.evaluation_results_class:Current Best Return = 275.4566345214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.3606936416185
INFO:tools.evaluation_results_class:Counted Episodes = 2595
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.9262390136719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.9262390136719
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.17735290527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15598.5576171875
INFO:tools.evaluation_results_class:Current Best Return = 274.9262390136719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.61645962732919
INFO:tools.evaluation_results_class:Counted Episodes = 2576
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.111328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.111328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.40328979492188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15738.2216796875
INFO:tools.evaluation_results_class:Current Best Return = 276.111328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.38886741399304
INFO:tools.evaluation_results_class:Counted Episodes = 2587
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.7431945800781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.7431945800781
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.7057647705078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15986.8408203125
INFO:tools.evaluation_results_class:Current Best Return = 275.7431945800781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.78054474708171
INFO:tools.evaluation_results_class:Counted Episodes = 2570
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.49420166015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.49420166015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.5940399169922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15597.5947265625
INFO:tools.evaluation_results_class:Current Best Return = 277.49420166015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.39258114374034
INFO:tools.evaluation_results_class:Counted Episodes = 2588
INFO:robust_rl.robust_rl_trainer:Iteration 37 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 290.97308349609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 291.97308349609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.46949768066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17420.62109375
INFO:tools.evaluation_results_class:Current Best Return = 510.28057861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.7384739279243
INFO:tools.evaluation_results_class:Counted Episodes = 4967
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.290237426757812
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 13.118309020996094
INFO:agents.father_agent:Step: 10, Training loss: 16.25692367553711
INFO:agents.father_agent:Step: 15, Training loss: 16.04026222229004
INFO:agents.father_agent:Step: 20, Training loss: 14.799178123474121
INFO:agents.father_agent:Step: 25, Training loss: 14.315418243408203

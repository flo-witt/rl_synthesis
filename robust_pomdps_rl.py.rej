diff a/robust_pomdps_rl.py b/robust_pomdps_rl.py	(rejected hunks)
@@ -1,5 +1,7 @@
 import argparse
 
+import stormpy
+
 # using Paynt for POMDP sketches
 
 import paynt.cli
@@ -35,13 +37,17 @@
 import numpy as np
 
 import logging
+
+from tools.evaluators import evaluate_policy_in_model
+
 logger = logging.getLogger(__name__)
 
 class RobustTrainer:
     def __init__(self, args: ArgsEmulator, use_one_hot_memory=False, latent_dim=2,
                  pomdp_sketch=None,
                  obs_evaluator=None, quotient_state_valuations=None,
-                 family_quotient_numpy: FamilyQuotientNumpy = None):
+                 family_quotient_numpy: FamilyQuotientNumpy = None,
+                 autlearn_extraction = True):
         self.args = args
         self.use_one_hot_memory = use_one_hot_memory
         self.model_name = args.model_name
@@ -49,6 +55,7 @@
         self.obs_evaluator = obs_evaluator
         self.quotient_state_valuations = quotient_state_valuations
         self.family_quotient_numpy = family_quotient_numpy
+        self.autlearn_extraction = autlearn_extraction
         self.direct_extractor = self.init_extractor(latent_dim)
         self.benchmark_stats = self.BenchmarkStats(
             fsc_size=3**latent_dim, num_training_steps_per_iteration=301)
@@ -91,7 +98,9 @@
         direct_extractor = SelfInterpretableExtractor(memory_len=latent_dim, is_one_hot=self.use_one_hot_memory,
                                                       use_residual_connection=True, training_epochs=20001,
                                                       num_data_steps=4001, get_best_policy_flag=False, model_name=self.model_name,
-                                                      max_episode_len=self.args.max_steps, family_quotient_numpy=self.family_quotient_numpy)
+                                                      max_episode_len=self.args.max_steps,
+                                                      family_quotient_numpy=self.family_quotient_numpy,
+                                                      autlearn_extraction=self.autlearn_extraction)
         return direct_extractor
 
     def extract_fsc(self, agent: Recurrent_PPO_agent, environment, quotient, num_data_steps=4001, training_epochs=10001) -> paynt.quotient.fsc.FSC:
@@ -260,7 +269,7 @@
         evaluations.append(one_by_one.evaluate(assignment, keep_value_only=True))
     return evaluations, hole_assignments_to_test
 
-def initialize_extractor(pomdp_sketch, args_emulated, family_quotient_numpy):
+def initialize_extractor(pomdp_sketch, args_emulated, family_quotient_numpy,autlearn_extraction):
     quotient_sv = pomdp_sketch.quotient_mdp.state_valuations
     quotient_obs = pomdp_sketch.obs_evaluator
 
@@ -269,15 +278,28 @@
     # Currently latent_dim is hardcoded to 2 (3 ** i => 9-FSC)
     # If use one-hot memory, then the size of FSC is equal to the latent_dim.
     extractor = RobustTrainer(args_emulated, use_one_hot_memory=True, latent_dim=64, quotient_state_valuations=quotient_sv,
-                              obs_evaluator=quotient_obs, pomdp_sketch=pomdp_sketch, family_quotient_numpy=family_quotient_numpy)
+                              obs_evaluator=quotient_obs, pomdp_sketch=pomdp_sketch, family_quotient_numpy=family_quotient_numpy,
+                              autlearn_extraction = autlearn_extraction)
     
     return extractor
 
 
+from paynt.parser.prism_parser import PrismParser
+from paynt.verification.property import construct_property, Property, OptimalityProperty
+def construct_reachability_spec(sketch_path):
+    prism, _ = PrismParser.load_sketch_prism(sketch_path)
+
+    prop = construct_property(stormpy.parse_properties_for_prism_program(f"Pmin=? [F goal]", prism)[0], 0)
+
+    spec = paynt.verification.property.Specification([prop])
+
+    return spec
 
 def main():
     args_cmd = parse_args()
-
+    num_samples_learn = 40000
+    nr_pomdps = 10
+    autlearn_extraction = True
     paynt.utils.timer.GlobalTimer.start()
 
     profiling = True
@@ -301,30 +323,38 @@
     # Masked_training is used to train the agent with masking, i.e. the agent will be forbidden to take illegal actions.
     args_emulated = init_args(
         prism_path=prism_path, properties_path=properties_path, batched_vec_storm=True, masked_training=False)
+    args_emulated.max_steps = 100
     args_emulated.model_name = project_path.split("/")[-1]
     # pomdp = initialize_prism_model(prism_path, properties_path, constants="")
 
     hole_assignment = pomdp_sketch.family.pick_any()
     pomdp, _, _ = assignment_to_pomdp(pomdp_sketch, hole_assignment)
 
-    extractor = initialize_extractor(pomdp_sketch, args_emulated, family_quotient_numpy)
+    extractor = initialize_extractor(pomdp_sketch, args_emulated, family_quotient_numpy,autlearn_extraction)
 
     agent = extractor.generate_agent(pomdp, args_emulated)
+    last_hole = None
+    for i in range(nr_pomdps):
+        hole_assignment = pomdp_sketch.family.pick_random()
 
-    hole_assignment = pomdp_sketch.family.pick_any()
-    pomdp, _, _ = assignment_to_pomdp(pomdp_sketch, hole_assignment)
+        pomdp, _, _ = assignment_to_pomdp(pomdp_sketch, hole_assignment)
+        extractor.add_new_pomdp(pomdp,agent)
+        last_hole = hole_assignment
+
 
     # Using None for the POMDP here means that the extractor will use the existing environment from initialization.
-    extractor.train_on_new_pomdp(pomdp, agent, nr_iterations=1001)
+    # extractor.train_on_new_pomdp(pomdp, agent, nr_iterations=201)
+    extractor.train_on_new_pomdp(None, agent, nr_iterations=501)
 
     # -------------------------------------------------------------------------
     # Different extraction method should be used here!!!
     fsc = extractor.extract_fsc(
-        agent, agent.environment, training_epochs=20001, quotient=pomdp_sketch, num_data_steps=8001)
+        agent, agent.environment, training_epochs=20001, quotient=pomdp_sketch, num_data_steps=num_samples_learn)
     # -------------------------------------------------------------------------
     dtmc_sketch = pomdp_sketch.build_dtmc_sketch(
         fsc, negate_specification=True)
 
+
     synthesizer = paynt.synthesizer.synthesizer_ar.SynthesizerAR(dtmc_sketch)
     synthesizer.synthesize(keep_optimum=True)
     print("Best value", synthesizer.best_assignment_value)
@@ -335,25 +365,39 @@
     extractor.save_stats(json_path)
 
     for i in range(50):
-        pomdp, _, _ = assignment_to_pomdp(pomdp_sketch, hole_assignment)
 
         extractor.train_on_new_pomdp(pomdp, agent, nr_iterations=501)
+        # extractor.train_on_new_pomdp(None, agent, nr_iterations=201)
         # -------------------------------------------------------------------------
         # Different extraction method should be used here!!!
         fsc = extractor.extract_fsc(
-            agent, agent.environment, quotient=pomdp_sketch, num_data_steps=3001, training_epochs=10001)
+            agent, agent.environment, quotient=pomdp_sketch, num_data_steps=num_samples_learn, training_epochs=10001)
         # -------------------------------------------------------------------------
 
         dtmc_sketch = pomdp_sketch.build_dtmc_sketch(
             fsc, negate_specification=True)
         synthesizer = paynt.synthesizer.synthesizer_ar.SynthesizerAR(
             dtmc_sketch)
+
         hole_assignment = synthesizer.synthesize(keep_optimum=True)
-        print("Best value", synthesizer.best_assignment_value)
+
+        one_by_one = paynt.synthesizer.synthesizer_onebyone.SynthesizerOneByOne(dtmc_sketch)
+        worst_case_eval = one_by_one.evaluate(last_hole, keep_value_only=True)
+
+        print(f"HOLES:{hole_assignment}")
+        print("Best value: ", synthesizer.best_assignment_value)
+        print("Worst-case eval last hole:", worst_case_eval)
+        if hole_assignment is None:
+            continue
         extractor.benchmark_stats.add_family_performance(
             synthesizer.best_assignment_value)
         extractor.save_stats(json_path)
 
+        pomdp, _, _ = assignment_to_pomdp(pomdp_sketch, hole_assignment)
+        last_hole = hole_assignment
+
+        extractor.add_new_pomdp(pomdp,agent)
+
 
     if profiling:
         pr.disable()

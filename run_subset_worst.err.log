nohup: ignoring input
2025-08-19 12:53:37.526703: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-19 12:53:37.528435: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-19 12:53:37.558089: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-19 12:53:37.558134: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-19 12:53:37.559229: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-19 12:53:37.564712: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-19 12:53:37.564889: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-19 12:53:38.121972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/avoid/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/avoid/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"penalty"}min=? [F ((x = (5 - 1)) & (y = (5 - 1)))] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
DEBUG:paynt.parser.jani:keeping 65660/171860 choices with non-conflicting hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 17567 states and 61860 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"rewardmodel_penalty"}min=? [F ((x = (5 - 1)) & (y = (5 - 1)))] 
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -778.6832275390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -533.0281982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = -207.19847106933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6235489220563848
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 355140.25
INFO:tools.evaluation_results_class:Current Best Return = -778.6832275390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6235489220563848
INFO:tools.evaluation_results_class:Average Episode Length = 397.922056384743
INFO:tools.evaluation_results_class:Counted Episodes = 603
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
icy
./runner_worst_case_evaluation.sh: line 9: 2906488 Killed                  timeout 7200 python3 robust_pomdps_rl.py --project-path "$models_dir/$model" --batched-vec-storm --geometric-batched-vec-storm
2025-08-19 12:54:08.359301: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-19 12:54:08.361045: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-19 12:54:08.390788: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-19 12:54:08.390839: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-19 12:54:08.391951: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-19 12:54:08.397355: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-19 12:54:08.397549: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-19 12:54:08.965569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/avoid/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/avoid/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"penalty"}min=? [F ((x = (5 - 1)) & (y = (5 - 1)))] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
DEBUG:paynt.parser.jani:keeping 65660/171860 choices with non-conflicting hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 17567 states and 61860 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"rewardmodel_penalty"}min=? [F ((x = (5 - 1)) & (y = (5 - 1)))] 
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               INFO:tools.evaluation_results_class:Average Return = -115.84734344482422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.15264892578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.4297637939453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17314.8671875
INFO:tools.evaluation_results_class:Current Best Return = -115.84734344482422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.59828473413379
INFO:tools.evaluation_results_class:Counted Episodes = 2915
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -101.81230926513672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 298.18768310546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.76846313476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28271.505859375
INFO:tools.evaluation_results_class:Current Best Return = -101.81230926513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.67514609831557
INFO:tools.evaluation_results_class:Counted Episodes = 2909
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -123.33748626708984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.6625061035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.92666625976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15931.2783203125
INFO:tools.evaluation_results_class:Current Best Return = -123.33748626708984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.87413554633471
INFO:tools.evaluation_results_class:Counted Episodes = 2892
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.31687927246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.68312072753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.1492919921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24971.3828125
INFO:tools.evaluation_results_class:Current Best Return = -156.31687927246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.13750850918993
INFO:tools.evaluation_results_class:Counted Episodes = 2938
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -229.52845764160156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 170.47154235839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 99.70956420898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33946.46875
INFO:tools.evaluation_results_class:Current Best Return = -229.52845764160156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.2088827203331
INFO:tools.evaluation_results_class:Counted Episodes = 2882
INFO:robust_rl.robust_rl_trainer:Iteration 2 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.89959716796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.10040283203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.8810272216797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25684.5390625
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.04043126684636
INFO:tools.evaluation_results_class:Counted Episodes = 5936
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.889460563659668
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 13.007357597351074
INFO:agents.father_agent:Step: 10, Training loss: 12.281344413757324
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -146.8282470703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.1717529296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.45672607421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24103.95703125
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.836811289596156
INFO:tools.evaluation_results_class:Counted Episodes = 6661
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -171.8268280029297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 228.1731719970703
INFO:tools.evaluation_results_class:Average Discounted Reward = 154.6139373779297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31317.177734375
INFO:tools.evaluation_results_class:Current Best Return = -171.8268280029297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.99052671466465
INFO:tools.evaluation_results_class:Counted Episodes = 5278
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -149.8064727783203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.1935272216797
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.52931213378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25179.033203125
INFO:tools.evaluation_results_class:Current Best Return = -149.8064727783203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.107797823169825
INFO:tools.evaluation_results_class:Counted Episodes = 6707
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 217.21056370473903
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 3 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.4939727783203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.5060272216797
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.01576232910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24420.205078125
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.88380493678507
INFO:tools.evaluation_results_class:Counted Episodes = 6644
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.230365753173828
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 13.735182762145996
INFO:agents.father_agent:Step: 10, Training loss: 13.573777198791504
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -153.66787719726562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.33212280273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.34185791015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23627.83984375
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.931643087267126
INFO:tools.evaluation_results_class:Counted Episodes = 7139
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -167.5814208984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 232.4185791015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 163.59225463867188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27986.791015625
INFO:tools.evaluation_results_class:Current Best Return = -167.5814208984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.89663740268345
INFO:tools.evaluation_results_class:Counted Episodes = 6037
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -150.7337188720703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.2662811279297
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.2519989013672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22398.8671875
INFO:tools.evaluation_results_class:Current Best Return = -150.7337188720703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.50477259966311
INFO:tools.evaluation_results_class:Counted Episodes = 7124
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 204.06339591730168
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 4 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.69140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.30859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.0012969970703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24460.4375
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.834521313766594
INFO:tools.evaluation_results_class:Counted Episodes = 7155
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.227932929992676
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 14.709940910339355
INFO:agents.father_agent:Step: 10, Training loss: 13.967360496520996
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -149.91375732421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.08624267578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 188.98915100097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20522.521484375
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.62291525423729
INFO:tools.evaluation_results_class:Counted Episodes = 7375
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -168.86505126953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 231.13494873046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.42147827148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26291.8203125
INFO:tools.evaluation_results_class:Current Best Return = -168.86505126953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.79438968915845
INFO:tools.evaluation_results_class:Counted Episodes = 6595
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.74969482421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.25030517578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.9062957763672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21634.45703125
INFO:tools.evaluation_results_class:Current Best Return = -159.74969482421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.47995073217463
INFO:tools.evaluation_results_class:Counted Episodes = 7307
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 203.04771534206694
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 5 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.94107055664062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.05892944335938
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.3505401611328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23240.103515625
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.67789244975557
INFO:tools.evaluation_results_class:Counted Episodes = 7364
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.757059097290039
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 14.802274703979492
INFO:agents.father_agent:Step: 10, Training loss: 14.80069351196289
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -157.4361572265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.5638427734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.19554138183594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21376.546875
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.11097740339618
INFO:tools.evaluation_results_class:Counted Episodes = 7479
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.22232055664062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.77767944335938
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.80166625976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23133.421875
INFO:tools.evaluation_results_class:Current Best Return = -164.22232055664062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.678648765699435
INFO:tools.evaluation_results_class:Counted Episodes = 6927
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.93959045410156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.06040954589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.11260986328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20673.208984375
INFO:tools.evaluation_results_class:Current Best Return = -156.93959045410156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.22251725969198
INFO:tools.evaluation_results_class:Counted Episodes = 7532
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 194.1931691987319
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 6 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -161.03501892089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.96498107910156
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.1857147216797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21982.837890625
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.093557872226675
INFO:tools.evaluation_results_class:Counted Episodes = 7482
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.69527530670166
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 13.662940979003906
INFO:agents.father_agent:Step: 10, Training loss: 13.317527770996094
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -157.67837524414062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.32162475585938
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.7466583251953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20515.52734375
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.69346933368658
INFO:tools.evaluation_results_class:Counted Episodes = 7549
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.3028106689453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.6971893310547
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.80438232421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22646.58984375
INFO:tools.evaluation_results_class:Current Best Return = -164.3028106689453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.44039270687237
INFO:tools.evaluation_results_class:Counted Episodes = 7130
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.173828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.826171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.6586151123047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20984.404296875
INFO:tools.evaluation_results_class:Current Best Return = -165.173828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.6940005239717
INFO:tools.evaluation_results_class:Counted Episodes = 7634
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 200.31972051588338
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -101.34492492675781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 298.6550598144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.1804656982422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13194.525390625
INFO:tools.evaluation_results_class:Current Best Return = -101.34492492675781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.98416737348035
INFO:tools.evaluation_results_class:Counted Episodes = 3537
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -97.45610809326172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 302.54388427734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.1370391845703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12931.142578125
INFO:tools.evaluation_results_class:Current Best Return = -97.45610809326172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.54103517300543
INFO:tools.evaluation_results_class:Counted Episodes = 3497
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -98.67704772949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 301.32293701171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.90847778320312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20310.02734375
INFO:tools.evaluation_results_class:Current Best Return = -98.67704772949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.47785081451843
INFO:tools.evaluation_results_class:Counted Episodes = 3499
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -99.3832778930664
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.6167297363281
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.92213439941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10739.921875
INFO:tools.evaluation_results_class:Current Best Return = -99.3832778930664
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.31845322718226
INFO:tools.evaluation_results_class:Counted Episodes = 3517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.7928009033203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.20721435546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.4325714111328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20220.365234375
INFO:tools.evaluation_results_class:Current Best Return = -138.7928009033203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.214285714285715
INFO:tools.evaluation_results_class:Counted Episodes = 3528
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -198.7567596435547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 201.2432403564453
INFO:tools.evaluation_results_class:Average Discounted Reward = 137.5607147216797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25155.767578125
INFO:tools.evaluation_results_class:Current Best Return = -198.7567596435547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.996617812852314
INFO:tools.evaluation_results_class:Counted Episodes = 3548
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -201.54415893554688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 198.45584106445312
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.84829711914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24815.904296875
INFO:tools.evaluation_results_class:Current Best Return = -201.54415893554688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.28034188034188
INFO:tools.evaluation_results_class:Counted Episodes = 3510
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -201.65618896484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 198.34381103515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.3332061767578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25488.849609375
INFO:tools.evaluation_results_class:Current Best Return = -201.65618896484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.5052872249214
INFO:tools.evaluation_results_class:Counted Episodes = 3499
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -203.03628540039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 196.96371459960938
INFO:tools.evaluation_results_class:Average Discounted Reward = 133.45127868652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27562.34375
INFO:tools.evaluation_results_class:Current Best Return = -203.03628540039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.27210884353742
INFO:tools.evaluation_results_class:Counted Episodes = 3528
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -200.7781219482422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 199.2218780517578
INFO:tools.evaluation_results_class:Average Discounted Reward = 135.45895385742188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25746.34375
INFO:tools.evaluation_results_class:Current Best Return = -200.7781219482422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.18475488807027
INFO:tools.evaluation_results_class:Counted Episodes = 3529
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -203.2251434326172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 196.7748565673828
INFO:tools.evaluation_results_class:Average Discounted Reward = 133.30325317382812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25318.1484375
INFO:tools.evaluation_results_class:Current Best Return = -203.2251434326172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.0940243557066
INFO:tools.evaluation_results_class:Counted Episodes = 3531
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -97.74871063232422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 302.25128173828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.16966247558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12235.6904296875
INFO:tools.evaluation_results_class:Current Best Return = -97.74871063232422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.433034501494156
INFO:tools.evaluation_results_class:Counted Episodes = 3681
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -91.90721893310547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 308.0927734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 247.8598175048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12386.4189453125
INFO:tools.evaluation_results_class:Current Best Return = -91.90721893310547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.214768731403844
INFO:tools.evaluation_results_class:Counted Episodes = 3697
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -95.37993621826172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 304.62005615234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.25843811035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18604.765625
INFO:tools.evaluation_results_class:Current Best Return = -95.37993621826172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.75026680896478
INFO:tools.evaluation_results_class:Counted Episodes = 3748
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -91.42227172851562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 308.5777282714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.97592163085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10239.236328125
INFO:tools.evaluation_results_class:Current Best Return = -91.42227172851562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.112963959117806
INFO:tools.evaluation_results_class:Counted Episodes = 3718
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.30372619628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.6962585449219
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.01136779785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19755.80078125
INFO:tools.evaluation_results_class:Current Best Return = -138.30372619628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.95760665414542
INFO:tools.evaluation_results_class:Counted Episodes = 3727
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -201.7960662841797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 198.2039337158203
INFO:tools.evaluation_results_class:Average Discounted Reward = 137.01573181152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23265.583984375
INFO:tools.evaluation_results_class:Current Best Return = -201.7960662841797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.575113001861205
INFO:tools.evaluation_results_class:Counted Episodes = 3761
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -202.30404663085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 197.69595336914062
INFO:tools.evaluation_results_class:Average Discounted Reward = 136.6927032470703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22825.75
INFO:tools.evaluation_results_class:Current Best Return = -202.30404663085938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.4321249007674
INFO:tools.evaluation_results_class:Counted Episodes = 3779
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -199.57518005371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 200.42481994628906
INFO:tools.evaluation_results_class:Average Discounted Reward = 139.13916015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21565.099609375
INFO:tools.evaluation_results_class:Current Best Return = -199.57518005371094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.51047467515248
INFO:tools.evaluation_results_class:Counted Episodes = 3771
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -199.12454223632812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 200.87545776367188
INFO:tools.evaluation_results_class:Average Discounted Reward = 139.57577514648438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23342.01171875
INFO:tools.evaluation_results_class:Current Best Return = -199.12454223632812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.48598625066103
INFO:tools.evaluation_results_class:Counted Episodes = 3782
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -200.2711639404297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 199.7288360595703
INFO:tools.evaluation_results_class:Average Discounted Reward = 138.3019256591797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21187.060546875
INFO:tools.evaluation_results_class:Current Best Return = -200.2711639404297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.5579729371186
INFO:tools.evaluation_results_class:Counted Episodes = 3769
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -201.25233459472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 198.74766540527344
INFO:tools.evaluation_results_class:Average Discounted Reward = 137.24583435058594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22917.46484375
INFO:tools.evaluation_results_class:Current Best Return = -201.25233459472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.71169730881961
INFO:tools.evaluation_results_class:Counted Episodes = 3753
INFO:robust_rl.robust_rl_trainer:Iteration 7 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -161.81704711914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.18295288085938
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.8277587890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21757.3046875
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.67508261731659
INFO:tools.evaluation_results_class:Counted Episodes = 7565
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.005278587341309
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 13.326902389526367
INFO:agents.father_agent:Step: 10, Training loss: 11.907489776611328
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -157.63478088378906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.36521911621094
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.68592834472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20672.6875
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.53886693017128
INFO:tools.evaluation_results_class:Counted Episodes = 7590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -167.24224853515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 232.75775146484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.4558563232422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22745.388671875
INFO:tools.evaluation_results_class:Current Best Return = -167.24224853515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.905869324473976
INFO:tools.evaluation_results_class:Counted Episodes = 7224
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.93955993652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.06044006347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.80348205566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22386.287109375
INFO:tools.evaluation_results_class:Current Best Return = -160.93955993652344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.73203985317252
INFO:tools.evaluation_results_class:Counted Episodes = 7628
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 189.49246396195622
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 8 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -168.3472900390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 231.6527099609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.7408905029297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22983.31640625
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.61743725231176
INFO:tools.evaluation_results_class:Counted Episodes = 7570
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.600381851196289
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 11.970558166503906
INFO:agents.father_agent:Step: 10, Training loss: 12.384596824645996
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -160.2919921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.7080078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.35128784179688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20412.5546875
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.31030873888017
INFO:tools.evaluation_results_class:Counted Episodes = 7644
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.11050415039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.88949584960938
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.6382598876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23244.337890625
INFO:tools.evaluation_results_class:Current Best Return = -166.11050415039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.342974079126876
INFO:tools.evaluation_results_class:Counted Episodes = 7330
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -167.15655517578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 232.84344482421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.24464416503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21128.81640625
INFO:tools.evaluation_results_class:Current Best Return = -167.15655517578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.650536228093124
INFO:tools.evaluation_results_class:Counted Episodes = 7646
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 193.6892439224546
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 9 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.40455627441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.59544372558594
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.41110229492188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21859.177734375
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.196388852544814
INFO:tools.evaluation_results_class:Counted Episodes = 7643
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.332730293273926
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 10.769418716430664
INFO:agents.father_agent:Step: 10, Training loss: 10.682372093200684
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -156.21559143066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.78440856933594
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.4296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18661.119140625
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.361467889908255
INFO:tools.evaluation_results_class:Counted Episodes = 7630
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.0614776611328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.9385223388672
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.8112335205078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19386.619140625
INFO:tools.evaluation_results_class:Current Best Return = -162.0614776611328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.23351013191895
INFO:tools.evaluation_results_class:Counted Episodes = 7353
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.25405883789062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.74594116210938
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.72885131835938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19831.318359375
INFO:tools.evaluation_results_class:Current Best Return = -159.25405883789062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.368625924484235
INFO:tools.evaluation_results_class:Counted Episodes = 7707
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 176.78861019434314
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 10 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.27395629882812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.72604370117188
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.70652770996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19619.564453125
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.23970184386034
INFO:tools.evaluation_results_class:Counted Episodes = 7647
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.740506172180176
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 10.41070556640625
INFO:agents.father_agent:Step: 10, Training loss: 10.44365119934082
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -153.74366760253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.25633239746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 184.8997039794922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17678.611328125
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.233220161922176
INFO:tools.evaluation_results_class:Counted Episodes = 7658
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.29830932617188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.70169067382812
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.7654571533203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19521.111328125
INFO:tools.evaluation_results_class:Current Best Return = -160.29830932617188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.91424713031736
INFO:tools.evaluation_results_class:Counted Episodes = 7405
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -163.61294555664062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.38705444335938
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.23117065429688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18100.609375
INFO:tools.evaluation_results_class:Current Best Return = -163.61294555664062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.94276671502044
INFO:tools.evaluation_results_class:Counted Episodes = 7583
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 184.20013288626726
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 11 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.45428466796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.54571533203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.7145538330078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18316.2890625
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.40102321920504
INFO:tools.evaluation_results_class:Counted Episodes = 7623
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.113367080688477
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 9.744220733642578
INFO:agents.father_agent:Step: 10, Training loss: 9.865250587463379
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -158.9973602294922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.0026397705078
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.0251922607422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20628.1171875
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.45559947299078
INFO:tools.evaluation_results_class:Counted Episodes = 7590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.92422485351562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.07577514648438
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.0884552001953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20517.90625
INFO:tools.evaluation_results_class:Current Best Return = -160.92422485351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.25310072236609
INFO:tools.evaluation_results_class:Counted Episodes = 7337
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.5146026611328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.4853973388672
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.076416015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19879.234375
INFO:tools.evaluation_results_class:Current Best Return = -157.5146026611328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.675611991098314
INFO:tools.evaluation_results_class:Counted Episodes = 7639
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 173.97467572009043
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -94.62670135498047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 305.373291015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.02685546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11191.5302734375
INFO:tools.evaluation_results_class:Current Best Return = -94.62670135498047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.381626422425754
INFO:tools.evaluation_results_class:Counted Episodes = 3603
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.13819122314453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 315.86181640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 250.7469024658203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10960.970703125
INFO:tools.evaluation_results_class:Current Best Return = -84.13819122314453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.28279883381924
INFO:tools.evaluation_results_class:Counted Episodes = 3430
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.33575439453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 326.66424560546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 259.7179870605469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15900.0078125
INFO:tools.evaluation_results_class:Current Best Return = -73.33575439453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.14518475414606
INFO:tools.evaluation_results_class:Counted Episodes = 3437
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -94.29467010498047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 305.705322265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.5113525390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10430.4775390625
INFO:tools.evaluation_results_class:Current Best Return = -94.29467010498047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.25638179800222
INFO:tools.evaluation_results_class:Counted Episodes = 3604
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -128.0917205810547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.90826416015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.29275512695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17518.736328125
INFO:tools.evaluation_results_class:Current Best Return = -128.0917205810547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.914912027689645
INFO:tools.evaluation_results_class:Counted Episodes = 3467
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -174.05906677246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 225.94093322753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 162.48155212402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20691.302734375
INFO:tools.evaluation_results_class:Current Best Return = -174.05906677246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.73448181569592
INFO:tools.evaluation_results_class:Counted Episodes = 3657
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -174.049560546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 225.950439453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 162.46568298339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19861.13671875
INFO:tools.evaluation_results_class:Current Best Return = -174.049560546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.04405286343612
INFO:tools.evaluation_results_class:Counted Episodes = 3632
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -176.12493896484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 223.87506103515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 160.97235107421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22179.962890625
INFO:tools.evaluation_results_class:Current Best Return = -176.12493896484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.75669764898852
INFO:tools.evaluation_results_class:Counted Episodes = 3658
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -176.1997833251953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 223.8002166748047
INFO:tools.evaluation_results_class:Average Discounted Reward = 160.48220825195312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19862.564453125
INFO:tools.evaluation_results_class:Current Best Return = -176.1997833251953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.92864983534577
INFO:tools.evaluation_results_class:Counted Episodes = 3644
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -181.7942657470703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 218.2057342529297
INFO:tools.evaluation_results_class:Average Discounted Reward = 155.1562957763672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22853.642578125
INFO:tools.evaluation_results_class:Current Best Return = -181.7942657470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.99752134398237
INFO:tools.evaluation_results_class:Counted Episodes = 3631
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -179.2178497314453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 220.7821502685547
INFO:tools.evaluation_results_class:Average Discounted Reward = 157.7964324951172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21892.21484375
INFO:tools.evaluation_results_class:Current Best Return = -179.2178497314453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.70925470925471
INFO:tools.evaluation_results_class:Counted Episodes = 3663
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -180.20872497558594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 219.79127502441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 157.21920776367188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21383.802734375
INFO:tools.evaluation_results_class:Current Best Return = -180.20872497558594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.871091607240814
INFO:tools.evaluation_results_class:Counted Episodes = 3646
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -180.98069763183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 219.01930236816406
INFO:tools.evaluation_results_class:Average Discounted Reward = 156.66360473632812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21796.87109375
INFO:tools.evaluation_results_class:Current Best Return = -180.98069763183594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.58113617830932
INFO:tools.evaluation_results_class:Counted Episodes = 3679
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -174.9384765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 225.0615234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 162.28164672851562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20898.236328125
INFO:tools.evaluation_results_class:Current Best Return = -174.9384765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.64260322668854
INFO:tools.evaluation_results_class:Counted Episodes = 3657
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -180.51441955566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 219.48558044433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 156.67347717285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21787.451171875
INFO:tools.evaluation_results_class:Current Best Return = -180.51441955566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.94314748695413
INFO:tools.evaluation_results_class:Counted Episodes = 3641
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -177.8125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 222.1875
INFO:tools.evaluation_results_class:Average Discounted Reward = 159.82656860351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20334.087890625
INFO:tools.evaluation_results_class:Current Best Return = -177.8125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.349567099567096
INFO:tools.evaluation_results_class:Counted Episodes = 3696
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -92.5105972290039
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 307.4894104003906
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.52557373046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11297.3505859375
INFO:tools.evaluation_results_class:Current Best Return = -92.5105972290039
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.51669316375199
INFO:tools.evaluation_results_class:Counted Episodes = 3774
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.2933578491211
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 320.7066345214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 258.8855895996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10360.1357421875
INFO:tools.evaluation_results_class:Current Best Return = -79.2933578491211
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.225823878984336
INFO:tools.evaluation_results_class:Counted Episodes = 3702
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.18048858642578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 319.81951904296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 257.7550964355469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15657.748046875
INFO:tools.evaluation_results_class:Current Best Return = -80.18048858642578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.3869918699187
INFO:tools.evaluation_results_class:Counted Episodes = 3690
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -90.9100112915039
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 309.0899963378906
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.44313049316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10056.12109375
INFO:tools.evaluation_results_class:Current Best Return = -90.9100112915039
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.63365282215123
INFO:tools.evaluation_results_class:Counted Episodes = 3756
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -130.26409912109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.73590087890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.37103271484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18005.189453125
INFO:tools.evaluation_results_class:Current Best Return = -130.26409912109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.353579175704986
INFO:tools.evaluation_results_class:Counted Episodes = 3688
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -172.47584533691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 227.52415466308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.11947631835938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21573.126953125
INFO:tools.evaluation_results_class:Current Best Return = -172.47584533691406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.096638655462186
INFO:tools.evaluation_results_class:Counted Episodes = 3808
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -172.4434814453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 227.5565185546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.24392700195312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21190.509765625
INFO:tools.evaluation_results_class:Current Best Return = -172.4434814453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.23133543638276
INFO:tools.evaluation_results_class:Counted Episodes = 3804
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -175.959228515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 224.040771484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 162.6121826171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21121.501953125
INFO:tools.evaluation_results_class:Current Best Return = -175.959228515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.27617043661231
INFO:tools.evaluation_results_class:Counted Episodes = 3802
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.50209045410156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 226.49790954589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 165.38829040527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20628.2890625
INFO:tools.evaluation_results_class:Current Best Return = -173.50209045410156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.08337703198742
INFO:tools.evaluation_results_class:Counted Episodes = 3814
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.96495056152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 226.03504943847656
INFO:tools.evaluation_results_class:Average Discounted Reward = 164.4351043701172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21578.435546875
INFO:tools.evaluation_results_class:Current Best Return = -173.96495056152344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.35230566534914
INFO:tools.evaluation_results_class:Counted Episodes = 3795
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.39588928222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 221.60411071777344
INFO:tools.evaluation_results_class:Average Discounted Reward = 160.4282684326172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20645.49609375
INFO:tools.evaluation_results_class:Current Best Return = -178.39588928222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.12871808370624
INFO:tools.evaluation_results_class:Counted Episodes = 3799
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.50787353515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 226.49212646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 165.24044799804688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21656.470703125
INFO:tools.evaluation_results_class:Current Best Return = -173.50787353515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.128151260504204
INFO:tools.evaluation_results_class:Counted Episodes = 3808
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.31192016601562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 226.68807983398438
INFO:tools.evaluation_results_class:Average Discounted Reward = 165.0814208984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21023.29296875
INFO:tools.evaluation_results_class:Current Best Return = -173.31192016601562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.422151731430084
INFO:tools.evaluation_results_class:Counted Episodes = 3783
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -174.3570556640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 225.6429443359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 164.7896270751953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21046.921875
INFO:tools.evaluation_results_class:Current Best Return = -174.3570556640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.91341878106199
INFO:tools.evaluation_results_class:Counted Episodes = 3823
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -175.2747802734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 224.7252197265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 163.19174194335938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20059.072265625
INFO:tools.evaluation_results_class:Current Best Return = -175.2747802734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.465818759936404
INFO:tools.evaluation_results_class:Counted Episodes = 3774
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -171.35011291503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 228.64988708496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.0869903564453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19998.748046875
INFO:tools.evaluation_results_class:Current Best Return = -171.35011291503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.30450830477195
INFO:tools.evaluation_results_class:Counted Episodes = 3793
INFO:robust_rl.robust_rl_trainer:Iteration 12 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.10096740722656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.89903259277344
INFO:tools.evaluation_results_class:Average Discounted Reward = 184.73927307128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18976.30078125
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.18650274653414
INFO:tools.evaluation_results_class:Counted Episodes = 7646
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.593945503234863
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 9.914398193359375
INFO:agents.father_agent:Step: 10, Training loss: 8.996253967285156
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -158.33306884765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.66693115234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.3380889892578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21717.986328125
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.0338531254165
INFO:tools.evaluation_results_class:Counted Episodes = 7503
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.58126831054688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.41873168945312
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.99725341796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21422.76953125
INFO:tools.evaluation_results_class:Current Best Return = -164.58126831054688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.04941013185288
INFO:tools.evaluation_results_class:Counted Episodes = 7205
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.72190856933594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.27809143066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.19619750976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22361.9921875
INFO:tools.evaluation_results_class:Current Best Return = -170.72190856933594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.22741143691124
INFO:tools.evaluation_results_class:Counted Episodes = 7537
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 186.7585261846554
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 13 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.65008544921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.34991455078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.3140869140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19739.05859375
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.091188661585775
INFO:tools.evaluation_results_class:Counted Episodes = 7479
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.494853973388672
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Step: 5, Training loss: 8.07924747467041
INFO:agents.father_agent:Step: 10, Training loss: 9.06635856628418
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -147.54098510742188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.45901489257812
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.54969787597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17971.623046875
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.15426781519186
INFO:tools.evaluation_results_class:Counted Episodes = 7662
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.92825317382812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.07174682617188
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.6392364501953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18462.974609375
INFO:tools.evaluation_results_class:Current Best Return = -155.92825317382812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.664054982817866
INFO:tools.evaluation_results_class:Counted Episodes = 7275
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -152.27159118652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.72840881347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.8245849609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18057.435546875
INFO:tools.evaluation_results_class:Current Best Return = -152.27159118652344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.48756024488733
INFO:tools.evaluation_results_class:Counted Episodes = 7677
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 162.7644359291831
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 14 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -148.26593017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.73406982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.76504516601562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17556.767578125
INFO:tools.evaluation_results_class:Current Best Return = -144.9121551513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.23648737076299
INFO:tools.evaluation_results_class:Counted Episodes = 7641
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.18940544128418
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Step: 5, Training loss: 8.3843994140625
INFO:agents.father_agent:Step: 10, Training loss: 7.383127212524414
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -139.52383422851562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.4761657714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.26455688476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15210.4072265625
INFO:tools.evaluation_results_class:Current Best Return = -139.52383422851562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.59207920792079
INFO:tools.evaluation_results_class:Counted Episodes = 7575
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -151.42564392089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.57435607910156
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.4009552001953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16965.87109375
INFO:tools.evaluation_results_class:Current Best Return = -151.42564392089844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.31139736768412
INFO:tools.evaluation_results_class:Counted Episodes = 7142
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.88726806640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.11273193359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.6253204345703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15605.765625
INFO:tools.evaluation_results_class:Current Best Return = -145.88726806640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.99709570957096
INFO:tools.evaluation_results_class:Counted Episodes = 7575
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 167.28192417429653
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 15 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.59939575195312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.4006042480469
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.58804321289062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16707.833984375
INFO:tools.evaluation_results_class:Current Best Return = -139.52383422851562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.739384288747345
INFO:tools.evaluation_results_class:Counted Episodes = 7536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.117134094238281
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Step: 5, Training loss: 6.891852378845215
INFO:agents.father_agent:Step: 10, Training loss: 6.27970552444458
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -140.13079833984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.86920166015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.06085205078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16109.8154296875
INFO:tools.evaluation_results_class:Current Best Return = -139.52383422851562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.865602129075185
INFO:tools.evaluation_results_class:Counted Episodes = 7515
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -152.4458770751953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.5541229248047
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.99244689941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17432.28515625
INFO:tools.evaluation_results_class:Current Best Return = -152.4458770751953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.520471116096466
INFO:tools.evaluation_results_class:Counted Episodes = 7132
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.01161193847656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.98838806152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.26568603515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15780.064453125
INFO:tools.evaluation_results_class:Current Best Return = -145.01161193847656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.014259308159495
INFO:tools.evaluation_results_class:Counted Episodes = 7574
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 156.41005005943708
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 16 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -137.95492553710938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.0450744628906
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.37884521484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16504.271484375
INFO:tools.evaluation_results_class:Current Best Return = -137.95492553710938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.87262332136684
INFO:tools.evaluation_results_class:Counted Episodes = 7521
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.207854747772217
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Step: 5, Training loss: 5.435484409332275
INFO:agents.father_agent:Step: 10, Training loss: 5.211086750030518
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -137.3301239013672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.6698913574219
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.41029357910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17058.09765625
INFO:tools.evaluation_results_class:Current Best Return = -137.3301239013672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.70012206700122
INFO:tools.evaluation_results_class:Counted Episodes = 7373
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -150.12815856933594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.87184143066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 184.09645080566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18287.416015625
INFO:tools.evaluation_results_class:Current Best Return = -150.12815856933594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.0850640113798
INFO:tools.evaluation_results_class:Counted Episodes = 7030
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -146.76202392578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.23797607421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.64231872558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17992.505859375
INFO:tools.evaluation_results_class:Current Best Return = -146.76202392578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.591670014731484
INFO:tools.evaluation_results_class:Counted Episodes = 7467
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=3, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 159.7917969268769
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=3, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -101.37691497802734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 298.6230773925781
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.25315856933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12998.2841796875
INFO:tools.evaluation_results_class:Current Best Return = -101.37691497802734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.92798110979929
INFO:tools.evaluation_results_class:Counted Episodes = 3388
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -86.288330078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 313.711669921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.83187866210938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11465.111328125
INFO:tools.evaluation_results_class:Current Best Return = -86.288330078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.25780189959294
INFO:tools.evaluation_results_class:Counted Episodes = 2948
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.20420455932617
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 341.7958068847656
INFO:tools.evaluation_results_class:Average Discounted Reward = 260.738037109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16827.287109375
INFO:tools.evaluation_results_class:Current Best Return = -58.20420455932617
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.67285464098074
INFO:tools.evaluation_results_class:Counted Episodes = 2855
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -96.69278717041016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 303.3072204589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.38656616210938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9865.68359375
INFO:tools.evaluation_results_class:Current Best Return = -96.69278717041016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.84153166421208
INFO:tools.evaluation_results_class:Counted Episodes = 3395
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.44927215576172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.55072021484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.925537109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13530.4375
INFO:tools.evaluation_results_class:Current Best Return = -119.44927215576172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.70355731225296
INFO:tools.evaluation_results_class:Counted Episodes = 3036
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.7314910888672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.2685089111328
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.0395050048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17078.90625
INFO:tools.evaluation_results_class:Current Best Return = -156.7314910888672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.563006426376084
INFO:tools.evaluation_results_class:Counted Episodes = 3579
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.40916442871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.59083557128906
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.466552734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16741.25390625
INFO:tools.evaluation_results_class:Current Best Return = -157.40916442871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.539407490218
INFO:tools.evaluation_results_class:Counted Episodes = 3578
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.38230895996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.61769104003906
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.0828399658203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17204.513671875
INFO:tools.evaluation_results_class:Current Best Return = -156.38230895996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.981598461961
INFO:tools.evaluation_results_class:Counted Episodes = 3641
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.48789978027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.51210021972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.56402587890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17438.755859375
INFO:tools.evaluation_results_class:Current Best Return = -154.48789978027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.29051987767584
INFO:tools.evaluation_results_class:Counted Episodes = 3597
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.65493774414062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.34506225585938
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.49476623535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18240.373046875
INFO:tools.evaluation_results_class:Current Best Return = -154.65493774414062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.30656327887012
INFO:tools.evaluation_results_class:Counted Episodes = 3611
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.0135955810547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.9864044189453
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.10487365722656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17378.369140625
INFO:tools.evaluation_results_class:Current Best Return = -159.0135955810547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.36608381903969
INFO:tools.evaluation_results_class:Counted Episodes = 3603
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.79052734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.20947265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.24234008789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17573.40625
INFO:tools.evaluation_results_class:Current Best Return = -158.79052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.44735376044568
INFO:tools.evaluation_results_class:Counted Episodes = 3590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.60401916503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.39598083496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.68055725097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17071.0703125
INFO:tools.evaluation_results_class:Current Best Return = -154.60401916503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.91288815608684
INFO:tools.evaluation_results_class:Counted Episodes = 3639
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.8722381591797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.1277618408203
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.6048126220703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17346.13671875
INFO:tools.evaluation_results_class:Current Best Return = -154.8722381591797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.09070796460177
INFO:tools.evaluation_results_class:Counted Episodes = 3616
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.7725830078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.2274169921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.10084533691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17484.31640625
INFO:tools.evaluation_results_class:Current Best Return = -155.7725830078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.42507645259939
INFO:tools.evaluation_results_class:Counted Episodes = 3597
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.82809448242188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.17190551757812
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.8520965576172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17831.67578125
INFO:tools.evaluation_results_class:Current Best Return = -160.82809448242188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.13134657836645
INFO:tools.evaluation_results_class:Counted Episodes = 3624
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.71670532226562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.28329467773438
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.7241973876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17104.060546875
INFO:tools.evaluation_results_class:Current Best Return = -158.71670532226562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.23085460599334
INFO:tools.evaluation_results_class:Counted Episodes = 3604
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.60203552246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.39796447753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.87030029296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17083.21875
INFO:tools.evaluation_results_class:Current Best Return = -156.60203552246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.99503585217871
INFO:tools.evaluation_results_class:Counted Episodes = 3626
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -150.87216186523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.12783813476562
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.97750854492188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22871.859375
INFO:tools.evaluation_results_class:Current Best Return = -150.87216186523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.74886731391586
INFO:tools.evaluation_results_class:Counted Episodes = 3090
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -149.9417724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.0582275390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.0541534423828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22444.04296875
INFO:tools.evaluation_results_class:Current Best Return = -149.9417724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.700420575865415
INFO:tools.evaluation_results_class:Counted Episodes = 3091
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -150.3233642578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.6766357421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 188.72869873046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14482.9931640625
INFO:tools.evaluation_results_class:Current Best Return = -150.3233642578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.99774266365689
INFO:tools.evaluation_results_class:Counted Episodes = 3544
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -90.31636047363281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 309.68365478515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.71807861328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10373.2099609375
INFO:tools.evaluation_results_class:Current Best Return = -90.31636047363281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.344528710725896
INFO:tools.evaluation_results_class:Counted Episodes = 3692
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.30123138427734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 319.6987609863281
INFO:tools.evaluation_results_class:Average Discounted Reward = 255.32012939453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11166.3994140625
INFO:tools.evaluation_results_class:Current Best Return = -80.30123138427734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.481280365818805
INFO:tools.evaluation_results_class:Counted Episodes = 3499
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.8376235961914
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 327.1623840332031
INFO:tools.evaluation_results_class:Average Discounted Reward = 260.115966796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15144.28515625
INFO:tools.evaluation_results_class:Current Best Return = -72.8376235961914
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.0246020260492
INFO:tools.evaluation_results_class:Counted Episodes = 3455
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -89.35662078857422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 310.64337158203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.83267211914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9813.490234375
INFO:tools.evaluation_results_class:Current Best Return = -89.35662078857422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.488719760804564
INFO:tools.evaluation_results_class:Counted Episodes = 3679
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -124.05640411376953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.943603515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.4484100341797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14843.923828125
INFO:tools.evaluation_results_class:Current Best Return = -124.05640411376953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.11224489795919
INFO:tools.evaluation_results_class:Counted Episodes = 3528
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.03993225097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.96006774902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.34828186035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18132.87109375
INFO:tools.evaluation_results_class:Current Best Return = -154.03993225097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.69709874900186
INFO:tools.evaluation_results_class:Counted Episodes = 3757
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -151.86163330078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.13836669921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.5812530517578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16313.064453125
INFO:tools.evaluation_results_class:Current Best Return = -151.86163330078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.33720623184579
INFO:tools.evaluation_results_class:Counted Episodes = 3787
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.0223388671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.9776611328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 184.84219360351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16059.7724609375
INFO:tools.evaluation_results_class:Current Best Return = -153.0223388671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.14931650893796
INFO:tools.evaluation_results_class:Counted Episodes = 3804
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -151.19261169433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.80738830566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.17018127441406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17517.1015625
INFO:tools.evaluation_results_class:Current Best Return = -151.19261169433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.47183846971307
INFO:tools.evaluation_results_class:Counted Episodes = 3764
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.9695587158203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.0304412841797
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.22625732421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17701.958984375
INFO:tools.evaluation_results_class:Current Best Return = -157.9695587158203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.8525641025641
INFO:tools.evaluation_results_class:Counted Episodes = 3744
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.61676025390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.38323974609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.18214416503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17280.62890625
INFO:tools.evaluation_results_class:Current Best Return = -157.61676025390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.569148936170215
INFO:tools.evaluation_results_class:Counted Episodes = 3760
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -152.58111572265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.41888427734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 184.64602661132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17511.125
INFO:tools.evaluation_results_class:Current Best Return = -152.58111572265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.532978723404256
INFO:tools.evaluation_results_class:Counted Episodes = 3760
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.09628295898438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.90371704101562
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.55813598632812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16808.84765625
INFO:tools.evaluation_results_class:Current Best Return = -154.09628295898438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.37694539699288
INFO:tools.evaluation_results_class:Counted Episodes = 3791
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.3292236328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.6707763671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.79647827148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17414.267578125
INFO:tools.evaluation_results_class:Current Best Return = -156.3292236328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.11928533893852
INFO:tools.evaluation_results_class:Counted Episodes = 3806
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.539794921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.460205078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.16067504882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18283.943359375
INFO:tools.evaluation_results_class:Current Best Return = -154.539794921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.43342175066313
INFO:tools.evaluation_results_class:Counted Episodes = 3770
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.31077575683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.68922424316406
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.789794921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18327.57421875
INFO:tools.evaluation_results_class:Current Best Return = -157.31077575683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.23781933105083
INFO:tools.evaluation_results_class:Counted Episodes = 3797
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.15118408203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.84881591796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.51846313476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17494.064453125
INFO:tools.evaluation_results_class:Current Best Return = -156.15118408203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.428647074397674
INFO:tools.evaluation_results_class:Counted Episodes = 3777
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.85092163085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.14907836914062
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.23245239257812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18320.15625
INFO:tools.evaluation_results_class:Current Best Return = -155.85092163085938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.278100263852245
INFO:tools.evaluation_results_class:Counted Episodes = 3790
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.3598175048828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.64019775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.9303741455078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23106.564453125
INFO:tools.evaluation_results_class:Current Best Return = -143.3598175048828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.86816350028785
INFO:tools.evaluation_results_class:Counted Episodes = 3474
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -137.89218139648438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.1078186035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.257568359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21438.25
INFO:tools.evaluation_results_class:Current Best Return = -137.89218139648438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.83597578552897
INFO:tools.evaluation_results_class:Counted Episodes = 3469
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -163.3146209716797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.6853790283203
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.27462768554688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17604.92578125
INFO:tools.evaluation_results_class:Current Best Return = -163.3146209716797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.53813559322034
INFO:tools.evaluation_results_class:Counted Episodes = 3776
INFO:robust_rl.robust_rl_trainer:Iteration 17 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -132.2545623779297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.7454528808594
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.36001586914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15647.2548828125
INFO:tools.evaluation_results_class:Current Best Return = -132.2545623779297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.45971116210015
INFO:tools.evaluation_results_class:Counted Episodes = 7409
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.787919998168945
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 4.717423439025879
INFO:agents.father_agent:Step: 10, Training loss: 4.613257884979248
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -133.41539001464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.5846252441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.739501953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15725.1083984375
INFO:tools.evaluation_results_class:Current Best Return = -132.2545623779297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.026330224904
INFO:tools.evaluation_results_class:Counted Episodes = 7292
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -144.23892211914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.76107788085938
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.358154296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16869.416015625
INFO:tools.evaluation_results_class:Current Best Return = -144.23892211914062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.42229875161429
INFO:tools.evaluation_results_class:Counted Episodes = 6969
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -144.330810546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.669189453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.7804718017578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17689.80859375
INFO:tools.evaluation_results_class:Current Best Return = -144.330810546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.20649436713055
INFO:tools.evaluation_results_class:Counted Episodes = 7545
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=3, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 157.2490748728183
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=3, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 18 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -131.70379638671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.29620361328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.04034423828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14846.1015625
INFO:tools.evaluation_results_class:Current Best Return = -131.70379638671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.62655826558266
INFO:tools.evaluation_results_class:Counted Episodes = 7380
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.475204944610596
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 3.8694674968719482
INFO:agents.father_agent:Step: 10, Training loss: 3.7146151065826416
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -124.03553009033203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.9644775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.9010009765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14522.390625
INFO:tools.evaluation_results_class:Current Best Return = -124.03553009033203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.58746948738812
INFO:tools.evaluation_results_class:Counted Episodes = 7374
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -133.56268310546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.43731689453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.66845703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15609.79296875
INFO:tools.evaluation_results_class:Current Best Return = -133.56268310546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.09713307659393
INFO:tools.evaluation_results_class:Counted Episodes = 7011
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.0409393310547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.9590759277344
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.82752990722656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16875.10546875
INFO:tools.evaluation_results_class:Current Best Return = -141.0409393310547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.332801701222756
INFO:tools.evaluation_results_class:Counted Episodes = 7524
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=1, goright1_init=1, goright2_init=0, o1y=1, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 154.52820328575336
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=1, goright1_init=1, goright2_init=0, o1y=1, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 19 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -120.39839172363281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.6015930175781
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.10414123535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13865.3623046875
INFO:tools.evaluation_results_class:Current Best Return = -120.39839172363281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.842277308635246
INFO:tools.evaluation_results_class:Counted Episodes = 7342
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.351797342300415
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 2.8685712814331055
INFO:agents.father_agent:Step: 10, Training loss: 2.6925606727600098
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -119.7347640991211
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.2652282714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.38011169433594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13485.02734375
INFO:tools.evaluation_results_class:Current Best Return = -119.7347640991211
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.04806244009311
INFO:tools.evaluation_results_class:Counted Episodes = 7303
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -130.8469696044922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.15301513671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.6208038330078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15869.4169921875
INFO:tools.evaluation_results_class:Current Best Return = -130.8469696044922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.769164015041945
INFO:tools.evaluation_results_class:Counted Episodes = 6914
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.1371307373047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.86285400390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.83230590820312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17383.912109375
INFO:tools.evaluation_results_class:Current Best Return = -141.1371307373047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.99367088607595
INFO:tools.evaluation_results_class:Counted Episodes = 7584
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=1, goright1_init=1, goright2_init=0, o1y=1, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 150.9942444018648
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=1, goright1_init=1, goright2_init=0, o1y=1, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 20 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -117.38394165039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.6160583496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.34539794921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14013.56640625
INFO:tools.evaluation_results_class:Current Best Return = -117.38394165039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.508042151968944
INFO:tools.evaluation_results_class:Counted Episodes = 7212
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.6923511028289795
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 2.1716573238372803
INFO:agents.father_agent:Step: 10, Training loss: 2.08091139793396
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -114.77079772949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.22918701171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 218.954833984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13856.1025390625
INFO:tools.evaluation_results_class:Current Best Return = -114.77079772949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.35314091680815
INFO:tools.evaluation_results_class:Counted Episodes = 7068
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -131.3328857421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.6671142578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.72726440429688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15697.3330078125
INFO:tools.evaluation_results_class:Current Best Return = -131.3328857421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.40998531571219
INFO:tools.evaluation_results_class:Counted Episodes = 6810
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.61041259765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.38958740234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.6036376953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17043.9296875
INFO:tools.evaluation_results_class:Current Best Return = -136.61041259765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.29880043620501
INFO:tools.evaluation_results_class:Counted Episodes = 7336
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=3, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 156.56801823855812
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=3, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 21 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.30695343017578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.69305419921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.10142517089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13797.6015625
INFO:tools.evaluation_results_class:Current Best Return = -114.77079772949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.203131612357176
INFO:tools.evaluation_results_class:Counted Episodes = 7089
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.9890481233596802
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 1.8046932220458984
INFO:agents.father_agent:Step: 10, Training loss: 1.7127515077590942
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -117.56938934326172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.43060302734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.44602966308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14244.3115234375
INFO:tools.evaluation_results_class:Current Best Return = -114.77079772949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.904868494683825
INFO:tools.evaluation_results_class:Counted Episodes = 7148
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -128.51397705078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.48602294921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.00387573242188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15238.27734375
INFO:tools.evaluation_results_class:Current Best Return = -128.51397705078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.28830674667057
INFO:tools.evaluation_results_class:Counted Episodes = 6833
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -139.41439819335938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.5856018066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.8144073486328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17958.44140625
INFO:tools.evaluation_results_class:Current Best Return = -139.41439819335938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.11195652173913
INFO:tools.evaluation_results_class:Counted Episodes = 7360
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=1, o1y=3, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 155.54326725987087
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=1, o1y=3, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -101.0677719116211
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 298.9322204589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.2293243408203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15366.8515625
INFO:tools.evaluation_results_class:Current Best Return = -101.0677719116211
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.982835158330865
INFO:tools.evaluation_results_class:Counted Episodes = 3379
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.4941177368164
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 328.5058898925781
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.22132873535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8892.0546875
INFO:tools.evaluation_results_class:Current Best Return = -71.4941177368164
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.029411764705884
INFO:tools.evaluation_results_class:Counted Episodes = 2720
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -32.00898742675781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 367.9909973144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 273.7348937988281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7117.9912109375
INFO:tools.evaluation_results_class:Current Best Return = -32.00898742675781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.31535756154748
INFO:tools.evaluation_results_class:Counted Episodes = 2559
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -99.33840942382812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.6615905761719
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.9983367919922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10211.0263671875
INFO:tools.evaluation_results_class:Current Best Return = -99.33840942382812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.11311373523928
INFO:tools.evaluation_results_class:Counted Episodes = 3218
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -102.71631622314453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 297.28369140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.67861938476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10084.94140625
INFO:tools.evaluation_results_class:Current Best Return = -102.71631622314453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.144329896907216
INFO:tools.evaluation_results_class:Counted Episodes = 2813
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -132.41258239746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.58740234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.46401977539062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14428.70703125
INFO:tools.evaluation_results_class:Current Best Return = -132.41258239746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.93897746014294
INFO:tools.evaluation_results_class:Counted Episodes = 3638
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.8635711669922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.13641357421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.16696166992188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15946.24609375
INFO:tools.evaluation_results_class:Current Best Return = -134.8635711669922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.87016195443316
INFO:tools.evaluation_results_class:Counted Episodes = 3643
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.98361206054688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.0163879394531
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.3226776123047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16115.7392578125
INFO:tools.evaluation_results_class:Current Best Return = -134.98361206054688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.73258672493854
INFO:tools.evaluation_results_class:Counted Episodes = 3661
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -133.89219665527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.1078186035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.46926879882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16308.5361328125
INFO:tools.evaluation_results_class:Current Best Return = -133.89219665527344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.59497816593886
INFO:tools.evaluation_results_class:Counted Episodes = 3664
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -133.7325439453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.2674560546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.856201171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15105.5
INFO:tools.evaluation_results_class:Current Best Return = -133.7325439453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.2227435826663
INFO:tools.evaluation_results_class:Counted Episodes = 3623
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -131.6875457763672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.31243896484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.2617950439453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14909.8408203125
INFO:tools.evaluation_results_class:Current Best Return = -131.6875457763672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.74035567715458
INFO:tools.evaluation_results_class:Counted Episodes = 3655
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -135.23609924316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.7638854980469
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.9259490966797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15258.1923828125
INFO:tools.evaluation_results_class:Current Best Return = -135.23609924316406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.81895371131197
INFO:tools.evaluation_results_class:Counted Episodes = 3651
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -129.0692901611328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.9306945800781
INFO:tools.evaluation_results_class:Average Discounted Reward = 206.1488800048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15035.6025390625
INFO:tools.evaluation_results_class:Current Best Return = -129.0692901611328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.403260869565216
INFO:tools.evaluation_results_class:Counted Episodes = 3680
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -132.94955444335938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.0504455566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.10653686523438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15679.5390625
INFO:tools.evaluation_results_class:Current Best Return = -132.94955444335938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.88785302988758
INFO:tools.evaluation_results_class:Counted Episodes = 3647
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -130.177978515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.822021484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.56576538085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14691.2080078125
INFO:tools.evaluation_results_class:Current Best Return = -130.177978515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.86075254051085
INFO:tools.evaluation_results_class:Counted Episodes = 3641
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -130.6499481201172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.3500671386719
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.1204376220703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15125.2822265625
INFO:tools.evaluation_results_class:Current Best Return = -130.6499481201172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.81469298245614
INFO:tools.evaluation_results_class:Counted Episodes = 3648
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -131.04501342773438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.9549865722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.2041778564453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15898.1611328125
INFO:tools.evaluation_results_class:Current Best Return = -131.04501342773438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.582264665757165
INFO:tools.evaluation_results_class:Counted Episodes = 3665
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -131.876953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.123046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.95420837402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15000.474609375
INFO:tools.evaluation_results_class:Current Best Return = -131.876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.845372150508105
INFO:tools.evaluation_results_class:Counted Episodes = 3641
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -131.0580291748047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.9419860839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.9639892578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19180.037109375
INFO:tools.evaluation_results_class:Current Best Return = -131.0580291748047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.26059763724809
INFO:tools.evaluation_results_class:Counted Episodes = 2878
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -132.2312469482422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.76873779296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.14544677734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20035.20703125
INFO:tools.evaluation_results_class:Current Best Return = -132.2312469482422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.141666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 2880
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -128.5758819580078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.4241027832031
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.3373260498047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12117.2607421875
INFO:tools.evaluation_results_class:Current Best Return = -128.5758819580078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.33021276595745
INFO:tools.evaluation_results_class:Counted Episodes = 3525
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -128.99142456054688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.0085754394531
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.6104736328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12178.8662109375
INFO:tools.evaluation_results_class:Current Best Return = -128.99142456054688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.48443301913739
INFO:tools.evaluation_results_class:Counted Episodes = 3501
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.50262451171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.49737548828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.0933074951172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17349.732421875
INFO:tools.evaluation_results_class:Current Best Return = -134.50262451171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.43747810858144
INFO:tools.evaluation_results_class:Counted Episodes = 2855
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -135.287353515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.712646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.36575317382812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18014.869140625
INFO:tools.evaluation_results_class:Current Best Return = -135.287353515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.22168172341904
INFO:tools.evaluation_results_class:Counted Episodes = 2878
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -126.59207916259766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.4079284667969
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.46148681640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11935.626953125
INFO:tools.evaluation_results_class:Current Best Return = -126.59207916259766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.94794908062235
INFO:tools.evaluation_results_class:Counted Episodes = 3535
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -128.14552307128906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.8544921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.29571533203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11441.7099609375
INFO:tools.evaluation_results_class:Current Best Return = -128.14552307128906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.03835307388607
INFO:tools.evaluation_results_class:Counted Episodes = 3546
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -90.35311889648438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 309.6468811035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 244.3537139892578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10760.2373046875
INFO:tools.evaluation_results_class:Current Best Return = -90.35311889648438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.175041413583656
INFO:tools.evaluation_results_class:Counted Episodes = 3622
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.79694366455078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 320.20306396484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 254.73541259765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11026.8115234375
INFO:tools.evaluation_results_class:Current Best Return = -79.79694366455078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.589186012342054
INFO:tools.evaluation_results_class:Counted Episodes = 3403
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.598995208740234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 340.4010009765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 269.7056884765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12211.638671875
INFO:tools.evaluation_results_class:Current Best Return = -59.598995208740234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.885342789598106
INFO:tools.evaluation_results_class:Counted Episodes = 3384
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -94.8023681640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 305.1976318359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.9142303466797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10890.578125
INFO:tools.evaluation_results_class:Current Best Return = -94.8023681640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.9980732177264
INFO:tools.evaluation_results_class:Counted Episodes = 3633
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -128.75762939453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.24237060546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.35678100585938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16226.59765625
INFO:tools.evaluation_results_class:Current Best Return = -128.75762939453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.09270560883464
INFO:tools.evaluation_results_class:Counted Episodes = 3441
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -144.58447265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.41552734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.73182678222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17061.24609375
INFO:tools.evaluation_results_class:Current Best Return = -144.58447265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.830254350736276
INFO:tools.evaluation_results_class:Counted Episodes = 3735
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.42030334472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.5797119140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.9761199951172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16906.630859375
INFO:tools.evaluation_results_class:Current Best Return = -142.42030334472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.557917109458025
INFO:tools.evaluation_results_class:Counted Episodes = 3764
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -139.5130157470703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.4869689941406
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.54100036621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15004.5634765625
INFO:tools.evaluation_results_class:Current Best Return = -139.5130157470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.618490967056324
INFO:tools.evaluation_results_class:Counted Episodes = 3764
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.2763671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.7236328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.70057678222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16632.19921875
INFO:tools.evaluation_results_class:Current Best Return = -142.2763671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.82750333778371
INFO:tools.evaluation_results_class:Counted Episodes = 3745
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.6878204345703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.31219482421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.65264892578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17353.5234375
INFO:tools.evaluation_results_class:Current Best Return = -143.6878204345703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.70514529458811
INFO:tools.evaluation_results_class:Counted Episodes = 3751
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.69041442871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.3095703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.72828674316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16937.826171875
INFO:tools.evaluation_results_class:Current Best Return = -143.69041442871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.680277555377636
INFO:tools.evaluation_results_class:Counted Episodes = 3747
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.43548583984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.56451416015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.23033142089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17128.783203125
INFO:tools.evaluation_results_class:Current Best Return = -138.43548583984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.896145610278374
INFO:tools.evaluation_results_class:Counted Episodes = 3736
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.7156982421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.2843017578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.53904724121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17644.9296875
INFO:tools.evaluation_results_class:Current Best Return = -145.7156982421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.851018220793144
INFO:tools.evaluation_results_class:Counted Episodes = 3732
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.98556518554688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.0144348144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.7170867919922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16269.44140625
INFO:tools.evaluation_results_class:Current Best Return = -138.98556518554688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.817210048102616
INFO:tools.evaluation_results_class:Counted Episodes = 3742
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -139.55677795410156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.4432067871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.94131469726562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15364.86328125
INFO:tools.evaluation_results_class:Current Best Return = -139.55677795410156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.01184068891281
INFO:tools.evaluation_results_class:Counted Episodes = 3716
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.09169006347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.9082946777344
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.40289306640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15296.4951171875
INFO:tools.evaluation_results_class:Current Best Return = -141.09169006347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.006991126646945
INFO:tools.evaluation_results_class:Counted Episodes = 3719
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -144.5277862548828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.4722137451172
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.77716064453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16246.904296875
INFO:tools.evaluation_results_class:Current Best Return = -144.5277862548828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.858898984500264
INFO:tools.evaluation_results_class:Counted Episodes = 3742
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.9311065673828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.0688781738281
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.4243927001953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18159.751953125
INFO:tools.evaluation_results_class:Current Best Return = -143.9311065673828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.717489986648864
INFO:tools.evaluation_results_class:Counted Episodes = 3745
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -126.47484588623047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.525146484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.93667602539062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18626.3515625
INFO:tools.evaluation_results_class:Current Best Return = -126.47484588623047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.68873197999412
INFO:tools.evaluation_results_class:Counted Episodes = 3399
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -123.75538635253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.2445983886719
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.96817016601562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19475.50390625
INFO:tools.evaluation_results_class:Current Best Return = -123.75538635253906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.2090856144438
INFO:tools.evaluation_results_class:Counted Episodes = 3434
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.26991271972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.73008728027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.40577697753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17473.171875
INFO:tools.evaluation_results_class:Current Best Return = -154.26991271972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.19913885898816
INFO:tools.evaluation_results_class:Counted Episodes = 3716
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.0975341796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.9024658203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.69180297851562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16470.13671875
INFO:tools.evaluation_results_class:Current Best Return = -153.0975341796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.90353697749196
INFO:tools.evaluation_results_class:Counted Episodes = 3732
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -139.17666625976562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.8233337402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.39321899414062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20142.578125
INFO:tools.evaluation_results_class:Current Best Return = -139.17666625976562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.48815443112021
INFO:tools.evaluation_results_class:Counted Episodes = 3419
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -131.85379028320312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.1462097167969
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.4535675048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18843.861328125
INFO:tools.evaluation_results_class:Current Best Return = -131.85379028320312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.48443922489724
INFO:tools.evaluation_results_class:Counted Episodes = 3406
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.61082458496094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.38917541503906
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.0269012451172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16769.765625
INFO:tools.evaluation_results_class:Current Best Return = -153.61082458496094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.9812382739212
INFO:tools.evaluation_results_class:Counted Episodes = 3731
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.2826690673828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.7173309326172
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.81773376464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18437.3828125
INFO:tools.evaluation_results_class:Current Best Return = -157.2826690673828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.57253127495342
INFO:tools.evaluation_results_class:Counted Episodes = 3757
INFO:robust_rl.robust_rl_trainer:Iteration 22 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -115.8826904296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.1173095703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 219.15711975097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13730.6328125
INFO:tools.evaluation_results_class:Current Best Return = -114.77079772949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.88422818791946
INFO:tools.evaluation_results_class:Counted Episodes = 7152
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.710091471672058
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 1.4203161001205444
INFO:agents.father_agent:Step: 10, Training loss: 1.6141481399536133
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -110.51065826416016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.4893493652344
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.7034912109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11884.86328125
INFO:tools.evaluation_results_class:Current Best Return = -110.51065826416016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.237178605802285
INFO:tools.evaluation_results_class:Counted Episodes = 7273
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -122.18942260742188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.8105773925781
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.52420043945312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13867.607421875
INFO:tools.evaluation_results_class:Current Best Return = -122.18942260742188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.77893368010403
INFO:tools.evaluation_results_class:Counted Episodes = 6921
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -135.38096618652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.6190490722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.3904266357422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16226.6142578125
INFO:tools.evaluation_results_class:Current Best Return = -135.38096618652344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.36857523302264
INFO:tools.evaluation_results_class:Counted Episodes = 7510
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=1, o1y=3, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 147.96292044615154
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=1, o1y=3, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 23 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -111.81925964355469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 288.18072509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.43505859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12129.4150390625
INFO:tools.evaluation_results_class:Current Best Return = -110.51065826416016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.34326710816777
INFO:tools.evaluation_results_class:Counted Episodes = 7248
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.406448245048523
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 1.5669249296188354
INFO:agents.father_agent:Step: 10, Training loss: 1.2499747276306152
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -107.13374328613281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.8662414550781
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.22174072265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11955.8212890625
INFO:tools.evaluation_results_class:Current Best Return = -107.13374328613281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.9082260772244
INFO:tools.evaluation_results_class:Counted Episodes = 7148
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -118.0335693359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.9664306640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.6721649169922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13320.8388671875
INFO:tools.evaluation_results_class:Current Best Return = -118.0335693359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.352734203196015
INFO:tools.evaluation_results_class:Counted Episodes = 6821
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.88864135742188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.1113586425781
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.15354919433594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17355.974609375
INFO:tools.evaluation_results_class:Current Best Return = -136.88864135742188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.75302989496364
INFO:tools.evaluation_results_class:Counted Episodes = 7426
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=1, o1y=3, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 146.91767922040006
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=1, o1y=3, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 24 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -107.62918090820312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.3708190917969
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.82032775878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11762.27734375
INFO:tools.evaluation_results_class:Current Best Return = -107.13374328613281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.8885310180647
INFO:tools.evaluation_results_class:Counted Episodes = 7141
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.6560887098312378
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 1.3591209650039673
INFO:agents.father_agent:Step: 10, Training loss: 1.3424348831176758
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -110.87649536132812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.1235046386719
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.94981384277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12351.76953125
INFO:tools.evaluation_results_class:Current Best Return = -107.13374328613281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.04270309130122
INFO:tools.evaluation_results_class:Counted Episodes = 6955
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.98118591308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.018798828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.7176055908203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13654.5107421875
INFO:tools.evaluation_results_class:Current Best Return = -119.98118591308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.1006270528516
INFO:tools.evaluation_results_class:Counted Episodes = 6698
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -144.5672607421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.4327392578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.58477783203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18763.03515625
INFO:tools.evaluation_results_class:Current Best Return = -144.5672607421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.45914876146161
INFO:tools.evaluation_results_class:Counted Episodes = 7307
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=1, o1y=3, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 155.687864410204
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=1, o1y=3, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 25 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -107.232421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.767578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.54263305664062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11842.708984375
INFO:tools.evaluation_results_class:Current Best Return = -107.13374328613281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.925107604017214
INFO:tools.evaluation_results_class:Counted Episodes = 6970
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.2810537815093994
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 1.6641573905944824
INFO:agents.father_agent:Step: 10, Training loss: 1.233851432800293
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -107.38621520996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.61376953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.29806518554688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12073.3466796875
INFO:tools.evaluation_results_class:Current Best Return = -107.13374328613281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.90091992158045
INFO:tools.evaluation_results_class:Counted Episodes = 6631
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -121.05840301513672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.94158935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.58555603027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13764.4521484375
INFO:tools.evaluation_results_class:Current Best Return = -121.05840301513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.50300508552936
INFO:tools.evaluation_results_class:Counted Episodes = 6489
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.09893798828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 230.90106201171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.71266174316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20749.31640625
INFO:tools.evaluation_results_class:Current Best Return = -169.09893798828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.026372059871704
INFO:tools.evaluation_results_class:Counted Episodes = 7015
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 185.2851883626447
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 26 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -106.88941955566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.110595703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.3095245361328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11909.3984375
INFO:tools.evaluation_results_class:Current Best Return = -106.88941955566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.3281807372176
INFO:tools.evaluation_results_class:Counted Episodes = 6728
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.5132437944412231
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 1.5709607601165771
INFO:agents.father_agent:Step: 10, Training loss: 1.446153998374939
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -107.64925384521484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.3507385253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.42886352539062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11811.169921875
INFO:tools.evaluation_results_class:Current Best Return = -106.88941955566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.56736938588451
INFO:tools.evaluation_results_class:Counted Episodes = 6546
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -118.1275863647461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.8724060058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.65908813476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13802.5205078125
INFO:tools.evaluation_results_class:Current Best Return = -118.1275863647461
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.76632411067194
INFO:tools.evaluation_results_class:Counted Episodes = 6325
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.05807495117188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 221.94192504882812
INFO:tools.evaluation_results_class:Average Discounted Reward = 160.4263153076172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22715.3828125
INFO:tools.evaluation_results_class:Current Best Return = -178.05807495117188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.58639573425565
INFO:tools.evaluation_results_class:Counted Episodes = 6939
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=3, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 195.56507659528643
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=3, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.25353240966797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 326.7464599609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 257.27349853515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12517.7060546875
INFO:tools.evaluation_results_class:Current Best Return = -73.25353240966797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.49684589966957
INFO:tools.evaluation_results_class:Counted Episodes = 3329
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.38731384277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 340.6126708984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.6448516845703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5424.11572265625
INFO:tools.evaluation_results_class:Current Best Return = -59.38731384277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.75348627980208
INFO:tools.evaluation_results_class:Counted Episodes = 2223
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -22.363929748535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 377.6360778808594
INFO:tools.evaluation_results_class:Average Discounted Reward = 267.14422607421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2463.129150390625
INFO:tools.evaluation_results_class:Current Best Return = -22.363929748535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.04243542435424
INFO:tools.evaluation_results_class:Counted Episodes = 2168
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -95.77137756347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 304.2286071777344
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.70925903320312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11039.380859375
INFO:tools.evaluation_results_class:Current Best Return = -95.77137756347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.625698324022345
INFO:tools.evaluation_results_class:Counted Episodes = 2327
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.19129180908203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 330.8087158203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.3439483642578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5589.81787109375
INFO:tools.evaluation_results_class:Current Best Return = -69.19129180908203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.92260334212841
INFO:tools.evaluation_results_class:Counted Episodes = 2274
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -123.60074615478516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.3992614746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.56051635742188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15513.029296875
INFO:tools.evaluation_results_class:Current Best Return = -123.60074615478516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.97780340155664
INFO:tools.evaluation_results_class:Counted Episodes = 3469
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.22235107421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.77764892578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.1710205078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14514.6611328125
INFO:tools.evaluation_results_class:Current Best Return = -119.22235107421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.46413260931695
INFO:tools.evaluation_results_class:Counted Episodes = 3499
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.90455627441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.0954284667969
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.9505615234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15219.0458984375
INFO:tools.evaluation_results_class:Current Best Return = -119.90455627441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.692462023502436
INFO:tools.evaluation_results_class:Counted Episodes = 3489
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -120.23151397705078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.76849365234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.2586669921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14775.4677734375
INFO:tools.evaluation_results_class:Current Best Return = -120.23151397705078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.42706251784185
INFO:tools.evaluation_results_class:Counted Episodes = 3503
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -118.89064025878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.2826385498047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14622.009765625
INFO:tools.evaluation_results_class:Current Best Return = -118.89064025878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.57028342399084
INFO:tools.evaluation_results_class:Counted Episodes = 3493
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -121.5558090209961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.4441833496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.933837890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15215.1796875
INFO:tools.evaluation_results_class:Current Best Return = -121.5558090209961
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.585783614045106
INFO:tools.evaluation_results_class:Counted Episodes = 3503
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -122.97686004638672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.02313232421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.59051513671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15414.9169921875
INFO:tools.evaluation_results_class:Current Best Return = -122.97686004638672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.53356183947444
INFO:tools.evaluation_results_class:Counted Episodes = 3501
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -123.24300384521484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.7569885253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.30374145507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15426.814453125
INFO:tools.evaluation_results_class:Current Best Return = -123.24300384521484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.46944603083952
INFO:tools.evaluation_results_class:Counted Episodes = 3502
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -118.54403686523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.4559631347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.7886505126953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14938.2451171875
INFO:tools.evaluation_results_class:Current Best Return = -118.54403686523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.330681818181816
INFO:tools.evaluation_results_class:Counted Episodes = 3520
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -123.13744354248047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.862548828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.2595977783203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15561.7373046875
INFO:tools.evaluation_results_class:Current Best Return = -123.13744354248047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.614060258249644
INFO:tools.evaluation_results_class:Counted Episodes = 3485
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.62837982177734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.3716125488281
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.1411895751953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14332.3818359375
INFO:tools.evaluation_results_class:Current Best Return = -119.62837982177734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.818652849740936
INFO:tools.evaluation_results_class:Counted Episodes = 3474
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.86933898925781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.1306457519531
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.79693603515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14996.890625
INFO:tools.evaluation_results_class:Current Best Return = -119.86933898925781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.33475661827498
INFO:tools.evaluation_results_class:Counted Episodes = 3513
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -123.79822540283203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.2017822265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.61227416992188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15655.8544921875
INFO:tools.evaluation_results_class:Current Best Return = -123.79822540283203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.675265118945255
INFO:tools.evaluation_results_class:Counted Episodes = 3489
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -127.21188354492188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.7881164550781
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.15438842773438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22213.271484375
INFO:tools.evaluation_results_class:Current Best Return = -127.21188354492188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 71.86930693069307
INFO:tools.evaluation_results_class:Counted Episodes = 2020
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -128.39173889160156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.6082763671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.7184600830078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22008.0703125
INFO:tools.evaluation_results_class:Current Best Return = -128.39173889160156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 71.48031496062993
INFO:tools.evaluation_results_class:Counted Episodes = 2032
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -110.94764709472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.0523376464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.10055541992188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9966.154296875
INFO:tools.evaluation_results_class:Current Best Return = -110.94764709472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.962732919254655
INFO:tools.evaluation_results_class:Counted Episodes = 3381
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -111.90387725830078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 288.09613037109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.47389221191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9915.9462890625
INFO:tools.evaluation_results_class:Current Best Return = -111.90387725830078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.77660199882422
INFO:tools.evaluation_results_class:Counted Episodes = 3402
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -126.17967987060547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.8203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.1577911376953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23092.05078125
INFO:tools.evaluation_results_class:Current Best Return = -126.17967987060547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.7978560490046
INFO:tools.evaluation_results_class:Counted Episodes = 1959
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -126.49442291259766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.5055847167969
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.94427490234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23069.4765625
INFO:tools.evaluation_results_class:Current Best Return = -126.49442291259766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.62474645030426
INFO:tools.evaluation_results_class:Counted Episodes = 1972
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -112.15934753417969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.8406677246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.05001831054688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9922.353515625
INFO:tools.evaluation_results_class:Current Best Return = -112.15934753417969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.06884272997033
INFO:tools.evaluation_results_class:Counted Episodes = 3370
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -115.74375915527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.2562561035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.83612060546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9878.5859375
INFO:tools.evaluation_results_class:Current Best Return = -115.74375915527344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.651936872309896
INFO:tools.evaluation_results_class:Counted Episodes = 3485
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -114.20746612548828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.79254150390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.5357208251953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9584.16796875
INFO:tools.evaluation_results_class:Current Best Return = -114.20746612548828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.06597222222222
INFO:tools.evaluation_results_class:Counted Episodes = 3456
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -116.39146423339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.6085205078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.12107849121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9918.4228515625
INFO:tools.evaluation_results_class:Current Best Return = -116.39146423339844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.7368117613145
INFO:tools.evaluation_results_class:Counted Episodes = 3469
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -116.26498413085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.7350158691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 219.73008728027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9553.4794921875
INFO:tools.evaluation_results_class:Current Best Return = -116.26498413085938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.061106284390384
INFO:tools.evaluation_results_class:Counted Episodes = 3453
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -121.6911392211914
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.3088684082031
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.05303955078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15474.041015625
INFO:tools.evaluation_results_class:Current Best Return = -121.6911392211914
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.52
INFO:tools.evaluation_results_class:Counted Episodes = 3500
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -112.02251434326172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.97747802734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.2269287109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9794.91015625
INFO:tools.evaluation_results_class:Current Best Return = -112.02251434326172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.0
INFO:tools.evaluation_results_class:Counted Episodes = 3376
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.40721893310547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 320.5927734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.59669494628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8198.7705078125
INFO:tools.evaluation_results_class:Current Best Return = -79.40721893310547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.99348919798757
INFO:tools.evaluation_results_class:Counted Episodes = 3379
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.67782592773438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 315.3221740722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.29251098632812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12038.7421875
INFO:tools.evaluation_results_class:Current Best Return = -84.67782592773438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.35503685503686
INFO:tools.evaluation_results_class:Counted Episodes = 3256
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.1839485168457
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 337.8160400390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 266.62835693359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12303.2978515625
INFO:tools.evaluation_results_class:Current Best Return = -62.1839485168457
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.554854223023746
INFO:tools.evaluation_results_class:Counted Episodes = 3327
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -114.38131713867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.6186828613281
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.4130859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14566.41796875
INFO:tools.evaluation_results_class:Current Best Return = -114.38131713867188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.58166862514689
INFO:tools.evaluation_results_class:Counted Episodes = 3404
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.88905334472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.11094665527344
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.0685272216797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20821.580078125
INFO:tools.evaluation_results_class:Current Best Return = -145.88905334472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.48779017184203
INFO:tools.evaluation_results_class:Counted Episodes = 3317
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -193.0689239501953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 206.9310760498047
INFO:tools.evaluation_results_class:Average Discounted Reward = 143.71926879882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23998.869140625
INFO:tools.evaluation_results_class:Current Best Return = -193.0689239501953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.067796610169495
INFO:tools.evaluation_results_class:Counted Episodes = 3540
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.77133178710938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 209.22866821289062
INFO:tools.evaluation_results_class:Average Discounted Reward = 146.2674102783203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23667.44921875
INFO:tools.evaluation_results_class:Current Best Return = -190.77133178710938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.821548821548824
INFO:tools.evaluation_results_class:Counted Episodes = 3564
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -189.92539978027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 210.07460021972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 146.8032989501953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24083.841796875
INFO:tools.evaluation_results_class:Current Best Return = -189.92539978027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.86373873873874
INFO:tools.evaluation_results_class:Counted Episodes = 3552
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -198.7190704345703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 201.2809295654297
INFO:tools.evaluation_results_class:Average Discounted Reward = 138.62783813476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27033.970703125
INFO:tools.evaluation_results_class:Current Best Return = -198.7190704345703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.29284903518729
INFO:tools.evaluation_results_class:Counted Episodes = 3524
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -192.4746856689453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 207.5253143310547
INFO:tools.evaluation_results_class:Average Discounted Reward = 144.71031188964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23103.4375
INFO:tools.evaluation_results_class:Current Best Return = -192.4746856689453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.84251968503937
INFO:tools.evaluation_results_class:Counted Episodes = 3556
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.14181518554688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 208.85818481445312
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.99925231933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24709.904296875
INFO:tools.evaluation_results_class:Current Best Return = -191.14181518554688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.729881823297696
INFO:tools.evaluation_results_class:Counted Episodes = 3554
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.42945861816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 208.57054138183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.80734252929688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24090.810546875
INFO:tools.evaluation_results_class:Current Best Return = -191.42945861816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.7796514896009
INFO:tools.evaluation_results_class:Counted Episodes = 3558
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.65228271484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 208.34771728515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.25595092773438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25066.74609375
INFO:tools.evaluation_results_class:Current Best Return = -191.65228271484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.91990975747321
INFO:tools.evaluation_results_class:Counted Episodes = 3546
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -192.9887237548828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 207.0112762451172
INFO:tools.evaluation_results_class:Average Discounted Reward = 144.0761260986328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25237.1640625
INFO:tools.evaluation_results_class:Current Best Return = -192.9887237548828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.94361432196222
INFO:tools.evaluation_results_class:Counted Episodes = 3547
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.23402404785156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 209.76597595214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 146.97544860839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23365.93359375
INFO:tools.evaluation_results_class:Current Best Return = -190.23402404785156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.77020557589412
INFO:tools.evaluation_results_class:Counted Episodes = 3551
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.19175720214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 211.80824279785156
INFO:tools.evaluation_results_class:Average Discounted Reward = 148.77894592285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23978.974609375
INFO:tools.evaluation_results_class:Current Best Return = -188.19175720214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.75217269414073
INFO:tools.evaluation_results_class:Counted Episodes = 3567
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -193.38232421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 206.61767578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 143.4526824951172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24505.189453125
INFO:tools.evaluation_results_class:Current Best Return = -193.38232421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.695932678821876
INFO:tools.evaluation_results_class:Counted Episodes = 3565
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -189.3131103515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 210.6868896484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 147.92694091796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23959.537109375
INFO:tools.evaluation_results_class:Current Best Return = -189.3131103515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.67312552653749
INFO:tools.evaluation_results_class:Counted Episodes = 3561
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -149.34019470214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.65980529785156
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.6078338623047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22669.759765625
INFO:tools.evaluation_results_class:Current Best Return = -149.34019470214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.49710610932476
INFO:tools.evaluation_results_class:Counted Episodes = 3110
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.62762451171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.37237548828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.27957153320312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23651.345703125
INFO:tools.evaluation_results_class:Current Best Return = -154.62762451171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.728299451435944
INFO:tools.evaluation_results_class:Counted Episodes = 3099
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -194.85585021972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 205.14414978027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 148.7060546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22818.255859375
INFO:tools.evaluation_results_class:Current Best Return = -194.85585021972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.43040935672514
INFO:tools.evaluation_results_class:Counted Episodes = 3420
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -198.26263427734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 201.73736572265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.7999267578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24992.19921875
INFO:tools.evaluation_results_class:Current Best Return = -198.26263427734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.32749050540462
INFO:tools.evaluation_results_class:Counted Episodes = 3423
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -148.73089599609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.26910400390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.06382751464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20676.3828125
INFO:tools.evaluation_results_class:Current Best Return = -148.73089599609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.097975179621166
INFO:tools.evaluation_results_class:Counted Episodes = 3062
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -148.3852081298828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.6147918701172
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.04234313964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20787.34765625
INFO:tools.evaluation_results_class:Current Best Return = -148.3852081298828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.70616725863739
INFO:tools.evaluation_results_class:Counted Episodes = 3097
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.2297821044922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 209.7702178955078
INFO:tools.evaluation_results_class:Average Discounted Reward = 153.1894989013672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22441.0234375
INFO:tools.evaluation_results_class:Current Best Return = -190.2297821044922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.196044211751015
INFO:tools.evaluation_results_class:Counted Episodes = 3438
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.88255310058594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 208.11744689941406
INFO:tools.evaluation_results_class:Average Discounted Reward = 153.24612426757812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24931.73046875
INFO:tools.evaluation_results_class:Current Best Return = -191.88255310058594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.13023255813953
INFO:tools.evaluation_results_class:Counted Episodes = 3440
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -185.22190856933594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 214.77809143066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 159.60910034179688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23549.1328125
INFO:tools.evaluation_results_class:Current Best Return = -185.22190856933594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.59862385321101
INFO:tools.evaluation_results_class:Counted Episodes = 3488
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.89537048339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 211.10462951660156
INFO:tools.evaluation_results_class:Average Discounted Reward = 155.7861785888672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22874.990234375
INFO:tools.evaluation_results_class:Current Best Return = -188.89537048339844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.843920666858295
INFO:tools.evaluation_results_class:Counted Episodes = 3479
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.46890258789062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 208.53109741210938
INFO:tools.evaluation_results_class:Average Discounted Reward = 153.26646423339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23265.724609375
INFO:tools.evaluation_results_class:Current Best Return = -191.46890258789062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.21976744186046
INFO:tools.evaluation_results_class:Counted Episodes = 3440
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.78369140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 209.21630859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 146.75241088867188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25830.85546875
INFO:tools.evaluation_results_class:Current Best Return = -190.78369140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.64583917063603
INFO:tools.evaluation_results_class:Counted Episodes = 3569
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -195.10055541992188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 204.89944458007812
INFO:tools.evaluation_results_class:Average Discounted Reward = 149.34771728515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23402.22265625
INFO:tools.evaluation_results_class:Current Best Return = -195.10055541992188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.96018599244406
INFO:tools.evaluation_results_class:Counted Episodes = 3441
INFO:robust_rl.robust_rl_trainer:Iteration 27 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -106.34659576416016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.6534118652344
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.9907684326172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12014.1142578125
INFO:tools.evaluation_results_class:Current Best Return = -106.34659576416016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.38822811832876
INFO:tools.evaluation_results_class:Counted Episodes = 6558
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.4892491102218628
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 1.4498975276947021
INFO:agents.father_agent:Step: 10, Training loss: 1.6669578552246094
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -96.23347473144531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 303.7665100097656
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.9922332763672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10161.3857421875
INFO:tools.evaluation_results_class:Current Best Return = -96.23347473144531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.829107430748934
INFO:tools.evaluation_results_class:Counted Episodes = 6823
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -106.27603149414062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.7239685058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.1468048095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11839.0439453125
INFO:tools.evaluation_results_class:Current Best Return = -106.27603149414062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.5455778213048
INFO:tools.evaluation_results_class:Counted Episodes = 6637
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -176.73158264160156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 223.26841735839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 163.40200805664062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21705.396484375
INFO:tools.evaluation_results_class:Current Best Return = -176.73158264160156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.95457063711911
INFO:tools.evaluation_results_class:Counted Episodes = 7220
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 189.89142410509552
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 28 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -95.13912963867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 304.8608703613281
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.164306640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10304.2001953125
INFO:tools.evaluation_results_class:Current Best Return = -95.13912963867188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.633449883449885
INFO:tools.evaluation_results_class:Counted Episodes = 6864
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.5992416143417358
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 1.6510612964630127
INFO:agents.father_agent:Step: 10, Training loss: 1.5575264692306519
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -90.24166107177734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 309.7583312988281
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.04290771484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9065.6337890625
INFO:tools.evaluation_results_class:Current Best Return = -90.24166107177734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.16790736145575
INFO:tools.evaluation_results_class:Counted Episodes = 7254
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -97.73817443847656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 302.2618408203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.7293243408203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10495.4287109375
INFO:tools.evaluation_results_class:Current Best Return = -97.73817443847656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.75074142070329
INFO:tools.evaluation_results_class:Counted Episodes = 7081
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -167.9774932861328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 232.0225067138672
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.79225158691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19946.92578125
INFO:tools.evaluation_results_class:Current Best Return = -167.9774932861328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.51516815374056
INFO:tools.evaluation_results_class:Counted Episodes = 7285
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 185.96963670537505
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 29 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -88.59066009521484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 311.4093322753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.66445922851562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9005.904296875
INFO:tools.evaluation_results_class:Current Best Return = -88.59066009521484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.03533278553821
INFO:tools.evaluation_results_class:Counted Episodes = 7302
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.6709975004196167
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 1.6850364208221436
INFO:agents.father_agent:Step: 10, Training loss: 1.544559359550476
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -86.69638061523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 313.3036193847656
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.45936584472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8465.419921875
INFO:tools.evaluation_results_class:Current Best Return = -86.69638061523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.60534527201194
INFO:tools.evaluation_results_class:Counted Episodes = 7371
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -94.7622299194336
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 305.2377624511719
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.39671325683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10221.0693359375
INFO:tools.evaluation_results_class:Current Best Return = -94.7622299194336
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.66465090709181
INFO:tools.evaluation_results_class:Counted Episodes = 7276
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -183.48020935058594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 216.51979064941406
INFO:tools.evaluation_results_class:Average Discounted Reward = 156.8738250732422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23498.4609375
INFO:tools.evaluation_results_class:Current Best Return = -183.48020935058594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.87711043454193
INFO:tools.evaluation_results_class:Counted Episodes = 7226
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 193.6426280380236
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 30 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -86.84485626220703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 313.1551513671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.21334838867188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8786.19921875
INFO:tools.evaluation_results_class:Current Best Return = -86.69638061523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.535371297173
INFO:tools.evaluation_results_class:Counted Episodes = 7393
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.3937644958496094
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 1.4828640222549438
INFO:agents.father_agent:Step: 10, Training loss: 1.4070743322372437
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -85.545166015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 314.454833984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 251.12869262695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8802.126953125
INFO:tools.evaluation_results_class:Current Best Return = -85.545166015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.1256837891928
INFO:tools.evaluation_results_class:Counted Episodes = 7495
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -89.08341217041016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 310.9165954589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.7146759033203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9262.76953125
INFO:tools.evaluation_results_class:Current Best Return = -89.08341217041016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.89647725738966
INFO:tools.evaluation_results_class:Counted Episodes = 7409
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -182.514404296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 217.485595703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 157.93539428710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23581.177734375
INFO:tools.evaluation_results_class:Current Best Return = -182.514404296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.54787379972565
INFO:tools.evaluation_results_class:Counted Episodes = 7290
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 191.1868868975067
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 31 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -86.90727996826172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 313.09271240234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.87582397460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9111.6611328125
INFO:tools.evaluation_results_class:Current Best Return = -85.545166015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.096459585838346
INFO:tools.evaluation_results_class:Counted Episodes = 7485
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.272646427154541
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 1.2295000553131104
INFO:agents.father_agent:Step: 10, Training loss: 1.0511456727981567
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -79.60417175292969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 320.39581298828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 257.01190185546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7589.07568359375
INFO:tools.evaluation_results_class:Current Best Return = -79.60417175292969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.867127292054214
INFO:tools.evaluation_results_class:Counted Episodes = 7526
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -85.80664825439453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 314.193359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 250.37538146972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9155.4521484375
INFO:tools.evaluation_results_class:Current Best Return = -85.80664825439453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.41846810781959
INFO:tools.evaluation_results_class:Counted Episodes = 7494
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
2025-08-19 14:49:03.448669: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-19 14:49:03.450760: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-19 14:49:03.484389: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-19 14:49:03.484446: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-19 14:49:03.485848: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-19 14:49:03.492125: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-19 14:49:03.492363: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-19 14:49:04.124346: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/avoid/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/avoid/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"penalty"}min=? [F ((x = (5 - 1)) & (y = (5 - 1)))] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
DEBUG:paynt.parser.jani:keeping 65660/171860 choices with non-conflicting hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 17567 states and 61860 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"rewardmodel_penalty"}min=? [F ((x = (5 - 1)) & (y = (5 - 1)))] 
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -643.291748046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -300.73394775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = -173.92401123046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8598971722365039
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 267016.09375
INFO:tools.evaluation_results_class:Current Best Return = -643.291748046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8598971722365039
INFO:tools.evaluation_results_class:Average Episode Length = 291.23778920308484
INFO:tools.evaluation_results_class:Counted Episodes = 778
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 885.6740112304688
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 3.2405807971954346
INFO:agents.father_agent:Step: 10, Training loss: 5.364970684051514
INFO:agents.father_agent:Step: 15, Training loss: 7.047971725463867
INFO:agents.father_agent:Step: 20, Training loss: 6.2008280754089355
INFO:agents.father_agent:Step: 25, Training loss: 7.6472249031066895
INFO:agents.father_agent:Step: 30, Training loss: 5.250232219696045
INFO:agents.father_agent:Step: 35, Training loss: 4.904771327972412
INFO:agents.father_agent:Step: 40, Training loss: 5.329902172088623
INFO:agents.father_agent:Step: 45, Training loss: 7.502767086029053
INFO:agents.father_agent:Step: 50, Training loss: 6.529438495635986
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 43.75%
INFO:agents.father_agent:Step: 55, Training loss: 6.800492286682129
INFO:agents.father_agent:Step: 60, Training loss: 6.682924747467041
INFO:agents.father_agent:Step: 65, Training loss: 6.252479076385498
INFO:agents.father_agent:Step: 70, Training loss: 6.243983745574951
INFO:agents.father_agent:Step: 75, Training loss: 7.187719821929932
INFO:agents.father_agent:Step: 80, Training loss: 7.416475772857666
INFO:agents.father_agent:Step: 85, Training loss: 8.625040054321289
INFO:agents.father_agent:Step: 90, Training loss: 10.603516578674316
INFO:agents.father_agent:Step: 95, Training loss: 9.358911514282227
INFO:agents.father_agent:Step: 100, Training loss: 11.894013404846191
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -181.0958251953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 218.9041748046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 143.0353240966797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30636.57421875
INFO:tools.evaluation_results_class:Current Best Return = -181.0958251953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.591555014210314
INFO:tools.evaluation_results_class:Counted Episodes = 4926
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -184.0931396484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 215.9068603515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 140.4063262939453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30326.05859375
INFO:tools.evaluation_results_class:Current Best Return = -181.0958251953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.79488229273286
INFO:tools.evaluation_results_class:Counted Episodes = 4885
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -214.16810607910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 185.83189392089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 101.22348022460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38170.234375
INFO:tools.evaluation_results_class:Current Best Return = -214.16810607910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.68423813270391
INFO:tools.evaluation_results_class:Counted Episodes = 3813
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.53204345703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 221.46795654296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 144.9882049560547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29249.591796875
INFO:tools.evaluation_results_class:Current Best Return = -178.53204345703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.60907643312102
INFO:tools.evaluation_results_class:Counted Episodes = 5024
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 267.60356694571067
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -151.31460571289062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.68539428710938
INFO:tools.evaluation_results_class:Average Discounted Reward = 160.45843505859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25163.791015625
INFO:tools.evaluation_results_class:Current Best Return = -151.31460571289062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.70992366412214
INFO:tools.evaluation_results_class:Counted Episodes = 1834
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.76785278320312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.23214721679688
INFO:tools.evaluation_results_class:Average Discounted Reward = 164.10447692871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22147.498046875
INFO:tools.evaluation_results_class:Current Best Return = -145.76785278320312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.60762942779292
INFO:tools.evaluation_results_class:Counted Episodes = 1835
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -115.51619720458984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.4837951660156
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.09300231933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17456.1015625
INFO:tools.evaluation_results_class:Current Best Return = -115.51619720458984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 76.34125269978402
INFO:tools.evaluation_results_class:Counted Episodes = 1852
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -194.82308959960938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 205.17691040039062
INFO:tools.evaluation_results_class:Average Discounted Reward = 127.72618103027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33150.625
INFO:tools.evaluation_results_class:Current Best Return = -194.82308959960938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.04638619201727
INFO:tools.evaluation_results_class:Counted Episodes = 1854
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -273.3358459472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 126.66415405273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 47.04744338989258
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41313.12109375
INFO:tools.evaluation_results_class:Current Best Return = -273.3358459472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 76.56620021528525
INFO:tools.evaluation_results_class:Counted Episodes = 1858
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -292.5543212890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 107.44567108154297
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.884801864624023
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 58715.23046875
INFO:tools.evaluation_results_class:Current Best Return = -292.5543212890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 74.7496062992126
INFO:tools.evaluation_results_class:Counted Episodes = 1905
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.71002197265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.28997802734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 188.40164184570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21492.314453125
INFO:tools.evaluation_results_class:Current Best Return = -136.71002197265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.663803680981594
INFO:tools.evaluation_results_class:Counted Episodes = 2445
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -108.80089569091797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 291.1990966796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.162353515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16055.8525390625
INFO:tools.evaluation_results_class:Current Best Return = -108.80089569091797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.591169255928044
INFO:tools.evaluation_results_class:Counted Episodes = 2446
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -99.81204223632812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.1879577636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 218.90277099609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13798.466796875
INFO:tools.evaluation_results_class:Current Best Return = -99.81204223632812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.55118755118755
INFO:tools.evaluation_results_class:Counted Episodes = 2442
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.1840362548828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.8159637451172
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.295654296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20825.05859375
INFO:tools.evaluation_results_class:Current Best Return = -154.1840362548828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.25821972734563
INFO:tools.evaluation_results_class:Counted Episodes = 2494
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -229.71900939941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 170.28099060058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 98.88553619384766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31785.783203125
INFO:tools.evaluation_results_class:Current Best Return = -229.71900939941406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.94579513750498
INFO:tools.evaluation_results_class:Counted Episodes = 2509
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -276.96124267578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 123.03877258300781
INFO:tools.evaluation_results_class:Average Discounted Reward = 50.328697204589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 51989.07421875
INFO:tools.evaluation_results_class:Current Best Return = -276.96124267578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.73021001615509
INFO:tools.evaluation_results_class:Counted Episodes = 2476
INFO:robust_rl.robust_rl_trainer:Iteration 2 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -199.5305938720703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 200.4694061279297
INFO:tools.evaluation_results_class:Average Discounted Reward = 125.25874328613281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35293.18359375
INFO:tools.evaluation_results_class:Current Best Return = -181.0958251953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.910240718074256
INFO:tools.evaluation_results_class:Counted Episodes = 4902
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.712512016296387
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Step: 5, Training loss: 12.008097648620605
INFO:agents.father_agent:Step: 10, Training loss: 12.5473051071167
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -171.37062072753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 228.62937927246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 158.9470672607422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26045.59375
INFO:tools.evaluation_results_class:Current Best Return = -171.37062072753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.8835800807537
INFO:tools.evaluation_results_class:Counted Episodes = 5944
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -195.14749145507812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 204.85250854492188
INFO:tools.evaluation_results_class:Average Discounted Reward = 125.53560638427734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32497.25
INFO:tools.evaluation_results_class:Current Best Return = -195.14749145507812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.67630421118793
INFO:tools.evaluation_results_class:Counted Episodes = 4773
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.78765869140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 230.21234130859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 159.79885864257812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25770.544921875
INFO:tools.evaluation_results_class:Current Best Return = -169.78765869140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.67359946326736
INFO:tools.evaluation_results_class:Counted Episodes = 5962
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 232.6330069287514
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 3 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -180.60772705078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 219.39227294921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 149.7184295654297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27419.98046875
INFO:tools.evaluation_results_class:Current Best Return = -171.37062072753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.08159136884693
INFO:tools.evaluation_results_class:Counted Episodes = 5932
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.246665000915527
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Step: 5, Training loss: 13.677053451538086
INFO:agents.father_agent:Step: 10, Training loss: 14.1727933883667
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -159.23095703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.76904296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.2540740966797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18880.58984375
INFO:tools.evaluation_results_class:Current Best Return = -159.23095703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.06408706166868
INFO:tools.evaluation_results_class:Counted Episodes = 6616
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -179.28489685058594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 220.71510314941406
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.41415405273438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25425.3125
INFO:tools.evaluation_results_class:Current Best Return = -179.28489685058594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.91162205855937
INFO:tools.evaluation_results_class:Counted Episodes = 5567
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.68756103515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.31243896484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.69076538085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20410.908203125
INFO:tools.evaluation_results_class:Current Best Return = -158.68756103515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.275501046963804
INFO:tools.evaluation_results_class:Counted Episodes = 6686
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 203.51935336720157
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 4 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.0594024658203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 230.9405975341797
INFO:tools.evaluation_results_class:Average Discounted Reward = 163.7671661376953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22026.392578125
INFO:tools.evaluation_results_class:Current Best Return = -159.23095703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.15562964085468
INFO:tools.evaluation_results_class:Counted Episodes = 6599
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.28536605834961
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 14.433074951171875
INFO:agents.father_agent:Step: 10, Training loss: 15.41604995727539
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -153.39230346679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.60769653320312
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.98123168945312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16835.98828125
INFO:tools.evaluation_results_class:Current Best Return = -153.39230346679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.96565598505532
INFO:tools.evaluation_results_class:Counted Episodes = 6959
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.9557342529297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.0442657470703
INFO:tools.evaluation_results_class:Average Discounted Reward = 162.19822692871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18104.94140625
INFO:tools.evaluation_results_class:Current Best Return = -164.9557342529297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.79935379644588
INFO:tools.evaluation_results_class:Counted Episodes = 6190
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -152.6918182373047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.3081817626953
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.78646850585938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16400.345703125
INFO:tools.evaluation_results_class:Current Best Return = -152.6918182373047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.674794401962195
INFO:tools.evaluation_results_class:Counted Episodes = 6931
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=1, o2x_init=1, goright1_init=0, goright2_init=0, o1y=4, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 202.51504456192808
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=1, o2x_init=1, goright1_init=0, goright2_init=0, o1y=4, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 5 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.1980438232422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.8019561767578
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.92938232421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22243.255859375
INFO:tools.evaluation_results_class:Current Best Return = -153.39230346679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.97269330267318
INFO:tools.evaluation_results_class:Counted Episodes = 6958
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.426926612854004
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 15.47110652923584
INFO:agents.father_agent:Step: 10, Training loss: 14.660407066345215
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -154.7843017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.2156982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.16734313964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22104.939453125
INFO:tools.evaluation_results_class:Current Best Return = -153.39230346679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.10428943401398
INFO:tools.evaluation_results_class:Counted Episodes = 7297
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.9636688232422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.0363311767578
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.25135803222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23554.96875
INFO:tools.evaluation_results_class:Current Best Return = -162.9636688232422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.63790493490766
INFO:tools.evaluation_results_class:Counted Episodes = 6606
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.7940216064453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.2059783935547
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.52391052246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22285.158203125
INFO:tools.evaluation_results_class:Current Best Return = -157.7940216064453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.11690427698574
INFO:tools.evaluation_results_class:Counted Episodes = 7365
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 191.34369586864298
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 6 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.8687286376953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.1312713623047
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.97230529785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21718.896484375
INFO:tools.evaluation_results_class:Current Best Return = -153.39230346679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.46054995163742
INFO:tools.evaluation_results_class:Counted Episodes = 7237
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.623948097229004
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 15.926244735717773
INFO:agents.father_agent:Step: 10, Training loss: 12.47611141204834
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -149.17591857910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.82408142089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 188.885498046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20361.044921875
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.62567713976165
INFO:tools.evaluation_results_class:Counted Episodes = 7384
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.21881103515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.78118896484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.43637084960938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23038.517578125
INFO:tools.evaluation_results_class:Current Best Return = -159.21881103515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.55661055963171
INFO:tools.evaluation_results_class:Counted Episodes = 6951
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.6366729736328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.3633270263672
INFO:tools.evaluation_results_class:Average Discounted Reward = 184.16751098632812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22240.1171875
INFO:tools.evaluation_results_class:Current Best Return = -154.6366729736328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.52955335651244
INFO:tools.evaluation_results_class:Counted Episodes = 7478
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 197.5091637673079
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -122.3949203491211
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.6050720214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.45494079589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20549.73828125
INFO:tools.evaluation_results_class:Current Best Return = -122.3949203491211
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.80495917750227
INFO:tools.evaluation_results_class:Counted Episodes = 3307
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.4448013305664
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 328.5552062988281
INFO:tools.evaluation_results_class:Average Discounted Reward = 258.94158935546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7268.74462890625
INFO:tools.evaluation_results_class:Current Best Return = -71.4448013305664
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.21526361782697
INFO:tools.evaluation_results_class:Counted Episodes = 3433
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -93.88619232177734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 306.1138000488281
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.77017211914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10154.875
INFO:tools.evaluation_results_class:Current Best Return = -93.88619232177734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.38754117999401
INFO:tools.evaluation_results_class:Counted Episodes = 3339
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -107.07919311523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.9208068847656
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.03208923339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11381.1708984375
INFO:tools.evaluation_results_class:Current Best Return = -107.07919311523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.222089907710625
INFO:tools.evaluation_results_class:Counted Episodes = 3359
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -167.60504150390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 232.39495849609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.48538208007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16734.5390625
INFO:tools.evaluation_results_class:Current Best Return = -167.60504150390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.114583333333336
INFO:tools.evaluation_results_class:Counted Episodes = 3456
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -200.07691955566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 199.92308044433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 135.53182983398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24057.3359375
INFO:tools.evaluation_results_class:Current Best Return = -200.07691955566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.61538461538461
INFO:tools.evaluation_results_class:Counted Episodes = 3497
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -207.44850158691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 192.55149841308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 128.2678985595703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23925.876953125
INFO:tools.evaluation_results_class:Current Best Return = -207.44850158691406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.99768518518518
INFO:tools.evaluation_results_class:Counted Episodes = 3456
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -200.99282836914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 199.00717163085938
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.71360778808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23406.19921875
INFO:tools.evaluation_results_class:Current Best Return = -200.99282836914062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.65556831228473
INFO:tools.evaluation_results_class:Counted Episodes = 3484
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -118.7548599243164
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.2451477050781
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.00489807128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 39041.8359375
INFO:tools.evaluation_results_class:Current Best Return = -118.7548599243164
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.87363304981774
INFO:tools.evaluation_results_class:Counted Episodes = 3292
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.7124481201172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.2875518798828
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.52415466308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21528.8828125
INFO:tools.evaluation_results_class:Current Best Return = -165.7124481201172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.375822860562536
INFO:tools.evaluation_results_class:Counted Episodes = 3342
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -206.73922729492188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 193.26077270507812
INFO:tools.evaluation_results_class:Average Discounted Reward = 129.3299560546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25199.07421875
INFO:tools.evaluation_results_class:Current Best Return = -206.73922729492188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.68925904652499
INFO:tools.evaluation_results_class:Counted Episodes = 3482
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -124.26842498779297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.7315673828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 218.56399536132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22060.05859375
INFO:tools.evaluation_results_class:Current Best Return = -124.26842498779297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.653684505463715
INFO:tools.evaluation_results_class:Counted Episodes = 3569
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.33599853515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 332.66400146484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 267.443359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6435.826171875
INFO:tools.evaluation_results_class:Current Best Return = -67.33599853515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.64906666666667
INFO:tools.evaluation_results_class:Counted Episodes = 3750
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -95.8560791015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 304.1439208984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.67596435546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10358.3916015625
INFO:tools.evaluation_results_class:Current Best Return = -95.8560791015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.01736972704715
INFO:tools.evaluation_results_class:Counted Episodes = 3627
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -94.24607849121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 305.75390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.65858459472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9311.1640625
INFO:tools.evaluation_results_class:Current Best Return = -94.24607849121094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.902392081385756
INFO:tools.evaluation_results_class:Counted Episodes = 3637
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.77537536621094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.22462463378906
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.30270385742188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14913.98828125
INFO:tools.evaluation_results_class:Current Best Return = -158.77537536621094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.33937753721245
INFO:tools.evaluation_results_class:Counted Episodes = 3695
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -194.06887817382812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 205.93112182617188
INFO:tools.evaluation_results_class:Average Discounted Reward = 143.8091278076172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21569.037109375
INFO:tools.evaluation_results_class:Current Best Return = -194.06887817382812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.173796072101155
INFO:tools.evaluation_results_class:Counted Episodes = 3717
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -195.07183837890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 204.92816162109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 143.09957885742188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22361.0390625
INFO:tools.evaluation_results_class:Current Best Return = -195.07183837890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.09631423190745
INFO:tools.evaluation_results_class:Counted Episodes = 3717
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -203.5711212158203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 196.4288787841797
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.77651977539062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24754.1875
INFO:tools.evaluation_results_class:Current Best Return = -203.5711212158203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.49877617623062
INFO:tools.evaluation_results_class:Counted Episodes = 3677
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -118.07534790039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.9246520996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.7045135498047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37903.97265625
INFO:tools.evaluation_results_class:Current Best Return = -118.07534790039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.2
INFO:tools.evaluation_results_class:Counted Episodes = 3610
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.10963439941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.89036560058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.80429077148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23303.482421875
INFO:tools.evaluation_results_class:Current Best Return = -166.10963439941406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.21594684385382
INFO:tools.evaluation_results_class:Counted Episodes = 3612
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -197.16806030273438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 202.83193969726562
INFO:tools.evaluation_results_class:Average Discounted Reward = 141.2660675048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23097.3828125
INFO:tools.evaluation_results_class:Current Best Return = -197.16806030273438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.972617449664426
INFO:tools.evaluation_results_class:Counted Episodes = 3725
INFO:robust_rl.robust_rl_trainer:Iteration 7 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.64434814453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.35565185546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.0
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21262.576171875
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.47158860844918
INFO:tools.evaluation_results_class:Counted Episodes = 7409
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.142701148986816
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 13.682039260864258
INFO:agents.father_agent:Step: 10, Training loss: 13.256037712097168
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -155.91307067871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.08692932128906
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.4947509765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22053.7578125
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.53670406921725
INFO:tools.evaluation_results_class:Counted Episodes = 7397
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.18521118164062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.81478881835938
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.5904541015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22330.564453125
INFO:tools.evaluation_results_class:Current Best Return = -160.18521118164062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.99729921819474
INFO:tools.evaluation_results_class:Counted Episodes = 7035
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.9927520751953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.0072479248047
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.2760009765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22392.923828125
INFO:tools.evaluation_results_class:Current Best Return = -155.9927520751953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.738651625033576
INFO:tools.evaluation_results_class:Counted Episodes = 7446
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 197.5577919801831
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 8 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.9741973876953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.0258026123047
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.76158142089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21085.9921875
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.745247148288975
INFO:tools.evaluation_results_class:Counted Episodes = 7364
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.812912940979004
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 13.210174560546875
INFO:agents.father_agent:Step: 10, Training loss: 13.916618347167969
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -155.2774658203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.7225341796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.4608154296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19932.5546875
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.73896912680535
INFO:tools.evaluation_results_class:Counted Episodes = 7547
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.27272033691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.72727966308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.57785034179688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22048.509765625
INFO:tools.evaluation_results_class:Current Best Return = -162.27272033691406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.011086474501106
INFO:tools.evaluation_results_class:Counted Episodes = 7216
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.75538635253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.24461364746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.3180694580078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21695.96484375
INFO:tools.evaluation_results_class:Current Best Return = -160.75538635253906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.88321935823251
INFO:tools.evaluation_results_class:Counted Episodes = 7604
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 191.12289383877865
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 9 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -163.38710021972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.61289978027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.83892822265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21202.84375
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.563746862201086
INFO:tools.evaluation_results_class:Counted Episodes = 7569
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.660244941711426
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 11.712894439697266
INFO:agents.father_agent:Step: 10, Training loss: 11.628074645996094
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -162.8825225830078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.1174774169922
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.01617431640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19543.29296875
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.86737541528239
INFO:tools.evaluation_results_class:Counted Episodes = 7525
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.7557373046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.2442626953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.732421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20777.46875
INFO:tools.evaluation_results_class:Current Best Return = -165.7557373046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.11838265944144
INFO:tools.evaluation_results_class:Counted Episodes = 7197
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.65475463867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.34524536132812
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.1790008544922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20836.783203125
INFO:tools.evaluation_results_class:Current Best Return = -164.65475463867188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.13015873015873
INFO:tools.evaluation_results_class:Counted Episodes = 7560
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 189.0479117184495
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 10 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -163.70947265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.29052734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.49293518066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20818.314453125
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.9696
INFO:tools.evaluation_results_class:Counted Episodes = 7500
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.06820011138916
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 11.456819534301758
INFO:agents.father_agent:Step: 10, Training loss: 11.345016479492188
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -161.3777618408203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.6222381591797
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.8376922607422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21851.8359375
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.97707278059184
INFO:tools.evaluation_results_class:Counted Episodes = 7502
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.0492706298828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.9507293701172
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.15237426757812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22806.3984375
INFO:tools.evaluation_results_class:Current Best Return = -166.0492706298828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.03664122137405
INFO:tools.evaluation_results_class:Counted Episodes = 7205
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.42665100097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.57334899902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.07896423339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21381.8671875
INFO:tools.evaluation_results_class:Current Best Return = -162.42665100097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.059740946338884
INFO:tools.evaluation_results_class:Counted Episodes = 7566
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 182.65897724684837
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 11 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -161.7512969970703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.2487030029297
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.22731018066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20079.9375
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.80619433736541
INFO:tools.evaluation_results_class:Counted Episodes = 7523
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.339594841003418
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 10.363424301147461
INFO:agents.father_agent:Step: 10, Training loss: 9.908208847045898
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -160.5555877685547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.4444122314453
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.4210205078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19745.052734375
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.72990183072433
INFO:tools.evaluation_results_class:Counted Episodes = 7538
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.16539001464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.83460998535156
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.31861877441406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20296.541015625
INFO:tools.evaluation_results_class:Current Best Return = -162.16539001464844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.04332731565061
INFO:tools.evaluation_results_class:Counted Episodes = 7201
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.8941192626953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.1058807373047
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.88848876953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21183.685546875
INFO:tools.evaluation_results_class:Current Best Return = -160.8941192626953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.088565763384004
INFO:tools.evaluation_results_class:Counted Episodes = 7565
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 185.0931870047805
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.9718017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.0281982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.95257568359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21971.908203125
INFO:tools.evaluation_results_class:Current Best Return = -119.9718017578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.20073551946062
INFO:tools.evaluation_results_class:Counted Episodes = 3263
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.0580062866211
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 327.9419860839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 261.0754699707031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6974.50244140625
INFO:tools.evaluation_results_class:Current Best Return = -72.0580062866211
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.364973633083544
INFO:tools.evaluation_results_class:Counted Episodes = 3603
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -91.0415267944336
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 308.9584655761719
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.950439453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10562.359375
INFO:tools.evaluation_results_class:Current Best Return = -91.0415267944336
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.65898757199151
INFO:tools.evaluation_results_class:Counted Episodes = 3299
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -97.78465270996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 302.2153625488281
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.7349853515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9704.7392578125
INFO:tools.evaluation_results_class:Current Best Return = -97.78465270996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.267697798929206
INFO:tools.evaluation_results_class:Counted Episodes = 3362
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.8926544189453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.1073455810547
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.07461547851562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13901.650390625
INFO:tools.evaluation_results_class:Current Best Return = -155.8926544189453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.41045606229144
INFO:tools.evaluation_results_class:Counted Episodes = 3596
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -181.32272338867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 218.67727661132812
INFO:tools.evaluation_results_class:Average Discounted Reward = 154.75955200195312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18856.068359375
INFO:tools.evaluation_results_class:Current Best Return = -181.32272338867188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.893523600439075
INFO:tools.evaluation_results_class:Counted Episodes = 3644
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -185.3329620361328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 214.6670379638672
INFO:tools.evaluation_results_class:Average Discounted Reward = 151.60028076171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19810.685546875
INFO:tools.evaluation_results_class:Current Best Return = -185.3329620361328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.74273176083379
INFO:tools.evaluation_results_class:Counted Episodes = 3646
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -186.7547149658203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 213.2452850341797
INFO:tools.evaluation_results_class:Average Discounted Reward = 150.02854919433594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19892.13671875
INFO:tools.evaluation_results_class:Current Best Return = -186.7547149658203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.76973504506965
INFO:tools.evaluation_results_class:Counted Episodes = 3661
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -110.26332092285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.7366943359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.1501007080078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40271.8828125
INFO:tools.evaluation_results_class:Current Best Return = -110.26332092285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.50354173082846
INFO:tools.evaluation_results_class:Counted Episodes = 3247
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.71327209472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.28672790527344
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.34854125976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23801.98046875
INFO:tools.evaluation_results_class:Current Best Return = -164.71327209472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.884015307624374
INFO:tools.evaluation_results_class:Counted Episodes = 3397
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -184.37705993652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 215.62294006347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 152.28515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19652.85546875
INFO:tools.evaluation_results_class:Current Best Return = -184.37705993652344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.816684961580684
INFO:tools.evaluation_results_class:Counted Episodes = 3644
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -182.27239990234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 217.72760009765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 153.96981811523438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18728.87109375
INFO:tools.evaluation_results_class:Current Best Return = -182.27239990234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.92798240791644
INFO:tools.evaluation_results_class:Counted Episodes = 3638
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -187.48373413085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 212.51626586914062
INFO:tools.evaluation_results_class:Average Discounted Reward = 148.9585418701172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20100.546875
INFO:tools.evaluation_results_class:Current Best Return = -187.48373413085938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.96966354109211
INFO:tools.evaluation_results_class:Counted Episodes = 3626
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.6244354248047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.3755645751953
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.2644500732422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22544.076171875
INFO:tools.evaluation_results_class:Current Best Return = -162.6244354248047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.51816271389973
INFO:tools.evaluation_results_class:Counted Episodes = 3331
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.33607482910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 211.66392517089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 148.61683654785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20671.0546875
INFO:tools.evaluation_results_class:Current Best Return = -188.33607482910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.895112575507966
INFO:tools.evaluation_results_class:Counted Episodes = 3642
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -187.28399658203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 212.71600341796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 149.72760009765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20148.423828125
INFO:tools.evaluation_results_class:Current Best Return = -187.28399658203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.74582763337893
INFO:tools.evaluation_results_class:Counted Episodes = 3655
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -120.35205078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.64794921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.7882843017578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21679.802734375
INFO:tools.evaluation_results_class:Current Best Return = -120.35205078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.7170129140932
INFO:tools.evaluation_results_class:Counted Episodes = 3562
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.10367584228516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 332.8963317871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 267.9168395996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6108.82080078125
INFO:tools.evaluation_results_class:Current Best Return = -67.10367584228516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.422903993652476
INFO:tools.evaluation_results_class:Counted Episodes = 3781
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -94.67707824707031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 305.32293701171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.76260375976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9875.103515625
INFO:tools.evaluation_results_class:Current Best Return = -94.67707824707031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.556051310652535
INFO:tools.evaluation_results_class:Counted Episodes = 3586
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -92.16694641113281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 307.83306884765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 244.1216583251953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9339.2744140625
INFO:tools.evaluation_results_class:Current Best Return = -92.16694641113281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.40133779264214
INFO:tools.evaluation_results_class:Counted Episodes = 3588
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -146.68527221679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.31472778320312
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.40606689453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13429.4140625
INFO:tools.evaluation_results_class:Current Best Return = -146.68527221679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.537480063795854
INFO:tools.evaluation_results_class:Counted Episodes = 3762
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -181.34103393554688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 218.65896606445312
INFO:tools.evaluation_results_class:Average Discounted Reward = 156.61737060546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18770.333984375
INFO:tools.evaluation_results_class:Current Best Return = -181.34103393554688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.538216560509554
INFO:tools.evaluation_results_class:Counted Episodes = 3768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -183.9800262451172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 216.0199737548828
INFO:tools.evaluation_results_class:Average Discounted Reward = 154.72756958007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19222.630859375
INFO:tools.evaluation_results_class:Current Best Return = -183.9800262451172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.110410094637224
INFO:tools.evaluation_results_class:Counted Episodes = 3804
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -184.45176696777344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 215.54823303222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 153.40847778320312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18533.58203125
INFO:tools.evaluation_results_class:Current Best Return = -184.45176696777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.592346532022326
INFO:tools.evaluation_results_class:Counted Episodes = 3763
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -115.68421173095703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.3157958984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.82180786132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40997.34765625
INFO:tools.evaluation_results_class:Current Best Return = -115.68421173095703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.101867572156195
INFO:tools.evaluation_results_class:Counted Episodes = 3534
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.49337768554688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.50662231445312
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.44677734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23176.3671875
INFO:tools.evaluation_results_class:Current Best Return = -164.49337768554688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.22295805739515
INFO:tools.evaluation_results_class:Counted Episodes = 3624
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -183.0806121826172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 216.9193878173828
INFO:tools.evaluation_results_class:Average Discounted Reward = 155.75523376464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19648.01171875
INFO:tools.evaluation_results_class:Current Best Return = -183.0806121826172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.91808427113321
INFO:tools.evaluation_results_class:Counted Episodes = 3821
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -183.39231872558594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 216.60768127441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 155.1315155029297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19237.115234375
INFO:tools.evaluation_results_class:Current Best Return = -183.39231872558594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.206415987378385
INFO:tools.evaluation_results_class:Counted Episodes = 3803
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -185.83096313476562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 214.16903686523438
INFO:tools.evaluation_results_class:Average Discounted Reward = 152.7132110595703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20136.65234375
INFO:tools.evaluation_results_class:Current Best Return = -185.83096313476562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.202738283307006
INFO:tools.evaluation_results_class:Counted Episodes = 3798
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.10812377929688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.89187622070312
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.8008575439453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23145.439453125
INFO:tools.evaluation_results_class:Current Best Return = -165.10812377929688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.22816745217632
INFO:tools.evaluation_results_class:Counted Episodes = 3607
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -182.4879913330078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 217.5120086669922
INFO:tools.evaluation_results_class:Average Discounted Reward = 155.7323760986328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19856.3046875
INFO:tools.evaluation_results_class:Current Best Return = -182.4879913330078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.30625164864152
INFO:tools.evaluation_results_class:Counted Episodes = 3791
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.12841796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 211.87158203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 150.79568481445312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19654.025390625
INFO:tools.evaluation_results_class:Current Best Return = -188.12841796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.183157894736844
INFO:tools.evaluation_results_class:Counted Episodes = 3800
INFO:robust_rl.robust_rl_trainer:Iteration 12 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.3466339111328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.6533660888672
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.208740234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19207.482421875
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.77253446447508
INFO:tools.evaluation_results_class:Counted Episodes = 7544
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.580181121826172
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 8.59416675567627
INFO:agents.father_agent:Step: 10, Training loss: 9.98482894897461
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -158.04246520996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.95753479003906
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.6285858154297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19900.265625
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.37174821404502
INFO:tools.evaluation_results_class:Counted Episodes = 7419
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.24658203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.75341796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.5743408203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22270.189453125
INFO:tools.evaluation_results_class:Current Best Return = -166.24658203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.7666901905434
INFO:tools.evaluation_results_class:Counted Episodes = 7085
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -161.84207153320312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.15792846679688
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.85885620117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21444.640625
INFO:tools.evaluation_results_class:Current Best Return = -161.84207153320312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.833041369087724
INFO:tools.evaluation_results_class:Counted Episodes = 7421
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 184.73063891253003
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 13 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.02146911621094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.97853088378906
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.26805114746094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20010.21484375
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.20123423665146
INFO:tools.evaluation_results_class:Counted Episodes = 7454
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.864272117614746
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 8.64709186553955
INFO:agents.father_agent:Step: 10, Training loss: 8.716148376464844
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -161.4099884033203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.5900115966797
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.92845153808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21060.5625
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.11805648507563
INFO:tools.evaluation_results_class:Counted Episodes = 7471
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.41302490234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.58697509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.9005889892578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21138.326171875
INFO:tools.evaluation_results_class:Current Best Return = -165.41302490234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.84532577903683
INFO:tools.evaluation_results_class:Counted Episodes = 7060
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.16856384277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.83143615722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.0269012451172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20348.146484375
INFO:tools.evaluation_results_class:Current Best Return = -162.16856384277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.0830248545743
INFO:tools.evaluation_results_class:Counted Episodes = 7564
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 178.90010247507453
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 14 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -161.68553161621094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.31446838378906
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.25802612304688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19062.646484375
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.107090399252236
INFO:tools.evaluation_results_class:Counted Episodes = 7489
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.955097198486328
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 8.3695068359375
INFO:agents.father_agent:Step: 10, Training loss: 8.499616622924805
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -159.95570373535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.04429626464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.66348266601562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18737.052734375
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.250436183062675
INFO:tools.evaluation_results_class:Counted Episodes = 7451
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -161.39247131347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.60752868652344
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.2591552734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18758.958984375
INFO:tools.evaluation_results_class:Current Best Return = -161.39247131347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.19110096976612
INFO:tools.evaluation_results_class:Counted Episodes = 7012
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -163.07373046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.92626953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.08274841308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19341.033203125
INFO:tools.evaluation_results_class:Current Best Return = -163.07373046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.88831870357866
INFO:tools.evaluation_results_class:Counted Episodes = 7405
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 189.3238749608983
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 15 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.0353546142578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.9646453857422
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.36135864257812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19862.494140625
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.42051282051282
INFO:tools.evaluation_results_class:Counted Episodes = 7410
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.796555519104004
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 7.577728748321533
INFO:agents.father_agent:Step: 10, Training loss: 7.107139587402344
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -163.03501892089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.96498107910156
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.97793579101562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19845.41796875
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.52386747802569
INFO:tools.evaluation_results_class:Counted Episodes = 7395
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.1538848876953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.8461151123047
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.5762481689453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19512.349609375
INFO:tools.evaluation_results_class:Current Best Return = -165.1538848876953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.34841110793015
INFO:tools.evaluation_results_class:Counted Episodes = 6986
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.0823211669922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.9176788330078
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.01947021484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19572.572265625
INFO:tools.evaluation_results_class:Current Best Return = -164.0823211669922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.61544639271851
INFO:tools.evaluation_results_class:Counted Episodes = 7471
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 183.33218912919403
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 16 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.60116577148438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.39883422851562
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.30242919921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19860.935546875
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.68890697043667
INFO:tools.evaluation_results_class:Counted Episodes = 7374
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.146605491638184
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 6.657969951629639
INFO:agents.father_agent:Step: 10, Training loss: 6.496681213378906
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -158.84117126464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.15882873535156
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.652587890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19627.8984375
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.01012311901505
INFO:tools.evaluation_results_class:Counted Episodes = 7310
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.70074462890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.29925537109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.27316284179688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20474.91015625
INFO:tools.evaluation_results_class:Current Best Return = -162.70074462890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.0253017304057
INFO:tools.evaluation_results_class:Counted Episodes = 6877
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.0695343017578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.9304656982422
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.61268615722656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19970.04296875
INFO:tools.evaluation_results_class:Current Best Return = -159.0695343017578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.940858763165004
INFO:tools.evaluation_results_class:Counted Episodes = 7406
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 172.9566939759365
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -114.21910095214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.7808837890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.9908447265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21280.9453125
INFO:tools.evaluation_results_class:Current Best Return = -114.21910095214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.543649435124955
INFO:tools.evaluation_results_class:Counted Episodes = 2921
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -78.04462432861328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 321.95538330078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 254.0494384765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6969.92041015625
INFO:tools.evaluation_results_class:Current Best Return = -78.04462432861328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.935381048971315
INFO:tools.evaluation_results_class:Counted Episodes = 3451
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -89.73231506347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 310.2677001953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.0062713623047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11668.37890625
INFO:tools.evaluation_results_class:Current Best Return = -89.73231506347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.90667552308203
INFO:tools.evaluation_results_class:Counted Episodes = 3011
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -99.55377197265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.44622802734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.650146484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9325.2080078125
INFO:tools.evaluation_results_class:Current Best Return = -99.55377197265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.17456685191239
INFO:tools.evaluation_results_class:Counted Episodes = 3059
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.93275451660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.06724548339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.08319091796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13121.03515625
INFO:tools.evaluation_results_class:Current Best Return = -153.93275451660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.106086956521736
INFO:tools.evaluation_results_class:Counted Episodes = 3450
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -175.1149444580078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 224.8850555419922
INFO:tools.evaluation_results_class:Average Discounted Reward = 159.85369873046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17569.42578125
INFO:tools.evaluation_results_class:Current Best Return = -175.1149444580078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.70171012054948
INFO:tools.evaluation_results_class:Counted Episodes = 3567
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -176.19097900390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 223.80902099609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 158.48927307128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18406.814453125
INFO:tools.evaluation_results_class:Current Best Return = -176.19097900390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.93004231311706
INFO:tools.evaluation_results_class:Counted Episodes = 3545
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -176.54031372070312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 223.45968627929688
INFO:tools.evaluation_results_class:Average Discounted Reward = 158.45265197753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17154.87109375
INFO:tools.evaluation_results_class:Current Best Return = -176.54031372070312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.71556550951848
INFO:tools.evaluation_results_class:Counted Episodes = 3572
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -97.13066101074219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 302.8693542480469
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.5353240966797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37102.4140625
INFO:tools.evaluation_results_class:Current Best Return = -97.13066101074219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.49519890260631
INFO:tools.evaluation_results_class:Counted Episodes = 2916
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -172.93557739257812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 227.06442260742188
INFO:tools.evaluation_results_class:Average Discounted Reward = 163.04855346679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25664.052734375
INFO:tools.evaluation_results_class:Current Best Return = -172.93557739257812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.944462075531575
INFO:tools.evaluation_results_class:Counted Episodes = 3151
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -176.24493408203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 223.75506591796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 158.51724243164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17763.169921875
INFO:tools.evaluation_results_class:Current Best Return = -176.24493408203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.91544532130778
INFO:tools.evaluation_results_class:Counted Episodes = 3548
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -175.89569091796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 224.10430908203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 158.86904907226562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17605.59765625
INFO:tools.evaluation_results_class:Current Best Return = -175.89569091796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.91091062870031
INFO:tools.evaluation_results_class:Counted Episodes = 3547
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -174.48544311523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 225.51455688476562
INFO:tools.evaluation_results_class:Average Discounted Reward = 160.5817413330078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18178.2109375
INFO:tools.evaluation_results_class:Current Best Return = -174.48544311523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.7011751538892
INFO:tools.evaluation_results_class:Counted Episodes = 3574
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.31072998046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 230.68927001953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.445556640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24813.26953125
INFO:tools.evaluation_results_class:Current Best Return = -169.31072998046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.897160883280755
INFO:tools.evaluation_results_class:Counted Episodes = 3170
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -176.67543029785156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 223.32456970214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 158.01832580566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18371.419921875
INFO:tools.evaluation_results_class:Current Best Return = -176.67543029785156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.91751412429379
INFO:tools.evaluation_results_class:Counted Episodes = 3540
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -176.36366271972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 223.63633728027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 158.94252014160156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18360.1015625
INFO:tools.evaluation_results_class:Current Best Return = -176.36366271972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.47976555958694
INFO:tools.evaluation_results_class:Counted Episodes = 3583
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -176.3455352783203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 223.6544647216797
INFO:tools.evaluation_results_class:Average Discounted Reward = 158.8447265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17757.490234375
INFO:tools.evaluation_results_class:Current Best Return = -176.3455352783203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.638803466592115
INFO:tools.evaluation_results_class:Counted Episodes = 3577
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.46153259277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 221.53846740722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 156.9284210205078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19123.66796875
INFO:tools.evaluation_results_class:Current Best Return = -178.46153259277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.67664335664335
INFO:tools.evaluation_results_class:Counted Episodes = 3575
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.45712280273438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 226.54287719726562
INFO:tools.evaluation_results_class:Average Discounted Reward = 162.8221893310547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26549.806640625
INFO:tools.evaluation_results_class:Current Best Return = -173.45712280273438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.020330368487926
INFO:tools.evaluation_results_class:Counted Episodes = 3148
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.96949768066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.03050231933594
INFO:tools.evaluation_results_class:Average Discounted Reward = 164.72096252441406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25282.388671875
INFO:tools.evaluation_results_class:Current Best Return = -170.96949768066406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.980940279542565
INFO:tools.evaluation_results_class:Counted Episodes = 3148
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -179.37673950195312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 220.62326049804688
INFO:tools.evaluation_results_class:Average Discounted Reward = 156.10450744628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19447.822265625
INFO:tools.evaluation_results_class:Current Best Return = -179.37673950195312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.59286112660346
INFO:tools.evaluation_results_class:Counted Episodes = 3586
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -117.29442596435547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.70556640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.0709228515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22593.955078125
INFO:tools.evaluation_results_class:Current Best Return = -117.29442596435547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.64220453875626
INFO:tools.evaluation_results_class:Counted Episodes = 3393
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.09862518310547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 330.9013671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 265.04931640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6451.6748046875
INFO:tools.evaluation_results_class:Current Best Return = -69.09862518310547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.33450418805728
INFO:tools.evaluation_results_class:Counted Episodes = 3701
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -93.88631439208984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 306.1136779785156
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.64976501464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11047.763671875
INFO:tools.evaluation_results_class:Current Best Return = -93.88631439208984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.01128145791149
INFO:tools.evaluation_results_class:Counted Episodes = 3457
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -89.13423919677734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 310.8657531738281
INFO:tools.evaluation_results_class:Average Discounted Reward = 244.53701782226562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8475.904296875
INFO:tools.evaluation_results_class:Current Best Return = -89.13423919677734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.031177829099306
INFO:tools.evaluation_results_class:Counted Episodes = 3464
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.21910095214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.7809143066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.73545837402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12826.462890625
INFO:tools.evaluation_results_class:Current Best Return = -142.21910095214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.32621044089802
INFO:tools.evaluation_results_class:Counted Episodes = 3697
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -175.51980590820312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 224.48019409179688
INFO:tools.evaluation_results_class:Average Discounted Reward = 162.15216064453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17736.224609375
INFO:tools.evaluation_results_class:Current Best Return = -175.51980590820312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.548524328636
INFO:tools.evaluation_results_class:Counted Episodes = 3761
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.89144897460938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.10855102539062
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.39022827148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17225.029296875
INFO:tools.evaluation_results_class:Current Best Return = -170.89144897460938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.56581740976645
INFO:tools.evaluation_results_class:Counted Episodes = 3768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -175.06027221679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 224.93972778320312
INFO:tools.evaluation_results_class:Average Discounted Reward = 162.48867797851562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18046.99609375
INFO:tools.evaluation_results_class:Current Best Return = -175.06027221679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.649495485926714
INFO:tools.evaluation_results_class:Counted Episodes = 3766
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.2691879272461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.7308044433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.58070373535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 49010.2734375
INFO:tools.evaluation_results_class:Current Best Return = -119.2691879272461
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.185603807257586
INFO:tools.evaluation_results_class:Counted Episodes = 3362
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -163.5767059326172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.4232940673828
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.85679626464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24202.822265625
INFO:tools.evaluation_results_class:Current Best Return = -163.5767059326172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.04478474429356
INFO:tools.evaluation_results_class:Counted Episodes = 3461
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -172.0162353515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 227.9837646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 165.1187744140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17563.759765625
INFO:tools.evaluation_results_class:Current Best Return = -172.0162353515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.736491881820605
INFO:tools.evaluation_results_class:Counted Episodes = 3757
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.5950927734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.4049072265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.98104858398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17063.30859375
INFO:tools.evaluation_results_class:Current Best Return = -170.5950927734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.42525092445853
INFO:tools.evaluation_results_class:Counted Episodes = 3786
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -172.24575805664062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 227.75424194335938
INFO:tools.evaluation_results_class:Average Discounted Reward = 165.02749633789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16447.1796875
INFO:tools.evaluation_results_class:Current Best Return = -172.24575805664062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.56900212314225
INFO:tools.evaluation_results_class:Counted Episodes = 3768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.5730743408203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.4269256591797
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.58383178710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25347.384765625
INFO:tools.evaluation_results_class:Current Best Return = -164.5730743408203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.815713460427496
INFO:tools.evaluation_results_class:Counted Episodes = 3462
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.959228515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 226.040771484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 163.73751831054688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16301.6171875
INFO:tools.evaluation_results_class:Current Best Return = -173.959228515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.493248610007946
INFO:tools.evaluation_results_class:Counted Episodes = 3777
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -176.38136291503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 223.61863708496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 161.5665740966797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18417.576171875
INFO:tools.evaluation_results_class:Current Best Return = -176.38136291503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.55095541401274
INFO:tools.evaluation_results_class:Counted Episodes = 3768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.7559814453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 226.2440185546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 163.77725219726562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17393.619140625
INFO:tools.evaluation_results_class:Current Best Return = -173.7559814453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.686337054758106
INFO:tools.evaluation_results_class:Counted Episodes = 3762
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -171.74600219726562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 228.25399780273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 165.53602600097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17678.443359375
INFO:tools.evaluation_results_class:Current Best Return = -171.74600219726562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.66488794023479
INFO:tools.evaluation_results_class:Counted Episodes = 3748
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.9062042236328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.0937957763672
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.66961669921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25844.01171875
INFO:tools.evaluation_results_class:Current Best Return = -164.9062042236328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.5101515584787
INFO:tools.evaluation_results_class:Counted Episodes = 3497
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.67898559570312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.32101440429688
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.2236785888672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26887.734375
INFO:tools.evaluation_results_class:Current Best Return = -166.67898559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.534936998854526
INFO:tools.evaluation_results_class:Counted Episodes = 3492
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.57872009277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 226.42127990722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 164.61947631835938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16799.873046875
INFO:tools.evaluation_results_class:Current Best Return = -173.57872009277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.16692996313849
INFO:tools.evaluation_results_class:Counted Episodes = 3798
INFO:robust_rl.robust_rl_trainer:Iteration 17 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.62989807128906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.37010192871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.4866180419922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19441.548828125
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.968464163822524
INFO:tools.evaluation_results_class:Counted Episodes = 7325
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.863338470458984
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 5.762004375457764
INFO:agents.father_agent:Step: 10, Training loss: 5.767354488372803
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -158.43087768554688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.56912231445312
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.6586456298828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19248.19140625
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.145412023662125
INFO:tools.evaluation_results_class:Counted Episodes = 7269
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.33328247070312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.66671752929688
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.71243286132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20628.427734375
INFO:tools.evaluation_results_class:Current Best Return = -165.33328247070312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.26206493126645
INFO:tools.evaluation_results_class:Counted Episodes = 6838
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.31800842285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.68199157714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.93789672851562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18983.509765625
INFO:tools.evaluation_results_class:Current Best Return = -155.31800842285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.94310041897554
INFO:tools.evaluation_results_class:Counted Episodes = 7399
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 171.84010364800292
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 18 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.8594207763672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.1405792236328
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.7947998046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18448.07421875
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.095323000960086
INFO:tools.evaluation_results_class:Counted Episodes = 7291
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.843870639801025
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 5.534638404846191
INFO:agents.father_agent:Step: 10, Training loss: 5.035128116607666
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -156.64002990722656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.35997009277344
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.60427856445312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18993.62890625
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.687973997833154
INFO:tools.evaluation_results_class:Counted Episodes = 7384
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -161.87046813964844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.12953186035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.41842651367188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19930.1328125
INFO:tools.evaluation_results_class:Current Best Return = -161.87046813964844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.62757824895428
INFO:tools.evaluation_results_class:Counted Episodes = 6933
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.7849884033203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.2150115966797
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.81568908691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19678.46875
INFO:tools.evaluation_results_class:Current Best Return = -157.7849884033203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.7719250976694
INFO:tools.evaluation_results_class:Counted Episodes = 7423
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 169.32189668218217
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 19 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.6211395263672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.3788604736328
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.91661071777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19133.166015625
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.88546915429661
INFO:tools.evaluation_results_class:Counted Episodes = 7343
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.650548458099365
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 5.027201175689697
INFO:agents.father_agent:Step: 10, Training loss: 4.381374835968018
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -155.65638732910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.34361267089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.98057556152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18692.154296875
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.82580821170372
INFO:tools.evaluation_results_class:Counted Episodes = 7331
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.12460327148438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.87539672851562
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.5589599609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19298.380859375
INFO:tools.evaluation_results_class:Current Best Return = -162.12460327148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.58388489208633
INFO:tools.evaluation_results_class:Counted Episodes = 6950
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.05795288085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.94204711914062
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.91024780273438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19095.501953125
INFO:tools.evaluation_results_class:Current Best Return = -160.05795288085938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.98659444820582
INFO:tools.evaluation_results_class:Counted Episodes = 7385
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 171.15119661927983
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 20 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -161.30618286132812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.69381713867188
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.63465881347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20194.716796875
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.30770290753755
INFO:tools.evaluation_results_class:Counted Episodes = 7257
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.9584717750549316
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 3.7086143493652344
INFO:agents.father_agent:Step: 10, Training loss: 3.544426202774048
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -152.28672790527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.71327209472656
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.3733367919922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18619.15234375
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.75997277059224
INFO:tools.evaluation_results_class:Counted Episodes = 7345
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.7458038330078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.2541961669922
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.48422241210938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19078.162109375
INFO:tools.evaluation_results_class:Current Best Return = -156.7458038330078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.04068278805121
INFO:tools.evaluation_results_class:Counted Episodes = 7030
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.70399475097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.29600524902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.96533203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18969.64453125
INFO:tools.evaluation_results_class:Current Best Return = -157.70399475097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.4615795803822
INFO:tools.evaluation_results_class:Counted Episodes = 7483
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 164.24155087979685
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 21 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.83712768554688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.16287231445312
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.0562744140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19051.1875
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.82253444277725
INFO:tools.evaluation_results_class:Counted Episodes = 7331
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.4726881980895996
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 3.1456077098846436
INFO:agents.father_agent:Step: 10, Training loss: 2.622437000274658
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -149.6998748779297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.3001251220703
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.03550720214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18000.19921875
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.16453959105256
INFO:tools.evaluation_results_class:Counted Episodes = 7287
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.10585021972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.89414978027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.80026245117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18498.77734375
INFO:tools.evaluation_results_class:Current Best Return = -158.10585021972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.04879783753023
INFO:tools.evaluation_results_class:Counted Episodes = 7029
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.9934539794922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.0065460205078
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.0911102294922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19567.75390625
INFO:tools.evaluation_results_class:Current Best Return = -154.9934539794922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.32541956610724
INFO:tools.evaluation_results_class:Counted Episodes = 7329
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 169.08834343730692
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -110.55654907226562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.4434509277344
INFO:tools.evaluation_results_class:Average Discounted Reward = 219.7246551513672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19341.599609375
INFO:tools.evaluation_results_class:Current Best Return = -110.55654907226562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.58851839119973
INFO:tools.evaluation_results_class:Counted Episodes = 2909
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.4265365600586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 322.5734558105469
INFO:tools.evaluation_results_class:Average Discounted Reward = 253.89007568359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6863.53515625
INFO:tools.evaluation_results_class:Current Best Return = -77.4265365600586
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.2150130928135
INFO:tools.evaluation_results_class:Counted Episodes = 3437
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.1092529296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 318.8907470703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.24501037597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10308.275390625
INFO:tools.evaluation_results_class:Current Best Return = -81.1092529296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.29490616621984
INFO:tools.evaluation_results_class:Counted Episodes = 2984
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -97.5103530883789
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 302.4896545410156
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.10350036621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9018.390625
INFO:tools.evaluation_results_class:Current Best Return = -97.5103530883789
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.98447606727038
INFO:tools.evaluation_results_class:Counted Episodes = 3092
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -144.2701873779297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.7298126220703
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.34115600585938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12048.9853515625
INFO:tools.evaluation_results_class:Current Best Return = -144.2701873779297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.35722411831627
INFO:tools.evaluation_results_class:Counted Episodes = 3516
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -168.74508666992188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 231.25491333007812
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.81077575683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18447.384765625
INFO:tools.evaluation_results_class:Current Best Return = -168.74508666992188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.596069868995635
INFO:tools.evaluation_results_class:Counted Episodes = 3664
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.5373992919922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.4626007080078
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.2526092529297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17401.615234375
INFO:tools.evaluation_results_class:Current Best Return = -165.5373992919922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.93729372937294
INFO:tools.evaluation_results_class:Counted Episodes = 3636
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.46826171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17468.1328125
INFO:tools.evaluation_results_class:Current Best Return = -166.234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.109887083448086
INFO:tools.evaluation_results_class:Counted Episodes = 3631
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.806396484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 319.193603515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.92495727539062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30285.01171875
INFO:tools.evaluation_results_class:Current Best Return = -80.806396484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.845397048489104
INFO:tools.evaluation_results_class:Counted Episodes = 2846
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.37010192871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.62989807128906
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.29190063476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23021.419921875
INFO:tools.evaluation_results_class:Current Best Return = -159.37010192871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.59592215013902
INFO:tools.evaluation_results_class:Counted Episodes = 3237
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -167.04164123535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 232.95835876464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.88705444335938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17911.296875
INFO:tools.evaluation_results_class:Current Best Return = -167.04164123535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.054605626034196
INFO:tools.evaluation_results_class:Counted Episodes = 3626
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.3754119873047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 230.6245880126953
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.70632934570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18344.193359375
INFO:tools.evaluation_results_class:Current Best Return = -169.3754119873047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.997799779977996
INFO:tools.evaluation_results_class:Counted Episodes = 3636
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.52801513671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.47198486328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.0656280517578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17221.859375
INFO:tools.evaluation_results_class:Current Best Return = -165.52801513671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.10681755451284
INFO:tools.evaluation_results_class:Counted Episodes = 3623
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.1773681640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.8226318359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.53662109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22482.951171875
INFO:tools.evaluation_results_class:Current Best Return = -159.1773681640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.85488372093023
INFO:tools.evaluation_results_class:Counted Episodes = 3225
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -174.8831329345703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 225.1168670654297
INFO:tools.evaluation_results_class:Average Discounted Reward = 161.4506378173828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18305.875
INFO:tools.evaluation_results_class:Current Best Return = -174.8831329345703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.01212789415656
INFO:tools.evaluation_results_class:Counted Episodes = 3628
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.52536010742188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 230.47463989257812
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.0791778564453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18911.439453125
INFO:tools.evaluation_results_class:Current Best Return = -169.52536010742188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.28401219174287
INFO:tools.evaluation_results_class:Counted Episodes = 3609
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.7677459716797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.2322540283203
INFO:tools.evaluation_results_class:Average Discounted Reward = 165.07473754882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17749.8671875
INFO:tools.evaluation_results_class:Current Best Return = -170.7677459716797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.06655619994476
INFO:tools.evaluation_results_class:Counted Episodes = 3621
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.28602600097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.71397399902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.8566436767578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16408.490234375
INFO:tools.evaluation_results_class:Current Best Return = -164.28602600097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.74499039253362
INFO:tools.evaluation_results_class:Counted Episodes = 3643
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.1212921142578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.8787078857422
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.09146118164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22049.90625
INFO:tools.evaluation_results_class:Current Best Return = -160.1212921142578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.32863705972435
INFO:tools.evaluation_results_class:Counted Episodes = 3265
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.2175750732422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.7824249267578
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.213623046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20736.150390625
INFO:tools.evaluation_results_class:Current Best Return = -155.2175750732422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.466502765826675
INFO:tools.evaluation_results_class:Counted Episodes = 3254
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.5650634765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.4349365234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 165.70864868164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17601.146484375
INFO:tools.evaluation_results_class:Current Best Return = -170.5650634765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.96011004126547
INFO:tools.evaluation_results_class:Counted Episodes = 3635
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.10572814941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 230.89427185058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.1903533935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17563.61328125
INFO:tools.evaluation_results_class:Current Best Return = -169.10572814941406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.77293892084361
INFO:tools.evaluation_results_class:Counted Episodes = 3651
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -151.72178649902344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.27821350097656
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.54388427734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21502.564453125
INFO:tools.evaluation_results_class:Current Best Return = -151.72178649902344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.199511897498475
INFO:tools.evaluation_results_class:Counted Episodes = 3278
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.13626098632812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.86373901367188
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.84759521484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21015.537109375
INFO:tools.evaluation_results_class:Current Best Return = -153.13626098632812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.83276556209353
INFO:tools.evaluation_results_class:Counted Episodes = 3229
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.4404296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 230.5595703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.51235961914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16364.7998046875
INFO:tools.evaluation_results_class:Current Best Return = -169.4404296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.052399338113624
INFO:tools.evaluation_results_class:Counted Episodes = 3626
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -171.75985717773438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 228.24014282226562
INFO:tools.evaluation_results_class:Average Discounted Reward = 164.333984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18767.25390625
INFO:tools.evaluation_results_class:Current Best Return = -171.75985717773438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.0206782464847
INFO:tools.evaluation_results_class:Counted Episodes = 3627
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -116.52108764648438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.4789123535156
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.14842224121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20727.9765625
INFO:tools.evaluation_results_class:Current Best Return = -116.52108764648438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.62272993555946
INFO:tools.evaluation_results_class:Counted Episodes = 3414
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.67154693603516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 330.3284606933594
INFO:tools.evaluation_results_class:Average Discounted Reward = 265.3857421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6659.416015625
INFO:tools.evaluation_results_class:Current Best Return = -69.67154693603516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.72690976843226
INFO:tools.evaluation_results_class:Counted Episodes = 3757
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -90.33391571044922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 309.66607666015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.14334106445312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10089.5390625
INFO:tools.evaluation_results_class:Current Best Return = -90.33391571044922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.32399299474606
INFO:tools.evaluation_results_class:Counted Episodes = 3426
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -88.58033752441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 311.419677734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.4785614013672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8447.8955078125
INFO:tools.evaluation_results_class:Current Best Return = -88.58033752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.082366589327144
INFO:tools.evaluation_results_class:Counted Episodes = 3448
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -140.0519561767578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.94805908203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.2504425048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13572.28515625
INFO:tools.evaluation_results_class:Current Best Return = -140.0519561767578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.883237626469786
INFO:tools.evaluation_results_class:Counted Episodes = 3657
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.35780334472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.64219665527344
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.6097412109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17738.494140625
INFO:tools.evaluation_results_class:Current Best Return = -165.35780334472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.042093901780895
INFO:tools.evaluation_results_class:Counted Episodes = 3706
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -168.4519500732422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 231.5480499267578
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.18251037597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18175.283203125
INFO:tools.evaluation_results_class:Current Best Return = -168.4519500732422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.812066203950884
INFO:tools.evaluation_results_class:Counted Episodes = 3746
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -167.36790466308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 232.63209533691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.7563934326172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17744.400390625
INFO:tools.evaluation_results_class:Current Best Return = -167.36790466308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.044611663531306
INFO:tools.evaluation_results_class:Counted Episodes = 3721
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -110.47242736816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.527587890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.0579071044922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40277.19140625
INFO:tools.evaluation_results_class:Current Best Return = -110.47242736816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.824830433500445
INFO:tools.evaluation_results_class:Counted Episodes = 3391
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -150.2502899169922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.7497100830078
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.91905212402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21608.126953125
INFO:tools.evaluation_results_class:Current Best Return = -150.2502899169922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.644366197183096
INFO:tools.evaluation_results_class:Counted Episodes = 3408
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -167.7267303466797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 232.2732696533203
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.5653533935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17710.12109375
INFO:tools.evaluation_results_class:Current Best Return = -167.7267303466797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.92973987664253
INFO:tools.evaluation_results_class:Counted Episodes = 3729
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -167.62176513671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 232.37823486328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.50714111328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18327.1328125
INFO:tools.evaluation_results_class:Current Best Return = -167.62176513671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.077586206896555
INFO:tools.evaluation_results_class:Counted Episodes = 3712
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -171.465087890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 228.534912109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.63026428222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19099.86328125
INFO:tools.evaluation_results_class:Current Best Return = -171.465087890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.65565031982943
INFO:tools.evaluation_results_class:Counted Episodes = 3752
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.77606201171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.22393798828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.9380340576172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23825.66796875
INFO:tools.evaluation_results_class:Current Best Return = -153.77606201171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.45284671532847
INFO:tools.evaluation_results_class:Counted Episodes = 3425
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.11087036132812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 230.88912963867188
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.47161865234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18834.853515625
INFO:tools.evaluation_results_class:Current Best Return = -169.11087036132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.88109266202464
INFO:tools.evaluation_results_class:Counted Episodes = 3734
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.4473114013672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.5526885986328
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.21499633789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17706.16015625
INFO:tools.evaluation_results_class:Current Best Return = -166.4473114013672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.56410937085214
INFO:tools.evaluation_results_class:Counted Episodes = 3767
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.66595458984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.33404541015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.81044006347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18887.662109375
INFO:tools.evaluation_results_class:Current Best Return = -170.66595458984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.9726321438154
INFO:tools.evaluation_results_class:Counted Episodes = 3727
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -168.6199493408203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 231.3800506591797
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.7993621826172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17233.0703125
INFO:tools.evaluation_results_class:Current Best Return = -168.6199493408203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.835784969243115
INFO:tools.evaluation_results_class:Counted Episodes = 3739
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.4757537841797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.5242462158203
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.4995574951172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23909.091796875
INFO:tools.evaluation_results_class:Current Best Return = -157.4757537841797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.637378783426385
INFO:tools.evaluation_results_class:Counted Episodes = 3403
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.14479064941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.85520935058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.6306915283203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24118.447265625
INFO:tools.evaluation_results_class:Current Best Return = -154.14479064941406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.577960140679956
INFO:tools.evaluation_results_class:Counted Episodes = 3412
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.6893768310547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.3106231689453
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.08607482910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18227.78515625
INFO:tools.evaluation_results_class:Current Best Return = -170.6893768310547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.91630901287554
INFO:tools.evaluation_results_class:Counted Episodes = 3728
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.29844665527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.70155334472656
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.80325317382812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17699.732421875
INFO:tools.evaluation_results_class:Current Best Return = -164.29844665527344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.88083735909823
INFO:tools.evaluation_results_class:Counted Episodes = 3726
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.37303161621094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.62696838378906
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.5302734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23794.677734375
INFO:tools.evaluation_results_class:Current Best Return = -155.37303161621094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.524868344060856
INFO:tools.evaluation_results_class:Counted Episodes = 3418
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.35401916503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.64598083496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.52496337890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23199.3671875
INFO:tools.evaluation_results_class:Current Best Return = -156.35401916503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.54612005856515
INFO:tools.evaluation_results_class:Counted Episodes = 3415
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.7017822265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.2982177734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.93788146972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17751.515625
INFO:tools.evaluation_results_class:Current Best Return = -166.7017822265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.71885836222993
INFO:tools.evaluation_results_class:Counted Episodes = 3749
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -171.4840850830078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 228.5159149169922
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.3009796142578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17954.353515625
INFO:tools.evaluation_results_class:Current Best Return = -171.4840850830078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.73843273602567
INFO:tools.evaluation_results_class:Counted Episodes = 3739
INFO:robust_rl.robust_rl_trainer:Iteration 22 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -149.9889678955078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.0110321044922
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.1841278076172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17877.177734375
INFO:tools.evaluation_results_class:Current Best Return = -149.17591857910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.80555555555556
INFO:tools.evaluation_results_class:Counted Episodes = 7344
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.6448452472686768
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Step: 5, Training loss: 2.577685832977295
INFO:agents.father_agent:Step: 10, Training loss: 2.0876071453094482
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -147.05751037597656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.94248962402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.45741271972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17610.5
INFO:tools.evaluation_results_class:Current Best Return = -147.05751037597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.32942760942761
INFO:tools.evaluation_results_class:Counted Episodes = 7425
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.26849365234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.73150634765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.32357788085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17977.6171875
INFO:tools.evaluation_results_class:Current Best Return = -155.26849365234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.33561739616837
INFO:tools.evaluation_results_class:Counted Episodes = 7151
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.56304931640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.43695068359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.16519165039062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19925.990234375
INFO:tools.evaluation_results_class:Current Best Return = -157.56304931640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.00310852817948
INFO:tools.evaluation_results_class:Counted Episodes = 7399
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 165.55099122740037
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 23 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -149.2386932373047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.7613067626953
INFO:tools.evaluation_results_class:Average Discounted Reward = 188.8228759765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18462.822265625
INFO:tools.evaluation_results_class:Current Best Return = -147.05751037597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.139074618882056
INFO:tools.evaluation_results_class:Counted Episodes = 7478
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.0415148735046387
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Step: 5, Training loss: 1.8387149572372437
INFO:agents.father_agent:Step: 10, Training loss: 1.6516532897949219
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -148.93966674804688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.06033325195312
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.2641143798828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17494.115234375
INFO:tools.evaluation_results_class:Current Best Return = -147.05751037597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.11157190635451
INFO:tools.evaluation_results_class:Counted Episodes = 7475
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.89547729492188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.10452270507812
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.0397491455078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18254.365234375
INFO:tools.evaluation_results_class:Current Best Return = -154.89547729492188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.0911744266852
INFO:tools.evaluation_results_class:Counted Episodes = 7195
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.473876953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.526123046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.70980834960938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18344.099609375
INFO:tools.evaluation_results_class:Current Best Return = -156.473876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.51236135239877
INFO:tools.evaluation_results_class:Counted Episodes = 7483
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 170.97059439048195
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 24 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -147.7942352294922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.2057647705078
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.46998596191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17543.484375
INFO:tools.evaluation_results_class:Current Best Return = -147.05751037597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.14655056932351
INFO:tools.evaluation_results_class:Counted Episodes = 7465
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.0659408569335938
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 1.5251151323318481
INFO:agents.father_agent:Step: 10, Training loss: 1.7302147150039673
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -144.1297149658203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.8702850341797
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.5911102294922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17643.62109375
INFO:tools.evaluation_results_class:Current Best Return = -144.1297149658203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.3997849173276
INFO:tools.evaluation_results_class:Counted Episodes = 7439
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.24380493164062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.75619506835938
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.19105529785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18397.5859375
INFO:tools.evaluation_results_class:Current Best Return = -153.24380493164062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.941266103338414
INFO:tools.evaluation_results_class:Counted Episodes = 7219
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.43536376953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.56463623046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.84796142578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20725.564453125
INFO:tools.evaluation_results_class:Current Best Return = -157.43536376953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.04010840108401
INFO:tools.evaluation_results_class:Counted Episodes = 7380
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 160.81612036313487
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 25 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -146.85072326660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.14927673339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.295654296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17900.263671875
INFO:tools.evaluation_results_class:Current Best Return = -144.1297149658203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.388477587831474
INFO:tools.evaluation_results_class:Counted Episodes = 7429
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.6105327606201172
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 2.072929859161377
INFO:agents.father_agent:Step: 10, Training loss: 1.7166812419891357
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -139.6320343017578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.3679504394531
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.59255981445312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16976.455078125
INFO:tools.evaluation_results_class:Current Best Return = -139.6320343017578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.603896103896105
INFO:tools.evaluation_results_class:Counted Episodes = 7392
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -149.94326782226562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.05673217773438
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.2260284423828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18571.912109375
INFO:tools.evaluation_results_class:Current Best Return = -149.94326782226562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.15408149075233
INFO:tools.evaluation_results_class:Counted Episodes = 7191
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -148.94737243652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.05262756347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.33291625976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18989.84375
INFO:tools.evaluation_results_class:Current Best Return = -148.94737243652344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.47629223988246
INFO:tools.evaluation_results_class:Counted Episodes = 7487
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=3
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 159.0815407577105
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=3
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 26 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.85565185546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.14434814453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.5106658935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16332.8388671875
INFO:tools.evaluation_results_class:Current Best Return = -138.85565185546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.58415707515233
INFO:tools.evaluation_results_class:Counted Episodes = 7385
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.7606921195983887
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 1.6236172914505005
INFO:agents.father_agent:Step: 10, Training loss: 1.8167989253997803
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -136.67698669433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.322998046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.4884796142578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15474.4130859375
INFO:tools.evaluation_results_class:Current Best Return = -136.67698669433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.76559315124337
INFO:tools.evaluation_results_class:Counted Episodes = 7359
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.05130004882812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.94869995117188
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.7196044921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16938.4765625
INFO:tools.evaluation_results_class:Current Best Return = -145.05130004882812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.32033542976939
INFO:tools.evaluation_results_class:Counted Episodes = 7155
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -150.58535766601562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.41464233398438
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.91403198242188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18071.3203125
INFO:tools.evaluation_results_class:Current Best Return = -150.58535766601562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.55348774936404
INFO:tools.evaluation_results_class:Counted Episodes = 7469
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=3
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 160.65190829764663
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=3
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -106.3013916015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.6986083984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.8944091796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19451.669921875
INFO:tools.evaluation_results_class:Current Best Return = -106.3013916015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.76536682088566
INFO:tools.evaluation_results_class:Counted Episodes = 3026
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.86456298828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 318.13543701171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.21810913085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7410.84375
INFO:tools.evaluation_results_class:Current Best Return = -81.86456298828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.42042042042042
INFO:tools.evaluation_results_class:Counted Episodes = 3330
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.67610168457031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 318.3238830566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.57994079589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10200.4775390625
INFO:tools.evaluation_results_class:Current Best Return = -81.67610168457031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.64911125740619
INFO:tools.evaluation_results_class:Counted Episodes = 3038
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -93.70516204833984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 306.2948303222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.78919982910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9067.14453125
INFO:tools.evaluation_results_class:Current Best Return = -93.70516204833984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.355555555555554
INFO:tools.evaluation_results_class:Counted Episodes = 3195
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -133.97348022460938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.0265197753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.5968475341797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11487.2802734375
INFO:tools.evaluation_results_class:Current Best Return = -133.97348022460938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.46079270031366
INFO:tools.evaluation_results_class:Counted Episodes = 3507
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.2968292236328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.7031707763672
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.1869659423828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17235.48046875
INFO:tools.evaluation_results_class:Current Best Return = -157.2968292236328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.075310344827585
INFO:tools.evaluation_results_class:Counted Episodes = 3625
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.3344268798828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.6655731201172
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.08645629882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16479.498046875
INFO:tools.evaluation_results_class:Current Best Return = -154.3344268798828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.043693322341305
INFO:tools.evaluation_results_class:Counted Episodes = 3639
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.79934692382812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.20065307617188
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.52418518066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16362.658203125
INFO:tools.evaluation_results_class:Current Best Return = -155.79934692382812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.02803738317757
INFO:tools.evaluation_results_class:Counted Episodes = 3638
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.57351684570312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 316.4264831542969
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.41937255859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33864.66796875
INFO:tools.evaluation_results_class:Current Best Return = -83.57351684570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.24642614023145
INFO:tools.evaluation_results_class:Counted Episodes = 2938
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.6262969970703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.37371826171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.0871124267578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19743.984375
INFO:tools.evaluation_results_class:Current Best Return = -143.6262969970703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.862333825701626
INFO:tools.evaluation_results_class:Counted Episodes = 3385
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.06907653808594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.93092346191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.75656127929688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16304.341796875
INFO:tools.evaluation_results_class:Current Best Return = -156.06907653808594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.73684210526316
INFO:tools.evaluation_results_class:Counted Episodes = 3648
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -151.10276794433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.89723205566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 184.17454528808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15421.162109375
INFO:tools.evaluation_results_class:Current Best Return = -151.10276794433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.86782082989832
INFO:tools.evaluation_results_class:Counted Episodes = 3639
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -152.87875366210938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.12124633789062
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.3844451904297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16054.2578125
INFO:tools.evaluation_results_class:Current Best Return = -152.87875366210938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.023422430421604
INFO:tools.evaluation_results_class:Counted Episodes = 3629
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.7372283935547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.26275634765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.15858459472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19937.34765625
INFO:tools.evaluation_results_class:Current Best Return = -143.7372283935547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.68995889606577
INFO:tools.evaluation_results_class:Counted Episodes = 3406
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.39663696289062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.60336303710938
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.31788635253906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16797.791015625
INFO:tools.evaluation_results_class:Current Best Return = -158.39663696289062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.929810074318745
INFO:tools.evaluation_results_class:Counted Episodes = 3633
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.97430419921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.02569580078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.92947387695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15874.5234375
INFO:tools.evaluation_results_class:Current Best Return = -153.97430419921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.727173318753415
INFO:tools.evaluation_results_class:Counted Episodes = 3658
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.5835418701172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.4164581298828
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.96470642089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16132.2373046875
INFO:tools.evaluation_results_class:Current Best Return = -153.5835418701172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.929577464788736
INFO:tools.evaluation_results_class:Counted Episodes = 3621
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -148.6812286376953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.3187713623047
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.44189453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15533.9892578125
INFO:tools.evaluation_results_class:Current Best Return = -148.6812286376953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.96455070074196
INFO:tools.evaluation_results_class:Counted Episodes = 3639
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -144.69151306152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.30848693847656
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.17149353027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20750.58984375
INFO:tools.evaluation_results_class:Current Best Return = -144.69151306152344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.94853593611358
INFO:tools.evaluation_results_class:Counted Episodes = 3381
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.96803283691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.03196716308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.16110229492188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20899.9375
INFO:tools.evaluation_results_class:Current Best Return = -145.96803283691406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.69199178644764
INFO:tools.evaluation_results_class:Counted Episodes = 3409
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.7472381591797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.2527618408203
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.56915283203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15366.65625
INFO:tools.evaluation_results_class:Current Best Return = -154.7472381591797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.117127071823205
INFO:tools.evaluation_results_class:Counted Episodes = 3620
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.07861328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.92138671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.5726318359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17082.103515625
INFO:tools.evaluation_results_class:Current Best Return = -159.07861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.030068965517245
INFO:tools.evaluation_results_class:Counted Episodes = 3625
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.25987243652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.7401428222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.66879272460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20126.642578125
INFO:tools.evaluation_results_class:Current Best Return = -143.25987243652344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.79846788450206
INFO:tools.evaluation_results_class:Counted Episodes = 3394
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.10829162597656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.8917236328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.12899780273438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18537.197265625
INFO:tools.evaluation_results_class:Current Best Return = -141.10829162597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.85659486574211
INFO:tools.evaluation_results_class:Counted Episodes = 3389
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.76986694335938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.23013305664062
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.38671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15632.3525390625
INFO:tools.evaluation_results_class:Current Best Return = -153.76986694335938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.310994184436446
INFO:tools.evaluation_results_class:Counted Episodes = 3611
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.48480224609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.51519775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.89700317382812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16855.822265625
INFO:tools.evaluation_results_class:Current Best Return = -157.48480224609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.238674033149174
INFO:tools.evaluation_results_class:Counted Episodes = 3620
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.04156494140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.95843505859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.38125610351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16635.734375
INFO:tools.evaluation_results_class:Current Best Return = -157.04156494140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.014588494357284
INFO:tools.evaluation_results_class:Counted Episodes = 3633
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -140.7690887451172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.23089599609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.3544158935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19271.716796875
INFO:tools.evaluation_results_class:Current Best Return = -140.7690887451172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.72737955346651
INFO:tools.evaluation_results_class:Counted Episodes = 3404
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -139.94747924804688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.0525207519531
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.7130584716797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17988.91796875
INFO:tools.evaluation_results_class:Current Best Return = -139.94747924804688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.512910798122064
INFO:tools.evaluation_results_class:Counted Episodes = 3408
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.12417602539062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.87582397460938
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.9102325439453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15053.9033203125
INFO:tools.evaluation_results_class:Current Best Return = -153.12417602539062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.7887323943662
INFO:tools.evaluation_results_class:Counted Episodes = 3479
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.5941925048828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.4058074951172
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.24664306640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15464.1513671875
INFO:tools.evaluation_results_class:Current Best Return = -155.5941925048828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.234252539912916
INFO:tools.evaluation_results_class:Counted Episodes = 3445
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.1378402709961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.8621520996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.6622314453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22080.09765625
INFO:tools.evaluation_results_class:Current Best Return = -119.1378402709961
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.89728453364817
INFO:tools.evaluation_results_class:Counted Episodes = 3388
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.75755310058594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 326.2424621582031
INFO:tools.evaluation_results_class:Average Discounted Reward = 259.0747985839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7403.8623046875
INFO:tools.evaluation_results_class:Current Best Return = -73.75755310058594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.616331096196866
INFO:tools.evaluation_results_class:Counted Episodes = 3576
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -94.59164428710938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 305.4083557128906
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.35362243652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10314.09765625
INFO:tools.evaluation_results_class:Current Best Return = -94.59164428710938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.805763688760806
INFO:tools.evaluation_results_class:Counted Episodes = 3470
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -95.74915313720703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 304.2508544921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.59271240234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10204.29296875
INFO:tools.evaluation_results_class:Current Best Return = -95.74915313720703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.029312288613305
INFO:tools.evaluation_results_class:Counted Episodes = 3548
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.5055694580078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.49444580078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.7450408935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12979.4814453125
INFO:tools.evaluation_results_class:Current Best Return = -136.5055694580078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.39391469709318
INFO:tools.evaluation_results_class:Counted Episodes = 3681
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.48538208007812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.51461791992188
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.3555145263672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16132.6650390625
INFO:tools.evaluation_results_class:Current Best Return = -156.48538208007812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.0166264414052
INFO:tools.evaluation_results_class:Counted Episodes = 3729
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.77651977539062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.22348022460938
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.31382751464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17833.251953125
INFO:tools.evaluation_results_class:Current Best Return = -162.77651977539062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.09800753904146
INFO:tools.evaluation_results_class:Counted Episodes = 3714
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.3455810546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.6544189453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.93905639648438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15859.439453125
INFO:tools.evaluation_results_class:Current Best Return = -160.3455810546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.84986595174263
INFO:tools.evaluation_results_class:Counted Episodes = 3730
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -114.53424072265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.46575927734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.8846893310547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42674.671875
INFO:tools.evaluation_results_class:Current Best Return = -114.53424072265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.96501630595909
INFO:tools.evaluation_results_class:Counted Episodes = 3373
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -139.98806762695312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.0119323730469
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.38023376464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19567.01953125
INFO:tools.evaluation_results_class:Current Best Return = -139.98806762695312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.34406215316315
INFO:tools.evaluation_results_class:Counted Episodes = 3604
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.63507080078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.36492919921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.2227783203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16424.8046875
INFO:tools.evaluation_results_class:Current Best Return = -156.63507080078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.046186895810955
INFO:tools.evaluation_results_class:Counted Episodes = 3724
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.63272094726562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.36727905273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.49571228027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16842.228515625
INFO:tools.evaluation_results_class:Current Best Return = -156.63272094726562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.80593424218124
INFO:tools.evaluation_results_class:Counted Episodes = 3741
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.56942749023438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.43057250976562
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.30096435546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16118.251953125
INFO:tools.evaluation_results_class:Current Best Return = -159.56942749023438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.037206794284174
INFO:tools.evaluation_results_class:Counted Episodes = 3709
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.8791961669922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.1208038330078
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.75900268554688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20574.810546875
INFO:tools.evaluation_results_class:Current Best Return = -145.8791961669922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.624161073825505
INFO:tools.evaluation_results_class:Counted Episodes = 3576
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.4091033935547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.5908966064453
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.39871215820312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16446.728515625
INFO:tools.evaluation_results_class:Current Best Return = -158.4091033935547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.13196875841638
INFO:tools.evaluation_results_class:Counted Episodes = 3713
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.45993041992188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.54006958007812
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.5157470703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16475.23046875
INFO:tools.evaluation_results_class:Current Best Return = -162.45993041992188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.13232920925228
INFO:tools.evaluation_results_class:Counted Episodes = 3718
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.58465576171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.41534423828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.44593811035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16485.255859375
INFO:tools.evaluation_results_class:Current Best Return = -157.58465576171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.0220016098739
INFO:tools.evaluation_results_class:Counted Episodes = 3727
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.05030822753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.94969177246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.5167236328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16908.11328125
INFO:tools.evaluation_results_class:Current Best Return = -157.05030822753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.17594834543987
INFO:tools.evaluation_results_class:Counted Episodes = 3717
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.27391052246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.72607421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.10177612304688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19291.087890625
INFO:tools.evaluation_results_class:Current Best Return = -141.27391052246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.37233157748822
INFO:tools.evaluation_results_class:Counted Episodes = 3607
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.86526489257812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.1347351074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.69032287597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18894.384765625
INFO:tools.evaluation_results_class:Current Best Return = -142.86526489257812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.30579428888273
INFO:tools.evaluation_results_class:Counted Episodes = 3607
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.65945434570312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.34054565429688
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.98382568359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17274.66015625
INFO:tools.evaluation_results_class:Current Best Return = -162.65945434570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.74819566960706
INFO:tools.evaluation_results_class:Counted Episodes = 3741
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.41514587402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.58485412597656
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.85403442382812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16197.9716796875
INFO:tools.evaluation_results_class:Current Best Return = -159.41514587402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.767665952890795
INFO:tools.evaluation_results_class:Counted Episodes = 3736
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.31895446777344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.6810607910156
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.2850799560547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19152.90234375
INFO:tools.evaluation_results_class:Current Best Return = -143.31895446777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.50793209017534
INFO:tools.evaluation_results_class:Counted Episodes = 3593
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.1714324951172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.8285675048828
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.315673828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18955.29296875
INFO:tools.evaluation_results_class:Current Best Return = -145.1714324951172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.465407057515975
INFO:tools.evaluation_results_class:Counted Episodes = 3599
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -163.65789794921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.34210205078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.6618194580078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17412.19140625
INFO:tools.evaluation_results_class:Current Best Return = -163.65789794921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.995703544575726
INFO:tools.evaluation_results_class:Counted Episodes = 3724
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.3852996826172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.6147003173828
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.20706176757812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15966.3388671875
INFO:tools.evaluation_results_class:Current Best Return = -157.3852996826172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.07323640280021
INFO:tools.evaluation_results_class:Counted Episodes = 3714
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.452880859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.547119140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.40321350097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16427.916015625
INFO:tools.evaluation_results_class:Current Best Return = -158.452880859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.035541195476576
INFO:tools.evaluation_results_class:Counted Episodes = 3714
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.90032958984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.09967041015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.39244079589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19237.384765625
INFO:tools.evaluation_results_class:Current Best Return = -142.90032958984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.46770601336303
INFO:tools.evaluation_results_class:Counted Episodes = 3592
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -146.67335510253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.32664489746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.90248107910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19249.439453125
INFO:tools.evaluation_results_class:Current Best Return = -146.67335510253906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.47826086956522
INFO:tools.evaluation_results_class:Counted Episodes = 3588
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.74559020996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.25440979003906
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.63491821289062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17963.34765625
INFO:tools.evaluation_results_class:Current Best Return = -162.74559020996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.09251101321586
INFO:tools.evaluation_results_class:Counted Episodes = 3632
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.00827026367188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.99172973632812
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.93739318847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16648.3828125
INFO:tools.evaluation_results_class:Current Best Return = -159.00827026367188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.018743109151046
INFO:tools.evaluation_results_class:Counted Episodes = 3628
INFO:robust_rl.robust_rl_trainer:Iteration 27 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -137.97633361816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.0236511230469
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.58811950683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16850.400390625
INFO:tools.evaluation_results_class:Current Best Return = -136.67698669433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.779439760674464
INFO:tools.evaluation_results_class:Counted Episodes = 7354
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.7004106044769287
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 1.5861327648162842
INFO:agents.father_agent:Step: 10, Training loss: 1.906528353691101
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -132.5531463623047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.44683837890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.00253295898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16589.18359375
INFO:tools.evaluation_results_class:Current Best Return = -132.5531463623047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.5603249830738
INFO:tools.evaluation_results_class:Counted Episodes = 7385
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -146.1110076904297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.8889923095703
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.94772338867188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17776.84765625
INFO:tools.evaluation_results_class:Current Best Return = -146.1110076904297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.573837290993396
INFO:tools.evaluation_results_class:Counted Episodes = 7117
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.32369995117188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.67630004882812
INFO:tools.evaluation_results_class:Average Discounted Reward = 184.88201904296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19794.224609375
INFO:tools.evaluation_results_class:Current Best Return = -153.32369995117188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.67766776677668
INFO:tools.evaluation_results_class:Counted Episodes = 7272
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 155.70301049681458
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 28 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -133.7847137451172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.2153015136719
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.37069702148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15592.41015625
INFO:tools.evaluation_results_class:Current Best Return = -132.5531463623047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.66160520607375
INFO:tools.evaluation_results_class:Counted Episodes = 7376
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.0511505603790283
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 1.7426748275756836
INFO:agents.father_agent:Step: 10, Training loss: 1.8711864948272705
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -127.31749725341797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.6824951171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.47796630859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15252.537109375
INFO:tools.evaluation_results_class:Current Best Return = -127.31749725341797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.687661191801276
INFO:tools.evaluation_results_class:Counted Episodes = 7367
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -139.6813201904297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.3186950683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.3149871826172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16411.88671875
INFO:tools.evaluation_results_class:Current Best Return = -139.6813201904297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.685263454494226
INFO:tools.evaluation_results_class:Counted Episodes = 7098
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -149.8077392578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.1922607421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 188.16746520996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18314.32421875
INFO:tools.evaluation_results_class:Current Best Return = -149.8077392578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.26202479901894
INFO:tools.evaluation_results_class:Counted Episodes = 7339
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 153.19754067020008
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=1, goright1_init=0, goright2_init=0, o1y=1, o2y=4
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 29 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -127.25241088867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.7475891113281
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.49697875976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16006.255859375
INFO:tools.evaluation_results_class:Current Best Return = -127.25241088867188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.79015367877057
INFO:tools.evaluation_results_class:Counted Episodes = 7353
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.003326892852783
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 2.100044012069702
INFO:agents.father_agent:Step: 10, Training loss: 1.9808045625686646
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -120.96627807617188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.0337219238281
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.43734741210938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14635.3037109375
INFO:tools.evaluation_results_class:Current Best Return = -120.96627807617188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.783791134076694
INFO:tools.evaluation_results_class:Counted Episodes = 7354
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -137.9547882080078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.04522705078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.5072784423828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17182.287109375
INFO:tools.evaluation_results_class:Current Best Return = -137.9547882080078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.509969109800615
INFO:tools.evaluation_results_class:Counted Episodes = 7122
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.58673095703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.41326904296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.25482177734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17386.818359375
INFO:tools.evaluation_results_class:Current Best Return = -143.58673095703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.07663095076631
INFO:tools.evaluation_results_class:Counted Episodes = 7373
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=3
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 152.6266798506881
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=1, o2y=3
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 30 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -124.31211853027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.6878967285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.6748504638672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15555.4697265625
INFO:tools.evaluation_results_class:Current Best Return = -120.96627807617188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.788980865789114
INFO:tools.evaluation_results_class:Counted Episodes = 7369
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.873698115348816
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 1.9672044515609741
INFO:agents.father_agent:Step: 10, Training loss: 1.8454097509384155
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -116.07555389404297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.9244384765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.42941284179688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13875.3837890625
INFO:tools.evaluation_results_class:Current Best Return = -116.07555389404297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.691806836679326
INFO:tools.evaluation_results_class:Counted Episodes = 7372
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -129.33758544921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.66241455078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.75254821777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16002.8857421875
INFO:tools.evaluation_results_class:Current Best Return = -129.33758544921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.44973396807617
INFO:tools.evaluation_results_class:Counted Episodes = 7142
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.39639282226562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.6036071777344
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.28082275390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19082.654296875
INFO:tools.evaluation_results_class:Current Best Return = -143.39639282226562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.720968629609246
INFO:tools.evaluation_results_class:Counted Episodes = 7268
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 1600
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=3, o2y=1
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 149.3188717522201
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x_init=0, o2x_init=0, goright1_init=0, goright2_init=0, o1y=3, o2y=1
INFO:environment.vectorized_sim_initializer:Compiling model avoid...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 31 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -116.55278015136719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.44720458984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 219.79981994628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13977.20703125
INFO:tools.evaluation_results_class:Current Best Return = -116.07555389404297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.95526459356247
INFO:tools.evaluation_results_class:Counted Episodes = 7332
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
2025-08-19 16:49:03.727131: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-19 16:49:03.729056: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-19 16:49:03.759773: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-19 16:49:03.759815: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-19 16:49:03.761081: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-19 16:49:03.766784: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-19 16:49:03.766966: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-19 16:49:04.299322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/network/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/network/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"packets_sent"}max=? [F "done"] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 5480 states and 71240 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"packets_sent"}max=? [F "label_done"] 
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.447291374206543
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.4729118347168
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.769832611083984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15.428773880004883
INFO:tools.evaluation_results_class:Current Best Return = 5.447291374206543
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.63396778916545
INFO:tools.evaluation_results_class:Counted Episodes = 2732
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 81.40416717529297
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 12.858328819274902
INFO:agents.father_agent:Step: 10, Training loss: 9.650858879089355
INFO:agents.father_agent:Step: 15, Training loss: 10.81035041809082
INFO:agents.father_agent:Step: 20, Training loss: 11.256792068481445
INFO:agents.father_agent:Step: 25, Training loss: 11.599759101867676
INFO:agents.father_agent:Step: 30, Training loss: 11.621877670288086
INFO:agents.father_agent:Step: 35, Training loss: 9.010126113891602
INFO:agents.father_agent:Step: 40, Training loss: 11.169729232788086
INFO:agents.father_agent:Step: 45, Training loss: 13.657069206237793
INFO:agents.father_agent:Step: 50, Training loss: 12.007791519165039
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 39.06%
INFO:agents.father_agent:Step: 55, Training loss: 10.338520050048828
INFO:agents.father_agent:Step: 60, Training loss: 12.004069328308105
INFO:agents.father_agent:Step: 65, Training loss: 13.773896217346191
INFO:agents.father_agent:Step: 70, Training loss: 12.046667098999023
INFO:agents.father_agent:Step: 75, Training loss: 11.081572532653809
INFO:agents.father_agent:Step: 80, Training loss: 13.410232543945312
INFO:agents.father_agent:Step: 85, Training loss: 9.513961791992188
INFO:agents.father_agent:Step: 90, Training loss: 10.721946716308594
INFO:agents.father_agent:Step: 95, Training loss: 14.571661949157715
INFO:agents.father_agent:Step: 100, Training loss: 10.335399627685547
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.121888637542725
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 73.21888732910156
INFO:tools.evaluation_results_class:Average Discounted Reward = 54.0178337097168
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18.63045883178711
INFO:tools.evaluation_results_class:Current Best Return = 7.121888637542725
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.63396778916545
INFO:tools.evaluation_results_class:Counted Episodes = 2732
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 7.165080547332764
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 73.65080261230469
INFO:tools.evaluation_results_class:Average Discounted Reward = 54.428428649902344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18.59170913696289
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.63396778916545
INFO:tools.evaluation_results_class:Counted Episodes = 2732
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.577777862548828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.77777862548828
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.30992889404297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16.752979278564453
INFO:tools.evaluation_results_class:Current Best Return = 6.577777862548828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.99444444444444
INFO:tools.evaluation_results_class:Counted Episodes = 2880
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.347916603088379
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 75.47916412353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 55.394020080566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18.995622634887695
INFO:tools.evaluation_results_class:Current Best Return = 7.347916603088379
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.99444444444444
INFO:tools.evaluation_results_class:Counted Episodes = 2880
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.3692192835347328
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.672175407409668
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.72175598144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.86369323730469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8153076171875
INFO:tools.evaluation_results_class:Current Best Return = 3.672175407409668
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.92307692307692
INFO:tools.evaluation_results_class:Counted Episodes = 3328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.58984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 127.8984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 85.03472137451172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.15338659286499
INFO:tools.evaluation_results_class:Current Best Return = 12.58984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 176.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 62.78125
INFO:tools.evaluation_results_class:Average Discounted Reward = 51.891815185546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.532958984375
INFO:tools.evaluation_results_class:Current Best Return = 6.078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.875
INFO:tools.evaluation_results_class:Counted Episodes = 2048
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.231770515441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 104.31771087646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 68.3806381225586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.7718030214309692
INFO:tools.evaluation_results_class:Current Best Return = 10.231770515441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 188.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 11.802734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 120.02734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.92068481445312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.0138206481933594
INFO:tools.evaluation_results_class:Current Best Return = 11.802734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 2.92041015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 31.2041015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 27.84842300415039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1264781951904297
INFO:tools.evaluation_results_class:Current Best Return = 2.92041015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.27734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.7734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 39.200904846191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.4125635623931885
INFO:tools.evaluation_results_class:Current Best Return = 4.27734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.92307692307692
INFO:tools.evaluation_results_class:Counted Episodes = 3328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 13.981770515441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 141.8177032470703
INFO:tools.evaluation_results_class:Average Discounted Reward = 94.61006927490234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.9553966522216797
INFO:tools.evaluation_results_class:Current Best Return = 13.981770515441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 176.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.99951171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.9951171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.47336196899414
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.769042491912842
INFO:tools.evaluation_results_class:Current Best Return = 6.99951171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.875
INFO:tools.evaluation_results_class:Counted Episodes = 2048
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.794270515441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.94271087646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.32392120361328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.340488076210022
INFO:tools.evaluation_results_class:Current Best Return = 10.794270515441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 188.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.5390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 127.390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.056640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.57269287109375
INFO:tools.evaluation_results_class:Current Best Return = 12.5390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3623046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.623046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.807361602783203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.753500461578369
INFO:tools.evaluation_results_class:Current Best Return = 3.3623046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 2 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.741979598999023
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.419795989990234
INFO:tools.evaluation_results_class:Average Discounted Reward = 46.00675964355469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15.977023124694824
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 74.13408280778722
INFO:tools.evaluation_results_class:Counted Episodes = 3647
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.035966873168945
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 12.687304496765137
INFO:agents.father_agent:Step: 10, Training loss: 10.915743827819824
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 5.813819408416748
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 60.1381950378418
INFO:tools.evaluation_results_class:Average Discounted Reward = 46.57834243774414
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16.31768035888672
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 74.13408280778722
INFO:tools.evaluation_results_class:Counted Episodes = 3647
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.42660665512085
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.26606750488281
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.932159423828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15.305179595947266
INFO:tools.evaluation_results_class:Current Best Return = 5.42660665512085
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 76.8391959798995
INFO:tools.evaluation_results_class:Counted Episodes = 3781
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.919862270355225
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 61.19862365722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 46.97581481933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17.184003829956055
INFO:tools.evaluation_results_class:Current Best Return = 5.919862270355225
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 76.8391959798995
INFO:tools.evaluation_results_class:Counted Episodes = 3781
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.4008602813157807
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 3 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.172882080078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 53.72882080078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.901344299316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13.538942337036133
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.37292817679558
INFO:tools.evaluation_results_class:Counted Episodes = 4344
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.904667854309082
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 11.209489822387695
INFO:agents.father_agent:Step: 10, Training loss: 10.888501167297363
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 5.019337177276611
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 52.1933708190918
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.54912567138672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13.638668060302734
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.37292817679558
INFO:tools.evaluation_results_class:Counted Episodes = 4344
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.748768329620361
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 49.48768615722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.93354415893555
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12.801190376281738
INFO:tools.evaluation_results_class:Current Best Return = 4.748768329620361
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.48186296462158
INFO:tools.evaluation_results_class:Counted Episodes = 4466
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.1374831199646
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 53.37483215332031
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.21843338012695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13.64970588684082
INFO:tools.evaluation_results_class:Current Best Return = 5.1374831199646
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.48186296462158
INFO:tools.evaluation_results_class:Counted Episodes = 4466
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.4285698018352946
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 4 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.715962886810303
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 49.159629821777344
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.02935028076172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11.510465621948242
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.45396498455201
INFO:tools.evaluation_results_class:Counted Episodes = 4855
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.977263450622559
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 11.228531837463379
INFO:agents.father_agent:Step: 10, Training loss: 11.441085815429688
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.602471828460693
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.024715423583984
INFO:tools.evaluation_results_class:Average Discounted Reward = 39.007049560546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11.465864181518555
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.45396498455201
INFO:tools.evaluation_results_class:Counted Episodes = 4855
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.4926276206970215
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.92627716064453
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.73653793334961
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11.338210105895996
INFO:tools.evaluation_results_class:Current Best Return = 4.4926276206970215
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.3340739244597
INFO:tools.evaluation_results_class:Counted Episodes = 4951
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.906887531280518
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 51.06887435913086
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.34811019897461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12.032332420349121
INFO:tools.evaluation_results_class:Current Best Return = 4.906887531280518
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.3340739244597
INFO:tools.evaluation_results_class:Counted Episodes = 4951
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.465969093008711
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 5 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.526504993438721
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 47.265052795410156
INFO:tools.evaluation_results_class:Average Discounted Reward = 39.0155143737793
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10.192122459411621
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.38318818629307
INFO:tools.evaluation_results_class:Counted Episodes = 5282
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.789815902709961
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 11.688928604125977
INFO:agents.father_agent:Step: 10, Training loss: 13.819339752197266
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.4782280921936035
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.78227996826172
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.60750961303711
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10.161301612854004
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.38318818629307
INFO:tools.evaluation_results_class:Counted Episodes = 5282
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.217052936553955
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.170528411865234
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.17292022705078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9.484862327575684
INFO:tools.evaluation_results_class:Current Best Return = 4.217052936553955
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.68452270620946
INFO:tools.evaluation_results_class:Counted Episodes = 5395
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.578127861022949
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 47.781280517578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 39.234458923339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10.361042022705078
INFO:tools.evaluation_results_class:Current Best Return = 4.578127861022949
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.68452270620946
INFO:tools.evaluation_results_class:Counted Episodes = 5395
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.490695455670442
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 6 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.309016704559326
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.09016799926758
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.661869049072266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.883386611938477
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.50585729499468
INFO:tools.evaluation_results_class:Counted Episodes = 5634
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.204350471496582
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 10.906632423400879
INFO:agents.father_agent:Step: 10, Training loss: 14.859892845153809
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.307596683502197
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.075965881347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.65926742553711
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.904851913452148
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.50585729499468
INFO:tools.evaluation_results_class:Counted Episodes = 5634
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.060896873474121
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.608970642089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.378353118896484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.205155372619629
INFO:tools.evaluation_results_class:Current Best Return = 4.060896873474121
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.64316873146048
INFO:tools.evaluation_results_class:Counted Episodes = 5731
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.288256645202637
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.882568359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.32725524902344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9.039922714233398
INFO:tools.evaluation_results_class:Current Best Return = 4.288256645202637
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.64316873146048
INFO:tools.evaluation_results_class:Counted Episodes = 5731
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.498614397182082
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.011117935180664
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.11117935180664
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.896060943603516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8853933811187744
INFO:tools.evaluation_results_class:Current Best Return = 4.011117935180664
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.92307692307692
INFO:tools.evaluation_results_class:Counted Episodes = 3328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 13.188801765441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 133.8880157470703
INFO:tools.evaluation_results_class:Average Discounted Reward = 89.07352447509766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.1896138191223145
INFO:tools.evaluation_results_class:Current Best Return = 13.188801765441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 176.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.50048828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.0048828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 55.363685607910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.654296398162842
INFO:tools.evaluation_results_class:Current Best Return = 6.50048828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.875
INFO:tools.evaluation_results_class:Counted Episodes = 2048
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 106.53125
INFO:tools.evaluation_results_class:Average Discounted Reward = 69.71295928955078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.4379068613052368
INFO:tools.evaluation_results_class:Current Best Return = 10.453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 188.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.11328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 123.1328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.14093017578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.6863861083984375
INFO:tools.evaluation_results_class:Current Best Return = 12.11328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.178466796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.78466796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.206579208374023
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2227883338928223
INFO:tools.evaluation_results_class:Current Best Return = 3.178466796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.16357421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.6357421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.035736083984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.193946361541748
INFO:tools.evaluation_results_class:Current Best Return = 3.16357421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.144775390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.44775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.88079833984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1604366302490234
INFO:tools.evaluation_results_class:Current Best Return = 3.144775390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.15869140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.5869140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.99333381652832
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2609500885009766
INFO:tools.evaluation_results_class:Current Best Return = 3.15869140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.125732421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.25732421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.705528259277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.239318370819092
INFO:tools.evaluation_results_class:Current Best Return = 3.125732421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.155029296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.55029296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.973081588745117
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.193983554840088
INFO:tools.evaluation_results_class:Current Best Return = 3.155029296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.437800407409668
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.37800598144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.63821029663086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.314039945602417
INFO:tools.evaluation_results_class:Current Best Return = 4.437800407409668
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.92307692307692
INFO:tools.evaluation_results_class:Counted Episodes = 3328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.032551765441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 142.3255157470703
INFO:tools.evaluation_results_class:Average Discounted Reward = 94.7640380859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.221596717834473
INFO:tools.evaluation_results_class:Current Best Return = 14.032551765441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 176.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.34814453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 75.4814453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.320716857910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.803112030029297
INFO:tools.evaluation_results_class:Current Best Return = 7.34814453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.875
INFO:tools.evaluation_results_class:Counted Episodes = 2048
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.842448234558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 110.42447662353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.64511108398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.2629376649856567
INFO:tools.evaluation_results_class:Current Best Return = 10.842448234558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 188.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.5
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 127.0
INFO:tools.evaluation_results_class:Average Discounted Reward = 77.59307098388672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.50390625
INFO:tools.evaluation_results_class:Current Best Return = 12.5
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5068359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.068359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.11371994018555
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.704542636871338
INFO:tools.evaluation_results_class:Current Best Return = 3.5068359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.454833984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.54833984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.64271545410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6732535362243652
INFO:tools.evaluation_results_class:Current Best Return = 3.454833984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.49755859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.9755859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.02507019042969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7285099029541016
INFO:tools.evaluation_results_class:Current Best Return = 3.49755859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.495361328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.95361328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.00716018676758
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.611794948577881
INFO:tools.evaluation_results_class:Current Best Return = 3.495361328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4638671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.638671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.72996520996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7096314430236816
INFO:tools.evaluation_results_class:Current Best Return = 3.4638671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.470703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.70703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.79097366333008
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6851768493652344
INFO:tools.evaluation_results_class:Current Best Return = 3.470703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 7 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.209691047668457
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.09690856933594
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.20756912231445
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.91652250289917
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.38409589734932
INFO:tools.evaluation_results_class:Counted Episodes = 5923
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.354084014892578
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 12.365936279296875
INFO:agents.father_agent:Step: 10, Training loss: 15.349635124206543
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.246496677398682
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.4649658203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.52946853637695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.07970905303955
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.38409589734932
INFO:tools.evaluation_results_class:Counted Episodes = 5923
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9843332767486572
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.84333419799805
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.03136444091797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.693421840667725
INFO:tools.evaluation_results_class:Current Best Return = 3.9843332767486572
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.45466666666667
INFO:tools.evaluation_results_class:Counted Episodes = 6000
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.318166732788086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.18166732788086
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.97478485107422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.269269943237305
INFO:tools.evaluation_results_class:Current Best Return = 4.318166732788086
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.45466666666667
INFO:tools.evaluation_results_class:Counted Episodes = 6000
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5110127612336703
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 8 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.18512487411499
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.85124969482422
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.272918701171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.205417633056641
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.810328028580706
INFO:tools.evaluation_results_class:Counted Episodes = 6158
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.801308631896973
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 12.908638000488281
INFO:agents.father_agent:Step: 10, Training loss: 12.211755752563477
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.112861156463623
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.12861251831055
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.62974166870117
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.447639465332031
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.810328028580706
INFO:tools.evaluation_results_class:Counted Episodes = 6158
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9072413444519043
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.07241439819336
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.710243225097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.099213123321533
INFO:tools.evaluation_results_class:Current Best Return = 3.9072413444519043
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.64658763216918
INFO:tools.evaluation_results_class:Counted Episodes = 6242
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.163569450378418
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.63569259643555
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.938819885253906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.717077255249023
INFO:tools.evaluation_results_class:Current Best Return = 4.163569450378418
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.64658763216918
INFO:tools.evaluation_results_class:Counted Episodes = 6242
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5039635541706775
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 9 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.0465006828308105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.46500778198242
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.251197814941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.186834812164307
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.63366960907945
INFO:tools.evaluation_results_class:Counted Episodes = 6344
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.489703178405762
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 13.223928451538086
INFO:agents.father_agent:Step: 10, Training loss: 13.189915657043457
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.084962368011475
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.8496208190918
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.59197998046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.739156246185303
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.63366960907945
INFO:tools.evaluation_results_class:Counted Episodes = 6344
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.755703926086426
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.557037353515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.63179016113281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.536004543304443
INFO:tools.evaluation_results_class:Current Best Return = 3.755703926086426
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.245382585751976
INFO:tools.evaluation_results_class:Counted Episodes = 6443
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.057736873626709
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.577369689941406
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.25660705566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.9722981452941895
INFO:tools.evaluation_results_class:Current Best Return = 4.057736873626709
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.245382585751976
INFO:tools.evaluation_results_class:Counted Episodes = 6443
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5084802770970707
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 10 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.061070919036865
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.61071014404297
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.597633361816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.495886325836182
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.614546570507905
INFO:tools.evaluation_results_class:Counted Episodes = 6517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 16.496416091918945
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 14.436702728271484
INFO:agents.father_agent:Step: 10, Training loss: 14.535853385925293
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.077642917633057
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.776432037353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.78842544555664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.107828140258789
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.614546570507905
INFO:tools.evaluation_results_class:Counted Episodes = 6517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7212343215942383
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.21234130859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.43974304199219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.083712100982666
INFO:tools.evaluation_results_class:Current Best Return = 3.7212343215942383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.342909256725946
INFO:tools.evaluation_results_class:Counted Episodes = 6579
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.011703968048096
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.11703872680664
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.0426025390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.367547988891602
INFO:tools.evaluation_results_class:Current Best Return = 4.011703968048096
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.342909256725946
INFO:tools.evaluation_results_class:Counted Episodes = 6579
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.515671923953139
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 11 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9418115615844727
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.41811752319336
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.71039581298828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.641485214233398
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.758248350329936
INFO:tools.evaluation_results_class:Counted Episodes = 6668
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 19.852622985839844
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 15.770712852478027
INFO:agents.father_agent:Step: 10, Training loss: 14.5503568649292
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.9391121864318848
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.39112091064453
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.67452621459961
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.133065700531006
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.758248350329936
INFO:tools.evaluation_results_class:Counted Episodes = 6668
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6653783321380615
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.65378189086914
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.16088104248047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.65330171585083
INFO:tools.evaluation_results_class:Current Best Return = 3.6653783321380615
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.40775977404489
INFO:tools.evaluation_results_class:Counted Episodes = 6727
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.919280529022217
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.192806243896484
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.350120544433594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.03585147857666
INFO:tools.evaluation_results_class:Current Best Return = 3.919280529022217
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.40775977404489
INFO:tools.evaluation_results_class:Counted Episodes = 6727
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.516603855151259
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.079927921295166
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.799278259277344
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.50767517089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8535876274108887
INFO:tools.evaluation_results_class:Current Best Return = 4.079927921295166
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.92307692307692
INFO:tools.evaluation_results_class:Counted Episodes = 3328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 13.549479484558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 137.4947967529297
INFO:tools.evaluation_results_class:Average Discounted Reward = 91.38143157958984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.9142181873321533
INFO:tools.evaluation_results_class:Current Best Return = 13.549479484558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 176.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.63330078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 68.3330078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 56.45503616333008
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.7576212882995605
INFO:tools.evaluation_results_class:Current Best Return = 6.63330078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.875
INFO:tools.evaluation_results_class:Counted Episodes = 2048
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.631510734558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 108.31510162353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 70.98214721679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.2665590047836304
INFO:tools.evaluation_results_class:Current Best Return = 10.631510734558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 188.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 124.65625
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.99153900146484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.702880859375
INFO:tools.evaluation_results_class:Current Best Return = 12.265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.21923828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.1923828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.563568115234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.227324962615967
INFO:tools.evaluation_results_class:Current Best Return = 3.21923828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.221923828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.21923828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.58453369140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2571463584899902
INFO:tools.evaluation_results_class:Current Best Return = 3.221923828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2490234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.490234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.84193992614746
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2294907569885254
INFO:tools.evaluation_results_class:Current Best Return = 3.2490234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.24169921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.4169921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.767597198486328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2462687492370605
INFO:tools.evaluation_results_class:Current Best Return = 3.24169921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.23193359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.3193359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.68017578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.266031265258789
INFO:tools.evaluation_results_class:Current Best Return = 3.23193359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.232177734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.32177734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.689552307128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2778806686401367
INFO:tools.evaluation_results_class:Current Best Return = 3.232177734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.198974609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.98974609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.372573852539062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.282430648803711
INFO:tools.evaluation_results_class:Current Best Return = 3.198974609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.194580078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.94580078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.329750061035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2568163871765137
INFO:tools.evaluation_results_class:Current Best Return = 3.194580078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.183837890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.83837890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.230274200439453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2716236114501953
INFO:tools.evaluation_results_class:Current Best Return = 3.183837890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.190673828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.90673828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.29534912109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2719931602478027
INFO:tools.evaluation_results_class:Current Best Return = 3.190673828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2021484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.021484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.39403533935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2398972511291504
INFO:tools.evaluation_results_class:Current Best Return = 3.2021484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.521334171295166
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 47.213340759277344
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.349510192871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.388967990875244
INFO:tools.evaluation_results_class:Current Best Return = 4.521334171295166
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.92307692307692
INFO:tools.evaluation_results_class:Counted Episodes = 3328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.194010734558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 143.9401092529297
INFO:tools.evaluation_results_class:Average Discounted Reward = 95.75150299072266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.8542871475219727
INFO:tools.evaluation_results_class:Current Best Return = 14.194010734558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 176.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.3662109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 75.662109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.61175537109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.547529220581055
INFO:tools.evaluation_results_class:Current Best Return = 7.3662109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.875
INFO:tools.evaluation_results_class:Counted Episodes = 2048
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.876301765441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 110.76302337646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.91937255859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.1865216493606567
INFO:tools.evaluation_results_class:Current Best Return = 10.876301765441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 188.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.6171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 128.171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.63916778564453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.37689208984375
INFO:tools.evaluation_results_class:Current Best Return = 12.6171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5107421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.107421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.18304443359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6829895973205566
INFO:tools.evaluation_results_class:Current Best Return = 3.5107421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.533447265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.33447265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.364967346191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7137250900268555
INFO:tools.evaluation_results_class:Current Best Return = 3.533447265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.520263671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.20263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.24412536621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.633378505706787
INFO:tools.evaluation_results_class:Current Best Return = 3.520263671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.521484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.21484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.249969482421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6386985778808594
INFO:tools.evaluation_results_class:Current Best Return = 3.521484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.51708984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.1708984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.219154357910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6798839569091797
INFO:tools.evaluation_results_class:Current Best Return = 3.51708984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.53466796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.3466796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.37156677246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.66041898727417
INFO:tools.evaluation_results_class:Current Best Return = 3.53466796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.500244140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.00244140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.05677795410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6650390625
INFO:tools.evaluation_results_class:Current Best Return = 3.500244140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.48681640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.8681640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.93574142456055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.669748306274414
INFO:tools.evaluation_results_class:Current Best Return = 3.48681640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.502197265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.02197265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.06037139892578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.659174919128418
INFO:tools.evaluation_results_class:Current Best Return = 3.502197265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.50439453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.0439453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.0921745300293
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5966601371765137
INFO:tools.evaluation_results_class:Current Best Return = 3.50439453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5126953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.126953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.16835021972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.686850070953369
INFO:tools.evaluation_results_class:Current Best Return = 3.5126953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 12 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8745224475860596
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.74522399902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.262088775634766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.579319000244141
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.01821921833676
INFO:tools.evaluation_results_class:Counted Episodes = 6806
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 18.747570037841797
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 16.62812042236328
INFO:agents.father_agent:Step: 10, Training loss: 15.809621810913086
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.926976203918457
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.26976013183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.70390701293945
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.430018901824951
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.01821921833676
INFO:tools.evaluation_results_class:Counted Episodes = 6806
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6109089851379395
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.109092712402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.82297134399414
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.030426025390625
INFO:tools.evaluation_results_class:Current Best Return = 3.6109089851379395
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.51287272727273
INFO:tools.evaluation_results_class:Counted Episodes = 6875
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.011345386505127
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.11345291137695
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.37466812133789
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.678998947143555
INFO:tools.evaluation_results_class:Current Best Return = 4.011345386505127
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.51287272727273
INFO:tools.evaluation_results_class:Counted Episodes = 6875
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.522822743928955
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 13 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.898761510848999
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.987613677978516
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.56256103515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.256311416625977
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.30760368663594
INFO:tools.evaluation_results_class:Counted Episodes = 6944
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.267650604248047
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 18.60036849975586
INFO:agents.father_agent:Step: 10, Training loss: 11.841684341430664
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.9110023975372314
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.110023498535156
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.734981536865234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.245535850524902
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.30760368663594
INFO:tools.evaluation_results_class:Counted Episodes = 6944
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.507575750350952
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.07575607299805
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.07045364379883
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.840851783752441
INFO:tools.evaluation_results_class:Current Best Return = 3.507575750350952
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.80903373356203
INFO:tools.evaluation_results_class:Counted Episodes = 6996
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8516294956207275
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.51629638671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.04166793823242
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.347054481506348
INFO:tools.evaluation_results_class:Current Best Return = 3.8516294956207275
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.80903373356203
INFO:tools.evaluation_results_class:Counted Episodes = 6996
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5190045023583525
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 14 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7956037521362305
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.95603942871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.742069244384766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.12293815612793
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.994005138452756
INFO:tools.evaluation_results_class:Counted Episodes = 7006
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 20.75229835510254
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 16.798503875732422
INFO:agents.father_agent:Step: 10, Training loss: 15.983072280883789
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.84384822845459
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.438480377197266
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.15328598022461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.169450759887695
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.994005138452756
INFO:tools.evaluation_results_class:Counted Episodes = 7006
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.426007032394409
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.26007080078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.43157196044922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.658942222595215
INFO:tools.evaluation_results_class:Current Best Return = 3.426007032394409
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.36650176678445
INFO:tools.evaluation_results_class:Counted Episodes = 7075
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7765371799468994
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.7653694152832
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.47917938232422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.192042827606201
INFO:tools.evaluation_results_class:Current Best Return = 3.7765371799468994
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.36650176678445
INFO:tools.evaluation_results_class:Counted Episodes = 7075
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5166233645778164
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 15 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.796213150024414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.96213150024414
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.863712310791016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.8278961181640625
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.38316970546985
INFO:tools.evaluation_results_class:Counted Episodes = 7130
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.266782760620117
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 17.24755096435547
INFO:agents.father_agent:Step: 10, Training loss: 13.767616271972656
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.8800840377807617
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.80084228515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.615623474121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.826994895935059
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.38316970546985
INFO:tools.evaluation_results_class:Counted Episodes = 7130
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3432462215423584
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.43246078491211
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.82196617126465
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.3596696853637695
INFO:tools.evaluation_results_class:Current Best Return = 3.3432462215423584
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.70150083379655
INFO:tools.evaluation_results_class:Counted Episodes = 7196
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.836575984954834
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.36575698852539
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.162879943847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.520540714263916
INFO:tools.evaluation_results_class:Current Best Return = 3.836575984954834
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.70150083379655
INFO:tools.evaluation_results_class:Counted Episodes = 7196
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.524649582072016
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 16 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8197336196899414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.19733428955078
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.145751953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.57213830947876
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.026644462947544
INFO:tools.evaluation_results_class:Counted Episodes = 7206
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.67750358581543
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 18.31536102294922
INFO:agents.father_agent:Step: 10, Training loss: 13.07446002960205
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.753122329711914
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.53122329711914
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.516075134277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.601693630218506
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.026644462947544
INFO:tools.evaluation_results_class:Counted Episodes = 7206
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3891260623931885
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.891258239746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.2760066986084
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.045552730560303
INFO:tools.evaluation_results_class:Current Best Return = 3.3891260623931885
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.32869924294563
INFO:tools.evaluation_results_class:Counted Episodes = 7265
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8165175914764404
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.16517639160156
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.115047454833984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.792349338531494
INFO:tools.evaluation_results_class:Current Best Return = 3.8165175914764404
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.32869924294563
INFO:tools.evaluation_results_class:Counted Episodes = 7265
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5307602824705144
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.011117935180664
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.11117935180664
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.90113830566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7970519065856934
INFO:tools.evaluation_results_class:Current Best Return = 4.011117935180664
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.92307692307692
INFO:tools.evaluation_results_class:Counted Episodes = 3328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 13.266926765441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 134.6692657470703
INFO:tools.evaluation_results_class:Average Discounted Reward = 89.47513580322266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.08369779586792
INFO:tools.evaluation_results_class:Current Best Return = 13.266926765441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 176.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.52001953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.2001953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 55.58661651611328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.123622894287109
INFO:tools.evaluation_results_class:Current Best Return = 6.52001953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.875
INFO:tools.evaluation_results_class:Counted Episodes = 2048
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.442708015441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 106.42708587646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 69.7678451538086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.6086968183517456
INFO:tools.evaluation_results_class:Current Best Return = 10.442708015441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 188.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.005859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 122.05859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 74.30506896972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.9784812927246094
INFO:tools.evaluation_results_class:Current Best Return = 12.005859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.157470703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.57470703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.99254608154297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.181013584136963
INFO:tools.evaluation_results_class:Current Best Return = 3.157470703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.167724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.67724609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.111629486083984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1688899993896484
INFO:tools.evaluation_results_class:Current Best Return = 3.167724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.1806640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.806640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.223060607910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.207594394683838
INFO:tools.evaluation_results_class:Current Best Return = 3.1806640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.1904296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.904296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.314897537231445
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.199087619781494
INFO:tools.evaluation_results_class:Current Best Return = 3.1904296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.191162109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.91162109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.311168670654297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1844043731689453
INFO:tools.evaluation_results_class:Current Best Return = 3.191162109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.19189453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.9189453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.316478729248047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2087817192077637
INFO:tools.evaluation_results_class:Current Best Return = 3.19189453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.18017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.8017578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.211681365966797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.206794261932373
INFO:tools.evaluation_results_class:Current Best Return = 3.18017578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2080078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.080078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.467435836791992
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1705994606018066
INFO:tools.evaluation_results_class:Current Best Return = 3.2080078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.175048828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.75048828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.193967819213867
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.187863826751709
INFO:tools.evaluation_results_class:Current Best Return = 3.175048828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.169189453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.69189453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.123668670654297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2382211685180664
INFO:tools.evaluation_results_class:Current Best Return = 3.169189453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.1748046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.748046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.181724548339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.217978000640869
INFO:tools.evaluation_results_class:Current Best Return = 3.1748046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.122314453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.22314453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.685590744018555
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.133721351623535
INFO:tools.evaluation_results_class:Current Best Return = 3.122314453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.16064453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.6064453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.04045867919922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1880602836608887
INFO:tools.evaluation_results_class:Current Best Return = 3.16064453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.16845703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.6845703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.10230255126953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2069735527038574
INFO:tools.evaluation_results_class:Current Best Return = 3.16845703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.17041015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.7041015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.114356994628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2043588161468506
INFO:tools.evaluation_results_class:Current Best Return = 3.17041015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.13671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.3671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.811031341552734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2303314208984375
INFO:tools.evaluation_results_class:Current Best Return = 3.13671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.49338960647583
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.933895111083984
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.144691467285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.4747159481048584
INFO:tools.evaluation_results_class:Current Best Return = 4.49338960647583
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.92307692307692
INFO:tools.evaluation_results_class:Counted Episodes = 3328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.12109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 143.2109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 95.34488677978516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.799138307571411
INFO:tools.evaluation_results_class:Current Best Return = 14.12109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 176.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.33984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 75.3984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.37529373168945
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.5134124755859375
INFO:tools.evaluation_results_class:Current Best Return = 7.33984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.875
INFO:tools.evaluation_results_class:Counted Episodes = 2048
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.845051765441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 110.45052337646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.6341323852539
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.253334879875183
INFO:tools.evaluation_results_class:Current Best Return = 10.845051765441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 188.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.572265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 127.72265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.32058715820312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.3814964294433594
INFO:tools.evaluation_results_class:Current Best Return = 12.572265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4892578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.892578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.978179931640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5199036598205566
INFO:tools.evaluation_results_class:Current Best Return = 3.4892578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.15625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.240806579589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.655029296875
INFO:tools.evaluation_results_class:Current Best Return = 3.515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.505126953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.05126953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.14891052246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.623021125793457
INFO:tools.evaluation_results_class:Current Best Return = 3.505126953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.497314453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.97314453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.082923889160156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.633294105529785
INFO:tools.evaluation_results_class:Current Best Return = 3.497314453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.515380859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.15380859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.243873596191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6374588012695312
INFO:tools.evaluation_results_class:Current Best Return = 3.515380859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5009765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.009765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.11170959472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5932602882385254
INFO:tools.evaluation_results_class:Current Best Return = 3.5009765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.526123046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.26123046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.3060188293457
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5315446853637695
INFO:tools.evaluation_results_class:Current Best Return = 3.526123046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.52197265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.2197265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.26641082763672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5585994720458984
INFO:tools.evaluation_results_class:Current Best Return = 3.52197265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.51171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.1171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.18201446533203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6087493896484375
INFO:tools.evaluation_results_class:Current Best Return = 3.51171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.544677734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.44677734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.47410583496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6127500534057617
INFO:tools.evaluation_results_class:Current Best Return = 3.544677734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.539306640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.39306640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.43157958984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.615642547607422
INFO:tools.evaluation_results_class:Current Best Return = 3.539306640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.51220703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.1220703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.173316955566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6463351249694824
INFO:tools.evaluation_results_class:Current Best Return = 3.51220703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.514404296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.14404296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.20000457763672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6326050758361816
INFO:tools.evaluation_results_class:Current Best Return = 3.514404296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.513427734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.13427734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.190853118896484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.606265068054199
INFO:tools.evaluation_results_class:Current Best Return = 3.513427734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.489501953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.89501953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.98356628417969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6351442337036133
INFO:tools.evaluation_results_class:Current Best Return = 3.489501953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4892578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.892578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.97093963623047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6170716285705566
INFO:tools.evaluation_results_class:Current Best Return = 3.4892578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 17 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8008787631988525
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.0087890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.998817443847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.312177658081055
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.67756110958528
INFO:tools.evaluation_results_class:Counted Episodes = 7282
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.70478057861328
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 15.482446670532227
INFO:agents.father_agent:Step: 10, Training loss: 16.509660720825195
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.7443010807037354
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.44300842285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.56020736694336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.295782566070557
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.67756110958528
INFO:tools.evaluation_results_class:Counted Episodes = 7282
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.530637264251709
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.306373596191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.613075256347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.101185321807861
INFO:tools.evaluation_results_class:Current Best Return = 3.530637264251709
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.91830065359477
INFO:tools.evaluation_results_class:Counted Episodes = 7344
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8650598526000977
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.65060043334961
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.63508224487305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.573975086212158
INFO:tools.evaluation_results_class:Current Best Return = 3.8650598526000977
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.91830065359477
INFO:tools.evaluation_results_class:Counted Episodes = 7344
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5396208358414563
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 18 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7497620582580566
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.49761962890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.68964385986328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.238183498382568
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.32418105205927
INFO:tools.evaluation_results_class:Counted Episodes = 7357
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 16.994779586791992
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 13.668567657470703
INFO:agents.father_agent:Step: 10, Training loss: 16.125995635986328
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.748131036758423
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.4813117980957
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.64311218261719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.200120449066162
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.32418105205927
INFO:tools.evaluation_results_class:Counted Episodes = 7357
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.444279432296753
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.44279479980469
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.86774444580078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.8866686820983887
INFO:tools.evaluation_results_class:Current Best Return = 3.444279432296753
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.56179168915273
INFO:tools.evaluation_results_class:Counted Episodes = 7412
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7678089141845703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.6780891418457
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.752601623535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.375526428222656
INFO:tools.evaluation_results_class:Current Best Return = 3.7678089141845703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.56179168915273
INFO:tools.evaluation_results_class:Counted Episodes = 7412
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.533740590154557
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 19 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.777193784713745
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.77193832397461
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.93159866333008
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.94779634475708
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.04461517724761
INFO:tools.evaluation_results_class:Counted Episodes = 7419
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.763219833374023
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 16.15711784362793
INFO:agents.father_agent:Step: 10, Training loss: 15.149800300598145
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.6589837074279785
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.58983612060547
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.86565017700195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.336869239807129
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.04461517724761
INFO:tools.evaluation_results_class:Counted Episodes = 7419
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.423710584640503
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.23710632324219
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.777469635009766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.8117618560791016
INFO:tools.evaluation_results_class:Current Best Return = 3.423710584640503
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.3007367716008
INFO:tools.evaluation_results_class:Counted Episodes = 7465
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6999330520629883
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.99932861328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.20781707763672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.294152736663818
INFO:tools.evaluation_results_class:Current Best Return = 3.6999330520629883
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.3007367716008
INFO:tools.evaluation_results_class:Counted Episodes = 7465
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.536397087310373
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 20 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.869844675064087
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.698448181152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.84060287475586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.995914936065674
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.835565077664704
INFO:tools.evaluation_results_class:Counted Episodes = 7468
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 19.09258270263672
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 13.059222221374512
INFO:agents.father_agent:Step: 10, Training loss: 11.548849105834961
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.668318271636963
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.68318176269531
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.027427673339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.116420269012451
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.835565077664704
INFO:tools.evaluation_results_class:Counted Episodes = 7468
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4085519313812256
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.08552169799805
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.710365295410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7591538429260254
INFO:tools.evaluation_results_class:Current Best Return = 3.4085519313812256
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.08725189822832
INFO:tools.evaluation_results_class:Counted Episodes = 7507
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.664846181869507
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.648460388183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.95525360107422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.218829154968262
INFO:tools.evaluation_results_class:Current Best Return = 3.664846181869507
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.08725189822832
INFO:tools.evaluation_results_class:Counted Episodes = 7507
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5315443381721607
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 21 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.756950855255127
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.56950759887695
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.87091064453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.922435998916626
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.62445124384728
INFO:tools.evaluation_results_class:Counted Episodes = 7517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.545289993286133
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 12.974159240722656
INFO:agents.father_agent:Step: 10, Training loss: 21.515535354614258
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.694559097290039
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.94559097290039
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.30481719970703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.802941083908081
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.62445124384728
INFO:tools.evaluation_results_class:Counted Episodes = 7517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2653439044952393
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.653438568115234
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.416887283325195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.5769474506378174
INFO:tools.evaluation_results_class:Current Best Return = 3.2653439044952393
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.82328042328042
INFO:tools.evaluation_results_class:Counted Episodes = 7560
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7186508178710938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.18650817871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.49412536621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.195842742919922
INFO:tools.evaluation_results_class:Current Best Return = 3.7186508178710938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.82328042328042
INFO:tools.evaluation_results_class:Counted Episodes = 7560
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.535590359381371
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.060997486114502
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.60997772216797
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.352901458740234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.982757568359375
INFO:tools.evaluation_results_class:Current Best Return = 4.060997486114502
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.92307692307692
INFO:tools.evaluation_results_class:Counted Episodes = 3328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 13.322916984558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 135.2291717529297
INFO:tools.evaluation_results_class:Average Discounted Reward = 89.91776275634766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.184787273406982
INFO:tools.evaluation_results_class:Current Best Return = 13.322916984558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 176.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.65087890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 68.5087890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 56.639625549316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.966494083404541
INFO:tools.evaluation_results_class:Current Best Return = 6.65087890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.875
INFO:tools.evaluation_results_class:Counted Episodes = 2048
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.536458015441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 107.36458587646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 70.26485443115234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.5611706972122192
INFO:tools.evaluation_results_class:Current Best Return = 10.536458015441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 188.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.23046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 124.3046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.66776275634766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.9312591552734375
INFO:tools.evaluation_results_class:Current Best Return = 12.23046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.225830078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.25830078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.61827278137207
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.323756694793701
INFO:tools.evaluation_results_class:Current Best Return = 3.225830078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.20166015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.0166015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.38640785217285
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2908759117126465
INFO:tools.evaluation_results_class:Current Best Return = 3.20166015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2001953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.001953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.367868423461914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.279745578765869
INFO:tools.evaluation_results_class:Current Best Return = 3.2001953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2099609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.099609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.46269989013672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.300642490386963
INFO:tools.evaluation_results_class:Current Best Return = 3.2099609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.198974609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.98974609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.368911743164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.323934555053711
INFO:tools.evaluation_results_class:Current Best Return = 3.198974609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.21142578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.1142578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.472625732421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.309791088104248
INFO:tools.evaluation_results_class:Current Best Return = 3.21142578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.239013671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.39013671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.748958587646484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2194838523864746
INFO:tools.evaluation_results_class:Current Best Return = 3.239013671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.21875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.1875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.574039459228516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.212890625
INFO:tools.evaluation_results_class:Current Best Return = 3.21875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.242919921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.42919921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.78189468383789
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2615466117858887
INFO:tools.evaluation_results_class:Current Best Return = 3.242919921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.23779296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.3779296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.724544525146484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2237277030944824
INFO:tools.evaluation_results_class:Current Best Return = 3.23779296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.23388671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.3388671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.692890167236328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2587733268737793
INFO:tools.evaluation_results_class:Current Best Return = 3.23388671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.256591796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.56591796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.896324157714844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2918267250061035
INFO:tools.evaluation_results_class:Current Best Return = 3.256591796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.26025390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.6025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.925350189208984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2520923614501953
INFO:tools.evaluation_results_class:Current Best Return = 3.26025390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.242431640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.42431640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.76474380493164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2486000061035156
INFO:tools.evaluation_results_class:Current Best Return = 3.242431640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.251220703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.51220703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.85647201538086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2623276710510254
INFO:tools.evaluation_results_class:Current Best Return = 3.251220703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.28515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.8515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.161022186279297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.3044281005859375
INFO:tools.evaluation_results_class:Current Best Return = 3.28515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.21142578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.1142578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.477510452270508
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.292212963104248
INFO:tools.evaluation_results_class:Current Best Return = 3.21142578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.21533203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.1533203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.518508911132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.34230375289917
INFO:tools.evaluation_results_class:Current Best Return = 3.21533203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.221923828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.21923828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.58066749572754
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2874197959899902
INFO:tools.evaluation_results_class:Current Best Return = 3.221923828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.220458984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.20458984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.55852699279785
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2695136070251465
INFO:tools.evaluation_results_class:Current Best Return = 3.220458984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.209228515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.09228515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.467994689941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.210862636566162
INFO:tools.evaluation_results_class:Current Best Return = 3.209228515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.498197078704834
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.981971740722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.186885833740234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3737947940826416
INFO:tools.evaluation_results_class:Current Best Return = 4.498197078704834
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.92307692307692
INFO:tools.evaluation_results_class:Counted Episodes = 3328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.231770515441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 144.3177032470703
INFO:tools.evaluation_results_class:Average Discounted Reward = 95.9834976196289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.797844648361206
INFO:tools.evaluation_results_class:Current Best Return = 14.231770515441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 176.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.3046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 75.046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.104469299316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.70111083984375
INFO:tools.evaluation_results_class:Current Best Return = 7.3046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.875
INFO:tools.evaluation_results_class:Counted Episodes = 2048
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 110.59375
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.7605209350586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.2093912363052368
INFO:tools.evaluation_results_class:Current Best Return = 10.859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 188.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.576171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 127.76171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.21984100341797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.3691978454589844
INFO:tools.evaluation_results_class:Current Best Return = 12.576171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.533447265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.33447265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.38671112060547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7244672775268555
INFO:tools.evaluation_results_class:Current Best Return = 3.533447265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.492431640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.92431640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.00935363769531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5736732482910156
INFO:tools.evaluation_results_class:Current Best Return = 3.492431640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.510986328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.10986328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.16752624511719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6263442039489746
INFO:tools.evaluation_results_class:Current Best Return = 3.510986328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.51025390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.1025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.161354064941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6522388458251953
INFO:tools.evaluation_results_class:Current Best Return = 3.51025390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5185546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.185546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.22613525390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.614401340484619
INFO:tools.evaluation_results_class:Current Best Return = 3.5185546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.522216796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.22216796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.26227569580078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6059517860412598
INFO:tools.evaluation_results_class:Current Best Return = 3.522216796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.482177734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.82177734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.939231872558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6881589889526367
INFO:tools.evaluation_results_class:Current Best Return = 3.482177734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.0
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.09253692626953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.65185546875
INFO:tools.evaluation_results_class:Current Best Return = 3.5
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5224609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.224609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.29829788208008
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.574690341949463
INFO:tools.evaluation_results_class:Current Best Return = 3.5224609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.501708984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.01708984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.112144470214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.663083553314209
INFO:tools.evaluation_results_class:Current Best Return = 3.501708984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.531982421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.31982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.38121795654297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6840357780456543
INFO:tools.evaluation_results_class:Current Best Return = 3.531982421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.56394958496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.60931396484375
INFO:tools.evaluation_results_class:Current Best Return = 3.5546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.548828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.48828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.51258850097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6489830017089844
INFO:tools.evaluation_results_class:Current Best Return = 3.548828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.55126953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.5126953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.53453063964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.688777446746826
INFO:tools.evaluation_results_class:Current Best Return = 3.55126953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.548583984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.48583984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.50627517700195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5562338829040527
INFO:tools.evaluation_results_class:Current Best Return = 3.548583984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.53857421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.3857421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.41461944580078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.663550853729248
INFO:tools.evaluation_results_class:Current Best Return = 3.53857421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.52001953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.2001953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.235721588134766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.657802104949951
INFO:tools.evaluation_results_class:Current Best Return = 3.52001953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5068359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.068359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.11901092529297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.620069980621338
INFO:tools.evaluation_results_class:Current Best Return = 3.5068359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.536376953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.36376953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.390907287597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6744582653045654
INFO:tools.evaluation_results_class:Current Best Return = 3.536376953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.51025390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.1025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.151580810546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6673755645751953
INFO:tools.evaluation_results_class:Current Best Return = 3.51025390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.532470703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.32470703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.3478889465332
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.636152744293213
INFO:tools.evaluation_results_class:Current Best Return = 3.532470703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 22 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.739854574203491
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.39854431152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.752235412597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.011900901794434
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.40951751487112
INFO:tools.evaluation_results_class:Counted Episodes = 7565
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.206073760986328
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 14.678659439086914
INFO:agents.father_agent:Step: 10, Training loss: 13.986029624938965
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.618638515472412
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.18638610839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.68246841430664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.699507474899292
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.40951751487112
INFO:tools.evaluation_results_class:Counted Episodes = 7565
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.419638156890869
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.196380615234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.83771514892578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.4493629932403564
INFO:tools.evaluation_results_class:Current Best Return = 3.419638156890869
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.49606712113267
INFO:tools.evaluation_results_class:Counted Episodes = 7628
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.691793441772461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.91793441772461
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.310523986816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.873938798904419
INFO:tools.evaluation_results_class:Current Best Return = 3.691793441772461
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.49606712113267
INFO:tools.evaluation_results_class:Counted Episodes = 7628
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5416700105849923
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 23 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7886862754821777
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.88685989379883
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.25
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.852970838546753
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.18086363039769
INFO:tools.evaluation_results_class:Counted Episodes = 7619
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.994609832763672
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 13.093154907226562
INFO:agents.father_agent:Step: 10, Training loss: 15.340749740600586
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.7577109336853027
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.577110290527344
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.96083450317383
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.90034556388855
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.18086363039769
INFO:tools.evaluation_results_class:Counted Episodes = 7619
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.441991090774536
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.4199104309082
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.07560729980469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.34122633934021
INFO:tools.evaluation_results_class:Current Best Return = 3.441991090774536
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.368434805330544
INFO:tools.evaluation_results_class:Counted Episodes = 7654
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6335248947143555
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.33525085449219
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.7916374206543
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.552003860473633
INFO:tools.evaluation_results_class:Current Best Return = 3.6335248947143555
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.368434805330544
INFO:tools.evaluation_results_class:Counted Episodes = 7654
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5604916121100088
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 24 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6965057849884033
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.965057373046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.37028121948242
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.9239888191223145
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.092919774898576
INFO:tools.evaluation_results_class:Counted Episodes = 7641
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.839441299438477
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 14.351550102233887
INFO:agents.father_agent:Step: 10, Training loss: 15.971941947937012
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.791257619857788
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.91257858276367
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.24211502075195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.871490001678467
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.092919774898576
INFO:tools.evaluation_results_class:Counted Episodes = 7641
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3835437297821045
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.8354377746582
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.61531639099121
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3128602504730225
INFO:tools.evaluation_results_class:Current Best Return = 3.3835437297821045
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.2403332899362
INFO:tools.evaluation_results_class:Counted Episodes = 7681
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6737403869628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.737403869628906
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.16810607910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.794609308242798
INFO:tools.evaluation_results_class:Current Best Return = 3.6737403869628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.2403332899362
INFO:tools.evaluation_results_class:Counted Episodes = 7681
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.573768508471097
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 25 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7888295650482178
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.8882942199707
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.31710433959961
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.820268392562866
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.920062491863035
INFO:tools.evaluation_results_class:Counted Episodes = 7681
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.859926223754883
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 13.903149604797363
INFO:agents.father_agent:Step: 10, Training loss: 13.403584480285645
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.662153482437134
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.62153244018555
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.14396667480469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.5873303413391113
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.920062491863035
INFO:tools.evaluation_results_class:Counted Episodes = 7681
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4309542179107666
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.30954360961914
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.088890075683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3809969425201416
INFO:tools.evaluation_results_class:Current Best Return = 3.4309542179107666
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.9974140160331
INFO:tools.evaluation_results_class:Counted Episodes = 7734
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7712697982788086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.71269607543945
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.04456329345703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.555776357650757
INFO:tools.evaluation_results_class:Current Best Return = 3.7712697982788086
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.9974140160331
INFO:tools.evaluation_results_class:Counted Episodes = 7734
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5780864928482856
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 26 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.695348024368286
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.9534797668457
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.46664810180664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7144956588745117
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.78255798885577
INFO:tools.evaluation_results_class:Counted Episodes = 7717
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.24964714050293
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 14.711747169494629
INFO:agents.father_agent:Step: 10, Training loss: 17.15218734741211
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.6477906703948975
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.4779052734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.990238189697266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.615743637084961
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.78255798885577
INFO:tools.evaluation_results_class:Counted Episodes = 7717
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5795586109161377
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.79558563232422
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.442413330078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.1175897121429443
INFO:tools.evaluation_results_class:Current Best Return = 3.5795586109161377
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.92334494773519
INFO:tools.evaluation_results_class:Counted Episodes = 7749
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.734804391860962
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.348045349121094
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.808170318603516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3782453536987305
INFO:tools.evaluation_results_class:Current Best Return = 3.734804391860962
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.92334494773519
INFO:tools.evaluation_results_class:Counted Episodes = 7749
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5753250547457758
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.186598777770996
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.86598587036133
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.465782165527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.9113945960998535
INFO:tools.evaluation_results_class:Current Best Return = 4.186598777770996
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.92307692307692
INFO:tools.evaluation_results_class:Counted Episodes = 3328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 13.48828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 136.8828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 90.8123779296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.06757116317749
INFO:tools.evaluation_results_class:Current Best Return = 13.48828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 176.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 70.59375
INFO:tools.evaluation_results_class:Average Discounted Reward = 58.386234283447266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.695068359375
INFO:tools.evaluation_results_class:Current Best Return = 6.859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.875
INFO:tools.evaluation_results_class:Counted Episodes = 2048
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.622395515441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 108.22396087646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 70.83600616455078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.3339775800704956
INFO:tools.evaluation_results_class:Current Best Return = 10.622395515441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 188.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.3125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 125.125
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.40129089355469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.48828125
INFO:tools.evaluation_results_class:Current Best Return = 12.3125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3134765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.134765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.40568733215332
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2645249366760254
INFO:tools.evaluation_results_class:Current Best Return = 3.3134765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3037109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.037109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.320938110351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.260786533355713
INFO:tools.evaluation_results_class:Current Best Return = 3.3037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.289306640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.89306640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.1953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.272014617919922
INFO:tools.evaluation_results_class:Current Best Return = 3.289306640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.30322265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.0322265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.322418212890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2117671966552734
INFO:tools.evaluation_results_class:Current Best Return = 3.30322265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.96875
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.27204704284668
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.266845703125
INFO:tools.evaluation_results_class:Current Best Return = 3.296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.306640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.06640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.35956382751465
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2638816833496094
INFO:tools.evaluation_results_class:Current Best Return = 3.306640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.29052734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.9052734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.19432830810547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2832698822021484
INFO:tools.evaluation_results_class:Current Best Return = 3.29052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.27783203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.7783203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.08568572998047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.32368803024292
INFO:tools.evaluation_results_class:Current Best Return = 3.27783203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.318803787231445
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.31976318359375
INFO:tools.evaluation_results_class:Current Best Return = 3.3046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.29052734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.9052734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.194242477416992
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.3179378509521484
INFO:tools.evaluation_results_class:Current Best Return = 3.29052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2763671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.763671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.070331573486328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2717652320861816
INFO:tools.evaluation_results_class:Current Best Return = 3.2763671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.344703674316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.18060302734375
INFO:tools.evaluation_results_class:Current Best Return = 3.3046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.337207794189453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.25775146484375
INFO:tools.evaluation_results_class:Current Best Return = 3.3046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.312744140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.12744140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.41653060913086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.239349365234375
INFO:tools.evaluation_results_class:Current Best Return = 3.312744140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.96875
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.269393920898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.226318359375
INFO:tools.evaluation_results_class:Current Best Return = 3.296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.322021484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.22021484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.50619888305664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1924452781677246
INFO:tools.evaluation_results_class:Current Best Return = 3.322021484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.31640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.1640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.43695831298828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2993011474609375
INFO:tools.evaluation_results_class:Current Best Return = 3.31640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.291748046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.91748046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.22332763671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.317959785461426
INFO:tools.evaluation_results_class:Current Best Return = 3.291748046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.314453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.14453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.437786102294922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.3205528259277344
INFO:tools.evaluation_results_class:Current Best Return = 3.314453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.326171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.26171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.53706932067871
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2417564392089844
INFO:tools.evaluation_results_class:Current Best Return = 3.326171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3291015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.291015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.567394256591797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.3189377784729004
INFO:tools.evaluation_results_class:Current Best Return = 3.3291015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2861328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.861328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.163372039794922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2926392555236816
INFO:tools.evaluation_results_class:Current Best Return = 3.2861328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.308349609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.08349609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.365867614746094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2957897186279297
INFO:tools.evaluation_results_class:Current Best Return = 3.308349609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.96875
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.266355514526367
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.300537109375
INFO:tools.evaluation_results_class:Current Best Return = 3.296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.29443359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.9443359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.234981536865234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.279520034790039
INFO:tools.evaluation_results_class:Current Best Return = 3.29443359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.31884765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.1884765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.455245971679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.303609609603882
INFO:tools.evaluation_results_class:Current Best Return = 3.31884765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.507512092590332
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 47.07511901855469
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.28139877319336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.186241626739502
INFO:tools.evaluation_results_class:Current Best Return = 4.507512092590332
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.92307692307692
INFO:tools.evaluation_results_class:Counted Episodes = 3328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 14.196614265441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 143.9661407470703
INFO:tools.evaluation_results_class:Average Discounted Reward = 95.84351348876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.704832077026367
INFO:tools.evaluation_results_class:Current Best Return = 14.196614265441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 176.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.40380859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 76.0380859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.955345153808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.941919803619385
INFO:tools.evaluation_results_class:Current Best Return = 7.40380859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.875
INFO:tools.evaluation_results_class:Counted Episodes = 2048
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.907551765441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 111.07552337646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.98331451416016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.0656721591949463
INFO:tools.evaluation_results_class:Current Best Return = 10.907551765441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 188.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.587890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 127.87890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.21586608886719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.4219627380371094
INFO:tools.evaluation_results_class:Current Best Return = 12.587890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.585205078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.85205078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.851829528808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6255526542663574
INFO:tools.evaluation_results_class:Current Best Return = 3.585205078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5927734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.927734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.92892837524414
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6227402687072754
INFO:tools.evaluation_results_class:Current Best Return = 3.5927734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.587158203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.87158203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.887245178222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.640352725982666
INFO:tools.evaluation_results_class:Current Best Return = 3.587158203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.590087890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.90087890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.91205978393555
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.641787052154541
INFO:tools.evaluation_results_class:Current Best Return = 3.590087890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.564697265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.64697265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.68367004394531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.607142448425293
INFO:tools.evaluation_results_class:Current Best Return = 3.564697265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.56494140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.6494140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.69123458862305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5778141021728516
INFO:tools.evaluation_results_class:Current Best Return = 3.56494140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.556396484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.56396484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.5903205871582
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6408629417419434
INFO:tools.evaluation_results_class:Current Best Return = 3.556396484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.53076171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.3076171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.36042785644531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.584502696990967
INFO:tools.evaluation_results_class:Current Best Return = 3.53076171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.53125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.3125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.361671447753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.599609375
INFO:tools.evaluation_results_class:Current Best Return = 3.53125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.552001953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.52001953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.55568313598633
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6706361770629883
INFO:tools.evaluation_results_class:Current Best Return = 3.552001953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.53515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.3515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.39078903198242
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5925140380859375
INFO:tools.evaluation_results_class:Current Best Return = 3.53515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.57421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.7421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.77500915527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6697845458984375
INFO:tools.evaluation_results_class:Current Best Return = 3.57421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.577392578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.77392578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.807071685791016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6595377922058105
INFO:tools.evaluation_results_class:Current Best Return = 3.577392578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.551513671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.51513671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.56950378417969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6296706199645996
INFO:tools.evaluation_results_class:Current Best Return = 3.551513671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5615234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.615234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.667945861816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6036362648010254
INFO:tools.evaluation_results_class:Current Best Return = 3.5615234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.56884765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.6884765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.731346130371094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.622701644897461
INFO:tools.evaluation_results_class:Current Best Return = 3.56884765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.593017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.93017578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.9073600769043
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6075587272644043
INFO:tools.evaluation_results_class:Current Best Return = 3.593017578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.565673828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.65673828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.6632080078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5547690391540527
INFO:tools.evaluation_results_class:Current Best Return = 3.565673828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5595703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.595703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.61016845703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.622915744781494
INFO:tools.evaluation_results_class:Current Best Return = 3.5595703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.577392578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.77392578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.77571487426758
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5599284172058105
INFO:tools.evaluation_results_class:Current Best Return = 3.577392578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.568603515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.68603515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.706199645996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5773253440856934
INFO:tools.evaluation_results_class:Current Best Return = 3.568603515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.560546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.60546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.62083435058594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6384239196777344
INFO:tools.evaluation_results_class:Current Best Return = 3.560546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.547119140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.47119140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.49349594116211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6559829711914062
INFO:tools.evaluation_results_class:Current Best Return = 3.547119140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.560791015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.60791015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.62501525878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.662320613861084
INFO:tools.evaluation_results_class:Current Best Return = 3.560791015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5537109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.537109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.55419921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.562056064605713
INFO:tools.evaluation_results_class:Current Best Return = 3.5537109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.55322265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.5322265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.5560417175293
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5914058685302734
INFO:tools.evaluation_results_class:Current Best Return = 3.55322265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 27 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.712775468826294
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.12775421142578
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.67473220825195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.535653591156006
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.61299471445146
INFO:tools.evaluation_results_class:Counted Episodes = 7757
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.062599182128906
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 12.690433502197266
INFO:agents.father_agent:Step: 10, Training loss: 17.663755416870117
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.7424261569976807
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.42426300048828
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.95710754394531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.452026605606079
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.61299471445146
INFO:tools.evaluation_results_class:Counted Episodes = 7757
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.435657501220703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.35657501220703
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.20746612548828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.015662431716919
INFO:tools.evaluation_results_class:Current Best Return = 3.435657501220703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.67546782876185
INFO:tools.evaluation_results_class:Counted Episodes = 7802
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6882851123809814
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.882850646972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.408851623535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7231364250183105
INFO:tools.evaluation_results_class:Current Best Return = 3.6882851123809814
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.67546782876185
INFO:tools.evaluation_results_class:Counted Episodes = 7802
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.574946844456244
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 28 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6328577995300293
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.328575134277344
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.955501556396484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.139406204223633
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.528216994472295
INFO:tools.evaluation_results_class:Counted Episodes = 7779
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.269977569580078
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 13.65234088897705
INFO:agents.father_agent:Step: 10, Training loss: 19.057838439941406
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.675536632537842
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.755367279052734
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.34749984741211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.4149703979492188
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.528216994472295
INFO:tools.evaluation_results_class:Counted Episodes = 7779
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4659597873687744
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.65959930419922
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.48001480102539
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8463635444641113
INFO:tools.evaluation_results_class:Current Best Return = 3.4659597873687744
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.552177800485374
INFO:tools.evaluation_results_class:Counted Episodes = 7829
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.568655014038086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.68655014038086
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.384159088134766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.5013856887817383
INFO:tools.evaluation_results_class:Current Best Return = 3.568655014038086
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.552177800485374
INFO:tools.evaluation_results_class:Counted Episodes = 7829
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5831238259654006
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 29 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.797356367111206
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.97356414794922
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.45615768432617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3874518871307373
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.46714579055441
INFO:tools.evaluation_results_class:Counted Episodes = 7792
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.47252082824707
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 10.545228958129883
INFO:agents.father_agent:Step: 10, Training loss: 15.304828643798828
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.6895534992218018
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.89553451538086
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.559513092041016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.2322933673858643
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.46714579055441
INFO:tools.evaluation_results_class:Counted Episodes = 7792
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3510026931762695
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.51002502441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.402130126953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.0743958950042725
INFO:tools.evaluation_results_class:Current Best Return = 3.3510026931762695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.552177800485374
INFO:tools.evaluation_results_class:Counted Episodes = 7829
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7161834239959717
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.161834716796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.738807678222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.5354909896850586
INFO:tools.evaluation_results_class:Current Best Return = 3.7161834239959717
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.552177800485374
INFO:tools.evaluation_results_class:Counted Episodes = 7829
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.585612112666845
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 30 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.897717237472534
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.9771728515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.38715744018555
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.2988100051879883
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.27789822726693
INFO:tools.evaluation_results_class:Counted Episodes = 7841
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.004807472229004
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 12.13570499420166
INFO:agents.father_agent:Step: 10, Training loss: 11.905936241149902
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.743910312652588
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.43910217285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.03801727294922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.6268041133880615
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.27789822726693
INFO:tools.evaluation_results_class:Counted Episodes = 7841
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.384498119354248
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.8449821472168
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.748971939086914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.932974338531494
INFO:tools.evaluation_results_class:Current Best Return = 3.384498119354248
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.36823379923761
INFO:tools.evaluation_results_class:Counted Episodes = 7870
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6010165214538574
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.01016616821289
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.7249755859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.6026926040649414
INFO:tools.evaluation_results_class:Current Best Return = 3.6010165214538574
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.36823379923761
INFO:tools.evaluation_results_class:Counted Episodes = 7870
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5941729306420407
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 31 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6480774879455566
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.48077392578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.123233795166016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.361253499984741
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.21772345301757
INFO:tools.evaluation_results_class:Counted Episodes = 7854
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.093307495117188
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 12.246271133422852
INFO:agents.father_agent:Step: 10, Training loss: 15.313720703125
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.8273491859436035
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.27349090576172
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.82096862792969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.24088191986084
INFO:tools.evaluation_results_class:Current Best Return = 7.165080547332764
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.21772345301757
INFO:tools.evaluation_results_class:Counted Episodes = 7854
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6429836750030518
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.42983627319336
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.13691329956055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.914712905883789
INFO:tools.evaluation_results_class:Current Best Return = 3.6429836750030518
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.248226950354606
INFO:tools.evaluation_results_class:Counted Episodes = 7896
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.689842939376831
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.89842987060547
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.58639907836914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.144810676574707
INFO:tools.evaluation_results_class:Current Best Return = 3.689842939376831
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.248226950354606
INFO:tools.evaluation_results_class:Counted Episodes = 7896
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.601356443038882
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.240384578704834
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.403846740722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.92963409423828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.0257487297058105
INFO:tools.evaluation_results_class:Current Best Return = 4.240384578704834
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.92307692307692
INFO:tools.evaluation_results_class:Counted Episodes = 3328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 13.645833015441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 138.4583282470703
INFO:tools.evaluation_results_class:Average Discounted Reward = 91.85671997070312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.08289909362793
INFO:tools.evaluation_results_class:Current Best Return = 13.645833015441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 176.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.982421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.82421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.41289138793945
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.708674430847168
INFO:tools.evaluation_results_class:Current Best Return = 6.982421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.875
INFO:tools.evaluation_results_class:Counted Episodes = 2048
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.5703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 107.703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 70.60401153564453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.47943115234375
INFO:tools.evaluation_results_class:Current Best Return = 10.5703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 188.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.28515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 124.8515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.30899047851562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.6022796630859375
INFO:tools.evaluation_results_class:Current Best Return = 12.28515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 220.5
INFO:tools.evaluation_results_class:Counted Episodes = 512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.374267578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.74267578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.975862503051758
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.225402355194092
INFO:tools.evaluation_results_class:Current Best Return = 3.374267578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3916015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.916015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.121707916259766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2729172706604004
INFO:tools.evaluation_results_class:Current Best Return = 3.3916015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.396728515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.96728515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.174156188964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.365800380706787
INFO:tools.evaluation_results_class:Current Best Return = 3.396728515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.38671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.8671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.080177307128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.3377532958984375
INFO:tools.evaluation_results_class:Current Best Return = 3.38671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.39501953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.9501953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.15367889404297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.326381206512451
INFO:tools.evaluation_results_class:Current Best Return = 3.39501953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.373291015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.73291015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.96300506591797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.320371150970459
INFO:tools.evaluation_results_class:Current Best Return = 3.373291015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
2025-08-19 18:49:04.201295: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-19 18:49:04.203189: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-19 18:49:04.233791: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-19 18:49:04.233837: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-19 18:49:04.235113: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-19 18:49:04.240794: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-19 18:49:04.240988: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-19 18:49:04.771196: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/network/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/network/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"packets_sent"}max=? [F "done"] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 5480 states and 71240 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"packets_sent"}max=? [F "label_done"] 
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.943606853485107
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 51.436065673828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 39.80843734741211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.27094554901123
INFO:tools.evaluation_results_class:Current Best Return = 4.943606853485107
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.43908323281062
INFO:tools.evaluation_results_class:Counted Episodes = 3316
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 77.89917755126953
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 11.54080581665039
INFO:agents.father_agent:Step: 10, Training loss: 14.057847023010254
INFO:agents.father_agent:Step: 15, Training loss: 9.593164443969727
INFO:agents.father_agent:Step: 20, Training loss: 10.406230926513672
INFO:agents.father_agent:Step: 25, Training loss: 11.066544532775879
INFO:agents.father_agent:Step: 30, Training loss: 15.607590675354004
INFO:agents.father_agent:Step: 35, Training loss: 9.121844291687012
INFO:agents.father_agent:Step: 40, Training loss: 12.492100715637207
INFO:agents.father_agent:Step: 45, Training loss: 16.053569793701172
INFO:agents.father_agent:Step: 50, Training loss: 13.101057052612305
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 39.06%
INFO:agents.father_agent:Step: 55, Training loss: 8.568974494934082
INFO:agents.father_agent:Step: 60, Training loss: 13.330926895141602
INFO:agents.father_agent:Step: 65, Training loss: 15.858440399169922
INFO:agents.father_agent:Step: 70, Training loss: 12.253952026367188
INFO:agents.father_agent:Step: 75, Training loss: 13.658188819885254
INFO:agents.father_agent:Step: 80, Training loss: 11.307507514953613
INFO:agents.father_agent:Step: 85, Training loss: 16.565105438232422
INFO:agents.father_agent:Step: 90, Training loss: 14.466204643249512
INFO:agents.father_agent:Step: 95, Training loss: 13.880382537841797
INFO:agents.father_agent:Step: 100, Training loss: 11.457295417785645
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.96984338760376
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.69843292236328
INFO:tools.evaluation_results_class:Average Discounted Reward = 56.03590393066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11.859766006469727
INFO:tools.evaluation_results_class:Current Best Return = 6.96984338760376
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.43908323281062
INFO:tools.evaluation_results_class:Counted Episodes = 3316
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 7.057900905609131
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.57901000976562
INFO:tools.evaluation_results_class:Average Discounted Reward = 56.67266845703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11.86637020111084
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.43908323281062
INFO:tools.evaluation_results_class:Counted Episodes = 3316
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.248095989227295
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 64.48095703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 50.125144958496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10.146121978759766
INFO:tools.evaluation_results_class:Current Best Return = 6.248095989227295
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.3567662565905
INFO:tools.evaluation_results_class:Counted Episodes = 3414
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.0541887283325195
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.54188537597656
INFO:tools.evaluation_results_class:Average Discounted Reward = 56.5301513671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12.222312927246094
INFO:tools.evaluation_results_class:Current Best Return = 7.0541887283325195
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.3567662565905
INFO:tools.evaluation_results_class:Counted Episodes = 3414
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.416260673664608
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.860677242279053
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 60.60676956176758
INFO:tools.evaluation_results_class:Average Discounted Reward = 51.65256881713867
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.985363483428955
INFO:tools.evaluation_results_class:Current Best Return = 5.860677242279053
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.888888888888886
INFO:tools.evaluation_results_class:Counted Episodes = 2304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 9.217187881469727
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 94.171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.40129852294922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.095017433166504
INFO:tools.evaluation_results_class:Current Best Return = 9.217187881469727
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.9248046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 81.248046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 58.93049621582031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.6378999948501587
INFO:tools.evaluation_results_class:Current Best Return = 7.9248046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.75
INFO:tools.evaluation_results_class:Counted Episodes = 1024
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.833333015441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 110.33333587646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.26205444335938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7951390743255615
INFO:tools.evaluation_results_class:Current Best Return = 10.833333015441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 156.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.969400882720947
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.69400787353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 56.26227951049805
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.1637771129608154
INFO:tools.evaluation_results_class:Current Best Return = 6.969400882720947
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.051025390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 32.51025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.031444549560547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.119222640991211
INFO:tools.evaluation_results_class:Current Best Return = 3.051025390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.767795085906982
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.67794799804688
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.49715805053711
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.193042755126953
INFO:tools.evaluation_results_class:Current Best Return = 6.767795085906982
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.888888888888886
INFO:tools.evaluation_results_class:Counted Episodes = 2304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.589062690734863
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 107.890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 83.03470611572266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.060818195343018
INFO:tools.evaluation_results_class:Current Best Return = 10.589062690734863
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.580078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 87.80078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.82066345214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.4252281188964844
INFO:tools.evaluation_results_class:Current Best Return = 8.580078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.75
INFO:tools.evaluation_results_class:Counted Episodes = 1024
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 11.981770515441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 121.81771087646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 83.23522186279297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7496674060821533
INFO:tools.evaluation_results_class:Current Best Return = 11.981770515441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 156.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.881510257720947
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.81510162353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.39079666137695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.752887010574341
INFO:tools.evaluation_results_class:Current Best Return = 7.881510257720947
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.43359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.3359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.46577453613281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5995941162109375
INFO:tools.evaluation_results_class:Current Best Return = 3.43359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 2 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.745947360992432
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.45947265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 47.86213684082031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11.26379108428955
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.01766271473505
INFO:tools.evaluation_results_class:Counted Episodes = 4133
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.267229080200195
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 13.62038803100586
INFO:agents.father_agent:Step: 10, Training loss: 14.367067337036133
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 5.8828935623168945
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 60.82893753051758
INFO:tools.evaluation_results_class:Average Discounted Reward = 48.89580154418945
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11.734169006347656
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.01766271473505
INFO:tools.evaluation_results_class:Counted Episodes = 4133
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.3462700843811035
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.46269989013672
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.4229621887207
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9.839195251464844
INFO:tools.evaluation_results_class:Current Best Return = 5.3462700843811035
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 69.21752265861028
INFO:tools.evaluation_results_class:Counted Episodes = 4303
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.943992614746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 61.43992614746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.27871322631836
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11.650824546813965
INFO:tools.evaluation_results_class:Current Best Return = 5.943992614746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 69.21752265861028
INFO:tools.evaluation_results_class:Counted Episodes = 4303
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.4655053785618595
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 3 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.274274826049805
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.74274826049805
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.96656799316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10.251590728759766
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.77679697351829
INFO:tools.evaluation_results_class:Counted Episodes = 4758
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.57740592956543
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Step: 5, Training loss: 13.798691749572754
INFO:agents.father_agent:Step: 10, Training loss: 13.186555862426758
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 5.324716091156006
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.247161865234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 45.39240264892578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10.345378875732422
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.77679697351829
INFO:tools.evaluation_results_class:Counted Episodes = 4758
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.75805139541626
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 49.58051300048828
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.50790023803711
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.727232933044434
INFO:tools.evaluation_results_class:Current Best Return = 4.75805139541626
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.871993477374644
INFO:tools.evaluation_results_class:Counted Episodes = 4906
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 5.287810802459717
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.878108978271484
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.883548736572266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10.386385917663574
INFO:tools.evaluation_results_class:Current Best Return = 5.287810802459717
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.871993477374644
INFO:tools.evaluation_results_class:Counted Episodes = 4906
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.4972784795792218
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 4 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.844870567321777
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.44870376586914
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.955726623535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9.044583320617676
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.998849472674976
INFO:tools.evaluation_results_class:Counted Episodes = 5215
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.964103698730469
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Step: 5, Training loss: 12.332235336303711
INFO:agents.father_agent:Step: 10, Training loss: 14.473716735839844
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.857334613800049
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.57334518432617
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.07076644897461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9.21953296661377
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.998849472674976
INFO:tools.evaluation_results_class:Counted Episodes = 5215
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.370502471923828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.705020904541016
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.813934326171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.839676856994629
INFO:tools.evaluation_results_class:Current Best Return = 4.370502471923828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.070464767616194
INFO:tools.evaluation_results_class:Counted Episodes = 5336
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.946026802062988
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 51.460269927978516
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.634769439697266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9.421000480651855
INFO:tools.evaluation_results_class:Current Best Return = 4.946026802062988
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.070464767616194
INFO:tools.evaluation_results_class:Counted Episodes = 5336
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5101600854954405
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 5 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.663032054901123
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.63031768798828
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.908565521240234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.397178649902344
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.63425098319628
INFO:tools.evaluation_results_class:Counted Episodes = 5594
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.689693450927734
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Step: 5, Training loss: 9.89918041229248
INFO:agents.father_agent:Step: 10, Training loss: 12.705702781677246
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.645155429840088
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.45155334472656
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.7549934387207
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.269330024719238
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.63425098319628
INFO:tools.evaluation_results_class:Counted Episodes = 5594
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.243309497833252
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.4330940246582
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.204891204833984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.222066402435303
INFO:tools.evaluation_results_class:Current Best Return = 4.243309497833252
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.40038481721182
INFO:tools.evaluation_results_class:Counted Episodes = 5717
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.72450590133667
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 49.245059967041016
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.327903747558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.467569351196289
INFO:tools.evaluation_results_class:Current Best Return = 4.72450590133667
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.40038481721182
INFO:tools.evaluation_results_class:Counted Episodes = 5717
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.517565751091329
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 6 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.433265686035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.33265686035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 39.349365234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.222172737121582
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.19444444444444
INFO:tools.evaluation_results_class:Counted Episodes = 5904
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.5547513961792
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 12.327973365783691
INFO:agents.father_agent:Step: 10, Training loss: 12.616256713867188
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.543529987335205
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 47.435298919677734
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.2568244934082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.372427463531494
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.19444444444444
INFO:tools.evaluation_results_class:Counted Episodes = 5904
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.0687479972839355
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.68748092651367
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.04822540283203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.702347755432129
INFO:tools.evaluation_results_class:Current Best Return = 4.0687479972839355
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.82630355363666
INFO:tools.evaluation_results_class:Counted Episodes = 6022
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.51527738571167
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 47.152774810791016
INFO:tools.evaluation_results_class:Average Discounted Reward = 39.88291549682617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.00466537475586
INFO:tools.evaluation_results_class:Current Best Return = 4.51527738571167
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.82630355363666
INFO:tools.evaluation_results_class:Counted Episodes = 6022
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5129683880165232
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.141058921813965
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.41059112548828
INFO:tools.evaluation_results_class:Average Discounted Reward = 54.17808532714844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.02393913269043
INFO:tools.evaluation_results_class:Current Best Return = 6.141058921813965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.888888888888886
INFO:tools.evaluation_results_class:Counted Episodes = 2304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 9.478124618530273
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 96.78125
INFO:tools.evaluation_results_class:Average Discounted Reward = 74.73285675048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.222959041595459
INFO:tools.evaluation_results_class:Current Best Return = 9.478124618530273
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.0546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 82.546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.02989196777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.68841552734375
INFO:tools.evaluation_results_class:Current Best Return = 8.0546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.75
INFO:tools.evaluation_results_class:Counted Episodes = 1024
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 11.165364265441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 113.65364837646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 77.54523468017578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.9869773387908936
INFO:tools.evaluation_results_class:Current Best Return = 11.165364265441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 156.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.174479007720947
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 73.74478912353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 57.88434982299805
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.284661054611206
INFO:tools.evaluation_results_class:Current Best Return = 7.174479007720947
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.14013671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.4013671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.854267120361328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1908106803894043
INFO:tools.evaluation_results_class:Current Best Return = 3.14013671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.190673828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.90673828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.318355560302734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.0957236289978027
INFO:tools.evaluation_results_class:Current Best Return = 3.190673828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.22265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.2265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.601259231567383
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1403656005859375
INFO:tools.evaluation_results_class:Current Best Return = 3.22265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.200927734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.00927734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.398082733154297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.142489433288574
INFO:tools.evaluation_results_class:Current Best Return = 3.200927734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2216796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.216796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.587116241455078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.124197483062744
INFO:tools.evaluation_results_class:Current Best Return = 3.2216796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.216552734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.16552734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.547748565673828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.0988569259643555
INFO:tools.evaluation_results_class:Current Best Return = 3.216552734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.947916507720947
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.47916412353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 61.15943145751953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.9443359375
INFO:tools.evaluation_results_class:Current Best Return = 6.947916507720947
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.888888888888886
INFO:tools.evaluation_results_class:Counted Episodes = 2304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.619531631469727
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 108.1953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 83.43912506103516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.81383752822876
INFO:tools.evaluation_results_class:Current Best Return = 10.619531631469727
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.638671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 88.38671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.4030990600586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.3655357360839844
INFO:tools.evaluation_results_class:Current Best Return = 8.638671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.75
INFO:tools.evaluation_results_class:Counted Episodes = 1024
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.139323234558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 123.39322662353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 84.28205871582031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7292869091033936
INFO:tools.evaluation_results_class:Current Best Return = 12.139323234558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 156.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.008463859558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 82.08463287353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.41675567626953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.5695903301239014
INFO:tools.evaluation_results_class:Current Best Return = 8.008463859558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.535400390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.35400390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.39432907104492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.588590621948242
INFO:tools.evaluation_results_class:Current Best Return = 3.535400390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.515380859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.15380859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.208740234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5476150512695312
INFO:tools.evaluation_results_class:Current Best Return = 3.515380859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.513916015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.13916015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.198299407958984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6267600059509277
INFO:tools.evaluation_results_class:Current Best Return = 3.513916015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5302734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.302734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.358150482177734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5762314796447754
INFO:tools.evaluation_results_class:Current Best Return = 3.5302734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.504638671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.04638671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.1157341003418
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.567849636077881
INFO:tools.evaluation_results_class:Current Best Return = 3.504638671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.510986328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.10986328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.182525634765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5633559226989746
INFO:tools.evaluation_results_class:Current Best Return = 3.510986328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 7 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.333712100982666
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.337120056152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.706932067871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.734603404998779
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.36779743548125
INFO:tools.evaluation_results_class:Counted Episodes = 6161
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.744709968566895
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 10.61205768585205
INFO:agents.father_agent:Step: 10, Training loss: 12.459102630615234
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.228372097015381
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.283721923828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.80419158935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.763460636138916
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.36779743548125
INFO:tools.evaluation_results_class:Counted Episodes = 6161
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.976026773452759
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.76026916503906
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.53451156616211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.907049179077148
INFO:tools.evaluation_results_class:Current Best Return = 3.976026773452759
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.99952053699856
INFO:tools.evaluation_results_class:Counted Episodes = 6257
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.31868314743042
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.186832427978516
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.52082824707031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.107646942138672
INFO:tools.evaluation_results_class:Current Best Return = 4.31868314743042
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.99952053699856
INFO:tools.evaluation_results_class:Counted Episodes = 6257
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.4906359487180763
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 8 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.159918308258057
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.59918212890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.40537643432617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.30791711807251
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.96861268047709
INFO:tools.evaluation_results_class:Counted Episodes = 6372
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.361824989318848
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 13.160221099853516
INFO:agents.father_agent:Step: 10, Training loss: 13.11725902557373
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.152071475982666
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.520713806152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.3413200378418
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.373139381408691
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.96861268047709
INFO:tools.evaluation_results_class:Counted Episodes = 6372
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8496832847595215
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.49683380126953
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.64985656738281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.61018705368042
INFO:tools.evaluation_results_class:Current Best Return = 3.8496832847595215
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.44477058550904
INFO:tools.evaluation_results_class:Counted Episodes = 6473
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.123435974121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.23435974121094
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.019989013671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.666518688201904
INFO:tools.evaluation_results_class:Current Best Return = 4.123435974121094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.44477058550904
INFO:tools.evaluation_results_class:Counted Episodes = 6473
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.4702019162151
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 9 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.03148889541626
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.31488800048828
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.45283889770508
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.2961649894714355
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.919902170590035
INFO:tools.evaluation_results_class:Counted Episodes = 6542
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.101363182067871
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 14.370973587036133
INFO:agents.father_agent:Step: 10, Training loss: 12.691387176513672
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.080862045288086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.80862045288086
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.85247802734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.0196003913879395
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.919902170590035
INFO:tools.evaluation_results_class:Counted Episodes = 6542
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.682420253753662
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.82420349121094
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.36688232421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.2037787437438965
INFO:tools.evaluation_results_class:Current Best Return = 3.682420253753662
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.277543648404574
INFO:tools.evaluation_results_class:Counted Episodes = 6644
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.070289134979248
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.70288848876953
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.70987319946289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.2239885330200195
INFO:tools.evaluation_results_class:Current Best Return = 4.070289134979248
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.277543648404574
INFO:tools.evaluation_results_class:Counted Episodes = 6644
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.457875637898488
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 10 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.974167585372925
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.741676330566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.1019172668457
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.532108783721924
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.99656562639988
INFO:tools.evaluation_results_class:Counted Episodes = 6697
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.8257417678833
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 15.035894393920898
INFO:agents.father_agent:Step: 10, Training loss: 15.872333526611328
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.9659550189971924
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.659549713134766
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.99037170410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.841158866882324
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.99656562639988
INFO:tools.evaluation_results_class:Counted Episodes = 6697
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.648169994354248
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.4817008972168
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.152381896972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.910159111022949
INFO:tools.evaluation_results_class:Current Best Return = 3.648169994354248
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.43093270365998
INFO:tools.evaluation_results_class:Counted Episodes = 6776
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.027449607849121
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.274497985839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.495506286621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.656566619873047
INFO:tools.evaluation_results_class:Current Best Return = 4.027449607849121
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 44.43093270365998
INFO:tools.evaluation_results_class:Counted Episodes = 6776
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.457287394886141
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 11 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.033226013183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.33226013183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.72731399536133
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.279487133026123
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.23770491803279
INFO:tools.evaluation_results_class:Counted Episodes = 6832
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 18.599340438842773
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 14.575092315673828
INFO:agents.father_agent:Step: 10, Training loss: 15.047873497009277
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.961211919784546
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.612117767333984
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.12820816040039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.516498565673828
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.23770491803279
INFO:tools.evaluation_results_class:Counted Episodes = 6832
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.616978168487549
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.16978073120117
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.050506591796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.58138370513916
INFO:tools.evaluation_results_class:Current Best Return = 3.616978168487549
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.63189917427206
INFO:tools.evaluation_results_class:Counted Episodes = 6903
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9307546615600586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.30754852294922
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.754310607910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.491945743560791
INFO:tools.evaluation_results_class:Current Best Return = 3.9307546615600586
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 43.63189917427206
INFO:tools.evaluation_results_class:Counted Episodes = 6903
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.4687148788322664
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.032552242279053
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 62.32551956176758
INFO:tools.evaluation_results_class:Average Discounted Reward = 53.19852828979492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.113089561462402
INFO:tools.evaluation_results_class:Current Best Return = 6.032552242279053
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.888888888888886
INFO:tools.evaluation_results_class:Counted Episodes = 2304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 9.474218368530273
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 96.7421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 74.44746398925781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.275897979736328
INFO:tools.evaluation_results_class:Current Best Return = 9.474218368530273
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.1328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 83.328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.446372985839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.71673583984375
INFO:tools.evaluation_results_class:Current Best Return = 8.1328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.75
INFO:tools.evaluation_results_class:Counted Episodes = 1024
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 11.205729484558105
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 114.05728912353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 77.73871612548828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.908196210861206
INFO:tools.evaluation_results_class:Current Best Return = 11.205729484558105
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 156.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.212239742279053
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 74.12239837646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 58.121612548828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.099485397338867
INFO:tools.evaluation_results_class:Current Best Return = 7.212239742279053
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.08935546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 32.8935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.392807006835938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2005114555358887
INFO:tools.evaluation_results_class:Current Best Return = 3.08935546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.072265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 32.72265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.23911476135254
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1549339294433594
INFO:tools.evaluation_results_class:Current Best Return = 3.072265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.07568359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 32.7568359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.2608585357666
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.127573013305664
INFO:tools.evaluation_results_class:Current Best Return = 3.07568359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.09521484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 32.9521484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.45319175720215
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.111051321029663
INFO:tools.evaluation_results_class:Current Best Return = 3.09521484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.0830078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 32.830078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.341350555419922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2069764137268066
INFO:tools.evaluation_results_class:Current Best Return = 3.0830078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.087158203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 32.87158203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.373600006103516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.175753116607666
INFO:tools.evaluation_results_class:Current Best Return = 3.087158203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.18701171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.8701171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.267467498779297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.189635753631592
INFO:tools.evaluation_results_class:Current Best Return = 3.18701171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.169189453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.69189453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.11021614074707
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2133188247680664
INFO:tools.evaluation_results_class:Current Best Return = 3.169189453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.163330078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.63330078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.062252044677734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.176692485809326
INFO:tools.evaluation_results_class:Current Best Return = 3.163330078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.15185546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.5185546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.95956802368164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1698107719421387
INFO:tools.evaluation_results_class:Current Best Return = 3.15185546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.172607421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.72607421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.140548706054688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2033610343933105
INFO:tools.evaluation_results_class:Current Best Return = 3.172607421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.850260257720947
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 70.50260162353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.12371063232422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.103012561798096
INFO:tools.evaluation_results_class:Current Best Return = 6.850260257720947
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.888888888888886
INFO:tools.evaluation_results_class:Counted Episodes = 2304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.574999809265137
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 107.75
INFO:tools.evaluation_results_class:Average Discounted Reward = 83.13765716552734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6.319375038146973
INFO:tools.evaluation_results_class:Current Best Return = 10.574999809265137
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.619140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 88.19140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.0754623413086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.3959617614746094
INFO:tools.evaluation_results_class:Current Best Return = 8.619140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.75
INFO:tools.evaluation_results_class:Counted Episodes = 1024
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.16015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 123.6015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 84.47638702392578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.480860471725464
INFO:tools.evaluation_results_class:Current Best Return = 12.16015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 156.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.94921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 81.4921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.99100875854492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.451848268508911
INFO:tools.evaluation_results_class:Current Best Return = 7.94921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.485595703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.85595703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.943946838378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5979371070861816
INFO:tools.evaluation_results_class:Current Best Return = 3.485595703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.48974609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.8974609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.97715759277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6644458770751953
INFO:tools.evaluation_results_class:Current Best Return = 3.48974609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.49072265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.9072265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.98521423339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6918087005615234
INFO:tools.evaluation_results_class:Current Best Return = 3.49072265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.472412109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.72412109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.82271957397461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.722383975982666
INFO:tools.evaluation_results_class:Current Best Return = 3.472412109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.502685546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.02685546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.08817672729492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.665520668029785
INFO:tools.evaluation_results_class:Current Best Return = 3.502685546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.47705078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.7705078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.848052978515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6679301261901855
INFO:tools.evaluation_results_class:Current Best Return = 3.47705078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.446044921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.46044921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.5900764465332
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.58595609664917
INFO:tools.evaluation_results_class:Current Best Return = 3.446044921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.476806640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.76806640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.84855651855469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.513622283935547
INFO:tools.evaluation_results_class:Current Best Return = 3.476806640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.474853515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.74853515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.836509704589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.594094753265381
INFO:tools.evaluation_results_class:Current Best Return = 3.474853515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4755859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.755859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.8411865234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.618055820465088
INFO:tools.evaluation_results_class:Current Best Return = 3.4755859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.462158203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.62158203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.726470947265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.585482120513916
INFO:tools.evaluation_results_class:Current Best Return = 3.462158203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 12 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8723037242889404
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.72303771972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.4168815612793
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.199683666229248
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.570319240724764
INFO:tools.evaluation_results_class:Counted Episodes = 6954
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.521880149841309
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 16.74700164794922
INFO:agents.father_agent:Step: 10, Training loss: 18.072145462036133
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.9476559162139893
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.476558685302734
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.09123611450195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.268470764160156
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.570319240724764
INFO:tools.evaluation_results_class:Counted Episodes = 6954
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.561877727508545
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.618778228759766
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.65599060058594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.449300289154053
INFO:tools.evaluation_results_class:Current Best Return = 3.561877727508545
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.86173541963016
INFO:tools.evaluation_results_class:Counted Episodes = 7030
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8846373558044434
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.84637451171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.485450744628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.1635050773620605
INFO:tools.evaluation_results_class:Current Best Return = 3.8846373558044434
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.86173541963016
INFO:tools.evaluation_results_class:Counted Episodes = 7030
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.482901813360427
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 13 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7952232360839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.952232360839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.84669876098633
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.672735691070557
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.925946862634255
INFO:tools.evaluation_results_class:Counted Episodes = 7076
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 16.660282135009766
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 16.29781150817871
INFO:agents.father_agent:Step: 10, Training loss: 16.15332794189453
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.8551442623138428
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.55144119262695
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.350730895996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.938174247741699
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.925946862634255
INFO:tools.evaluation_results_class:Counted Episodes = 7076
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.474782943725586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.74782943725586
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.02922439575195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.099181652069092
INFO:tools.evaluation_results_class:Current Best Return = 3.474782943725586
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.234519473241804
INFO:tools.evaluation_results_class:Counted Episodes = 7138
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.978145122528076
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.78145217895508
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.425453186035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.883523464202881
INFO:tools.evaluation_results_class:Current Best Return = 3.978145122528076
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 42.234519473241804
INFO:tools.evaluation_results_class:Counted Episodes = 7138
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.484749484102407
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 14 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.822770595550537
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.22770690917969
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.15129089355469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.417832851409912
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.63881099270892
INFO:tools.evaluation_results_class:Counted Episodes = 7132
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.64475440979004
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 16.144580841064453
INFO:agents.father_agent:Step: 10, Training loss: 12.038227081298828
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.917975425720215
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.179752349853516
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.96338653564453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.767669200897217
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.63881099270892
INFO:tools.evaluation_results_class:Counted Episodes = 7132
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.576843023300171
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.768428802490234
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.95371627807617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.220494270324707
INFO:tools.evaluation_results_class:Current Best Return = 3.576843023300171
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.86575038178537
INFO:tools.evaluation_results_class:Counted Episodes = 7203
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8992085456848145
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.992088317871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.826358795166016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.832545757293701
INFO:tools.evaluation_results_class:Current Best Return = 3.8992085456848145
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.86575038178537
INFO:tools.evaluation_results_class:Counted Episodes = 7203
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.4906529786661427
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 15 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7185256481170654
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.18525695800781
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.337608337402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.270993232727051
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.07785753727222
INFO:tools.evaluation_results_class:Counted Episodes = 7244
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 20.574819564819336
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 15.828495979309082
INFO:agents.father_agent:Step: 10, Training loss: 14.895313262939453
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.8155715465545654
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.15571594238281
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.240135192871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.1984543800354
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.07785753727222
INFO:tools.evaluation_results_class:Counted Episodes = 7244
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4705238342285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.705238342285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.12368392944336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.804596185684204
INFO:tools.evaluation_results_class:Current Best Return = 3.4705238342285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.26808918068664
INFO:tools.evaluation_results_class:Counted Episodes = 7311
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8528244495391846
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.52824401855469
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.485137939453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.4554290771484375
INFO:tools.evaluation_results_class:Current Best Return = 3.8528244495391846
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 41.26808918068664
INFO:tools.evaluation_results_class:Counted Episodes = 7311
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.4880238331572824
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 16 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.790971279144287
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.90971374511719
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.02505111694336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.141806125640869
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.75266757865937
INFO:tools.evaluation_results_class:Counted Episodes = 7310
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 19.658302307128906
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 17.75857925415039
INFO:agents.father_agent:Step: 10, Training loss: 16.69333267211914
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.7124485969543457
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.124488830566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.35822677612305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.337833881378174
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.75266757865937
INFO:tools.evaluation_results_class:Counted Episodes = 7310
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.520683526992798
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.20683670043945
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.62392044067383
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.8315608501434326
INFO:tools.evaluation_results_class:Current Best Return = 3.520683526992798
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.926217279262175
INFO:tools.evaluation_results_class:Counted Episodes = 7373
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8498575687408447
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.49857711791992
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.504947662353516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.23908805847168
INFO:tools.evaluation_results_class:Current Best Return = 3.8498575687408447
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.926217279262175
INFO:tools.evaluation_results_class:Counted Episodes = 7373
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.493366043785295
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.118923664093018
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.18923568725586
INFO:tools.evaluation_results_class:Average Discounted Reward = 53.93335723876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.180301189422607
INFO:tools.evaluation_results_class:Current Best Return = 6.118923664093018
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.888888888888886
INFO:tools.evaluation_results_class:Counted Episodes = 2304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 9.489062309265137
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 96.890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 74.62413024902344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.946755409240723
INFO:tools.evaluation_results_class:Current Best Return = 9.489062309265137
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.15625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 83.5625
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.683624267578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.6279296875
INFO:tools.evaluation_results_class:Current Best Return = 8.15625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.75
INFO:tools.evaluation_results_class:Counted Episodes = 1024
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 11.130208015441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 113.30208587646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 77.33016204833984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.8866918087005615
INFO:tools.evaluation_results_class:Current Best Return = 11.130208015441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 156.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 74.34375
INFO:tools.evaluation_results_class:Average Discounted Reward = 58.264404296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3070476055145264
INFO:tools.evaluation_results_class:Current Best Return = 7.234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.144287109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.44287109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.904052734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1278629302978516
INFO:tools.evaluation_results_class:Current Best Return = 3.144287109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.095458984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 32.95458984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.45384979248047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1669135093688965
INFO:tools.evaluation_results_class:Current Best Return = 3.095458984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.12158203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.2158203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.699317932128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.120959758758545
INFO:tools.evaluation_results_class:Current Best Return = 3.12158203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.11572265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.1572265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.64236068725586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2073116302490234
INFO:tools.evaluation_results_class:Current Best Return = 3.11572265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.1025390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.5299015045166
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.173079013824463
INFO:tools.evaluation_results_class:Current Best Return = 3.1025390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.1123046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.123046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.613262176513672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.183187961578369
INFO:tools.evaluation_results_class:Current Best Return = 3.1123046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.0927734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 32.927734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.431907653808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1344590187072754
INFO:tools.evaluation_results_class:Current Best Return = 3.0927734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.098388671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 32.98388671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.48085594177246
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1028685569763184
INFO:tools.evaluation_results_class:Current Best Return = 3.098388671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.108154296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.08154296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.57215118408203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.120871067047119
INFO:tools.evaluation_results_class:Current Best Return = 3.108154296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.08837890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 32.8837890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.39996337890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.0986344814300537
INFO:tools.evaluation_results_class:Current Best Return = 3.08837890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.070068359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 32.70068359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.22509765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.0939674377441406
INFO:tools.evaluation_results_class:Current Best Return = 3.070068359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.17138671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.7138671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.147701263427734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.0839076042175293
INFO:tools.evaluation_results_class:Current Best Return = 3.17138671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.160888671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.60888671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.046489715576172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1237730979919434
INFO:tools.evaluation_results_class:Current Best Return = 3.160888671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.154541015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.54541015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.997501373291016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1018500328063965
INFO:tools.evaluation_results_class:Current Best Return = 3.154541015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.1884765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.884765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.283950805664062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1011948585510254
INFO:tools.evaluation_results_class:Current Best Return = 3.1884765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.20654296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.0654296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.45761489868164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1380038261413574
INFO:tools.evaluation_results_class:Current Best Return = 3.20654296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.879340171813965
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 70.79340362548828
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.50551986694336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.975024700164795
INFO:tools.evaluation_results_class:Current Best Return = 6.879340171813965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.888888888888886
INFO:tools.evaluation_results_class:Counted Episodes = 2304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.603906631469727
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 108.0390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 83.29423522949219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.911078929901123
INFO:tools.evaluation_results_class:Current Best Return = 10.603906631469727
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.5439453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 87.439453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.679595947265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.4511935710906982
INFO:tools.evaluation_results_class:Current Best Return = 8.5439453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.75
INFO:tools.evaluation_results_class:Counted Episodes = 1024
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.22265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 124.2265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 84.85338592529297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.537663698196411
INFO:tools.evaluation_results_class:Current Best Return = 12.22265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 156.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.853515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.53515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.228759765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.6054954528808594
INFO:tools.evaluation_results_class:Current Best Return = 7.853515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.48388671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.8388671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.95049285888672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6589198112487793
INFO:tools.evaluation_results_class:Current Best Return = 3.48388671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.15625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.22731399536133
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.486572265625
INFO:tools.evaluation_results_class:Current Best Return = 3.515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.52490234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.2490234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.313899993896484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.515981674194336
INFO:tools.evaluation_results_class:Current Best Return = 3.52490234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.154014587402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.51751708984375
INFO:tools.evaluation_results_class:Current Best Return = 3.5078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.49951171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.9951171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.092010498046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.506347179412842
INFO:tools.evaluation_results_class:Current Best Return = 3.49951171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.525390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.25390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.310035705566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5423240661621094
INFO:tools.evaluation_results_class:Current Best Return = 3.525390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.510498046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.10498046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.18315124511719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5345582962036133
INFO:tools.evaluation_results_class:Current Best Return = 3.510498046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.482177734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.82177734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.92973327636719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5880613327026367
INFO:tools.evaluation_results_class:Current Best Return = 3.482177734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.465087890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.65087890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.7779541015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.586183786392212
INFO:tools.evaluation_results_class:Current Best Return = 3.465087890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.50390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.0390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.113624572753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5795745849609375
INFO:tools.evaluation_results_class:Current Best Return = 3.50390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.512451171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.12451171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.20533752441406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.584317684173584
INFO:tools.evaluation_results_class:Current Best Return = 3.512451171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.489990234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.89990234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.986480712890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5296850204467773
INFO:tools.evaluation_results_class:Current Best Return = 3.489990234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.475341796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.75341796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.86924743652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.560427188873291
INFO:tools.evaluation_results_class:Current Best Return = 3.475341796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4833984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.833984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.9342155456543
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5402512550354004
INFO:tools.evaluation_results_class:Current Best Return = 3.4833984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.50439453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.0439453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.117156982421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5331835746765137
INFO:tools.evaluation_results_class:Current Best Return = 3.50439453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4892578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.892578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.98406982421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5213685035705566
INFO:tools.evaluation_results_class:Current Best Return = 3.4892578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 17 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.797044515609741
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.97044372558594
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.169471740722656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.289475917816162
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.43329718004338
INFO:tools.evaluation_results_class:Counted Episodes = 7376
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 16.24481773376465
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 16.85201644897461
INFO:agents.father_agent:Step: 10, Training loss: 13.857780456542969
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.7151572704315186
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.151573181152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.396480560302734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.208588123321533
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.43329718004338
INFO:tools.evaluation_results_class:Counted Episodes = 7376
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3808820247650146
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.80881881713867
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.413711547851562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.597736120223999
INFO:tools.evaluation_results_class:Current Best Return = 3.3808820247650146
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.58053240118311
INFO:tools.evaluation_results_class:Counted Episodes = 7438
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8069374561309814
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.069374084472656
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.16754150390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.175149440765381
INFO:tools.evaluation_results_class:Current Best Return = 3.8069374561309814
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.58053240118311
INFO:tools.evaluation_results_class:Counted Episodes = 7438
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.498899385245748
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 18 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7414371967315674
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.414371490478516
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.67548751831055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.9373092651367188
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.11511081262592
INFO:tools.evaluation_results_class:Counted Episodes = 7445
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.83346176147461
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 16.291452407836914
INFO:agents.father_agent:Step: 10, Training loss: 12.510698318481445
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.670248508453369
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.702484130859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.08773422241211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.899188756942749
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.11511081262592
INFO:tools.evaluation_results_class:Counted Episodes = 7445
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4357848167419434
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.357845306396484
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.973661422729492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.417206287384033
INFO:tools.evaluation_results_class:Current Best Return = 3.4357848167419434
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.2326139088729
INFO:tools.evaluation_results_class:Counted Episodes = 7506
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.782973527908325
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.829734802246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.061153411865234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.858708143234253
INFO:tools.evaluation_results_class:Current Best Return = 3.782973527908325
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.2326139088729
INFO:tools.evaluation_results_class:Counted Episodes = 7506
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.499211120904485
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 19 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.747633695602417
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.47633743286133
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.80895233154297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.16414737701416
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.855619250766566
INFO:tools.evaluation_results_class:Counted Episodes = 7501
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.81430435180664
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 16.077608108520508
INFO:agents.father_agent:Step: 10, Training loss: 12.385123252868652
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.708838939666748
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.08838653564453
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.416194915771484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7205843925476074
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.855619250766566
INFO:tools.evaluation_results_class:Counted Episodes = 7501
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2976410388946533
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.976409912109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.763723373413086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.533196449279785
INFO:tools.evaluation_results_class:Current Best Return = 3.2976410388946533
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.01987808110257
INFO:tools.evaluation_results_class:Counted Episodes = 7546
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.627882242202759
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.27882385253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.72307586669922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.8509268760681152
INFO:tools.evaluation_results_class:Current Best Return = 3.627882242202759
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 40.01987808110257
INFO:tools.evaluation_results_class:Counted Episodes = 7546
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.4875113788410173
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 20 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.667285203933716
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.6728515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.142295837402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7721216678619385
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.65217391304348
INFO:tools.evaluation_results_class:Counted Episodes = 7544
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 16.349519729614258
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 15.82962703704834
INFO:agents.father_agent:Step: 10, Training loss: 11.224991798400879
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.6497879028320312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.49787902832031
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.918853759765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.602431297302246
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.65217391304348
INFO:tools.evaluation_results_class:Counted Episodes = 7544
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.30899977684021
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.09000015258789
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.929569244384766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.1472387313842773
INFO:tools.evaluation_results_class:Current Best Return = 3.30899977684021
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.801423112399526
INFO:tools.evaluation_results_class:Counted Episodes = 7589
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.67189359664917
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.718936920166016
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.1274299621582
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7724359035491943
INFO:tools.evaluation_results_class:Current Best Return = 3.67189359664917
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.801423112399526
INFO:tools.evaluation_results_class:Counted Episodes = 7589
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.495018096978047
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 21 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6953485012054443
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.95348358154297
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.40483474731445
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.744056463241577
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.44775332718408
INFO:tools.evaluation_results_class:Counted Episodes = 7589
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.261810302734375
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 15.458051681518555
INFO:agents.father_agent:Step: 10, Training loss: 14.191044807434082
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.711819648742676
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.11819839477539
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.547367095947266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.5046448707580566
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.44775332718408
INFO:tools.evaluation_results_class:Counted Episodes = 7589
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.347569704055786
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.4756965637207
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.281349182128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.1731817722320557
INFO:tools.evaluation_results_class:Current Best Return = 3.347569704055786
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.579850648499935
INFO:tools.evaluation_results_class:Counted Episodes = 7633
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7551422119140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.551422119140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.879844665527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.9876015186309814
INFO:tools.evaluation_results_class:Current Best Return = 3.7551422119140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.579850648499935
INFO:tools.evaluation_results_class:Counted Episodes = 7633
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5041768873435215
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.11805534362793
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.18055725097656
INFO:tools.evaluation_results_class:Average Discounted Reward = 53.9422607421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.9365835189819336
INFO:tools.evaluation_results_class:Current Best Return = 6.11805534362793
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.888888888888886
INFO:tools.evaluation_results_class:Counted Episodes = 2304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 9.543749809265137
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 97.4375
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.0709457397461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.788710594177246
INFO:tools.evaluation_results_class:Current Best Return = 9.543749809265137
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.1650390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 83.650390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.681724548339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.643660306930542
INFO:tools.evaluation_results_class:Current Best Return = 8.1650390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.75
INFO:tools.evaluation_results_class:Counted Episodes = 1024
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 11.235676765441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 114.35677337646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.1960678100586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.734820604324341
INFO:tools.evaluation_results_class:Current Best Return = 11.235676765441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 156.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.324869632720947
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 75.24869537353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.04976272583008
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.093027114868164
INFO:tools.evaluation_results_class:Current Best Return = 7.324869632720947
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.127197265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.27197265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.757919311523438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.164240837097168
INFO:tools.evaluation_results_class:Current Best Return = 3.127197265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.16357421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.6357421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.068506240844727
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.115821361541748
INFO:tools.evaluation_results_class:Current Best Return = 3.16357421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.188232421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.88232421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.28399658203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.213347911834717
INFO:tools.evaluation_results_class:Current Best Return = 3.188232421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.1708984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.708984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.138408660888672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1504807472229004
INFO:tools.evaluation_results_class:Current Best Return = 3.1708984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.149169921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.49169921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.93358612060547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.120570659637451
INFO:tools.evaluation_results_class:Current Best Return = 3.149169921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.183349609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.83349609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.247461318969727
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1712169647216797
INFO:tools.evaluation_results_class:Current Best Return = 3.183349609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.160400390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.60400390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.048450469970703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.235746383666992
INFO:tools.evaluation_results_class:Current Best Return = 3.160400390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.147705078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.47705078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.93401527404785
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2128024101257324
INFO:tools.evaluation_results_class:Current Best Return = 3.147705078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.151123046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.51123046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.969221115112305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2151994705200195
INFO:tools.evaluation_results_class:Current Best Return = 3.151123046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.151123046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.51123046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.97041130065918
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2313127517700195
INFO:tools.evaluation_results_class:Current Best Return = 3.151123046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.14794921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.4794921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.938358306884766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2671732902526855
INFO:tools.evaluation_results_class:Current Best Return = 3.14794921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.136962890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.36962890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.8343505859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.181680679321289
INFO:tools.evaluation_results_class:Current Best Return = 3.136962890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.146240234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.46240234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.918807983398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.20639705657959
INFO:tools.evaluation_results_class:Current Best Return = 3.146240234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.142578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.42578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.875030517578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1754722595214844
INFO:tools.evaluation_results_class:Current Best Return = 3.142578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.135986328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.35986328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.820932388305664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2395644187927246
INFO:tools.evaluation_results_class:Current Best Return = 3.135986328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.12890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.2890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.753700256347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1752777099609375
INFO:tools.evaluation_results_class:Current Best Return = 3.12890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.221923828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.21923828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.588600158691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1839041709899902
INFO:tools.evaluation_results_class:Current Best Return = 3.221923828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.221923828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.21923828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.596355438232422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1482596397399902
INFO:tools.evaluation_results_class:Current Best Return = 3.221923828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2197265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.197265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.576499938964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1284775733947754
INFO:tools.evaluation_results_class:Current Best Return = 3.2197265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.18408203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.8408203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.258073806762695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.127734661102295
INFO:tools.evaluation_results_class:Current Best Return = 3.18408203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.197509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.97509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.377307891845703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1282262802124023
INFO:tools.evaluation_results_class:Current Best Return = 3.197509765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.884548664093018
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 70.84548950195312
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.52664566040039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.081288814544678
INFO:tools.evaluation_results_class:Current Best Return = 6.884548664093018
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.888888888888886
INFO:tools.evaluation_results_class:Counted Episodes = 2304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.711718559265137
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.1171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 84.10945892333984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.894237518310547
INFO:tools.evaluation_results_class:Current Best Return = 10.711718559265137
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.544921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 87.44921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.542510986328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.3729820251464844
INFO:tools.evaluation_results_class:Current Best Return = 8.544921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.75
INFO:tools.evaluation_results_class:Counted Episodes = 1024
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.1015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 123.015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 83.98098754882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.70062255859375
INFO:tools.evaluation_results_class:Current Best Return = 12.1015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 156.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.948567867279053
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 81.48567962646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.05309295654297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.5409746170043945
INFO:tools.evaluation_results_class:Current Best Return = 7.948567867279053
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.510009765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.10009765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.187408447265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6034154891967773
INFO:tools.evaluation_results_class:Current Best Return = 3.510009765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.45263671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.5263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.665733337402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6291041374206543
INFO:tools.evaluation_results_class:Current Best Return = 3.45263671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.453857421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.53857421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.677127838134766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.581367015838623
INFO:tools.evaluation_results_class:Current Best Return = 3.453857421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.59857177734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.63323974609375
INFO:tools.evaluation_results_class:Current Best Return = 3.4453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4443359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.443359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.597511291503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.593092441558838
INFO:tools.evaluation_results_class:Current Best Return = 3.4443359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.455078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.55078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.693138122558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5995445251464844
INFO:tools.evaluation_results_class:Current Best Return = 3.455078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.522705078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.22705078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.29218292236328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5492892265319824
INFO:tools.evaluation_results_class:Current Best Return = 3.522705078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.513671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.13671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.20655822753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5906333923339844
INFO:tools.evaluation_results_class:Current Best Return = 3.513671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.51904296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.1904296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.252098083496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5274691581726074
INFO:tools.evaluation_results_class:Current Best Return = 3.51904296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.50634765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.0634765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.139564514160156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.506795883178711
INFO:tools.evaluation_results_class:Current Best Return = 3.50634765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.0
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.09938430786133
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.47021484375
INFO:tools.evaluation_results_class:Current Best Return = 3.5
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.15580749511719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.63470458984375
INFO:tools.evaluation_results_class:Current Best Return = 3.5078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5224609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.224609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.282203674316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.621565341949463
INFO:tools.evaluation_results_class:Current Best Return = 3.5224609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.54345703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.4345703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.46968078613281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5845370292663574
INFO:tools.evaluation_results_class:Current Best Return = 3.54345703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.511962890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.11962890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.19013595581055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5740761756896973
INFO:tools.evaluation_results_class:Current Best Return = 3.511962890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.526123046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.26123046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.31330871582031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5598649978637695
INFO:tools.evaluation_results_class:Current Best Return = 3.526123046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5185546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.185546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.23847961425781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.563620090484619
INFO:tools.evaluation_results_class:Current Best Return = 3.5185546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.51171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.1171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.1763801574707
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5530853271484375
INFO:tools.evaluation_results_class:Current Best Return = 3.51171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4833984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.833984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.936851501464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6286301612854004
INFO:tools.evaluation_results_class:Current Best Return = 3.4833984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.51025390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.1025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.168975830078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5257740020751953
INFO:tools.evaluation_results_class:Current Best Return = 3.51025390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4912109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.912109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.00843048095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.559980869293213
INFO:tools.evaluation_results_class:Current Best Return = 3.4912109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 22 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.649901866912842
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.49901580810547
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.00494384765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.8432464599609375
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.24492468893255
INFO:tools.evaluation_results_class:Counted Episodes = 7635
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.68027114868164
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 15.009378433227539
INFO:agents.father_agent:Step: 10, Training loss: 12.520160675048828
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.694171667098999
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.941715240478516
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.38900375366211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.5629193782806396
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.24492468893255
INFO:tools.evaluation_results_class:Counted Episodes = 7635
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.362349033355713
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.62348937988281
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.476268768310547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.0680017471313477
INFO:tools.evaluation_results_class:Current Best Return = 3.362349033355713
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.26269975315058
INFO:tools.evaluation_results_class:Counted Episodes = 7697
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7673120498657227
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.673118591308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.09893035888672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7747509479522705
INFO:tools.evaluation_results_class:Current Best Return = 3.7673120498657227
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.26269975315058
INFO:tools.evaluation_results_class:Counted Episodes = 7697
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5104712995551997
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 23 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.574293613433838
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.74293518066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.332027435302734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.664196729660034
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.05339236879802
INFO:tools.evaluation_results_class:Counted Episodes = 7679
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.83264446258545
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 12.71155834197998
INFO:agents.father_agent:Step: 10, Training loss: 14.51893424987793
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.6761298179626465
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.761295318603516
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.26304244995117
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.4147069454193115
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.05339236879802
INFO:tools.evaluation_results_class:Counted Episodes = 7679
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.343822956085205
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.438228607177734
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.335790634155273
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.0582945346832275
INFO:tools.evaluation_results_class:Current Best Return = 3.343822956085205
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.14659414659415
INFO:tools.evaluation_results_class:Counted Episodes = 7722
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6072261333465576
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.072261810302734
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.666481018066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.493617534637451
INFO:tools.evaluation_results_class:Current Best Return = 3.6072261333465576
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.14659414659415
INFO:tools.evaluation_results_class:Counted Episodes = 7722
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.524761705132504
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 24 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6774444580078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.774444580078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.313873291015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.210592269897461
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.95195429165044
INFO:tools.evaluation_results_class:Counted Episodes = 7701
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 16.592071533203125
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 12.429346084594727
INFO:agents.father_agent:Step: 10, Training loss: 16.97894287109375
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.734839677810669
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.34839630126953
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.85921096801758
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.7655556201934814
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.95195429165044
INFO:tools.evaluation_results_class:Counted Episodes = 7701
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.323214054107666
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.232139587402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.160751342773438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.9884145259857178
INFO:tools.evaluation_results_class:Current Best Return = 3.323214054107666
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.04728071308617
INFO:tools.evaluation_results_class:Counted Episodes = 7741
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.684924364089966
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.8492431640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.406673431396484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.422623634338379
INFO:tools.evaluation_results_class:Current Best Return = 3.684924364089966
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.04728071308617
INFO:tools.evaluation_results_class:Counted Episodes = 7741
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.534572914555835
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 25 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.832062005996704
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.320621490478516
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.766727447509766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3663668632507324
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.80943762120233
INFO:tools.evaluation_results_class:Counted Episodes = 7735
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.007763862609863
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 13.228269577026367
INFO:agents.father_agent:Step: 10, Training loss: 10.731498718261719
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.6126697063446045
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.1266975402832
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.777191162109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.1678807735443115
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.80943762120233
INFO:tools.evaluation_results_class:Counted Episodes = 7735
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3544530868530273
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.544532775878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.48999786376953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.071124315261841
INFO:tools.evaluation_results_class:Current Best Return = 3.3544530868530273
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.847063359465366
INFO:tools.evaluation_results_class:Counted Episodes = 7781
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.703765630722046
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.037654876708984
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.57938766479492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.5211644172668457
INFO:tools.evaluation_results_class:Current Best Return = 3.703765630722046
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.847063359465366
INFO:tools.evaluation_results_class:Counted Episodes = 7781
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.532920537102598
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 26 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.559417963027954
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.594181060791016
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.279884338378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.4170632362365723
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.66396292004635
INFO:tools.evaluation_results_class:Counted Episodes = 7767
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.290387153625488
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 13.101957321166992
INFO:agents.father_agent:Step: 10, Training loss: 16.90701675415039
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.578730583190918
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.78730392456055
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.51698303222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.426497459411621
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.66396292004635
INFO:tools.evaluation_results_class:Counted Episodes = 7767
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3715567588806152
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.71556854248047
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.627573013305664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.0991010665893555
INFO:tools.evaluation_results_class:Current Best Return = 3.3715567588806152
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.73888532991672
INFO:tools.evaluation_results_class:Counted Episodes = 7805
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.708648204803467
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.086483001708984
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.64434814453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3646981716156006
INFO:tools.evaluation_results_class:Current Best Return = 3.708648204803467
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.73888532991672
INFO:tools.evaluation_results_class:Counted Episodes = 7805
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5465220119251355
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.238715171813965
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 64.38715362548828
INFO:tools.evaluation_results_class:Average Discounted Reward = 55.05703353881836
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.08190393447876
INFO:tools.evaluation_results_class:Current Best Return = 6.238715171813965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.888888888888886
INFO:tools.evaluation_results_class:Counted Episodes = 2304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 9.668749809265137
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 98.6875
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.1300277709961
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.596523284912109
INFO:tools.evaluation_results_class:Current Best Return = 9.668749809265137
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.2314453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 84.314453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 61.14702606201172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.6231906414031982
INFO:tools.evaluation_results_class:Current Best Return = 8.2314453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.75
INFO:tools.evaluation_results_class:Counted Episodes = 1024
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 11.294270515441895
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 114.94271087646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.45376586914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.718092203140259
INFO:tools.evaluation_results_class:Current Best Return = 11.294270515441895
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 156.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.358724117279053
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 75.58724212646484
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.24101257324219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.1414992809295654
INFO:tools.evaluation_results_class:Current Best Return = 7.358724117279053
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.246826171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.46826171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.81620979309082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2405905723571777
INFO:tools.evaluation_results_class:Current Best Return = 3.246826171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.196533203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.96533203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.392074584960938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.0856423377990723
INFO:tools.evaluation_results_class:Current Best Return = 3.196533203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.222900390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.22900390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.623607635498047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.115110397338867
INFO:tools.evaluation_results_class:Current Best Return = 3.222900390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.225830078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.25830078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.651214599609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.183131694793701
INFO:tools.evaluation_results_class:Current Best Return = 3.225830078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.207763671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.07763671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.48996925354004
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.140183925628662
INFO:tools.evaluation_results_class:Current Best Return = 3.207763671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.21337890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.1337890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.545948028564453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.181032180786133
INFO:tools.evaluation_results_class:Current Best Return = 3.21337890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.255859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.55859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.905887603759766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2021141052246094
INFO:tools.evaluation_results_class:Current Best Return = 3.255859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.25341796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.5341796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.882802963256836
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.237536907196045
INFO:tools.evaluation_results_class:Current Best Return = 3.25341796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.864425659179688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1909165382385254
INFO:tools.evaluation_results_class:Current Best Return = 3.2509765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.260009765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.60009765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.93936538696289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.0962133407592773
INFO:tools.evaluation_results_class:Current Best Return = 3.260009765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.251708984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.51708984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.872848510742188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.232785701751709
INFO:tools.evaluation_results_class:Current Best Return = 3.251708984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2294921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.294921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.672122955322266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2383484840393066
INFO:tools.evaluation_results_class:Current Best Return = 3.2294921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.208740234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.08740234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.48080825805664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.241827964782715
INFO:tools.evaluation_results_class:Current Best Return = 3.208740234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.20556640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.0556640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.460887908935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.214578628540039
INFO:tools.evaluation_results_class:Current Best Return = 3.20556640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.22607421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.2607421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.637374877929688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.324866771697998
INFO:tools.evaluation_results_class:Current Best Return = 3.22607421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.236083984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.36083984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.738391876220703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2511496543884277
INFO:tools.evaluation_results_class:Current Best Return = 3.236083984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.20703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.0703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.471113204956055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1724700927734375
INFO:tools.evaluation_results_class:Current Best Return = 3.20703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.19384765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.9384765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.35515594482422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.217794418334961
INFO:tools.evaluation_results_class:Current Best Return = 3.19384765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.205322265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.05322265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.428211212158203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1866025924682617
INFO:tools.evaluation_results_class:Current Best Return = 3.205322265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.19580078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.9580078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.361373901367188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1144938468933105
INFO:tools.evaluation_results_class:Current Best Return = 3.19580078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.213623046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.13623046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.530323028564453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1753129959106445
INFO:tools.evaluation_results_class:Current Best Return = 3.213623046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.28515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.8515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.17313575744629
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1271820068359375
INFO:tools.evaluation_results_class:Current Best Return = 3.28515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.2919921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.919921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.241249084472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.2179627418518066
INFO:tools.evaluation_results_class:Current Best Return = 3.2919921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.302490234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.02490234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.329120635986328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1543493270874023
INFO:tools.evaluation_results_class:Current Best Return = 3.302490234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.313720703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.13720703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.42371368408203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.1806321144104004
INFO:tools.evaluation_results_class:Current Best Return = 3.313720703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.296142578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.96142578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.276884078979492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.160102367401123
INFO:tools.evaluation_results_class:Current Best Return = 3.296142578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.990451335906982
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.90451049804688
INFO:tools.evaluation_results_class:Average Discounted Reward = 61.59857940673828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.019006729125977
INFO:tools.evaluation_results_class:Current Best Return = 6.990451335906982
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.888888888888886
INFO:tools.evaluation_results_class:Counted Episodes = 2304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 10.711718559265137
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.1171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 84.27217102050781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5.508299827575684
INFO:tools.evaluation_results_class:Current Best Return = 10.711718559265137
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.8
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 8.5810546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 87.810546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.00933837890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1.3371798992156982
INFO:tools.evaluation_results_class:Current Best Return = 8.5810546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.75
INFO:tools.evaluation_results_class:Counted Episodes = 1024
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 12.08984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 122.8984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 84.0470199584961
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.4255218505859375
INFO:tools.evaluation_results_class:Current Best Return = 12.08984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 156.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 81.53125
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.0377426147461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3363444805145264
INFO:tools.evaluation_results_class:Current Best Return = 7.953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.83333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.54248046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.4248046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.48280334472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.626124858856201
INFO:tools.evaluation_results_class:Current Best Return = 3.54248046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.551513671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.51513671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.56861114501953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5818190574645996
INFO:tools.evaluation_results_class:Current Best Return = 3.551513671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.58642578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.8642578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.884864807128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.607764720916748
INFO:tools.evaluation_results_class:Current Best Return = 3.58642578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.73664093017578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.60491943359375
INFO:tools.evaluation_results_class:Current Best Return = 3.5703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.546630859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.46630859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.51478576660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5500717163085938
INFO:tools.evaluation_results_class:Current Best Return = 3.546630859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.55517578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.5517578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.595130920410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.638556957244873
INFO:tools.evaluation_results_class:Current Best Return = 3.55517578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.518798828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.18798828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.27963638305664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6075568199157715
INFO:tools.evaluation_results_class:Current Best Return = 3.518798828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.505126953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.05126953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.150394439697266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.595189094543457
INFO:tools.evaluation_results_class:Current Best Return = 3.505126953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.514404296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.14404296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.22856140136719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5725464820861816
INFO:tools.evaluation_results_class:Current Best Return = 3.514404296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.49951171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.9951171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.096031188964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.586425304412842
INFO:tools.evaluation_results_class:Current Best Return = 3.49951171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5068359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.068359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.15682601928711
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.565870761871338
INFO:tools.evaluation_results_class:Current Best Return = 3.5068359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.556640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.56640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.60641098022461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5114402770996094
INFO:tools.evaluation_results_class:Current Best Return = 3.556640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.564453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.64453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.67683410644531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.4977989196777344
INFO:tools.evaluation_results_class:Current Best Return = 3.564453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.561279296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.61279296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.646339416503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5924363136291504
INFO:tools.evaluation_results_class:Current Best Return = 3.561279296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.56005859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.6005859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.64748001098633
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5222721099853516
INFO:tools.evaluation_results_class:Current Best Return = 3.56005859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.55810546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.5810546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.61701965332031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5351977348327637
INFO:tools.evaluation_results_class:Current Best Return = 3.55810546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.583251953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.83251953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.82651138305664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5751006603240967
INFO:tools.evaluation_results_class:Current Best Return = 3.583251953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.55224609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.5224609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.55619812011719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6090869903564453
INFO:tools.evaluation_results_class:Current Best Return = 3.55224609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.571533203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.71533203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.72900390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6027932167053223
INFO:tools.evaluation_results_class:Current Best Return = 3.571533203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.5732421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.732421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.74456787109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5825257301330566
INFO:tools.evaluation_results_class:Current Best Return = 3.5732421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.52685546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.2685546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.3348388671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5837512016296387
INFO:tools.evaluation_results_class:Current Best Return = 3.52685546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.559814453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.59814453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.626060485839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.49739933013916
INFO:tools.evaluation_results_class:Current Best Return = 3.559814453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.54638671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.4638671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.50794982910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5129847526550293
INFO:tools.evaluation_results_class:Current Best Return = 3.54638671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.541259765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.41259765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.45745849609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.618903160095215
INFO:tools.evaluation_results_class:Current Best Return = 3.541259765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.544677734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.44677734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.49036407470703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5277891159057617
INFO:tools.evaluation_results_class:Current Best Return = 3.544677734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.55419921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.5419921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.582515716552734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.5483317375183105
INFO:tools.evaluation_results_class:Current Best Return = 3.55419921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.9375
INFO:tools.evaluation_results_class:Counted Episodes = 4096
INFO:robust_rl.robust_rl_trainer:Iteration 27 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6546597480773926
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.54659652709961
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.267147064208984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.148782730102539
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.52390719138572
INFO:tools.evaluation_results_class:Counted Episodes = 7801
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.77133846282959
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Step: 5, Training loss: 13.985200881958008
INFO:agents.father_agent:Step: 10, Training loss: 8.574851989746094
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.7055506706237793
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.055503845214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.61679458618164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.247359275817871
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.52390719138572
INFO:tools.evaluation_results_class:Counted Episodes = 7801
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.258759021759033
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.587589263916016
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.678810119628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7773549556732178
INFO:tools.evaluation_results_class:Current Best Return = 3.258759021759033
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.52936679831826
INFO:tools.evaluation_results_class:Counted Episodes = 7849
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.68709397315979
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.87093734741211
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.46493911743164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.42279314994812
INFO:tools.evaluation_results_class:Current Best Return = 3.68709397315979
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.52936679831826
INFO:tools.evaluation_results_class:Counted Episodes = 7849
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.557326835580145
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 28 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.551067352294922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.51067352294922
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.25857925415039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.241640329360962
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.42554007414036
INFO:tools.evaluation_results_class:Counted Episodes = 7823
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.868583679199219
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Step: 5, Training loss: 14.129137992858887
INFO:agents.father_agent:Step: 10, Training loss: 22.833595275878906
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.5427584648132324
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.42758560180664
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.20675277709961
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.195378541946411
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.42554007414036
INFO:tools.evaluation_results_class:Counted Episodes = 7823
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.418149471282959
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.181495666503906
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.10608673095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.9415721893310547
INFO:tools.evaluation_results_class:Current Best Return = 3.418149471282959
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.433146924250124
INFO:tools.evaluation_results_class:Counted Episodes = 7868
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6244280338287354
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.24428176879883
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.934417724609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.267817258834839
INFO:tools.evaluation_results_class:Current Best Return = 3.6244280338287354
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.433146924250124
INFO:tools.evaluation_results_class:Counted Episodes = 7868
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.565015919670288
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 29 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.86791729927063
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.67917251586914
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.08737564086914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.067674398422241
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.37723328228688
INFO:tools.evaluation_results_class:Counted Episodes = 7836
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.714639663696289
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 12.773621559143066
INFO:agents.father_agent:Step: 10, Training loss: 14.38200569152832
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.621873378753662
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.21873474121094
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.92105484008789
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.176698923110962
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.37723328228688
INFO:tools.evaluation_results_class:Counted Episodes = 7836
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.4159889221191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.159889221191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.03847885131836
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8456363677978516
INFO:tools.evaluation_results_class:Current Best Return = 3.4159889221191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.433146924250124
INFO:tools.evaluation_results_class:Counted Episodes = 7868
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8792576789855957
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.79257583618164
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.209983825683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.4742367267608643
INFO:tools.evaluation_results_class:Current Best Return = 3.8792576789855957
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.433146924250124
INFO:tools.evaluation_results_class:Counted Episodes = 7868
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.5641354055042522
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 30 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8461732864379883
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.461734771728516
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.943973541259766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.171159029006958
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.190506409442825
INFO:tools.evaluation_results_class:Counted Episodes = 7879
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.55937385559082
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 13.053860664367676
INFO:agents.father_agent:Step: 10, Training loss: 13.045947074890137
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.6743240356445312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.74324035644531
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.42771911621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3078203201293945
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.190506409442825
INFO:tools.evaluation_results_class:Counted Episodes = 7879
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.3705294132232666
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.70529556274414
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.70147705078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.6543166637420654
INFO:tools.evaluation_results_class:Current Best Return = 3.3705294132232666
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.22936939213952
INFO:tools.evaluation_results_class:Counted Episodes = 7913
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7165424823760986
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.16542434692383
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.83005905151367
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.0715537071228027
INFO:tools.evaluation_results_class:Current Best Return = 3.7165424823760986
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.22936939213952
INFO:tools.evaluation_results_class:Counted Episodes = 7913
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.574439273735715
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 31 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7264318466186523
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.26431655883789
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.91244125366211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.236741304397583
INFO:tools.evaluation_results_class:Current Best Return = 7.057900905609131
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.14292954891029
INFO:tools.evaluation_results_class:Counted Episodes = 7892
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.502673149108887
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 12.434521675109863
INFO:agents.father_agent:Step: 10, Training loss: 11.969847679138184
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
2025-08-19 20:49:04.814730: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-19 20:49:04.816906: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-19 20:49:04.849279: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-19 20:49:04.849342: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-19 20:49:04.850850: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-19 20:49:04.857292: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-19 20:49:04.857547: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-19 20:49:05.466169: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/obstacles-10-2/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/obstacles-10-2/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"penalty"}min=? [F ((x = 10) & (y = 10))] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 326 states and 829 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"rewardmodel_penalty"}min=? [F ((x = 10) & (y = 10))] 
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -449.0242919921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -269.5798645019531
INFO:tools.evaluation_results_class:Average Discounted Reward = -135.73025512695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.5607638888888888
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 74887.984375
INFO:tools.evaluation_results_class:Current Best Return = -449.0242919921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.5607638888888888
INFO:tools.evaluation_results_class:Average Episode Length = 436.1666666666667
INFO:tools.evaluation_results_class:Counted Episodes = 576
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1531.0247802734375
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 2.893244504928589
INFO:agents.father_agent:Step: 10, Training loss: 4.168527126312256
INFO:agents.father_agent:Step: 15, Training loss: 3.8733127117156982
INFO:agents.father_agent:Step: 20, Training loss: 4.25652551651001
INFO:agents.father_agent:Step: 25, Training loss: 4.685868263244629
INFO:agents.father_agent:Step: 30, Training loss: 5.216664791107178
INFO:agents.father_agent:Step: 35, Training loss: 4.7714080810546875
INFO:agents.father_agent:Step: 40, Training loss: 4.495352745056152
INFO:agents.father_agent:Step: 45, Training loss: 6.347174644470215
INFO:agents.father_agent:Step: 50, Training loss: 5.106786727905273
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 55, Training loss: 2.9132533073425293
INFO:agents.father_agent:Step: 60, Training loss: 3.940152883529663
INFO:agents.father_agent:Step: 65, Training loss: 4.043621063232422
INFO:agents.father_agent:Step: 70, Training loss: 4.112527847290039
INFO:agents.father_agent:Step: 75, Training loss: 3.868932008743286
INFO:agents.father_agent:Step: 80, Training loss: 3.6529698371887207
INFO:agents.father_agent:Step: 85, Training loss: 3.369763135910034
INFO:agents.father_agent:Step: 90, Training loss: 3.326565742492676
INFO:agents.father_agent:Step: 95, Training loss: 5.132542133331299
INFO:agents.father_agent:Step: 100, Training loss: 3.639181613922119
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -130.02210998535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 185.6241455078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 54.54026412963867
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9863945578231292
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12412.6513671875
INFO:tools.evaluation_results_class:Current Best Return = -130.02210998535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9863945578231292
INFO:tools.evaluation_results_class:Average Episode Length = 197.58673469387756
INFO:tools.evaluation_results_class:Counted Episodes = 1176
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -133.3455352783203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 179.3699188232422
INFO:tools.evaluation_results_class:Average Discounted Reward = 55.7494010925293
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9772357723577236
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13266.45703125
INFO:tools.evaluation_results_class:Current Best Return = -130.02210998535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9863945578231292
INFO:tools.evaluation_results_class:Average Episode Length = 193.05609756097562
INFO:tools.evaluation_results_class:Counted Episodes = 1230
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.7963104248047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 132.49429321289062
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.566696166992188
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9727831431079894
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22780.884765625
INFO:tools.evaluation_results_class:Current Best Return = -178.7963104248047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9727831431079894
INFO:tools.evaluation_results_class:Average Episode Length = 231.53116769095698
INFO:tools.evaluation_results_class:Counted Episodes = 1139
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -130.36990356445312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 185.123046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 54.18805694580078
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9859154929577465
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13121.81640625
INFO:tools.evaluation_results_class:Current Best Return = -130.36990356445312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9859154929577465
INFO:tools.evaluation_results_class:Average Episode Length = 201.14158636026687
INFO:tools.evaluation_results_class:Counted Episodes = 1349
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=8, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 192.78078691219662
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=8, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -227.3520050048828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 87.52799987792969
INFO:tools.evaluation_results_class:Average Discounted Reward = -20.472991943359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.984
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29549.66015625
INFO:tools.evaluation_results_class:Current Best Return = -227.3520050048828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.984
INFO:tools.evaluation_results_class:Average Episode Length = 223.976
INFO:tools.evaluation_results_class:Counted Episodes = 500
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -206.77932739257812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 103.041748046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 4.814308166503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9681908548707754
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30961.919921875
INFO:tools.evaluation_results_class:Current Best Return = -206.77932739257812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9681908548707754
INFO:tools.evaluation_results_class:Average Episode Length = 231.81510934393637
INFO:tools.evaluation_results_class:Counted Episodes = 503
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -189.3259735107422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 124.78084564208984
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.422563076019287
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9815837937384899
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25973.486328125
INFO:tools.evaluation_results_class:Current Best Return = -189.3259735107422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9815837937384899
INFO:tools.evaluation_results_class:Average Episode Length = 209.08471454880294
INFO:tools.evaluation_results_class:Counted Episodes = 543
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.89483642578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 145.76290893554688
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.086814880371094
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9770554493307839
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16083.646484375
INFO:tools.evaluation_results_class:Current Best Return = -166.89483642578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9770554493307839
INFO:tools.evaluation_results_class:Average Episode Length = 218.17017208413003
INFO:tools.evaluation_results_class:Counted Episodes = 523
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.25367736816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 174.62867736816406
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.350276947021484
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9871323529411765
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10416.7587890625
INFO:tools.evaluation_results_class:Current Best Return = -141.25367736816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9871323529411765
INFO:tools.evaluation_results_class:Average Episode Length = 213.30882352941177
INFO:tools.evaluation_results_class:Counted Episodes = 544
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -223.930419921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 90.98011779785156
INFO:tools.evaluation_results_class:Average Discounted Reward = -16.41645050048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9840954274353877
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34599.1953125
INFO:tools.evaluation_results_class:Current Best Return = -223.930419921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9840954274353877
INFO:tools.evaluation_results_class:Average Episode Length = 221.8051689860835
INFO:tools.evaluation_results_class:Counted Episodes = 503
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -168.00511169433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 148.72401428222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.751073837280273
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.989778534923339
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20973.271484375
INFO:tools.evaluation_results_class:Current Best Return = -168.00511169433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.989778534923339
INFO:tools.evaluation_results_class:Average Episode Length = 197.48892674616695
INFO:tools.evaluation_results_class:Counted Episodes = 587
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -149.4120635986328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 165.76382446289062
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.1943359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9849246231155779
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14763.7568359375
INFO:tools.evaluation_results_class:Current Best Return = -149.4120635986328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9849246231155779
INFO:tools.evaluation_results_class:Average Episode Length = 196.5862646566164
INFO:tools.evaluation_results_class:Counted Episodes = 597
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -172.998291015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 143.18568420410156
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.20248031616211
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9880749574105622
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22442.91796875
INFO:tools.evaluation_results_class:Current Best Return = -172.998291015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9880749574105622
INFO:tools.evaluation_results_class:Average Episode Length = 199.69505962521293
INFO:tools.evaluation_results_class:Counted Episodes = 587
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -122.53289794921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 191.67762756347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 58.34794998168945
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9819078947368421
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9604.6533203125
INFO:tools.evaluation_results_class:Current Best Return = -122.53289794921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9819078947368421
INFO:tools.evaluation_results_class:Average Episode Length = 196.47697368421052
INFO:tools.evaluation_results_class:Counted Episodes = 608
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -98.6529769897461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 218.62747192382812
INFO:tools.evaluation_results_class:Average Discounted Reward = 92.9599609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9915014164305949
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5766.76171875
INFO:tools.evaluation_results_class:Current Best Return = -98.6529769897461
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9915014164305949
INFO:tools.evaluation_results_class:Average Episode Length = 165.8597733711048
INFO:tools.evaluation_results_class:Counted Episodes = 706
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.43589782714844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 150.2820587158203
INFO:tools.evaluation_results_class:Average Discounted Reward = 24.355424880981445
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9897435897435898
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18903.62109375
INFO:tools.evaluation_results_class:Current Best Return = -166.43589782714844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9897435897435898
INFO:tools.evaluation_results_class:Average Episode Length = 194.81367521367522
INFO:tools.evaluation_results_class:Counted Episodes = 585
INFO:robust_rl.robust_rl_trainer:Iteration 2 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -135.7455291748047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 178.5353240966797
INFO:tools.evaluation_results_class:Average Discounted Reward = 50.504302978515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9821276595744681
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13889.880859375
INFO:tools.evaluation_results_class:Current Best Return = -130.02210998535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9863945578231292
INFO:tools.evaluation_results_class:Average Episode Length = 196.2195744680851
INFO:tools.evaluation_results_class:Counted Episodes = 1175
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.581200122833252
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 4.0965256690979
INFO:agents.father_agent:Step: 10, Training loss: 4.044651031494141
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -134.11659240722656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 176.1255645751953
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.94700622558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9695067264573991
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13224.3642578125
INFO:tools.evaluation_results_class:Current Best Return = -130.02210998535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9863945578231292
INFO:tools.evaluation_results_class:Average Episode Length = 204.2699551569507
INFO:tools.evaluation_results_class:Counted Episodes = 1115
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -185.23529052734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 122.58477783203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 11.642305374145508
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9619377162629758
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26736.8828125
INFO:tools.evaluation_results_class:Current Best Return = -185.23529052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9619377162629758
INFO:tools.evaluation_results_class:Average Episode Length = 229.64705882352942
INFO:tools.evaluation_results_class:Counted Episodes = 1156
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -129.4818878173828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 184.72100830078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 57.335350036621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9818840579710145
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12588.0810546875
INFO:tools.evaluation_results_class:Current Best Return = -129.4818878173828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9818840579710145
INFO:tools.evaluation_results_class:Average Episode Length = 195.77463768115942
INFO:tools.evaluation_results_class:Counted Episodes = 1380
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=8, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 179.24273284927497
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=8, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 3 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.35421752929688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 168.6357879638672
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.70884323120117
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.971843778383288
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13791.947265625
INFO:tools.evaluation_results_class:Current Best Return = -130.02210998535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9863945578231292
INFO:tools.evaluation_results_class:Average Episode Length = 208.76021798365122
INFO:tools.evaluation_results_class:Counted Episodes = 1101
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.295195579528809
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 4.57654333114624
INFO:agents.father_agent:Step: 10, Training loss: 4.550659656524658
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -134.04074096679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 179.44021606445312
INFO:tools.evaluation_results_class:Average Discounted Reward = 50.78403091430664
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9796279893711249
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14316.765625
INFO:tools.evaluation_results_class:Current Best Return = -130.02210998535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9863945578231292
INFO:tools.evaluation_results_class:Average Episode Length = 204.07528786536759
INFO:tools.evaluation_results_class:Counted Episodes = 1129
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.18597412109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 146.50570678710938
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.146421432495117
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9771615008156607
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20143.896484375
INFO:tools.evaluation_results_class:Current Best Return = -166.18597412109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9771615008156607
INFO:tools.evaluation_results_class:Average Episode Length = 216.97063621533442
INFO:tools.evaluation_results_class:Counted Episodes = 1226
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -130.92103576660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 182.76158142089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 55.25156021118164
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9802581624905087
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15571.7998046875
INFO:tools.evaluation_results_class:Current Best Return = -130.92103576660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9802581624905087
INFO:tools.evaluation_results_class:Average Episode Length = 203.68109339407744
INFO:tools.evaluation_results_class:Counted Episodes = 1317
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=8, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 170.177783559866
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=8, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 4 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.92779541015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 173.830322265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.08462905883789
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9711191335740073
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16799.248046875
INFO:tools.evaluation_results_class:Current Best Return = -130.02210998535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9863945578231292
INFO:tools.evaluation_results_class:Average Episode Length = 205.76895306859205
INFO:tools.evaluation_results_class:Counted Episodes = 1108
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.200658798217773
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 3.91782546043396
INFO:agents.father_agent:Step: 10, Training loss: 5.002238750457764
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -122.42327880859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 195.64569091796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 58.89554214477539
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9939655172413793
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10569.583984375
INFO:tools.evaluation_results_class:Current Best Return = -122.42327880859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9939655172413793
INFO:tools.evaluation_results_class:Average Episode Length = 199.80603448275863
INFO:tools.evaluation_results_class:Counted Episodes = 1160
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.22508239746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 150.79049682617188
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.48697280883789
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9844236760124611
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23134.85546875
INFO:tools.evaluation_results_class:Current Best Return = -164.22508239746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9844236760124611
INFO:tools.evaluation_results_class:Average Episode Length = 208.56308411214954
INFO:tools.evaluation_results_class:Counted Episodes = 1284
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -126.19969940185547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 188.7928466796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 56.67649841308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9843517138599106
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10866.65234375
INFO:tools.evaluation_results_class:Current Best Return = -126.19969940185547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9843517138599106
INFO:tools.evaluation_results_class:Average Episode Length = 197.70789865871834
INFO:tools.evaluation_results_class:Counted Episodes = 1342
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=8, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 158.3819240536856
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=8, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 5 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -128.6857147216797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 186.1714324951172
INFO:tools.evaluation_results_class:Average Discounted Reward = 54.496543884277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9839285714285714
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12740.0751953125
INFO:tools.evaluation_results_class:Current Best Return = -122.42327880859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9939655172413793
INFO:tools.evaluation_results_class:Average Episode Length = 200.34642857142856
INFO:tools.evaluation_results_class:Counted Episodes = 1120
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.297420024871826
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 4.300335884094238
INFO:agents.father_agent:Step: 10, Training loss: 3.983764171600342
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -115.1139144897461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 203.96322631835938
INFO:tools.evaluation_results_class:Average Discounted Reward = 70.95743560791016
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9971160778658976
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10276.8896484375
INFO:tools.evaluation_results_class:Current Best Return = -115.1139144897461
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9971160778658976
INFO:tools.evaluation_results_class:Average Episode Length = 176.13410237923577
INFO:tools.evaluation_results_class:Counted Episodes = 1387
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.6607208251953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 171.38217163085938
INFO:tools.evaluation_results_class:Average Discounted Reward = 50.725372314453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9907590759075907
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19106.2421875
INFO:tools.evaluation_results_class:Current Best Return = -145.6607208251953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9907590759075907
INFO:tools.evaluation_results_class:Average Episode Length = 181.77227722772278
INFO:tools.evaluation_results_class:Counted Episodes = 1515
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -117.46866607666016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 200.44256591796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 68.17909240722656
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9934725848563969
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11088.595703125
INFO:tools.evaluation_results_class:Current Best Return = -117.46866607666016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9934725848563969
INFO:tools.evaluation_results_class:Average Episode Length = 180.10313315926894
INFO:tools.evaluation_results_class:Counted Episodes = 1532
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=8, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 144.95276625746112
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=8, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 6 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.67381286621094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 198.12098693847656
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.54222869873047
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9931087289433385
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11176.4345703125
INFO:tools.evaluation_results_class:Current Best Return = -115.1139144897461
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9971160778658976
INFO:tools.evaluation_results_class:Average Episode Length = 181.91577335375192
INFO:tools.evaluation_results_class:Counted Episodes = 1306
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.4185099601745605
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 5.11531400680542
INFO:agents.father_agent:Step: 10, Training loss: 5.934700012207031
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -115.83523559570312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 202.56704711914062
INFO:tools.evaluation_results_class:Average Discounted Reward = 71.79502868652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9950071326676176
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10790.3828125
INFO:tools.evaluation_results_class:Current Best Return = -115.1139144897461
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9971160778658976
INFO:tools.evaluation_results_class:Average Episode Length = 171.7567760342368
INFO:tools.evaluation_results_class:Counted Episodes = 1402
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.44873046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 175.133544921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 54.69271469116211
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9955696202531645
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19568.625
INFO:tools.evaluation_results_class:Current Best Return = -143.44873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9955696202531645
INFO:tools.evaluation_results_class:Average Episode Length = 174.5727848101266
INFO:tools.evaluation_results_class:Counted Episodes = 1580
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -109.18003845214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 209.25228881835938
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.63893127441406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9951010410287814
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10746.5126953125
INFO:tools.evaluation_results_class:Current Best Return = -109.18003845214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9951010410287814
INFO:tools.evaluation_results_class:Average Episode Length = 168.21800367421923
INFO:tools.evaluation_results_class:Counted Episodes = 1633
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=8, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 133.40233456166482
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=8, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.7113800048828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 164.42140197753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 47.50552749633789
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.997289972899729
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21610.875
INFO:tools.evaluation_results_class:Current Best Return = -154.7113800048828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.997289972899729
INFO:tools.evaluation_results_class:Average Episode Length = 166.04471544715446
INFO:tools.evaluation_results_class:Counted Episodes = 738
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -112.09125518798828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 206.28643798828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 82.9311752319336
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9949302915082383
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8897.8115234375
INFO:tools.evaluation_results_class:Current Best Return = -112.09125518798828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9949302915082383
INFO:tools.evaluation_results_class:Average Episode Length = 158.1533586818758
INFO:tools.evaluation_results_class:Counted Episodes = 789
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -161.8248291015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 157.73379516601562
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.62725067138672
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9986206896551724
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24736.861328125
INFO:tools.evaluation_results_class:Current Best Return = -161.8248291015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9986206896551724
INFO:tools.evaluation_results_class:Average Episode Length = 169.22758620689655
INFO:tools.evaluation_results_class:Counted Episodes = 725
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -98.54350280761719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 220.64942932128906
INFO:tools.evaluation_results_class:Average Discounted Reward = 90.69271850585938
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9974779319041615
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7276.2099609375
INFO:tools.evaluation_results_class:Current Best Return = -98.54350280761719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9974779319041615
INFO:tools.evaluation_results_class:Average Episode Length = 154.33795712484238
INFO:tools.evaluation_results_class:Counted Episodes = 793
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.1986083984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.05807495117188
INFO:tools.evaluation_results_class:Average Discounted Reward = 111.95381164550781
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9976771196283392
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3685.961669921875
INFO:tools.evaluation_results_class:Current Best Return = -83.1986083984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9976771196283392
INFO:tools.evaluation_results_class:Average Episode Length = 145.5156794425087
INFO:tools.evaluation_results_class:Counted Episodes = 861
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.0907745361328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 159.9092254638672
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.4443359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19484.64453125
INFO:tools.evaluation_results_class:Current Best Return = -160.0907745361328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 174.22836879432623
INFO:tools.evaluation_results_class:Counted Episodes = 705
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.96311950683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 159.72540283203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.652095794677734
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9959016393442623
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24136.58203125
INFO:tools.evaluation_results_class:Current Best Return = -158.96311950683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9959016393442623
INFO:tools.evaluation_results_class:Average Episode Length = 167.76092896174865
INFO:tools.evaluation_results_class:Counted Episodes = 732
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -149.976318359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 169.57798767089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.018226623535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9986072423398329
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18741.3203125
INFO:tools.evaluation_results_class:Current Best Return = -149.976318359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9986072423398329
INFO:tools.evaluation_results_class:Average Episode Length = 165.70612813370474
INFO:tools.evaluation_results_class:Counted Episodes = 718
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -148.93475341796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 170.63914489746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 53.306175231933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9986684420772304
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20093.05859375
INFO:tools.evaluation_results_class:Current Best Return = -148.93475341796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9986684420772304
INFO:tools.evaluation_results_class:Average Episode Length = 163.04260985352863
INFO:tools.evaluation_results_class:Counted Episodes = 751
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -151.59591674804688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 167.09796142578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 48.851139068603516
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9959183673469387
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21576.544921875
INFO:tools.evaluation_results_class:Current Best Return = -151.59591674804688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9959183673469387
INFO:tools.evaluation_results_class:Average Episode Length = 166.10068027210883
INFO:tools.evaluation_results_class:Counted Episodes = 735
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.55804443359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 160.99440002441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.105960845947266
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9986013986013986
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19273.779296875
INFO:tools.evaluation_results_class:Current Best Return = -158.55804443359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9986013986013986
INFO:tools.evaluation_results_class:Average Episode Length = 170.54265734265735
INFO:tools.evaluation_results_class:Counted Episodes = 715
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -122.45706176757812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 197.09971618652344
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.108665466308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9986149584487535
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11132.1259765625
INFO:tools.evaluation_results_class:Current Best Return = -122.45706176757812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9986149584487535
INFO:tools.evaluation_results_class:Average Episode Length = 170.72853185595568
INFO:tools.evaluation_results_class:Counted Episodes = 722
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -100.30921173095703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 218.8486785888672
INFO:tools.evaluation_results_class:Average Discounted Reward = 87.78643035888672
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9973684210526316
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5541.21044921875
INFO:tools.evaluation_results_class:Current Best Return = -100.30921173095703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9973684210526316
INFO:tools.evaluation_results_class:Average Episode Length = 162.62236842105264
INFO:tools.evaluation_results_class:Counted Episodes = 760
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -107.31627655029297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 212.68373107910156
INFO:tools.evaluation_results_class:Average Discounted Reward = 77.50774383544922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8694.8203125
INFO:tools.evaluation_results_class:Current Best Return = -107.31627655029297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.70734908136484
INFO:tools.evaluation_results_class:Counted Episodes = 762
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.72200012207031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.01316833496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 98.37721252441406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9960474308300395
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4732.71435546875
INFO:tools.evaluation_results_class:Current Best Return = -84.72200012207031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9960474308300395
INFO:tools.evaluation_results_class:Average Episode Length = 158.37022397891963
INFO:tools.evaluation_results_class:Counted Episodes = 759
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.86882019042969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.44300842285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 126.1460189819336
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9978494623655914
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2612.565673828125
INFO:tools.evaluation_results_class:Current Best Return = -72.86882019042969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978494623655914
INFO:tools.evaluation_results_class:Average Episode Length = 134.43225806451613
INFO:tools.evaluation_results_class:Counted Episodes = 930
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -117.85655212402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 201.7020721435547
INFO:tools.evaluation_results_class:Average Discounted Reward = 68.18768310546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9986206896551724
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11108.431640625
INFO:tools.evaluation_results_class:Current Best Return = -117.85655212402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9986206896551724
INFO:tools.evaluation_results_class:Average Episode Length = 168.90896551724137
INFO:tools.evaluation_results_class:Counted Episodes = 725
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -130.0733642578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 189.49185180664062
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.061988830566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.998641304347826
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12827.5703125
INFO:tools.evaluation_results_class:Current Best Return = -130.0733642578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.998641304347826
INFO:tools.evaluation_results_class:Average Episode Length = 170.39673913043478
INFO:tools.evaluation_results_class:Counted Episodes = 736
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -124.93939208984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 195.06060791015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.912410736083984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13403.7412109375
INFO:tools.evaluation_results_class:Current Best Return = -124.93939208984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 168.92011019283746
INFO:tools.evaluation_results_class:Counted Episodes = 726
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -122.93169403076172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 196.19398498535156
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.63255310058594
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9972677595628415
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12461.3662109375
INFO:tools.evaluation_results_class:Current Best Return = -122.93169403076172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9972677595628415
INFO:tools.evaluation_results_class:Average Episode Length = 169.5
INFO:tools.evaluation_results_class:Counted Episodes = 732
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -115.98808288574219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 203.16424560546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 71.26077270507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9973509933774835
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10866.525390625
INFO:tools.evaluation_results_class:Current Best Return = -115.98808288574219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9973509933774835
INFO:tools.evaluation_results_class:Average Episode Length = 163.84900662251655
INFO:tools.evaluation_results_class:Counted Episodes = 755
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -127.29036712646484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 191.80311584472656
INFO:tools.evaluation_results_class:Average Discounted Reward = 61.296470642089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9971671388101983
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13794.3154296875
INFO:tools.evaluation_results_class:Current Best Return = -127.29036712646484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9971671388101983
INFO:tools.evaluation_results_class:Average Episode Length = 173.9985835694051
INFO:tools.evaluation_results_class:Counted Episodes = 706
INFO:robust_rl.robust_rl_trainer:Iteration 7 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -113.43295288085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 205.88230895996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.41081237792969
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9978601997146933
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10985.015625
INFO:tools.evaluation_results_class:Current Best Return = -113.43295288085938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978601997146933
INFO:tools.evaluation_results_class:Average Episode Length = 175.67118402282455
INFO:tools.evaluation_results_class:Counted Episodes = 1402
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.6864402294158936
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 4.541313648223877
INFO:agents.father_agent:Step: 10, Training loss: 4.9932942390441895
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -111.89310455322266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 206.7827606201172
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.94316101074219
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9958620689655172
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11377.4736328125
INFO:tools.evaluation_results_class:Current Best Return = -111.89310455322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978601997146933
INFO:tools.evaluation_results_class:Average Episode Length = 167.9896551724138
INFO:tools.evaluation_results_class:Counted Episodes = 1450
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.2109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 175.84376525878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 61.17049789428711
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.993920972644377
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21843.2578125
INFO:tools.evaluation_results_class:Current Best Return = -142.2109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.993920972644377
INFO:tools.evaluation_results_class:Average Episode Length = 167.61702127659575
INFO:tools.evaluation_results_class:Counted Episodes = 1645
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -107.0461196899414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 212.0198516845703
INFO:tools.evaluation_results_class:Average Discounted Reward = 84.59364318847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.997081144191477
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9817.5126953125
INFO:tools.evaluation_results_class:Current Best Return = -107.0461196899414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.997081144191477
INFO:tools.evaluation_results_class:Average Episode Length = 162.55166374781086
INFO:tools.evaluation_results_class:Counted Episodes = 1713
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=8, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 132.59247015491468
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=8, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 8 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -110.63037109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 207.07736206054688
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.9318618774414
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9928366762177651
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10728.0546875
INFO:tools.evaluation_results_class:Current Best Return = -110.63037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978601997146933
INFO:tools.evaluation_results_class:Average Episode Length = 171.512893982808
INFO:tools.evaluation_results_class:Counted Episodes = 1396
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.571906566619873
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 4.770456790924072
INFO:agents.father_agent:Step: 10, Training loss: 6.186608791351318
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -116.3043441772461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 200.9586639404297
INFO:tools.evaluation_results_class:Average Discounted Reward = 80.77861022949219
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9914468995010691
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12215.9013671875
INFO:tools.evaluation_results_class:Current Best Return = -110.63037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978601997146933
INFO:tools.evaluation_results_class:Average Episode Length = 168.37776193870278
INFO:tools.evaluation_results_class:Counted Episodes = 1403
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -135.55593872070312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 183.15892028808594
INFO:tools.evaluation_results_class:Average Discounted Reward = 69.2868881225586
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9959839357429718
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18160.40625
INFO:tools.evaluation_results_class:Current Best Return = -135.55593872070312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9959839357429718
INFO:tools.evaluation_results_class:Average Episode Length = 160.09007458405048
INFO:tools.evaluation_results_class:Counted Episodes = 1743
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -116.34089660644531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 198.74916076660156
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.94309997558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9846564376250834
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11086.486328125
INFO:tools.evaluation_results_class:Current Best Return = -116.34089660644531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9846564376250834
INFO:tools.evaluation_results_class:Average Episode Length = 182.1721147431621
INFO:tools.evaluation_results_class:Counted Episodes = 1499
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=8, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 140.3265524855603
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=8, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 9 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -121.31171417236328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 193.26455688476562
INFO:tools.evaluation_results_class:Average Discounted Reward = 73.78018188476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9830508474576272
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12939.046875
INFO:tools.evaluation_results_class:Current Best Return = -110.63037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978601997146933
INFO:tools.evaluation_results_class:Average Episode Length = 179.31982313927782
INFO:tools.evaluation_results_class:Counted Episodes = 1357
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.150383472442627
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 5.805222034454346
INFO:agents.father_agent:Step: 10, Training loss: 5.770631313323975
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -123.50679779052734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 187.79615783691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 73.63760375976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9728217426059153
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12052.958984375
INFO:tools.evaluation_results_class:Current Best Return = -110.63037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978601997146933
INFO:tools.evaluation_results_class:Average Episode Length = 185.47242206235012
INFO:tools.evaluation_results_class:Counted Episodes = 1251
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -130.29640197753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 188.2853240966797
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.11473083496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9955678670360111
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16135.232421875
INFO:tools.evaluation_results_class:Current Best Return = -130.29640197753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9955678670360111
INFO:tools.evaluation_results_class:Average Episode Length = 153.24653739612188
INFO:tools.evaluation_results_class:Counted Episodes = 1805
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -118.04596710205078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 193.8069305419922
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.4189682006836
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9745403111739745
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10954.6552734375
INFO:tools.evaluation_results_class:Current Best Return = -118.04596710205078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9745403111739745
INFO:tools.evaluation_results_class:Average Episode Length = 189.64144271570015
INFO:tools.evaluation_results_class:Counted Episodes = 1414
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=8, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 150.18111068079423
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=8, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 10 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -121.01290130615234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 191.2451629638672
INFO:tools.evaluation_results_class:Average Discounted Reward = 74.20020294189453
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9758064516129032
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11562.4140625
INFO:tools.evaluation_results_class:Current Best Return = -110.63037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978601997146933
INFO:tools.evaluation_results_class:Average Episode Length = 183.8516129032258
INFO:tools.evaluation_results_class:Counted Episodes = 1240
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.777256488800049
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 4.786287307739258
INFO:agents.father_agent:Step: 10, Training loss: 5.1420722007751465
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -126.67565155029297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 179.9678192138672
INFO:tools.evaluation_results_class:Average Discounted Reward = 67.98858642578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9582608695652174
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13111.9140625
INFO:tools.evaluation_results_class:Current Best Return = -110.63037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978601997146933
INFO:tools.evaluation_results_class:Average Episode Length = 199.7086956521739
INFO:tools.evaluation_results_class:Counted Episodes = 1150
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -130.29159545898438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 188.12684631347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.14605712890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9950576606260296
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15941.11328125
INFO:tools.evaluation_results_class:Current Best Return = -130.29159545898438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9950576606260296
INFO:tools.evaluation_results_class:Average Episode Length = 153.00604063701263
INFO:tools.evaluation_results_class:Counted Episodes = 1821
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -132.34127807617188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 176.098388671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 61.971858978271484
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9638739431206764
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13216.9482421875
INFO:tools.evaluation_results_class:Current Best Return = -132.34127807617188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9638739431206764
INFO:tools.evaluation_results_class:Average Episode Length = 203.35511145272866
INFO:tools.evaluation_results_class:Counted Episodes = 1301
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=8, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 164.0001335829291
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=8, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 11 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -132.47506713867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 173.01904296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.268558502197266
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9546690843155031
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14019.8935546875
INFO:tools.evaluation_results_class:Current Best Return = -110.63037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978601997146933
INFO:tools.evaluation_results_class:Average Episode Length = 201.03807796917496
INFO:tools.evaluation_results_class:Counted Episodes = 1103
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.162100315093994
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 17.19%
INFO:agents.father_agent:Step: 5, Training loss: 5.629305839538574
INFO:agents.father_agent:Step: 10, Training loss: 5.721933364868164
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -134.4591522216797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 167.92095947265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 68.15701293945312
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9449378330373002
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15241.4775390625
INFO:tools.evaluation_results_class:Current Best Return = -110.63037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978601997146933
INFO:tools.evaluation_results_class:Average Episode Length = 200.34547069271758
INFO:tools.evaluation_results_class:Counted Episodes = 1126
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -123.93014526367188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 194.5222930908203
INFO:tools.evaluation_results_class:Average Discounted Reward = 84.7920913696289
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9951638903815153
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15786.4990234375
INFO:tools.evaluation_results_class:Current Best Return = -123.93014526367188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9951638903815153
INFO:tools.evaluation_results_class:Average Episode Length = 148.68511552928533
INFO:tools.evaluation_results_class:Counted Episodes = 1861
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.3246612548828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 171.46401977539062
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.281524658203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9555895865237366
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13771.150390625
INFO:tools.evaluation_results_class:Current Best Return = -134.3246612548828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9555895865237366
INFO:tools.evaluation_results_class:Average Episode Length = 205.23583460949465
INFO:tools.evaluation_results_class:Counted Episodes = 1306
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=8, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 168.27711436278537
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=8, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.42018127441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 185.2042236328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 77.85759735107422
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988262910798122
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18849.55859375
INFO:tools.evaluation_results_class:Current Best Return = -134.42018127441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988262910798122
INFO:tools.evaluation_results_class:Average Episode Length = 144.56807511737088
INFO:tools.evaluation_results_class:Counted Episodes = 852
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -90.78154754638672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 227.861083984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 117.09471893310547
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9957582184517497
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5107.53857421875
INFO:tools.evaluation_results_class:Current Best Return = -90.78154754638672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9957582184517497
INFO:tools.evaluation_results_class:Average Episode Length = 131.10286320254508
INFO:tools.evaluation_results_class:Counted Episodes = 943
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.2936248779297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 180.5742950439453
INFO:tools.evaluation_results_class:Average Discounted Reward = 74.56414794921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9964622641509434
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20353.13671875
INFO:tools.evaluation_results_class:Current Best Return = -138.2936248779297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9964622641509434
INFO:tools.evaluation_results_class:Average Episode Length = 149.31721698113208
INFO:tools.evaluation_results_class:Counted Episodes = 848
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -89.73040771484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 230.26959228515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 114.69546508789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7214.3935546875
INFO:tools.evaluation_results_class:Current Best Return = -89.73040771484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 131.56008359456635
INFO:tools.evaluation_results_class:Counted Episodes = 957
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.9778823852539
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.4322509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 139.46771240234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9981566820276497
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2965.643798828125
INFO:tools.evaluation_results_class:Current Best Return = -70.9778823852539
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9981566820276497
INFO:tools.evaluation_results_class:Average Episode Length = 119.07465437788018
INFO:tools.evaluation_results_class:Counted Episodes = 1085
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.18202209472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 185.45391845703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 77.31208038330078
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988623435722411
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17558.291015625
INFO:tools.evaluation_results_class:Current Best Return = -134.18202209472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988623435722411
INFO:tools.evaluation_results_class:Average Episode Length = 141.87144482366327
INFO:tools.evaluation_results_class:Counted Episodes = 879
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -133.4891357421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 186.5108642578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.54095458984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15622.8544921875
INFO:tools.evaluation_results_class:Current Best Return = -133.4891357421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 142.37485714285714
INFO:tools.evaluation_results_class:Counted Episodes = 875
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.68365478515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 182.2039337158203
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.3648452758789
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.996523754345307
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19600.794921875
INFO:tools.evaluation_results_class:Current Best Return = -136.68365478515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.996523754345307
INFO:tools.evaluation_results_class:Average Episode Length = 144.21900347624566
INFO:tools.evaluation_results_class:Counted Episodes = 863
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -131.66744995117188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 187.95518493652344
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.67976379394531
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988207547169812
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15002.7900390625
INFO:tools.evaluation_results_class:Current Best Return = -131.66744995117188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988207547169812
INFO:tools.evaluation_results_class:Average Episode Length = 145.70754716981133
INFO:tools.evaluation_results_class:Counted Episodes = 848
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.75717163085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 173.9461669921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 71.2882080078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.992822966507177
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21545.158203125
INFO:tools.evaluation_results_class:Current Best Return = -143.75717163085938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.992822966507177
INFO:tools.evaluation_results_class:Average Episode Length = 148.9988038277512
INFO:tools.evaluation_results_class:Counted Episodes = 836
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.54710388183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 182.36322021484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 77.758544921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.996594778660613
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17670.248046875
INFO:tools.evaluation_results_class:Current Best Return = -136.54710388183594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.996594778660613
INFO:tools.evaluation_results_class:Average Episode Length = 142.40976163450625
INFO:tools.evaluation_results_class:Counted Episodes = 881
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.3333282470703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 181.48236083984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 77.71337127685547
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9931740614334471
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16847.32421875
INFO:tools.evaluation_results_class:Current Best Return = -136.3333282470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9931740614334471
INFO:tools.evaluation_results_class:Average Episode Length = 144.92377701934015
INFO:tools.evaluation_results_class:Counted Episodes = 879
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -137.00819396972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 181.49298095703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.24878692626953
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9953161592505855
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17412.8515625
INFO:tools.evaluation_results_class:Current Best Return = -137.00819396972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9953161592505855
INFO:tools.evaluation_results_class:Average Episode Length = 146.61943793911007
INFO:tools.evaluation_results_class:Counted Episodes = 854
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -133.9262237548828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 185.3473358154297
INFO:tools.evaluation_results_class:Average Discounted Reward = 79.06446075439453
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9977298524404086
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16830.607421875
INFO:tools.evaluation_results_class:Current Best Return = -133.9262237548828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9977298524404086
INFO:tools.evaluation_results_class:Average Episode Length = 140.67423382519863
INFO:tools.evaluation_results_class:Counted Episodes = 881
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -147.21255493164062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 170.08212280273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 66.73942565917969
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9915458937198067
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18677.634765625
INFO:tools.evaluation_results_class:Current Best Return = -147.21255493164062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9915458937198067
INFO:tools.evaluation_results_class:Average Episode Length = 150.85265700483092
INFO:tools.evaluation_results_class:Counted Episodes = 828
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -137.50059509277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 181.7473602294922
INFO:tools.evaluation_results_class:Average Discounted Reward = 74.19435119628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9976498237367802
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16606.279296875
INFO:tools.evaluation_results_class:Current Best Return = -137.50059509277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9976498237367802
INFO:tools.evaluation_results_class:Average Episode Length = 143.63807285546417
INFO:tools.evaluation_results_class:Counted Episodes = 851
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -135.75552368164062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 176.09507751464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.601348876953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9745331069609507
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14777.2919921875
INFO:tools.evaluation_results_class:Current Best Return = -135.75552368164062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9745331069609507
INFO:tools.evaluation_results_class:Average Episode Length = 190.43293718166385
INFO:tools.evaluation_results_class:Counted Episodes = 589
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -101.13602447509766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 213.1240692138672
INFO:tools.evaluation_results_class:Average Discounted Reward = 98.96195220947266
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9820627802690582
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5424.12646484375
INFO:tools.evaluation_results_class:Current Best Return = -101.13602447509766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9820627802690582
INFO:tools.evaluation_results_class:Average Episode Length = 167.98056801195816
INFO:tools.evaluation_results_class:Counted Episodes = 669
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -132.66445922851562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 178.3437957763672
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.55888366699219
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.971900826446281
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15216.4013671875
INFO:tools.evaluation_results_class:Current Best Return = -132.66445922851562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.971900826446281
INFO:tools.evaluation_results_class:Average Episode Length = 189.8495867768595
INFO:tools.evaluation_results_class:Counted Episodes = 605
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -94.41455841064453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 218.49684143066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 96.16812133789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9778481012658228
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6436.5654296875
INFO:tools.evaluation_results_class:Current Best Return = -94.41455841064453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9778481012658228
INFO:tools.evaluation_results_class:Average Episode Length = 180.3006329113924
INFO:tools.evaluation_results_class:Counted Episodes = 632
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.75263214111328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.61578369140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 126.38204956054688
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9855263157894737
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3300.341552734375
INFO:tools.evaluation_results_class:Current Best Return = -75.75263214111328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9855263157894737
INFO:tools.evaluation_results_class:Average Episode Length = 149.37105263157895
INFO:tools.evaluation_results_class:Counted Episodes = 760
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.7081298828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 176.92372131347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.57462692260742
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9800995024875622
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15962.27734375
INFO:tools.evaluation_results_class:Current Best Return = -136.7081298828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9800995024875622
INFO:tools.evaluation_results_class:Average Episode Length = 190.33167495854062
INFO:tools.evaluation_results_class:Counted Episodes = 603
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -137.1999969482422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 174.26666259765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 61.54875564575195
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9733333333333334
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13439.8291015625
INFO:tools.evaluation_results_class:Current Best Return = -137.1999969482422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9733333333333334
INFO:tools.evaluation_results_class:Average Episode Length = 192.22666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 600
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -144.85989379882812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 165.6129608154297
INFO:tools.evaluation_results_class:Average Discounted Reward = 57.54997253417969
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9702276707530648
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15906.30859375
INFO:tools.evaluation_results_class:Current Best Return = -144.85989379882812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9702276707530648
INFO:tools.evaluation_results_class:Average Episode Length = 192.03502626970229
INFO:tools.evaluation_results_class:Counted Episodes = 571
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -129.71009826660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 178.8240966796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 70.17747497558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9641693811074918
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13738.2470703125
INFO:tools.evaluation_results_class:Current Best Return = -129.71009826660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9641693811074918
INFO:tools.evaluation_results_class:Average Episode Length = 188.43648208469057
INFO:tools.evaluation_results_class:Counted Episodes = 614
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.0436248779297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 170.78707885742188
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.40750503540039
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9650959860383944
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15558.0068359375
INFO:tools.evaluation_results_class:Current Best Return = -138.0436248779297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9650959860383944
INFO:tools.evaluation_results_class:Average Episode Length = 201.23560209424085
INFO:tools.evaluation_results_class:Counted Episodes = 573
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.45704650878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 172.54638671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 61.543094635009766
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9656357388316151
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14472.626953125
INFO:tools.evaluation_results_class:Current Best Return = -136.45704650878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9656357388316151
INFO:tools.evaluation_results_class:Average Episode Length = 201.81443298969072
INFO:tools.evaluation_results_class:Counted Episodes = 582
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.84226989746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 176.83251953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 67.03016662597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9739837398373984
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15194.6767578125
INFO:tools.evaluation_results_class:Current Best Return = -134.84226989746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9739837398373984
INFO:tools.evaluation_results_class:Average Episode Length = 186.08943089430895
INFO:tools.evaluation_results_class:Counted Episodes = 615
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.12606811523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 170.151611328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 57.336402893066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9727427597955707
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14645.2265625
INFO:tools.evaluation_results_class:Current Best Return = -141.12606811523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9727427597955707
INFO:tools.evaluation_results_class:Average Episode Length = 193.54684838160136
INFO:tools.evaluation_results_class:Counted Episodes = 587
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.1669464111328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 175.2913055419922
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.07632827758789
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.979557069846678
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13458.361328125
INFO:tools.evaluation_results_class:Current Best Return = -138.1669464111328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.979557069846678
INFO:tools.evaluation_results_class:Average Episode Length = 192.32197614991483
INFO:tools.evaluation_results_class:Counted Episodes = 587
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.97207641601562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 177.2094268798828
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.493141174316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9755671902268761
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15129.1025390625
INFO:tools.evaluation_results_class:Current Best Return = -134.97207641601562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9755671902268761
INFO:tools.evaluation_results_class:Average Episode Length = 194.09424083769633
INFO:tools.evaluation_results_class:Counted Episodes = 573
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -135.1406707763672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 176.1813507080078
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.49271011352539
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9728813559322034
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16151.734375
INFO:tools.evaluation_results_class:Current Best Return = -135.1406707763672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9728813559322034
INFO:tools.evaluation_results_class:Average Episode Length = 193.53389830508473
INFO:tools.evaluation_results_class:Counted Episodes = 590
INFO:robust_rl.robust_rl_trainer:Iteration 12 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -120.69540405273438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 189.32101440429688
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.7647933959961
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9688013136288999
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11221.44140625
INFO:tools.evaluation_results_class:Current Best Return = -110.63037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978601997146933
INFO:tools.evaluation_results_class:Average Episode Length = 184.12725779967158
INFO:tools.evaluation_results_class:Counted Episodes = 1218
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.8721208572387695
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 12.50%
INFO:agents.father_agent:Step: 5, Training loss: 5.360989570617676
INFO:agents.father_agent:Step: 10, Training loss: 6.9103779792785645
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -115.28457641601562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 202.7486114501953
INFO:tools.evaluation_results_class:Average Discounted Reward = 91.11405181884766
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9938537185003073
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11573.3115234375
INFO:tools.evaluation_results_class:Current Best Return = -110.63037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978601997146933
INFO:tools.evaluation_results_class:Average Episode Length = 146.51813153042409
INFO:tools.evaluation_results_class:Counted Episodes = 1627
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -125.5680923461914
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 194.0004425048828
INFO:tools.evaluation_results_class:Average Discounted Reward = 91.4270248413086
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9986516853932584
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16498.62109375
INFO:tools.evaluation_results_class:Current Best Return = -125.5680923461914
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9986516853932584
INFO:tools.evaluation_results_class:Average Episode Length = 128.1761797752809
INFO:tools.evaluation_results_class:Counted Episodes = 2225
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -125.11903381347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 192.65040588378906
INFO:tools.evaluation_results_class:Average Discounted Reward = 83.42295837402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9930294906166219
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14638.9404296875
INFO:tools.evaluation_results_class:Current Best Return = -125.11903381347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9930294906166219
INFO:tools.evaluation_results_class:Average Episode Length = 149.3544235924933
INFO:tools.evaluation_results_class:Counted Episodes = 1865
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=8, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 145.05581862547137
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=8, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 13 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -117.9088134765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 199.7568359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 90.27690124511719
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9927051671732523
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13690.58203125
INFO:tools.evaluation_results_class:Current Best Return = -110.63037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978601997146933
INFO:tools.evaluation_results_class:Average Episode Length = 148.72158054711247
INFO:tools.evaluation_results_class:Counted Episodes = 1645
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.572811603546143
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 14.06%
INFO:agents.father_agent:Step: 5, Training loss: 5.679058074951172
INFO:agents.father_agent:Step: 10, Training loss: 7.590554237365723
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -99.86231994628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 220.13768005371094
INFO:tools.evaluation_results_class:Average Discounted Reward = 130.70404052734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10616.337890625
INFO:tools.evaluation_results_class:Current Best Return = -99.86231994628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.99461239149954
INFO:tools.evaluation_results_class:Counted Episodes = 3341
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -110.39865112304688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 209.60134887695312
INFO:tools.evaluation_results_class:Average Discounted Reward = 118.05086517333984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12557.1611328125
INFO:tools.evaluation_results_class:Current Best Return = -110.39865112304688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.37530712530713
INFO:tools.evaluation_results_class:Counted Episodes = 3256
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -102.53364562988281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 217.4663543701172
INFO:tools.evaluation_results_class:Average Discounted Reward = 127.85883331298828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10983.7451171875
INFO:tools.evaluation_results_class:Current Best Return = -102.53364562988281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.44199535962878
INFO:tools.evaluation_results_class:Counted Episodes = 3448
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=7, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 112.99142444633289
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=7, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 14 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -103.30745697021484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 216.69253540039062
INFO:tools.evaluation_results_class:Average Discounted Reward = 126.7793960571289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11714.4375
INFO:tools.evaluation_results_class:Current Best Return = -99.86231994628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.42481662591688
INFO:tools.evaluation_results_class:Counted Episodes = 3272
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.825689792633057
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 15.62%
INFO:agents.father_agent:Step: 5, Training loss: 7.8888020515441895
INFO:agents.father_agent:Step: 10, Training loss: 8.967414855957031
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -78.15754699707031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.8424530029297
INFO:tools.evaluation_results_class:Average Discounted Reward = 165.58880615234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5423.96044921875
INFO:tools.evaluation_results_class:Current Best Return = -78.15754699707031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.357752341311134
INFO:tools.evaluation_results_class:Counted Episodes = 4805
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.21634674072266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.7836456298828
INFO:tools.evaluation_results_class:Average Discounted Reward = 157.40708923339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6421.62646484375
INFO:tools.evaluation_results_class:Current Best Return = -82.21634674072266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.53842736561528
INFO:tools.evaluation_results_class:Counted Episodes = 4502
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.25634002685547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.74366760253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 162.3184051513672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6500.884765625
INFO:tools.evaluation_results_class:Current Best Return = -82.25634002685547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.31357318070319
INFO:tools.evaluation_results_class:Counted Episodes = 4892
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 87.99853940582176
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 15 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.1497802734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.8502197265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 164.65853881835938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5547.537109375
INFO:tools.evaluation_results_class:Current Best Return = -78.15754699707031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.47285209070106
INFO:tools.evaluation_results_class:Counted Episodes = 4807
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.111605644226074
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 10.373525619506836
INFO:agents.father_agent:Step: 10, Training loss: 9.739620208740234
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -68.90011596679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.09988403320312
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.0375518798828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4143.97119140625
INFO:tools.evaluation_results_class:Current Best Return = -68.90011596679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.98501728774491
INFO:tools.evaluation_results_class:Counted Episodes = 5206
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.38617706298828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.6138153076172
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.47801208496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4677.18310546875
INFO:tools.evaluation_results_class:Current Best Return = -73.38617706298828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.46474617244158
INFO:tools.evaluation_results_class:Counted Episodes = 4964
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.71379089355469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.2862091064453
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.7237548828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4194.97705078125
INFO:tools.evaluation_results_class:Current Best Return = -69.71379089355469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.5176245210728
INFO:tools.evaluation_results_class:Counted Episodes = 5220
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 76.2569871894295
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 16 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.68375396728516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.3162384033203
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.10496520996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4125.9072265625
INFO:tools.evaluation_results_class:Current Best Return = -68.68375396728516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.06282520716901
INFO:tools.evaluation_results_class:Counted Episodes = 5189
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.76703929901123
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 8.457676887512207
INFO:agents.father_agent:Step: 10, Training loss: 9.157852172851562
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -66.72333526611328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.2766571044922
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.57241821289062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3916.396240234375
INFO:tools.evaluation_results_class:Current Best Return = -66.72333526611328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.400187441424556
INFO:tools.evaluation_results_class:Counted Episodes = 5335
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.65879821777344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.34120178222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.45596313476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4366.318359375
INFO:tools.evaluation_results_class:Current Best Return = -71.65879821777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.09173601703116
INFO:tools.evaluation_results_class:Counted Episodes = 5167
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.07234954833984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.9276580810547
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.65231323242188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4112.099609375
INFO:tools.evaluation_results_class:Current Best Return = -70.07234954833984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.17658027223569
INFO:tools.evaluation_results_class:Counted Episodes = 5363
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 73.99178030586617
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.30067443847656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.69932556152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.81919860839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4428.60595703125
INFO:tools.evaluation_results_class:Current Best Return = -69.30067443847656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.29350856232577
INFO:tools.evaluation_results_class:Counted Episodes = 2511
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.20193099975586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.7980651855469
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.25401306152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3342.62939453125
INFO:tools.evaluation_results_class:Current Best Return = -60.20193099975586
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.958687258687256
INFO:tools.evaluation_results_class:Counted Episodes = 2590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.12409973144531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.8759002685547
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.60934448242188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4294.35888671875
INFO:tools.evaluation_results_class:Current Best Return = -74.12409973144531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.73935742971887
INFO:tools.evaluation_results_class:Counted Episodes = 2490
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.13784790039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.86215209960938
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.1155242919922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4661.41796875
INFO:tools.evaluation_results_class:Current Best Return = -75.13784790039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.71169751870815
INFO:tools.evaluation_results_class:Counted Episodes = 2539
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.09788513183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.9021301269531
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.6744384765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3393.058837890625
INFO:tools.evaluation_results_class:Current Best Return = -62.09788513183594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.60506424792139
INFO:tools.evaluation_results_class:Counted Episodes = 2646
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.97674560546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.02325439453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.29713439941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4474.5390625
INFO:tools.evaluation_results_class:Current Best Return = -73.97674560546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.55813953488372
INFO:tools.evaluation_results_class:Counted Episodes = 2494
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.80223846435547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.19775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.61917114257812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4254.0654296875
INFO:tools.evaluation_results_class:Current Best Return = -69.80223846435547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.42433947157726
INFO:tools.evaluation_results_class:Counted Episodes = 2498
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.2981185913086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.70187377929688
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.78378295898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4225.56640625
INFO:tools.evaluation_results_class:Current Best Return = -68.2981185913086
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.65386154461785
INFO:tools.evaluation_results_class:Counted Episodes = 2499
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.56275177001953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.437255859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.10243225097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4080.09375
INFO:tools.evaluation_results_class:Current Best Return = -68.56275177001953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.16084193804607
INFO:tools.evaluation_results_class:Counted Episodes = 2518
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.37889862060547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.62110900878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.93365478515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4246.5947265625
INFO:tools.evaluation_results_class:Current Best Return = -68.37889862060547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.47082334132694
INFO:tools.evaluation_results_class:Counted Episodes = 2502
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.96702575683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.03297424316406
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.83367919921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4144.73779296875
INFO:tools.evaluation_results_class:Current Best Return = -67.96702575683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.88982710092481
INFO:tools.evaluation_results_class:Counted Episodes = 2487
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.50837707519531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.4916229248047
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.80137634277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4199.67919921875
INFO:tools.evaluation_results_class:Current Best Return = -68.50837707519531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.37350359138069
INFO:tools.evaluation_results_class:Counted Episodes = 2506
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.45545959472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.54454040527344
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.72894287109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4304.94384765625
INFO:tools.evaluation_results_class:Current Best Return = -70.45545959472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.69863563402889
INFO:tools.evaluation_results_class:Counted Episodes = 2492
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.4188003540039
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.58120727539062
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.89085388183594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4106.251953125
INFO:tools.evaluation_results_class:Current Best Return = -69.4188003540039
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.474
INFO:tools.evaluation_results_class:Counted Episodes = 2500
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.91231536865234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.08767700195312
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.584228515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4269.45458984375
INFO:tools.evaluation_results_class:Current Best Return = -68.91231536865234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.325229174970104
INFO:tools.evaluation_results_class:Counted Episodes = 2509
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.23963165283203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.76036071777344
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.2451934814453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4253.73876953125
INFO:tools.evaluation_results_class:Current Best Return = -69.23963165283203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.2914673046252
INFO:tools.evaluation_results_class:Counted Episodes = 2508
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.25823211669922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.74176025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.99627685546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4487.85546875
INFO:tools.evaluation_results_class:Current Best Return = -71.25823211669922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.69598393574297
INFO:tools.evaluation_results_class:Counted Episodes = 2490
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.04718017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.95281982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.64013671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4656.8037109375
INFO:tools.evaluation_results_class:Current Best Return = -72.04718017578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.381847261095565
INFO:tools.evaluation_results_class:Counted Episodes = 2501
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.42857360839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.57142639160156
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.13177490234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4864.61865234375
INFO:tools.evaluation_results_class:Current Best Return = -75.42857360839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.676670668267306
INFO:tools.evaluation_results_class:Counted Episodes = 2499
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -78.37429809570312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.62570190429688
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.34725952148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5155.3330078125
INFO:tools.evaluation_results_class:Current Best Return = -78.37429809570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.75785656728445
INFO:tools.evaluation_results_class:Counted Episodes = 2482
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.89562225341797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.1043701171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.7719268798828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4834.34130859375
INFO:tools.evaluation_results_class:Current Best Return = -75.89562225341797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.699718988358086
INFO:tools.evaluation_results_class:Counted Episodes = 2491
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.57955932617188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.42044067382812
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.35113525390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4254.06494140625
INFO:tools.evaluation_results_class:Current Best Return = -71.57955932617188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.723604826546
INFO:tools.evaluation_results_class:Counted Episodes = 2652
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.70709991455078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.29290771484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.30279541015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3235.91943359375
INFO:tools.evaluation_results_class:Current Best Return = -58.70709991455078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.31656804733728
INFO:tools.evaluation_results_class:Counted Episodes = 2704
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.18773651123047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.812255859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.07196044921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4170.41650390625
INFO:tools.evaluation_results_class:Current Best Return = -71.18773651123047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.95392231530845
INFO:tools.evaluation_results_class:Counted Episodes = 2626
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.75326538085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.24673461914062
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.85328674316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4260.9794921875
INFO:tools.evaluation_results_class:Current Best Return = -71.75326538085938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.037700634565134
INFO:tools.evaluation_results_class:Counted Episodes = 2679
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -61.56193542480469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.43804931640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.45106506347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3225.438720703125
INFO:tools.evaluation_results_class:Current Best Return = -61.56193542480469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.1491513181654
INFO:tools.evaluation_results_class:Counted Episodes = 2769
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.06735229492188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.93264770507812
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.37789916992188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4306.82177734375
INFO:tools.evaluation_results_class:Current Best Return = -71.06735229492188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.16856925418569
INFO:tools.evaluation_results_class:Counted Episodes = 2628
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.09293365478516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.9070587158203
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.0468292236328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4021.2890625
INFO:tools.evaluation_results_class:Current Best Return = -68.09293365478516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.618813751416695
INFO:tools.evaluation_results_class:Counted Episodes = 2647
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.56290435791016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.4370880126953
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.00442504882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3898.376953125
INFO:tools.evaluation_results_class:Current Best Return = -66.56290435791016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.105162523900574
INFO:tools.evaluation_results_class:Counted Episodes = 2615
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.828277587890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.1717224121094
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.67784118652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3878.592529296875
INFO:tools.evaluation_results_class:Current Best Return = -63.828277587890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.85405610310841
INFO:tools.evaluation_results_class:Counted Episodes = 2638
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.38935089111328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.6106414794922
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.2463836669922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3897.56884765625
INFO:tools.evaluation_results_class:Current Best Return = -67.38935089111328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.975665399239546
INFO:tools.evaluation_results_class:Counted Episodes = 2630
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.53277587890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.46722412109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.69195556640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4032.052490234375
INFO:tools.evaluation_results_class:Current Best Return = -66.53277587890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.27515243902439
INFO:tools.evaluation_results_class:Counted Episodes = 2624
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.4749526977539
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.52503967285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.71023559570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4151.29345703125
INFO:tools.evaluation_results_class:Current Best Return = -69.4749526977539
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.8030131826742
INFO:tools.evaluation_results_class:Counted Episodes = 2655
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.45465087890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.54534912109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.1831817626953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4273.69873046875
INFO:tools.evaluation_results_class:Current Best Return = -69.45465087890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.123094512195124
INFO:tools.evaluation_results_class:Counted Episodes = 2624
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.92418670654297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.07582092285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.77980041503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4103.14501953125
INFO:tools.evaluation_results_class:Current Best Return = -67.92418670654297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.973464746019715
INFO:tools.evaluation_results_class:Counted Episodes = 2638
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -65.55357360839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.44642639160156
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.9498291015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3800.22412109375
INFO:tools.evaluation_results_class:Current Best Return = -65.55357360839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.93579027355623
INFO:tools.evaluation_results_class:Counted Episodes = 2632
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.85676574707031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.1432342529297
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.91481018066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3996.273681640625
INFO:tools.evaluation_results_class:Current Best Return = -67.85676574707031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.94050776809397
INFO:tools.evaluation_results_class:Counted Episodes = 2639
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.59620666503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.40379333496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.18687438964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4123.978515625
INFO:tools.evaluation_results_class:Current Best Return = -67.59620666503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.96091081593928
INFO:tools.evaluation_results_class:Counted Episodes = 2635
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.02578735351562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.97421264648438
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.96104431152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4222.93798828125
INFO:tools.evaluation_results_class:Current Best Return = -69.02578735351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.809632157755026
INFO:tools.evaluation_results_class:Counted Episodes = 2637
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.39969635009766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.60031127929688
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.22528076171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4773.021484375
INFO:tools.evaluation_results_class:Current Best Return = -76.39969635009766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.76139817629179
INFO:tools.evaluation_results_class:Counted Episodes = 2632
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.06456756591797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.93544006347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.03274536132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4382.93212890625
INFO:tools.evaluation_results_class:Current Best Return = -73.06456756591797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.96202050892518
INFO:tools.evaluation_results_class:Counted Episodes = 2633
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.66188049316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.33811950683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.7085418701172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4387.4697265625
INFO:tools.evaluation_results_class:Current Best Return = -73.66188049316406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.665281450698906
INFO:tools.evaluation_results_class:Counted Episodes = 2647
INFO:robust_rl.robust_rl_trainer:Iteration 17 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.71868896484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.28131103515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.57745361328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3952.42041015625
INFO:tools.evaluation_results_class:Current Best Return = -66.72333526611328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.623918766453556
INFO:tools.evaluation_results_class:Counted Episodes = 5318
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.103394508361816
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 8.446717262268066
INFO:agents.father_agent:Step: 10, Training loss: 9.00949764251709
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -67.53253173828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.46746826171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.8202362060547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4027.31201171875
INFO:tools.evaluation_results_class:Current Best Return = -66.72333526611328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.57235023041475
INFO:tools.evaluation_results_class:Counted Episodes = 5425
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.2186508178711
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.78135681152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.06777954101562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4485.90478515625
INFO:tools.evaluation_results_class:Current Best Return = -72.2186508178711
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.949142747062226
INFO:tools.evaluation_results_class:Counted Episodes = 5191
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.38186645507812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.61813354492188
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.0723876953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4064.61181640625
INFO:tools.evaluation_results_class:Current Best Return = -69.38186645507812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.49502395871729
INFO:tools.evaluation_results_class:Counted Episodes = 5426
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 73.05074573616358
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=6
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 18 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.84117126464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.15882873535156
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.37632751464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4093.4794921875
INFO:tools.evaluation_results_class:Current Best Return = -66.72333526611328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.81784524250278
INFO:tools.evaluation_results_class:Counted Episodes = 5402
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.8507609367370605
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 8.536816596984863
INFO:agents.father_agent:Step: 10, Training loss: 8.872949600219727
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -66.6134262084961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.38658142089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.1771697998047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3919.494140625
INFO:tools.evaluation_results_class:Current Best Return = -66.6134262084961
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.10431387132763
INFO:tools.evaluation_results_class:Counted Episodes = 5378
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.57197570800781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.4280242919922
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.5703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4281.93505859375
INFO:tools.evaluation_results_class:Current Best Return = -69.57197570800781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.95535026943803
INFO:tools.evaluation_results_class:Counted Episodes = 5196
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.55833435058594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.44166564941406
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.77064514160156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4027.6591796875
INFO:tools.evaluation_results_class:Current Best Return = -67.55833435058594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.70422790782072
INFO:tools.evaluation_results_class:Counted Episodes = 5511
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=4, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 73.63998887979477
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=4, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 19 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.8780288696289
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.12196350097656
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.1761932373047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4029.161865234375
INFO:tools.evaluation_results_class:Current Best Return = -66.6134262084961
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.89913011290024
INFO:tools.evaluation_results_class:Counted Episodes = 5403
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.70642614364624
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 6.849838733673096
INFO:agents.father_agent:Step: 10, Training loss: 7.379714488983154
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -66.16312408447266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.83688354492188
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.92547607421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3962.74169921875
INFO:tools.evaluation_results_class:Current Best Return = -66.16312408447266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.6620884289746
INFO:tools.evaluation_results_class:Counted Episodes = 5315
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.76441955566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.23558044433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.93057250976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4294.85302734375
INFO:tools.evaluation_results_class:Current Best Return = -68.76441955566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.321525358110726
INFO:tools.evaluation_results_class:Counted Episodes = 5166
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -65.30510711669922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.6949005126953
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.49769592285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4020.029052734375
INFO:tools.evaluation_results_class:Current Best Return = -65.30510711669922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.87056638811514
INFO:tools.evaluation_results_class:Counted Episodes = 5385
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 71.1454185651067
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 20 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.74756622314453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.25242614746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.77073669433594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4112.4306640625
INFO:tools.evaluation_results_class:Current Best Return = -66.16312408447266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.40052356020942
INFO:tools.evaluation_results_class:Counted Episodes = 5348
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.321293830871582
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 6.802724361419678
INFO:agents.father_agent:Step: 10, Training loss: 5.973219394683838
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -64.6986083984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.3013916015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.6641845703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3996.78125
INFO:tools.evaluation_results_class:Current Best Return = -64.6986083984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.261456550675035
INFO:tools.evaluation_results_class:Counted Episodes = 5259
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.35282135009766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.6471710205078
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.94448852539062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4493.3515625
INFO:tools.evaluation_results_class:Current Best Return = -70.35282135009766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.91758132109359
INFO:tools.evaluation_results_class:Counted Episodes = 5011
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.7751235961914
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.22488403320312
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.02203369140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3967.43310546875
INFO:tools.evaluation_results_class:Current Best Return = -64.7751235961914
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.826307866014304
INFO:tools.evaluation_results_class:Counted Episodes = 5314
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 72.78399069829811
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 21 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.4579849243164
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.54200744628906
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.16868591308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3852.765380859375
INFO:tools.evaluation_results_class:Current Best Return = -64.4579849243164
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.01665404996215
INFO:tools.evaluation_results_class:Counted Episodes = 5284
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.067275047302246
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 6.168115615844727
INFO:agents.father_agent:Step: 10, Training loss: 5.108543872833252
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -66.43570709228516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.5642852783203
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.82969665527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4088.765380859375
INFO:tools.evaluation_results_class:Current Best Return = -64.4579849243164
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.78978129713424
INFO:tools.evaluation_results_class:Counted Episodes = 5304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.04031372070312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.95968627929688
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.8269500732422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4780.625
INFO:tools.evaluation_results_class:Current Best Return = -71.04031372070312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.58778472082241
INFO:tools.evaluation_results_class:Counted Episodes = 4961
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.81755065917969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.1824493408203
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.59278869628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4106.66748046875
INFO:tools.evaluation_results_class:Current Best Return = -66.81755065917969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.61987974445697
INFO:tools.evaluation_results_class:Counted Episodes = 5322
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 70.91800147612581
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.28411102294922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.71588134765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.31631469726562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4944.2646484375
INFO:tools.evaluation_results_class:Current Best Return = -74.28411102294922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.73413521360431
INFO:tools.evaluation_results_class:Counted Episodes = 2411
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.586387634277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.4136047363281
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.9856414794922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3468.31005859375
INFO:tools.evaluation_results_class:Current Best Return = -58.586387634277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.908981071284735
INFO:tools.evaluation_results_class:Counted Episodes = 2483
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.80721282958984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.19277954101562
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.78414916992188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5171.865234375
INFO:tools.evaluation_results_class:Current Best Return = -76.80721282958984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.658789386401324
INFO:tools.evaluation_results_class:Counted Episodes = 2412
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.52873229980469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.4712677001953
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.82199096679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4804.02587890625
INFO:tools.evaluation_results_class:Current Best Return = -74.52873229980469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.91954022988506
INFO:tools.evaluation_results_class:Counted Episodes = 2436
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.618751525878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.3812561035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 184.7230682373047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3594.55322265625
INFO:tools.evaluation_results_class:Current Best Return = -62.618751525878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.0296875
INFO:tools.evaluation_results_class:Counted Episodes = 2560
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.57064819335938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.42935180664062
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.99246215820312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4700.21435546875
INFO:tools.evaluation_results_class:Current Best Return = -73.57064819335938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.53027754415475
INFO:tools.evaluation_results_class:Counted Episodes = 2378
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.68939208984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.31060791015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.36656188964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4597.2119140625
INFO:tools.evaluation_results_class:Current Best Return = -70.68939208984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.08670033670034
INFO:tools.evaluation_results_class:Counted Episodes = 2376
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.88075256347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.11924743652344
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.306640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4771.40478515625
INFO:tools.evaluation_results_class:Current Best Return = -70.88075256347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.03305439330544
INFO:tools.evaluation_results_class:Counted Episodes = 2390
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.07522583007812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.92477416992188
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.1890411376953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4809.60107421875
INFO:tools.evaluation_results_class:Current Best Return = -69.07522583007812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.73275145469659
INFO:tools.evaluation_results_class:Counted Episodes = 2406
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.37053680419922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.62945556640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.86398315429688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4531.861328125
INFO:tools.evaluation_results_class:Current Best Return = -68.37053680419922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.60290456431535
INFO:tools.evaluation_results_class:Counted Episodes = 2410
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.07354736328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.92645263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.0872344970703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4824.943359375
INFO:tools.evaluation_results_class:Current Best Return = -70.07354736328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.9314667781028
INFO:tools.evaluation_results_class:Counted Episodes = 2393
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.20393371582031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.7960662841797
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.58592224121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4603.3125
INFO:tools.evaluation_results_class:Current Best Return = -67.20393371582031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.93174204355109
INFO:tools.evaluation_results_class:Counted Episodes = 2388
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.53549194335938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.46450805664062
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.885986328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4623.72216796875
INFO:tools.evaluation_results_class:Current Best Return = -70.53549194335938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.68700705687007
INFO:tools.evaluation_results_class:Counted Episodes = 2409
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.36033630371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.63966369628906
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.6832275390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4958.75
INFO:tools.evaluation_results_class:Current Best Return = -69.36033630371094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.045093945720254
INFO:tools.evaluation_results_class:Counted Episodes = 2395
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.54161834716797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.45838928222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.8905029296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4684.72216796875
INFO:tools.evaluation_results_class:Current Best Return = -69.54161834716797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.686542443064184
INFO:tools.evaluation_results_class:Counted Episodes = 2415
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.97531127929688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.02468872070312
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.87620544433594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4782.2861328125
INFO:tools.evaluation_results_class:Current Best Return = -68.97531127929688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.0907949790795
INFO:tools.evaluation_results_class:Counted Episodes = 2390
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.29611206054688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.70388793945312
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.22520446777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4689.517578125
INFO:tools.evaluation_results_class:Current Best Return = -69.29611206054688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.504549214226635
INFO:tools.evaluation_results_class:Counted Episodes = 2418
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.21833038330078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.7816619873047
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.61712646484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5282.828125
INFO:tools.evaluation_results_class:Current Best Return = -76.21833038330078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.92333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 2400
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.74708557128906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.25291442871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.89356994628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4865.38623046875
INFO:tools.evaluation_results_class:Current Best Return = -72.74708557128906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.64559068219634
INFO:tools.evaluation_results_class:Counted Episodes = 2404
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.23238372802734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.7676239013672
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.0308380126953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5103.7578125
INFO:tools.evaluation_results_class:Current Best Return = -74.23238372802734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.14513422818792
INFO:tools.evaluation_results_class:Counted Episodes = 2384
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.36683654785156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.63316345214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.75205993652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5054.60986328125
INFO:tools.evaluation_results_class:Current Best Return = -75.36683654785156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.425460636515915
INFO:tools.evaluation_results_class:Counted Episodes = 2388
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.50334167480469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.4966583251953
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.7196807861328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5140.21923828125
INFO:tools.evaluation_results_class:Current Best Return = -75.50334167480469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.23076923076923
INFO:tools.evaluation_results_class:Counted Episodes = 2392
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.32649993896484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.6735076904297
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.3661651611328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4482.6494140625
INFO:tools.evaluation_results_class:Current Best Return = -70.32649993896484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.630914826498426
INFO:tools.evaluation_results_class:Counted Episodes = 2536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.25543212890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.74456787109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.7354736328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5026.5947265625
INFO:tools.evaluation_results_class:Current Best Return = -77.25543212890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.094063545150505
INFO:tools.evaluation_results_class:Counted Episodes = 2392
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.6044692993164
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.39553833007812
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.11595153808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4418.36669921875
INFO:tools.evaluation_results_class:Current Best Return = -75.6044692993164
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.24720893141946
INFO:tools.evaluation_results_class:Counted Episodes = 2508
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.40576934814453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.59423828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.6656951904297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5040.23046875
INFO:tools.evaluation_results_class:Current Best Return = -75.40576934814453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.88132051817802
INFO:tools.evaluation_results_class:Counted Episodes = 2393
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.12657928466797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.87342834472656
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.95509338378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4056.1484375
INFO:tools.evaluation_results_class:Current Best Return = -66.12657928466797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.40229445506692
INFO:tools.evaluation_results_class:Counted Episodes = 2615
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.41984939575195
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.58013916015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.11228942871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2855.888671875
INFO:tools.evaluation_results_class:Current Best Return = -53.41984939575195
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.3434456928839
INFO:tools.evaluation_results_class:Counted Episodes = 2670
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.24932098388672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.75067138671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.99803161621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4038.447509765625
INFO:tools.evaluation_results_class:Current Best Return = -69.24932098388672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.05583559519194
INFO:tools.evaluation_results_class:Counted Episodes = 2579
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.18685913085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.81314086914062
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.60162353515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4467.53955078125
INFO:tools.evaluation_results_class:Current Best Return = -70.18685913085938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.988226357766806
INFO:tools.evaluation_results_class:Counted Episodes = 2633
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.74550247192383
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.2544860839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.67483520507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3654.917724609375
INFO:tools.evaluation_results_class:Current Best Return = -59.74550247192383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.97796547925083
INFO:tools.evaluation_results_class:Counted Episodes = 2723
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.52916717529297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.47084045410156
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.4429473876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4079.068603515625
INFO:tools.evaluation_results_class:Current Best Return = -67.52916717529297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.554489639293934
INFO:tools.evaluation_results_class:Counted Episodes = 2606
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.911415100097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.0885925292969
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.27914428710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4119.66162109375
INFO:tools.evaluation_results_class:Current Best Return = -63.911415100097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.272241313478425
INFO:tools.evaluation_results_class:Counted Episodes = 2619
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.12866973876953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.871337890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.56785583496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3942.065673828125
INFO:tools.evaluation_results_class:Current Best Return = -63.12866973876953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.71831530139104
INFO:tools.evaluation_results_class:Counted Episodes = 2588
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.87288284301758
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.1271057128906
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.72662353515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4041.60009765625
INFO:tools.evaluation_results_class:Current Best Return = -62.87288284301758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.81895223420647
INFO:tools.evaluation_results_class:Counted Episodes = 2596
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -61.55036544799805
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.44964599609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.30557250976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4069.0048828125
INFO:tools.evaluation_results_class:Current Best Return = -61.55036544799805
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.47529682114133
INFO:tools.evaluation_results_class:Counted Episodes = 2611
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.648983001708984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.35101318359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.22320556640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4016.8349609375
INFO:tools.evaluation_results_class:Current Best Return = -63.648983001708984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.63052672049212
INFO:tools.evaluation_results_class:Counted Episodes = 2601
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -61.10331344604492
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.8966979980469
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.31190490722656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3893.474853515625
INFO:tools.evaluation_results_class:Current Best Return = -61.10331344604492
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.845026985350806
INFO:tools.evaluation_results_class:Counted Episodes = 2594
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.31060028076172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.6894073486328
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.3556671142578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4106.859375
INFO:tools.evaluation_results_class:Current Best Return = -64.31060028076172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.804238921001925
INFO:tools.evaluation_results_class:Counted Episodes = 2595
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.319679260253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.6803283691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 184.35858154296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4055.702880859375
INFO:tools.evaluation_results_class:Current Best Return = -62.319679260253906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.64969372128637
INFO:tools.evaluation_results_class:Counted Episodes = 2612
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.54419708251953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.455810546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.31642150878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4032.98779296875
INFO:tools.evaluation_results_class:Current Best Return = -63.54419708251953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.59800153727902
INFO:tools.evaluation_results_class:Counted Episodes = 2602
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.39266586303711
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.6073303222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 184.00450134277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4094.472900390625
INFO:tools.evaluation_results_class:Current Best Return = -62.39266586303711
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.044401544401545
INFO:tools.evaluation_results_class:Counted Episodes = 2590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.238609313964844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.7613830566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.427490234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3942.057861328125
INFO:tools.evaluation_results_class:Current Best Return = -63.238609313964844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.68648648648649
INFO:tools.evaluation_results_class:Counted Episodes = 2590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.27964782714844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.72035217285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.70082092285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4356.40234375
INFO:tools.evaluation_results_class:Current Best Return = -67.27964782714844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.93935882580147
INFO:tools.evaluation_results_class:Counted Episodes = 2589
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.61493682861328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.3850555419922
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.69253540039062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4596.20166015625
INFO:tools.evaluation_results_class:Current Best Return = -70.61493682861328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.76087793608009
INFO:tools.evaluation_results_class:Counted Episodes = 2597
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.68016815185547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.31982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.5771026611328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4569.2685546875
INFO:tools.evaluation_results_class:Current Best Return = -70.68016815185547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.77507716049383
INFO:tools.evaluation_results_class:Counted Episodes = 2592
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.29639434814453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.70359802246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.21945190429688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4355.93310546875
INFO:tools.evaluation_results_class:Current Best Return = -69.29639434814453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.44593558282209
INFO:tools.evaluation_results_class:Counted Episodes = 2608
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.36571502685547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.63429260253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.64456176757812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4460.181640625
INFO:tools.evaluation_results_class:Current Best Return = -69.36571502685547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.998065015479874
INFO:tools.evaluation_results_class:Counted Episodes = 2584
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.1434326171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.8565673828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.3907012939453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4200.53271484375
INFO:tools.evaluation_results_class:Current Best Return = -69.1434326171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.91526045487895
INFO:tools.evaluation_results_class:Counted Episodes = 2726
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.62904357910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.37095642089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.09324645996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4428.42529296875
INFO:tools.evaluation_results_class:Current Best Return = -71.62904357910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.73921417565485
INFO:tools.evaluation_results_class:Counted Episodes = 2596
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.62785339355469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.3721466064453
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.16458129882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3860.167236328125
INFO:tools.evaluation_results_class:Current Best Return = -68.62785339355469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.236551215917466
INFO:tools.evaluation_results_class:Counted Episodes = 2714
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.19615936279297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.8038330078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.322998046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4207.84716796875
INFO:tools.evaluation_results_class:Current Best Return = -68.19615936279297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.65719769673704
INFO:tools.evaluation_results_class:Counted Episodes = 2605
INFO:robust_rl.robust_rl_trainer:Iteration 22 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -65.8742904663086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.12570190429688
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.3051300048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3885.57568359375
INFO:tools.evaluation_results_class:Current Best Return = -64.4579849243164
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.79890689785149
INFO:tools.evaluation_results_class:Counted Episodes = 5306
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.259077548980713
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 4.865475654602051
INFO:agents.father_agent:Step: 10, Training loss: 5.0267791748046875
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -64.62688446044922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.3731231689453
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.68637084960938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4073.58935546875
INFO:tools.evaluation_results_class:Current Best Return = -64.4579849243164
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.538989513822685
INFO:tools.evaluation_results_class:Counted Episodes = 5245
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.1894760131836
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.81053161621094
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.2790985107422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5155.28955078125
INFO:tools.evaluation_results_class:Current Best Return = -75.1894760131836
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.535149750415975
INFO:tools.evaluation_results_class:Counted Episodes = 4808
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.73978424072266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.26022338867188
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.30828857421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4529.77587890625
INFO:tools.evaluation_results_class:Current Best Return = -69.73978424072266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.28682759931572
INFO:tools.evaluation_results_class:Counted Episodes = 5261
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 75.69096220241593
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 23 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -65.86209869384766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.13790893554688
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.32705688476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4078.412109375
INFO:tools.evaluation_results_class:Current Best Return = -64.4579849243164
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.87189148455162
INFO:tools.evaluation_results_class:Counted Episodes = 5308
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.567611217498779
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 4.083916187286377
INFO:agents.father_agent:Step: 10, Training loss: 3.979635238647461
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -63.91543197631836
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.0845642089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.83084106445312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3986.268310546875
INFO:tools.evaluation_results_class:Current Best Return = -63.91543197631836
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.99423409571401
INFO:tools.evaluation_results_class:Counted Episodes = 5203
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.3443603515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.6556396484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.2766571044922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5024.46337890625
INFO:tools.evaluation_results_class:Current Best Return = -71.3443603515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.01840744570838
INFO:tools.evaluation_results_class:Counted Episodes = 4835
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.6029052734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.3970947265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.5831298828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4083.511474609375
INFO:tools.evaluation_results_class:Current Best Return = -64.6029052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.725782133433846
INFO:tools.evaluation_results_class:Counted Episodes = 5306
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 73.39143180800738
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 24 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.61271286010742
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.3872985839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 184.62347412109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3924.156982421875
INFO:tools.evaluation_results_class:Current Best Return = -62.61271286010742
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.467455621301774
INFO:tools.evaluation_results_class:Counted Episodes = 5239
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.6085007190704346
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 3.2564120292663574
INFO:agents.father_agent:Step: 10, Training loss: 3.5928475856781006
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -57.125789642333984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.87420654296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.30038452148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3377.1953125
INFO:tools.evaluation_results_class:Current Best Return = -57.125789642333984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.42879746835443
INFO:tools.evaluation_results_class:Counted Episodes = 5056
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.2558822631836
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.74412536621094
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.6890411376953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4354.02001953125
INFO:tools.evaluation_results_class:Current Best Return = -66.2558822631836
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.76742179072276
INFO:tools.evaluation_results_class:Counted Episodes = 4635
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.0782470703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.9217529296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.26837158203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3705.075439453125
INFO:tools.evaluation_results_class:Current Best Return = -59.0782470703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.75508607198748
INFO:tools.evaluation_results_class:Counted Episodes = 5112
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 69.76604466895702
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 25 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.29655075073242
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.7034606933594
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.4756622314453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3568.724365234375
INFO:tools.evaluation_results_class:Current Best Return = -57.125789642333984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.26660098522167
INFO:tools.evaluation_results_class:Counted Episodes = 5075
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.584824323654175
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 2.688541889190674
INFO:agents.father_agent:Step: 10, Training loss: 2.327259063720703
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -55.34510803222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.6549072265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.1441650390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3252.06591796875
INFO:tools.evaluation_results_class:Current Best Return = -55.34510803222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.93747462444174
INFO:tools.evaluation_results_class:Counted Episodes = 4926
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.35265350341797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.6473388671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.02951049804688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3935.075439453125
INFO:tools.evaluation_results_class:Current Best Return = -63.35265350341797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.09420452002685
INFO:tools.evaluation_results_class:Counted Episodes = 4469
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.434593200683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.5653991699219
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.5391845703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3335.6328125
INFO:tools.evaluation_results_class:Current Best Return = -56.434593200683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.80447373676852
INFO:tools.evaluation_results_class:Counted Episodes = 5007
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 66.28074029159143
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 26 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.66343688964844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.3365478515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 188.29202270507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3178.160888671875
INFO:tools.evaluation_results_class:Current Best Return = -54.66343688964844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.55770782889427
INFO:tools.evaluation_results_class:Counted Episodes = 4956
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.456381320953369
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 1.9608914852142334
INFO:agents.father_agent:Step: 10, Training loss: 1.8480461835861206
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -53.61601638793945
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.38397216796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.5573272705078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3140.593505859375
INFO:tools.evaluation_results_class:Current Best Return = -53.61601638793945
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.265065065065066
INFO:tools.evaluation_results_class:Counted Episodes = 4995
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -65.47355651855469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.5264434814453
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.343505859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4546.48486328125
INFO:tools.evaluation_results_class:Current Best Return = -65.47355651855469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.93723941189378
INFO:tools.evaluation_results_class:Counted Episodes = 4557
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.91291427612305
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.08709716796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.07797241210938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3431.1474609375
INFO:tools.evaluation_results_class:Current Best Return = -56.91291427612305
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.86309286568354
INFO:tools.evaluation_results_class:Counted Episodes = 5018
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 66.51503856124239
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.00825119018555
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.99176025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.25787353515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4326.21826171875
INFO:tools.evaluation_results_class:Current Best Return = -63.00825119018555
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.64252978918424
INFO:tools.evaluation_results_class:Counted Episodes = 2182
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.45861053466797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.5413818359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.88070678710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2676.056884765625
INFO:tools.evaluation_results_class:Current Best Return = -48.45861053466797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.58123063302347
INFO:tools.evaluation_results_class:Counted Episodes = 2259
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -65.1061019897461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.89389038085938
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.5519561767578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4313.05810546875
INFO:tools.evaluation_results_class:Current Best Return = -65.1061019897461
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.33287795992715
INFO:tools.evaluation_results_class:Counted Episodes = 2196
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.32837677001953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.67161560058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.3450927734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4201.0087890625
INFO:tools.evaluation_results_class:Current Best Return = -66.32837677001953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.43738738738739
INFO:tools.evaluation_results_class:Counted Episodes = 2220
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.54573059082031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.45428466796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.04873657226562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3571.993896484375
INFO:tools.evaluation_results_class:Current Best Return = -55.54573059082031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.79540528825314
INFO:tools.evaluation_results_class:Counted Episodes = 2307
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.396202087402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.6037902832031
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.33917236328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4030.6015625
INFO:tools.evaluation_results_class:Current Best Return = -63.396202087402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.01251158480075
INFO:tools.evaluation_results_class:Counted Episodes = 2158
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.65778732299805
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.34222412109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.05099487304688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3934.34912109375
INFO:tools.evaluation_results_class:Current Best Return = -59.65778732299805
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.9232889297198
INFO:tools.evaluation_results_class:Counted Episodes = 2177
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.149169921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.850830078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.21888732910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3813.500732421875
INFO:tools.evaluation_results_class:Current Best Return = -58.149169921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.97605893186004
INFO:tools.evaluation_results_class:Counted Episodes = 2172
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.55641555786133
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.4435729980469
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.87461853027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3702.353271484375
INFO:tools.evaluation_results_class:Current Best Return = -58.55641555786133
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.90855323020928
INFO:tools.evaluation_results_class:Counted Episodes = 2198
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.050113677978516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.94989013671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.89842224121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3506.7939453125
INFO:tools.evaluation_results_class:Current Best Return = -56.050113677978516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.44145785876994
INFO:tools.evaluation_results_class:Counted Episodes = 2195
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.01094055175781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.98907470703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.2130126953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3528.337890625
INFO:tools.evaluation_results_class:Current Best Return = -58.01094055175781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.2616226071103
INFO:tools.evaluation_results_class:Counted Episodes = 2194
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.636112213134766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.3638916015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.88890075683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3842.241943359375
INFO:tools.evaluation_results_class:Current Best Return = -57.636112213134766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.90787655458314
INFO:tools.evaluation_results_class:Counted Episodes = 2171
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.260231018066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.7397766113281
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.31166076660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3625.934326171875
INFO:tools.evaluation_results_class:Current Best Return = -57.260231018066406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.66298850574712
INFO:tools.evaluation_results_class:Counted Episodes = 2175
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.5873908996582
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.41259765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.74072265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3954.189697265625
INFO:tools.evaluation_results_class:Current Best Return = -60.5873908996582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.29160871580899
INFO:tools.evaluation_results_class:Counted Episodes = 2157
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.014732360839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.9852600097656
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.6520538330078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3601.617431640625
INFO:tools.evaluation_results_class:Current Best Return = -58.014732360839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.71086556169429
INFO:tools.evaluation_results_class:Counted Episodes = 2172
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.29258346557617
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.7074279785156
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.61558532714844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3581.247314453125
INFO:tools.evaluation_results_class:Current Best Return = -57.29258346557617
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.50228937728937
INFO:tools.evaluation_results_class:Counted Episodes = 2184
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.30620574951172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.69378662109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.80755615234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3840.85791015625
INFO:tools.evaluation_results_class:Current Best Return = -59.30620574951172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.38264388489209
INFO:tools.evaluation_results_class:Counted Episodes = 2224
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.93193054199219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.06805419921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.330322265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3797.794677734375
INFO:tools.evaluation_results_class:Current Best Return = -58.93193054199219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.45089081772498
INFO:tools.evaluation_results_class:Counted Episodes = 2189
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.91332244873047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.086669921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.61839294433594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4167.25146484375
INFO:tools.evaluation_results_class:Current Best Return = -59.91332244873047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.12317518248175
INFO:tools.evaluation_results_class:Counted Episodes = 2192
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.20895004272461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.7910461425781
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.97674560546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4079.411376953125
INFO:tools.evaluation_results_class:Current Best Return = -63.20895004272461
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.87592250922509
INFO:tools.evaluation_results_class:Counted Episodes = 2168
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.52973556518555
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.47027587890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.8520965576172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4316.92529296875
INFO:tools.evaluation_results_class:Current Best Return = -62.52973556518555
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.52698993595608
INFO:tools.evaluation_results_class:Counted Episodes = 2186
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.99954605102539
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.0004577636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.2640380859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4497.6376953125
INFO:tools.evaluation_results_class:Current Best Return = -63.99954605102539
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.76926563916591
INFO:tools.evaluation_results_class:Counted Episodes = 2206
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -61.162841796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.837158203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.83287048339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3946.493408203125
INFO:tools.evaluation_results_class:Current Best Return = -61.162841796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.10524036379385
INFO:tools.evaluation_results_class:Counted Episodes = 2309
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.6595344543457
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.3404541015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.70294189453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4040.388671875
INFO:tools.evaluation_results_class:Current Best Return = -63.6595344543457
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.01137915339099
INFO:tools.evaluation_results_class:Counted Episodes = 2197
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.31797790527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.68202209472656
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.58877563476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4517.236328125
INFO:tools.evaluation_results_class:Current Best Return = -72.31797790527344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.11337466784765
INFO:tools.evaluation_results_class:Counted Episodes = 2258
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.20993041992188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.79006958007812
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.43402099609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4333.201171875
INFO:tools.evaluation_results_class:Current Best Return = -66.20993041992188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.22905282331512
INFO:tools.evaluation_results_class:Counted Episodes = 2196
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.96836853027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.03163146972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.81942749023438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4410.10693359375
INFO:tools.evaluation_results_class:Current Best Return = -69.96836853027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.933216168717045
INFO:tools.evaluation_results_class:Counted Episodes = 2276
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.45179748535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.54820251464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.33953857421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4331.33203125
INFO:tools.evaluation_results_class:Current Best Return = -69.45179748535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.63838294091515
INFO:tools.evaluation_results_class:Counted Episodes = 2251
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.0978775024414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.90213012695312
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.61167907714844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4323.37744140625
INFO:tools.evaluation_results_class:Current Best Return = -71.0978775024414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.30602302922941
INFO:tools.evaluation_results_class:Counted Episodes = 2258
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.25556945800781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.7444305419922
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.8916778564453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4419.82958984375
INFO:tools.evaluation_results_class:Current Best Return = -71.25556945800781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.03101791175186
INFO:tools.evaluation_results_class:Counted Episodes = 2289
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.59032440185547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.40968322753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.16549682617188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4659.80712890625
INFO:tools.evaluation_results_class:Current Best Return = -73.59032440185547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.53084775854417
INFO:tools.evaluation_results_class:Counted Episodes = 2253
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.76919937133789
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.2308044433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.21389770507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3277.87451171875
INFO:tools.evaluation_results_class:Current Best Return = -55.76919937133789
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.04995871180842
INFO:tools.evaluation_results_class:Counted Episodes = 2422
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -42.037506103515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.9624938964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.4811248779297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2042.621826171875
INFO:tools.evaluation_results_class:Current Best Return = -42.037506103515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.85787603632057
INFO:tools.evaluation_results_class:Counted Episodes = 2533
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.67644500732422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.32354736328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.8726806640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3229.611083984375
INFO:tools.evaluation_results_class:Current Best Return = -56.67644500732422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.328512396694215
INFO:tools.evaluation_results_class:Counted Episodes = 2420
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.09050369262695
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.90948486328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 184.71408081054688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3620.2861328125
INFO:tools.evaluation_results_class:Current Best Return = -59.09050369262695
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.40625
INFO:tools.evaluation_results_class:Counted Episodes = 2464
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.14136505126953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.858642578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.212890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3053.05029296875
INFO:tools.evaluation_results_class:Current Best Return = -54.14136505126953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.754841208365605
INFO:tools.evaluation_results_class:Counted Episodes = 2582
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.35322952270508
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.6467590332031
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.86843872070312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3142.45654296875
INFO:tools.evaluation_results_class:Current Best Return = -53.35322952270508
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.60915780866721
INFO:tools.evaluation_results_class:Counted Episodes = 2446
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.99380874633789
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.0061950683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.5036163330078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2901.601806640625
INFO:tools.evaluation_results_class:Current Best Return = -49.99380874633789
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.34915394139497
INFO:tools.evaluation_results_class:Counted Episodes = 2423
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.34816360473633
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.6518249511719
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.97897338867188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2860.38623046875
INFO:tools.evaluation_results_class:Current Best Return = -49.34816360473633
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.511428571428574
INFO:tools.evaluation_results_class:Counted Episodes = 2450
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.603782653808594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.3962097167969
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.1594696044922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2898.780029296875
INFO:tools.evaluation_results_class:Current Best Return = -49.603782653808594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.03658035347308
INFO:tools.evaluation_results_class:Counted Episodes = 2433
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.42774963378906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.5722351074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.27981567382812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2733.94921875
INFO:tools.evaluation_results_class:Current Best Return = -48.42774963378906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.06486042692939
INFO:tools.evaluation_results_class:Counted Episodes = 2436
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.58340072631836
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.4165954589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.3833770751953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3040.714599609375
INFO:tools.evaluation_results_class:Current Best Return = -50.58340072631836
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.88511856091578
INFO:tools.evaluation_results_class:Counted Episodes = 2446
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.30756759643555
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.69244384765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.034423828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3078.354248046875
INFO:tools.evaluation_results_class:Current Best Return = -51.30756759643555
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.69406021155411
INFO:tools.evaluation_results_class:Counted Episodes = 2458
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.81855392456055
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.18145751953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.08209228515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2983.921630859375
INFO:tools.evaluation_results_class:Current Best Return = -50.81855392456055
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.97126436781609
INFO:tools.evaluation_results_class:Counted Episodes = 2436
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.74598693847656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.2540283203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.1457061767578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2881.0341796875
INFO:tools.evaluation_results_class:Current Best Return = -50.74598693847656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.98394400988061
INFO:tools.evaluation_results_class:Counted Episodes = 2429
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.86653137207031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.13348388671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.48931884765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2922.01123046875
INFO:tools.evaluation_results_class:Current Best Return = -50.86653137207031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.53591836734694
INFO:tools.evaluation_results_class:Counted Episodes = 2450
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.862449645996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.1375427246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.4078826904297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2903.684814453125
INFO:tools.evaluation_results_class:Current Best Return = -48.862449645996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.52367346938775
INFO:tools.evaluation_results_class:Counted Episodes = 2450
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.71369552612305
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.28631591796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.50840759277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2979.423583984375
INFO:tools.evaluation_results_class:Current Best Return = -48.71369552612305
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.505775577557756
INFO:tools.evaluation_results_class:Counted Episodes = 2424
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.775997161865234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.2239990234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.1348876953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3138.162109375
INFO:tools.evaluation_results_class:Current Best Return = -52.775997161865234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.182079736950264
INFO:tools.evaluation_results_class:Counted Episodes = 2433
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.10674285888672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.89324951171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.70753479003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3377.98095703125
INFO:tools.evaluation_results_class:Current Best Return = -54.10674285888672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.42821679768308
INFO:tools.evaluation_results_class:Counted Episodes = 2417
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.96291732788086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.0370788574219
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.4926300048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3465.86083984375
INFO:tools.evaluation_results_class:Current Best Return = -54.96291732788086
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.951380304903175
INFO:tools.evaluation_results_class:Counted Episodes = 2427
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.41728210449219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.5827331542969
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.8358154296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3383.305419921875
INFO:tools.evaluation_results_class:Current Best Return = -54.41728210449219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.935708435708435
INFO:tools.evaluation_results_class:Counted Episodes = 2442
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.751033782958984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.24896240234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.33038330078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3470.796630859375
INFO:tools.evaluation_results_class:Current Best Return = -55.751033782958984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.31596360628619
INFO:tools.evaluation_results_class:Counted Episodes = 2418
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.783546447753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.2164611816406
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.3869171142578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3680.52587890625
INFO:tools.evaluation_results_class:Current Best Return = -59.783546447753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.5682429834679
INFO:tools.evaluation_results_class:Counted Episodes = 2601
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.62091827392578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.37908935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.4889678955078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3315.397216796875
INFO:tools.evaluation_results_class:Current Best Return = -55.62091827392578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.465481603968584
INFO:tools.evaluation_results_class:Counted Episodes = 2419
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.61673736572266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.3832550048828
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.96417236328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3837.9951171875
INFO:tools.evaluation_results_class:Current Best Return = -66.61673736572266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.659591509811776
INFO:tools.evaluation_results_class:Counted Episodes = 2497
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.861202239990234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.1387939453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 183.84715270996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3611.03125
INFO:tools.evaluation_results_class:Current Best Return = -58.861202239990234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.198105436573314
INFO:tools.evaluation_results_class:Counted Episodes = 2428
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.14109802246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.85890197753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.7917022705078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3896.67578125
INFO:tools.evaluation_results_class:Current Best Return = -67.14109802246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.07193958664547
INFO:tools.evaluation_results_class:Counted Episodes = 2516
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.46583557128906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.5341491699219
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.50912475585938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3601.312255859375
INFO:tools.evaluation_results_class:Current Best Return = -62.46583557128906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.60168810289389
INFO:tools.evaluation_results_class:Counted Episodes = 2488
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -65.4428482055664
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.55715942382812
INFO:tools.evaluation_results_class:Average Discounted Reward = 180.24217224121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3653.654296875
INFO:tools.evaluation_results_class:Current Best Return = -65.4428482055664
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.41566746602718
INFO:tools.evaluation_results_class:Counted Episodes = 2502
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.39984130859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.60015869140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.72177124023438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3789.967041015625
INFO:tools.evaluation_results_class:Current Best Return = -67.39984130859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.2062923138192
INFO:tools.evaluation_results_class:Counted Episodes = 2511
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.65454864501953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.345458984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.51597595214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3851.287353515625
INFO:tools.evaluation_results_class:Current Best Return = -67.65454864501953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.02845849802372
INFO:tools.evaluation_results_class:Counted Episodes = 2530
INFO:robust_rl.robust_rl_trainer:Iteration 27 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.900577545166016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.09942626953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.8409881591797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3089.989501953125
INFO:tools.evaluation_results_class:Current Best Return = -53.61601638793945
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.782790976242765
INFO:tools.evaluation_results_class:Counted Episodes = 5009
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.8920929431915283
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 1.7548991441726685
INFO:agents.father_agent:Step: 10, Training loss: 1.647450566291809
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -54.57682418823242
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.4231872558594
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.71316528320312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3349.81982421875
INFO:tools.evaluation_results_class:Current Best Return = -53.61601638793945
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.399960653157585
INFO:tools.evaluation_results_class:Counted Episodes = 5083
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.50273895263672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.49725341796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.4807586669922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4204.66845703125
INFO:tools.evaluation_results_class:Current Best Return = -64.50273895263672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.71375301601228
INFO:tools.evaluation_results_class:Counted Episodes = 4559
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.948490142822266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.051513671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.7279510498047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3437.01953125
INFO:tools.evaluation_results_class:Current Best Return = -56.948490142822266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.256167357410696
INFO:tools.evaluation_results_class:Counted Episodes = 5067
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 64.63654346265452
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 28 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.24380111694336
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.7561950683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.4563751220703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3428.146728515625
INFO:tools.evaluation_results_class:Current Best Return = -53.61601638793945
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.23396300669028
INFO:tools.evaluation_results_class:Counted Episodes = 5082
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.372886300086975
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 1.2720228433609009
INFO:agents.father_agent:Step: 10, Training loss: 1.119565725326538
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -53.61334228515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.38665771484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.500244140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3143.83837890625
INFO:tools.evaluation_results_class:Current Best Return = -53.61334228515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.6722590714007
INFO:tools.evaluation_results_class:Counted Episodes = 5126
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.66553497314453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.33447265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.41897583007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4242.02685546875
INFO:tools.evaluation_results_class:Current Best Return = -62.66553497314453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.521407376006785
INFO:tools.evaluation_results_class:Counted Episodes = 4718
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.79210662841797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.2078857421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 188.44076538085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3255.425048828125
INFO:tools.evaluation_results_class:Current Best Return = -55.79210662841797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.647518561938256
INFO:tools.evaluation_results_class:Counted Episodes = 5118
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 66.93566192281138
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 29 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.26835250854492
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.7316589355469
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.53973388671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3183.988037109375
INFO:tools.evaluation_results_class:Current Best Return = -53.61334228515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.03071870170015
INFO:tools.evaluation_results_class:Counted Episodes = 5176
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.105103850364685
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 0.9280530214309692
INFO:agents.father_agent:Step: 10, Training loss: 0.874695360660553
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -53.62186813354492
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.3781433105469
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.84585571289062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3219.372802734375
INFO:tools.evaluation_results_class:Current Best Return = -53.61334228515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.526915887850464
INFO:tools.evaluation_results_class:Counted Episodes = 5350
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.22270202636719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.77728271484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.78269958496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4171.39306640625
INFO:tools.evaluation_results_class:Current Best Return = -63.22270202636719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.85393489905233
INFO:tools.evaluation_results_class:Counted Episodes = 4854
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.646121978759766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.3538818359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 187.71066284179688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3596.27001953125
INFO:tools.evaluation_results_class:Current Best Return = -58.646121978759766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.0631519059359
INFO:tools.evaluation_results_class:Counted Episodes = 5273
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 69.21896910261222
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 30 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.17824935913086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.8217468261719
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.9890899658203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3264.46142578125
INFO:tools.evaluation_results_class:Current Best Return = -53.61334228515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.90558912386707
INFO:tools.evaluation_results_class:Counted Episodes = 5296
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.8278802037239075
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 0.878736138343811
INFO:agents.father_agent:Step: 10, Training loss: 0.7244450449943542
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -55.997039794921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.0029602050781
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.61932373046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3364.579833984375
INFO:tools.evaluation_results_class:Current Best Return = -53.61334228515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.78476331360947
INFO:tools.evaluation_results_class:Counted Episodes = 5408
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.47580337524414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.5242004394531
INFO:tools.evaluation_results_class:Average Discounted Reward = 182.11984252929688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4053.882568359375
INFO:tools.evaluation_results_class:Current Best Return = -62.47580337524414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.278293501876355
INFO:tools.evaluation_results_class:Counted Episodes = 5063
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.183319091796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.8166809082031
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.1243438720703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3512.447265625
INFO:tools.evaluation_results_class:Current Best Return = -58.183319091796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.317873510540785
INFO:tools.evaluation_results_class:Counted Episodes = 5455
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 70.96777388756887
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 31 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.45091247558594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.5491027832031
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.3579864501953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3287.921630859375
INFO:tools.evaluation_results_class:Current Best Return = -53.61334228515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.575059863694975
INFO:tools.evaluation_results_class:Counted Episodes = 5429
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.7297231554985046
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 0.6883842349052429
INFO:agents.father_agent:Step: 10, Training loss: 0.6734657287597656
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -52.8232536315918
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.1767578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.99806213378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3146.912353515625
INFO:tools.evaluation_results_class:Current Best Return = -52.8232536315918
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.49575735692363
INFO:tools.evaluation_results_class:Counted Episodes = 5539
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.207542419433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.7924499511719
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.50743103027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3717.39990234375
INFO:tools.evaluation_results_class:Current Best Return = -60.207542419433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.15181956027293
INFO:tools.evaluation_results_class:Counted Episodes = 5276
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.342350006103516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.65765380859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 188.36746215820312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3552.87451171875
INFO:tools.evaluation_results_class:Current Best Return = -60.342350006103516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.17675013708646
INFO:tools.evaluation_results_class:Counted Episodes = 5471
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 72.38931555966776
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.049522399902344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.9504699707031
INFO:tools.evaluation_results_class:Average Discounted Reward = 191.9541473388672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3212.2373046875
INFO:tools.evaluation_results_class:Current Best Return = -53.049522399902344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.48562300319489
INFO:tools.evaluation_results_class:Counted Episodes = 2504
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -41.778465270996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.2215270996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.86318969726562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1972.4163818359375
INFO:tools.evaluation_results_class:Current Best Return = -41.778465270996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.19519752130132
INFO:tools.evaluation_results_class:Counted Episodes = 2582
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.406639099121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.5933532714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.41847229003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3677.57177734375
INFO:tools.evaluation_results_class:Current Best Return = -60.406639099121094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.48620551779288
INFO:tools.evaluation_results_class:Counted Episodes = 2501
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.75
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.25
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.038330078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3712.06103515625
INFO:tools.evaluation_results_class:Current Best Return = -60.75
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.2828125
INFO:tools.evaluation_results_class:Counted Episodes = 2560
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.14204025268555
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.85797119140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.52511596679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3165.236572265625
INFO:tools.evaluation_results_class:Current Best Return = -49.14204025268555
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.077303884234574
INFO:tools.evaluation_results_class:Counted Episodes = 2626
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.54538345336914
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.4546203613281
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.92518615722656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3248.9853515625
INFO:tools.evaluation_results_class:Current Best Return = -54.54538345336914
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.27468152866242
INFO:tools.evaluation_results_class:Counted Episodes = 2512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -44.91413879394531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.08587646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.46124267578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2584.674560546875
INFO:tools.evaluation_results_class:Current Best Return = -44.91413879394531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.280750798722046
INFO:tools.evaluation_results_class:Counted Episodes = 2504
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -44.418399810791016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.58160400390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.83238220214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2607.36279296875
INFO:tools.evaluation_results_class:Current Best Return = -44.418399810791016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.4728
INFO:tools.evaluation_results_class:Counted Episodes = 2500
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -44.287540435791016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.71246337890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.81512451171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2442.763916015625
INFO:tools.evaluation_results_class:Current Best Return = -44.287540435791016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.43769968051118
INFO:tools.evaluation_results_class:Counted Episodes = 2504
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -44.789119720458984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.21087646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.8578643798828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2682.347412109375
INFO:tools.evaluation_results_class:Current Best Return = -44.789119720458984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.08697378872121
INFO:tools.evaluation_results_class:Counted Episodes = 2518
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -44.9425048828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.0574951171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.63037109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2682.46435546875
INFO:tools.evaluation_results_class:Current Best Return = -44.9425048828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.07176843774782
INFO:tools.evaluation_results_class:Counted Episodes = 2522
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -45.632083892822266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.367919921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.5363006591797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2687.3642578125
INFO:tools.evaluation_results_class:Current Best Return = -45.632083892822266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.579409417398246
INFO:tools.evaluation_results_class:Counted Episodes = 2506
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -45.48291015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.51708984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.04881286621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2683.45068359375
INFO:tools.evaluation_results_class:Current Best Return = -45.48291015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.200715421303656
INFO:tools.evaluation_results_class:Counted Episodes = 2516
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -44.86311340332031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.13690185546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.51763916015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2519.955078125
INFO:tools.evaluation_results_class:Current Best Return = -44.86311340332031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.278551532033426
INFO:tools.evaluation_results_class:Counted Episodes = 2513
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -44.894989013671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.1050109863281
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.44117736816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2495.3857421875
INFO:tools.evaluation_results_class:Current Best Return = -44.894989013671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.28480509148767
INFO:tools.evaluation_results_class:Counted Episodes = 2514
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.9429931640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.0570068359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.03347778320312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2461.5126953125
INFO:tools.evaluation_results_class:Current Best Return = -43.9429931640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.58651144118828
INFO:tools.evaluation_results_class:Counted Episodes = 2491
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -42.70621109008789
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.2937927246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.3244171142578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2281.2626953125
INFO:tools.evaluation_results_class:Current Best Return = -42.70621109008789
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.34633757961783
INFO:tools.evaluation_results_class:Counted Episodes = 2512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -45.59407424926758
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.4059143066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.71006774902344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2672.3759765625
INFO:tools.evaluation_results_class:Current Best Return = -45.59407424926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.45796637309848
INFO:tools.evaluation_results_class:Counted Episodes = 2498
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.269981384277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.7300109863281
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.82884216308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2779.940185546875
INFO:tools.evaluation_results_class:Current Best Return = -48.269981384277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.06202783300199
INFO:tools.evaluation_results_class:Counted Episodes = 2515
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.54083251953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.45916748046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.13571166992188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3017.086669921875
INFO:tools.evaluation_results_class:Current Best Return = -50.54083251953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.526421136909526
INFO:tools.evaluation_results_class:Counted Episodes = 2498
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.13855743408203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.8614501953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.11083984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2778.00390625
INFO:tools.evaluation_results_class:Current Best Return = -48.13855743408203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.80364212193191
INFO:tools.evaluation_results_class:Counted Episodes = 2526
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.80152130126953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.198486328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 192.1501922607422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3137.718994140625
INFO:tools.evaluation_results_class:Current Best Return = -51.80152130126953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.38375350140056
INFO:tools.evaluation_results_class:Counted Episodes = 2499
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.188514709472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.8114929199219
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.8077850341797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3889.06005859375
INFO:tools.evaluation_results_class:Current Best Return = -62.188514709472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.47261050245561
INFO:tools.evaluation_results_class:Counted Episodes = 2647
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.42469024658203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.5753173828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 189.59918212890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3263.124755859375
INFO:tools.evaluation_results_class:Current Best Return = -55.42469024658203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.26008789452657
INFO:tools.evaluation_results_class:Counted Episodes = 2503
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.66500854492188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.33499145507812
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.63525390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4308.0947265625
INFO:tools.evaluation_results_class:Current Best Return = -71.66500854492188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.219633307868605
INFO:tools.evaluation_results_class:Counted Episodes = 2618
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.52199935913086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.4779968261719
INFO:tools.evaluation_results_class:Average Discounted Reward = 190.97242736816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3316.55908203125
INFO:tools.evaluation_results_class:Current Best Return = -53.52199935913086
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.5436
INFO:tools.evaluation_results_class:Counted Episodes = 2500
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.37277221679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.62722778320312
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.5040283203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4458.482421875
INFO:tools.evaluation_results_class:Current Best Return = -68.37277221679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.92823894491854
INFO:tools.evaluation_results_class:Counted Episodes = 2578
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.92357635498047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.076416015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.52798461914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4438.822265625
INFO:tools.evaluation_results_class:Current Best Return = -71.92357635498047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.607910906298
INFO:tools.evaluation_results_class:Counted Episodes = 2604
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.25411987304688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.74588012695312
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.3892364501953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4193.56494140625
INFO:tools.evaluation_results_class:Current Best Return = -69.25411987304688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.60674587964738
INFO:tools.evaluation_results_class:Counted Episodes = 2609
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.90323638916016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.0967559814453
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.808837890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4080.695556640625
INFO:tools.evaluation_results_class:Current Best Return = -69.90323638916016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.6449498843485
INFO:tools.evaluation_results_class:Counted Episodes = 2594
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.01197052001953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.988037109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 179.4705352783203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4169.2744140625
INFO:tools.evaluation_results_class:Current Best Return = -69.01197052001953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.63204633204633
INFO:tools.evaluation_results_class:Counted Episodes = 2590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.9865493774414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.01345825195312
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.10467529296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4525.95556640625
INFO:tools.evaluation_results_class:Current Best Return = -72.9865493774414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.44696387394312
INFO:tools.evaluation_results_class:Counted Episodes = 2602
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.90219116210938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.09780883789062
INFO:tools.evaluation_results_class:Average Discounted Reward = 177.90707397460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4175.87353515625
INFO:tools.evaluation_results_class:Current Best Return = -70.90219116210938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.467462456680785
INFO:tools.evaluation_results_class:Counted Episodes = 2597
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.02120971679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.97879028320312
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.0320587158203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4520.17431640625
INFO:tools.evaluation_results_class:Current Best Return = -73.02120971679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.684535287311995
INFO:tools.evaluation_results_class:Counted Episodes = 2593
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.63056945800781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.3694305419922
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.27857971191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4152.2646484375
INFO:tools.evaluation_results_class:Current Best Return = -70.63056945800781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.53072196620584
INFO:tools.evaluation_results_class:Counted Episodes = 2604
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.17774963378906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.82225036621094
INFO:tools.evaluation_results_class:Average Discounted Reward = 178.00747680664062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4353.80517578125
INFO:tools.evaluation_results_class:Current Best Return = -71.17774963378906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.2427370030581
INFO:tools.evaluation_results_class:Counted Episodes = 2616
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.461334228515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.5386657714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.0948944091797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2759.5673828125
INFO:tools.evaluation_results_class:Current Best Return = -49.461334228515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.623915503583554
INFO:tools.evaluation_results_class:Counted Episodes = 2651
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -40.249725341796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.7502746582031
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.62518310546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1838.128662109375
INFO:tools.evaluation_results_class:Current Best Return = -40.249725341796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.08874220755409
INFO:tools.evaluation_results_class:Counted Episodes = 2727
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.87632751464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.1236572265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.2509002685547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2725.0751953125
INFO:tools.evaluation_results_class:Current Best Return = -50.87632751464844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.93247344461305
INFO:tools.evaluation_results_class:Counted Episodes = 2636
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.9537467956543
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.0462646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.42596435546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3000.288818359375
INFO:tools.evaluation_results_class:Current Best Return = -53.9537467956543
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.021633718761656
INFO:tools.evaluation_results_class:Counted Episodes = 2681
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.73486328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.26513671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.89511108398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3366.606201171875
INFO:tools.evaluation_results_class:Current Best Return = -52.73486328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.74166965245432
INFO:tools.evaluation_results_class:Counted Episodes = 2791
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -47.06285858154297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.9371337890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.7133331298828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2602.800048828125
INFO:tools.evaluation_results_class:Current Best Return = -47.06285858154297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.205333333333336
INFO:tools.evaluation_results_class:Counted Episodes = 2625
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
2025-08-19 22:49:05.054445: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-19 22:49:05.056352: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-19 22:49:05.086965: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-19 22:49:05.087008: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-19 22:49:05.088268: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-19 22:49:05.093916: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-19 22:49:05.094099: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-19 22:49:05.629951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/obstacles-10-2/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/obstacles-10-2/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"penalty"}min=? [F ((x = 10) & (y = 10))] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 326 states and 829 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"rewardmodel_penalty"}min=? [F ((x = 10) & (y = 10))] 
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -548.3352661132812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -457.6072692871094
INFO:tools.evaluation_results_class:Average Discounted Reward = -173.60073852539062
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.2835249042145594
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 104797.09375
INFO:tools.evaluation_results_class:Current Best Return = -548.3352661132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.2835249042145594
INFO:tools.evaluation_results_class:Average Episode Length = 526.6417624521073
INFO:tools.evaluation_results_class:Counted Episodes = 522
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1376.1494140625
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 4.322603225708008
INFO:agents.father_agent:Step: 10, Training loss: 4.978445529937744
INFO:agents.father_agent:Step: 15, Training loss: 5.910616874694824
INFO:agents.father_agent:Step: 20, Training loss: 4.3473896980285645
INFO:agents.father_agent:Step: 25, Training loss: 5.974255084991455
INFO:agents.father_agent:Step: 30, Training loss: 4.111813068389893
INFO:agents.father_agent:Step: 35, Training loss: 5.476189613342285
INFO:agents.father_agent:Step: 40, Training loss: 4.02607536315918
INFO:agents.father_agent:Step: 45, Training loss: 6.006194114685059
INFO:agents.father_agent:Step: 50, Training loss: 4.365185260772705
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 55, Training loss: 2.965010404586792
INFO:agents.father_agent:Step: 60, Training loss: 4.12744665145874
INFO:agents.father_agent:Step: 65, Training loss: 3.3030638694763184
INFO:agents.father_agent:Step: 70, Training loss: 3.4500503540039062
INFO:agents.father_agent:Step: 75, Training loss: 3.8210344314575195
INFO:agents.father_agent:Step: 80, Training loss: 4.893309593200684
INFO:agents.father_agent:Step: 85, Training loss: 3.000016450881958
INFO:agents.father_agent:Step: 90, Training loss: 4.288357734680176
INFO:agents.father_agent:Step: 95, Training loss: 2.079240560531616
INFO:agents.father_agent:Step: 100, Training loss: 2.314993381500244
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -192.7152557373047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 19.894914627075195
INFO:tools.evaluation_results_class:Average Discounted Reward = -32.634891510009766
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6644067796610169
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13150.7763671875
INFO:tools.evaluation_results_class:Current Best Return = -192.7152557373047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6644067796610169
INFO:tools.evaluation_results_class:Average Episode Length = 406.10169491525426
INFO:tools.evaluation_results_class:Counted Episodes = 590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -181.04214477539062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.78606033325195
INFO:tools.evaluation_results_class:Average Discounted Reward = -23.659378051757812
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6807131280388979
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10534.90234375
INFO:tools.evaluation_results_class:Current Best Return = -181.04214477539062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6807131280388979
INFO:tools.evaluation_results_class:Average Episode Length = 394.60940032414914
INFO:tools.evaluation_results_class:Counted Episodes = 617
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -254.85757446289062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -25.51607894897461
INFO:tools.evaluation_results_class:Average Discounted Reward = -56.2287712097168
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7166921898928025
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30609.57421875
INFO:tools.evaluation_results_class:Current Best Return = -254.85757446289062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7166921898928025
INFO:tools.evaluation_results_class:Average Episode Length = 393.4563552833078
INFO:tools.evaluation_results_class:Counted Episodes = 653
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -177.88760375976562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 60.0374641418457
INFO:tools.evaluation_results_class:Average Discounted Reward = -15.46387004852295
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7435158501440923
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13271.8583984375
INFO:tools.evaluation_results_class:Current Best Return = -177.88760375976562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7435158501440923
INFO:tools.evaluation_results_class:Average Episode Length = 367.2449567723343
INFO:tools.evaluation_results_class:Counted Episodes = 694
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=8, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 312.74726900373213
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=8, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -253.15699768066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -25.989761352539062
INFO:tools.evaluation_results_class:Average Discounted Reward = -53.16012191772461
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7098976109215017
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27122.228515625
INFO:tools.evaluation_results_class:Current Best Return = -253.15699768066406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7098976109215017
INFO:tools.evaluation_results_class:Average Episode Length = 393.67235494880543
INFO:tools.evaluation_results_class:Counted Episodes = 293
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -253.9630889892578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -27.385906219482422
INFO:tools.evaluation_results_class:Average Discounted Reward = -59.59166717529297
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7080536912751678
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20898.943359375
INFO:tools.evaluation_results_class:Current Best Return = -253.9630889892578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7080536912751678
INFO:tools.evaluation_results_class:Average Episode Length = 399.2046979865772
INFO:tools.evaluation_results_class:Counted Episodes = 298
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -260.4884338378906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -46.0990104675293
INFO:tools.evaluation_results_class:Average Discounted Reward = -54.72343063354492
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.66996699669967
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27533.142578125
INFO:tools.evaluation_results_class:Current Best Return = -260.4884338378906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.66996699669967
INFO:tools.evaluation_results_class:Average Episode Length = 399.0924092409241
INFO:tools.evaluation_results_class:Counted Episodes = 303
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -219.36241149902344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 8.288590431213379
INFO:tools.evaluation_results_class:Average Discounted Reward = -41.27892303466797
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7114093959731543
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18121.453125
INFO:tools.evaluation_results_class:Current Best Return = -219.36241149902344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7114093959731543
INFO:tools.evaluation_results_class:Average Episode Length = 391.03355704697987
INFO:tools.evaluation_results_class:Counted Episodes = 298
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -233.5933380126953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -10.65999984741211
INFO:tools.evaluation_results_class:Average Discounted Reward = -46.200252532958984
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6966666666666667
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26501.583984375
INFO:tools.evaluation_results_class:Current Best Return = -233.5933380126953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6966666666666667
INFO:tools.evaluation_results_class:Average Episode Length = 395.53333333333336
INFO:tools.evaluation_results_class:Counted Episodes = 300
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -309.76751708984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -76.39171600341797
INFO:tools.evaluation_results_class:Average Discounted Reward = -85.11856079101562
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7292993630573248
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38894.96875
INFO:tools.evaluation_results_class:Current Best Return = -309.76751708984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7292993630573248
INFO:tools.evaluation_results_class:Average Episode Length = 389.1751592356688
INFO:tools.evaluation_results_class:Counted Episodes = 314
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.68670654296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.28797149658203
INFO:tools.evaluation_results_class:Average Discounted Reward = -8.803008079528809
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7436708860759493
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9223.28515625
INFO:tools.evaluation_results_class:Current Best Return = -170.68670654296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7436708860759493
INFO:tools.evaluation_results_class:Average Episode Length = 368.35759493670884
INFO:tools.evaluation_results_class:Counted Episodes = 316
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.31707763671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.65853881835938
INFO:tools.evaluation_results_class:Average Discounted Reward = -4.333075523376465
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7530487804878049
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10429.923828125
INFO:tools.evaluation_results_class:Current Best Return = -169.31707763671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7530487804878049
INFO:tools.evaluation_results_class:Average Episode Length = 353.9207317073171
INFO:tools.evaluation_results_class:Counted Episodes = 328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -174.3784637451172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.036922454833984
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.7317795753479
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7169230769230769
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10447.5634765625
INFO:tools.evaluation_results_class:Current Best Return = -174.3784637451172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7169230769230769
INFO:tools.evaluation_results_class:Average Episode Length = 363.1446153846154
INFO:tools.evaluation_results_class:Counted Episodes = 325
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.11962890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 75.51840209960938
INFO:tools.evaluation_results_class:Average Discounted Reward = -1.0016573667526245
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7269938650306749
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9332.712890625
INFO:tools.evaluation_results_class:Current Best Return = -157.11962890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7269938650306749
INFO:tools.evaluation_results_class:Average Episode Length = 366.1319018404908
INFO:tools.evaluation_results_class:Counted Episodes = 326
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.68338012695312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 93.09717559814453
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.7577076554298401
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.780564263322884
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9834.6611328125
INFO:tools.evaluation_results_class:Current Best Return = -156.68338012695312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.780564263322884
INFO:tools.evaluation_results_class:Average Episode Length = 347.36990595611286
INFO:tools.evaluation_results_class:Counted Episodes = 319
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -231.24436950683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -3.848874568939209
INFO:tools.evaluation_results_class:Average Discounted Reward = -51.02239990234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7106109324758842
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23394.677734375
INFO:tools.evaluation_results_class:Current Best Return = -231.24436950683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7106109324758842
INFO:tools.evaluation_results_class:Average Episode Length = 386.7009646302251
INFO:tools.evaluation_results_class:Counted Episodes = 311
INFO:robust_rl.robust_rl_trainer:Iteration 2 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -207.52951049804688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -3.5497469902038574
INFO:tools.evaluation_results_class:Average Discounted Reward = -41.53270721435547
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6374367622259697
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14539.8076171875
INFO:tools.evaluation_results_class:Current Best Return = -181.04214477539062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6807131280388979
INFO:tools.evaluation_results_class:Average Episode Length = 409.47723440134905
INFO:tools.evaluation_results_class:Counted Episodes = 593
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.312345027923584
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 3.03954815864563
INFO:agents.father_agent:Step: 10, Training loss: 2.712308406829834
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -182.36363220214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.49586868286133
INFO:tools.evaluation_results_class:Average Discounted Reward = -24.59889030456543
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6776859504132231
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10691.1962890625
INFO:tools.evaluation_results_class:Current Best Return = -181.04214477539062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6807131280388979
INFO:tools.evaluation_results_class:Average Episode Length = 396.63305785123964
INFO:tools.evaluation_results_class:Counted Episodes = 605
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -248.38267517089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -23.122835159301758
INFO:tools.evaluation_results_class:Average Discounted Reward = -58.22325134277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7039370078740157
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28055.048828125
INFO:tools.evaluation_results_class:Current Best Return = -248.38267517089844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7039370078740157
INFO:tools.evaluation_results_class:Average Episode Length = 396.14803149606297
INFO:tools.evaluation_results_class:Counted Episodes = 635
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -182.69276428222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.936485290527344
INFO:tools.evaluation_results_class:Average Discounted Reward = -21.92487907409668
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6957163958641064
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11869.1640625
INFO:tools.evaluation_results_class:Current Best Return = -182.69276428222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6957163958641064
INFO:tools.evaluation_results_class:Average Episode Length = 388.66765140324964
INFO:tools.evaluation_results_class:Counted Episodes = 677
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=8, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 308.7303900615335
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=8, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 3 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -195.73658752441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 21.759349822998047
INFO:tools.evaluation_results_class:Average Discounted Reward = -32.951568603515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6796747967479675
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15867.2060546875
INFO:tools.evaluation_results_class:Current Best Return = -181.04214477539062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6807131280388979
INFO:tools.evaluation_results_class:Average Episode Length = 396.74959349593496
INFO:tools.evaluation_results_class:Counted Episodes = 615
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.828176259994507
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 3.3501598834991455
INFO:agents.father_agent:Step: 10, Training loss: 5.096827030181885
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -183.65554809570312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.836509704589844
INFO:tools.evaluation_results_class:Average Discounted Reward = -21.82357406616211
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6984126984126984
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14951.73046875
INFO:tools.evaluation_results_class:Current Best Return = -181.04214477539062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6984126984126984
INFO:tools.evaluation_results_class:Average Episode Length = 377.37619047619046
INFO:tools.evaluation_results_class:Counted Episodes = 630
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -219.2371368408203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.39142990112305
INFO:tools.evaluation_results_class:Average Discounted Reward = -37.259944915771484
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7957142857142857
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23743.751953125
INFO:tools.evaluation_results_class:Current Best Return = -219.2371368408203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7957142857142857
INFO:tools.evaluation_results_class:Average Episode Length = 362.7542857142857
INFO:tools.evaluation_results_class:Counted Episodes = 700
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -182.96368408203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.66565704345703
INFO:tools.evaluation_results_class:Average Discounted Reward = -22.367950439453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7019667170953101
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12797.8681640625
INFO:tools.evaluation_results_class:Current Best Return = -182.96368408203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7019667170953101
INFO:tools.evaluation_results_class:Average Episode Length = 390.1573373676248
INFO:tools.evaluation_results_class:Counted Episodes = 661
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=8, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 282.138199183876
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=8, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 4 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -194.81117248535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.318554878234863
INFO:tools.evaluation_results_class:Average Discounted Reward = -31.438491821289062
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6535303776683087
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14575.927734375
INFO:tools.evaluation_results_class:Current Best Return = -181.04214477539062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6984126984126984
INFO:tools.evaluation_results_class:Average Episode Length = 397.57142857142856
INFO:tools.evaluation_results_class:Counted Episodes = 609
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.588627815246582
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 2.604450225830078
INFO:agents.father_agent:Step: 10, Training loss: 4.296224117279053
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -180.9884796142578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.064144134521484
INFO:tools.evaluation_results_class:Average Discounted Reward = -23.19197654724121
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6907894736842105
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11749.5078125
INFO:tools.evaluation_results_class:Current Best Return = -180.9884796142578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6984126984126984
INFO:tools.evaluation_results_class:Average Episode Length = 384.83388157894734
INFO:tools.evaluation_results_class:Counted Episodes = 608
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -216.73350524902344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.554508209228516
INFO:tools.evaluation_results_class:Average Discounted Reward = -31.299989700317383
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.819650067294751
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26763.41796875
INFO:tools.evaluation_results_class:Current Best Return = -216.73350524902344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.819650067294751
INFO:tools.evaluation_results_class:Average Episode Length = 339.52624495289365
INFO:tools.evaluation_results_class:Counted Episodes = 743
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.35243225097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 61.37535858154297
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.68559455871582
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7335243553008596
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13058.2001953125
INFO:tools.evaluation_results_class:Current Best Return = -173.35243225097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7335243553008596
INFO:tools.evaluation_results_class:Average Episode Length = 366.21489971346705
INFO:tools.evaluation_results_class:Counted Episodes = 698
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 261.4985418903114
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 5 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -187.95396423339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.10952377319336
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.792186737060547
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7126984126984127
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16395.97265625
INFO:tools.evaluation_results_class:Current Best Return = -180.9884796142578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7126984126984127
INFO:tools.evaluation_results_class:Average Episode Length = 376.9095238095238
INFO:tools.evaluation_results_class:Counted Episodes = 630
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.783227443695068
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 4.178869247436523
INFO:agents.father_agent:Step: 10, Training loss: 3.972836971282959
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -184.2120819091797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.044891357421875
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.291419982910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7476780185758514
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17045.599609375
INFO:tools.evaluation_results_class:Current Best Return = -180.9884796142578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7476780185758514
INFO:tools.evaluation_results_class:Average Episode Length = 370.23374613003097
INFO:tools.evaluation_results_class:Counted Episodes = 646
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -204.51632690429688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.46858978271484
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.659412384033203
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8655778894472361
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23590.599609375
INFO:tools.evaluation_results_class:Current Best Return = -204.51632690429688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8655778894472361
INFO:tools.evaluation_results_class:Average Episode Length = 321.54145728643215
INFO:tools.evaluation_results_class:Counted Episodes = 796
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -171.57373046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 87.5147476196289
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.114360809326172
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8096514745308311
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13907.4423828125
INFO:tools.evaluation_results_class:Current Best Return = -171.57373046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8096514745308311
INFO:tools.evaluation_results_class:Average Episode Length = 345.26005361930294
INFO:tools.evaluation_results_class:Counted Episodes = 746
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 247.02267028024852
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 6 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.99526977539062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.705047607421875
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.004947662353516
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7334384858044164
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17672.84765625
INFO:tools.evaluation_results_class:Current Best Return = -180.9884796142578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7476780185758514
INFO:tools.evaluation_results_class:Average Episode Length = 372.5
INFO:tools.evaluation_results_class:Counted Episodes = 634
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.851435661315918
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 5.003408908843994
INFO:agents.father_agent:Step: 10, Training loss: 4.6771087646484375
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -182.33804321289062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.661949157714844
INFO:tools.evaluation_results_class:Average Discounted Reward = -21.969486236572266
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.75
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14572.861328125
INFO:tools.evaluation_results_class:Current Best Return = -180.9884796142578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.75
INFO:tools.evaluation_results_class:Average Episode Length = 365.5204402515723
INFO:tools.evaluation_results_class:Counted Episodes = 636
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -203.61529541015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.2744369506836
INFO:tools.evaluation_results_class:Average Discounted Reward = -21.278165817260742
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8621553884711779
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27408.041015625
INFO:tools.evaluation_results_class:Current Best Return = -203.61529541015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8621553884711779
INFO:tools.evaluation_results_class:Average Episode Length = 324.8383458646617
INFO:tools.evaluation_results_class:Counted Episodes = 798
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -179.366943359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.37255096435547
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.041982650756836
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7773109243697479
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18670.818359375
INFO:tools.evaluation_results_class:Current Best Return = -179.366943359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7773109243697479
INFO:tools.evaluation_results_class:Average Episode Length = 355.6750700280112
INFO:tools.evaluation_results_class:Counted Episodes = 714
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 235.99126359666198
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -147.20263671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 139.1131591796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 20.641023635864258
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8947368421052632
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9627.0673828125
INFO:tools.evaluation_results_class:Current Best Return = -147.20263671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8947368421052632
INFO:tools.evaluation_results_class:Average Episode Length = 292.5131578947368
INFO:tools.evaluation_results_class:Counted Episodes = 380
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -172.2069854736328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 111.66397857666016
INFO:tools.evaluation_results_class:Average Discounted Reward = -1.3459686040878296
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8870967741935484
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13186.9541015625
INFO:tools.evaluation_results_class:Current Best Return = -172.2069854736328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8870967741935484
INFO:tools.evaluation_results_class:Average Episode Length = 304.6747311827957
INFO:tools.evaluation_results_class:Counted Episodes = 372
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.78683471679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 129.05526733398438
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.182722091674805
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9026315789473685
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12417.1826171875
INFO:tools.evaluation_results_class:Current Best Return = -159.78683471679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9026315789473685
INFO:tools.evaluation_results_class:Average Episode Length = 290.7763157894737
INFO:tools.evaluation_results_class:Counted Episodes = 380
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.73214721679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 147.06378173828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.437652587890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9056122448979592
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11809.1904296875
INFO:tools.evaluation_results_class:Current Best Return = -142.73214721679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9056122448979592
INFO:tools.evaluation_results_class:Average Episode Length = 288.9770408163265
INFO:tools.evaluation_results_class:Counted Episodes = 392
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -146.42745971679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 138.75389099121094
INFO:tools.evaluation_results_class:Average Discounted Reward = 17.296340942382812
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8911917098445595
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11889.115234375
INFO:tools.evaluation_results_class:Current Best Return = -146.42745971679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8911917098445595
INFO:tools.evaluation_results_class:Average Episode Length = 296.3860103626943
INFO:tools.evaluation_results_class:Counted Episodes = 386
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -235.73130798339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.1523551940918
INFO:tools.evaluation_results_class:Average Discounted Reward = -40.809547424316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8808864265927978
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33569.87109375
INFO:tools.evaluation_results_class:Current Best Return = -235.73130798339844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8808864265927978
INFO:tools.evaluation_results_class:Average Episode Length = 309.32409972299166
INFO:tools.evaluation_results_class:Counted Episodes = 361
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -230.70855712890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.92245864868164
INFO:tools.evaluation_results_class:Average Discounted Reward = -30.962553024291992
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8957219251336899
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32951.1328125
INFO:tools.evaluation_results_class:Current Best Return = -230.70855712890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8957219251336899
INFO:tools.evaluation_results_class:Average Episode Length = 299.20588235294116
INFO:tools.evaluation_results_class:Counted Episodes = 374
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -227.6411590576172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.65171432495117
INFO:tools.evaluation_results_class:Average Discounted Reward = -35.202144622802734
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9102902374670184
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27640.52734375
INFO:tools.evaluation_results_class:Current Best Return = -227.6411590576172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9102902374670184
INFO:tools.evaluation_results_class:Average Episode Length = 303.8548812664908
INFO:tools.evaluation_results_class:Counted Episodes = 379
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -236.310546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 49.9572639465332
INFO:tools.evaluation_results_class:Average Discounted Reward = -37.75086212158203
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8945868945868946
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35312.87890625
INFO:tools.evaluation_results_class:Current Best Return = -236.310546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8945868945868946
INFO:tools.evaluation_results_class:Average Episode Length = 314.25071225071224
INFO:tools.evaluation_results_class:Counted Episodes = 351
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -231.37158203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.912567138671875
INFO:tools.evaluation_results_class:Average Discounted Reward = -41.995574951171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8633879781420765
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28852.251953125
INFO:tools.evaluation_results_class:Current Best Return = -231.37158203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8633879781420765
INFO:tools.evaluation_results_class:Average Episode Length = 320.55737704918033
INFO:tools.evaluation_results_class:Counted Episodes = 366
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -220.0955047607422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 53.16292190551758
INFO:tools.evaluation_results_class:Average Discounted Reward = -32.14368438720703
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8539325842696629
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25046.955078125
INFO:tools.evaluation_results_class:Current Best Return = -220.0955047607422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8539325842696629
INFO:tools.evaluation_results_class:Average Episode Length = 320.938202247191
INFO:tools.evaluation_results_class:Counted Episodes = 356
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -126.28169250488281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 132.4225311279297
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.04547119140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8084507042253521
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5433.59912109375
INFO:tools.evaluation_results_class:Current Best Return = -126.28169250488281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8084507042253521
INFO:tools.evaluation_results_class:Average Episode Length = 323.4225352112676
INFO:tools.evaluation_results_class:Counted Episodes = 355
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.4773406982422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 94.94863891601562
INFO:tools.evaluation_results_class:Average Discounted Reward = -0.7521692514419556
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7794561933534743
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9010.4189453125
INFO:tools.evaluation_results_class:Current Best Return = -154.4773406982422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7794561933534743
INFO:tools.evaluation_results_class:Average Episode Length = 353.33836858006043
INFO:tools.evaluation_results_class:Counted Episodes = 331
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -132.51296997070312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 129.38905334472656
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.11506462097168
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8184438040345822
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5693.40234375
INFO:tools.evaluation_results_class:Current Best Return = -132.51296997070312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8184438040345822
INFO:tools.evaluation_results_class:Average Episode Length = 327.86455331412105
INFO:tools.evaluation_results_class:Counted Episodes = 347
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -126.55113983154297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 136.17613220214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.353124618530273
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8210227272727273
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5367.712890625
INFO:tools.evaluation_results_class:Current Best Return = -126.55113983154297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8210227272727273
INFO:tools.evaluation_results_class:Average Episode Length = 328.0
INFO:tools.evaluation_results_class:Counted Episodes = 352
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -131.0728302001953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 139.62745666503906
INFO:tools.evaluation_results_class:Average Discounted Reward = 20.9099178314209
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.84593837535014
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5467.73681640625
INFO:tools.evaluation_results_class:Current Best Return = -131.0728302001953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.84593837535014
INFO:tools.evaluation_results_class:Average Episode Length = 321.2212885154062
INFO:tools.evaluation_results_class:Counted Episodes = 357
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -185.35714721679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.88095092773438
INFO:tools.evaluation_results_class:Average Discounted Reward = -17.931669235229492
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7976190476190477
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15340.6220703125
INFO:tools.evaluation_results_class:Current Best Return = -185.35714721679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7976190476190477
INFO:tools.evaluation_results_class:Average Episode Length = 342.2142857142857
INFO:tools.evaluation_results_class:Counted Episodes = 336
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -194.6727294921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.38787841796875
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.082637786865234
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.793939393939394
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20681.111328125
INFO:tools.evaluation_results_class:Current Best Return = -194.6727294921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.793939393939394
INFO:tools.evaluation_results_class:Average Episode Length = 356.2909090909091
INFO:tools.evaluation_results_class:Counted Episodes = 330
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.30210876464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 73.69184112548828
INFO:tools.evaluation_results_class:Average Discounted Reward = -20.348217010498047
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8187311178247734
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16354.6630859375
INFO:tools.evaluation_results_class:Current Best Return = -188.30210876464844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8187311178247734
INFO:tools.evaluation_results_class:Average Episode Length = 340.5347432024169
INFO:tools.evaluation_results_class:Counted Episodes = 331
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.5294189453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 76.88235473632812
INFO:tools.evaluation_results_class:Average Discounted Reward = -16.035770416259766
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8294117647058824
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16706.50390625
INFO:tools.evaluation_results_class:Current Best Return = -188.5294189453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8294117647058824
INFO:tools.evaluation_results_class:Average Episode Length = 331.2352941176471
INFO:tools.evaluation_results_class:Counted Episodes = 340
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -184.7275390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 84.25797271728516
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.642843246459961
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8405797101449275
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16466.1640625
INFO:tools.evaluation_results_class:Current Best Return = -184.7275390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8405797101449275
INFO:tools.evaluation_results_class:Average Episode Length = 327.5420289855073
INFO:tools.evaluation_results_class:Counted Episodes = 345
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -194.412841796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 62.95718765258789
INFO:tools.evaluation_results_class:Average Discounted Reward = -23.584590911865234
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8042813455657493
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18807.470703125
INFO:tools.evaluation_results_class:Current Best Return = -194.412841796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8042813455657493
INFO:tools.evaluation_results_class:Average Episode Length = 346.06727828746176
INFO:tools.evaluation_results_class:Counted Episodes = 327
INFO:robust_rl.robust_rl_trainer:Iteration 7 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -184.72463989257812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.434783935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = -20.9277400970459
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7536231884057971
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17726.7890625
INFO:tools.evaluation_results_class:Current Best Return = -180.9884796142578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7536231884057971
INFO:tools.evaluation_results_class:Average Episode Length = 371.19967793880835
INFO:tools.evaluation_results_class:Counted Episodes = 621
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.4736714363098145
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 4.0156965255737305
INFO:agents.father_agent:Step: 10, Training loss: 5.144649028778076
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -167.50149536132812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 89.93134307861328
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.556822776794434
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8044776119402985
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12360.033203125
INFO:tools.evaluation_results_class:Current Best Return = -167.50149536132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8044776119402985
INFO:tools.evaluation_results_class:Average Episode Length = 344.41492537313434
INFO:tools.evaluation_results_class:Counted Episodes = 670
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -204.05555725097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 83.48067474365234
INFO:tools.evaluation_results_class:Average Discounted Reward = -20.770687103271484
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8985507246376812
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27893.0234375
INFO:tools.evaluation_results_class:Current Best Return = -204.05555725097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8985507246376812
INFO:tools.evaluation_results_class:Average Episode Length = 307.07487922705315
INFO:tools.evaluation_results_class:Counted Episodes = 828
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.32009887695312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 90.61111450195312
INFO:tools.evaluation_results_class:Average Discounted Reward = -2.076658248901367
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8029100529100529
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14470.1689453125
INFO:tools.evaluation_results_class:Current Best Return = -166.32009887695312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8029100529100529
INFO:tools.evaluation_results_class:Average Episode Length = 336.5079365079365
INFO:tools.evaluation_results_class:Counted Episodes = 756
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 231.10780758221415
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 8 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.1426239013672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.19492721557617
INFO:tools.evaluation_results_class:Average Discounted Reward = -16.139013290405273
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7416798732171157
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15308.9462890625
INFO:tools.evaluation_results_class:Current Best Return = -167.50149536132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8044776119402985
INFO:tools.evaluation_results_class:Average Episode Length = 369.107765451664
INFO:tools.evaluation_results_class:Counted Episodes = 631
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.0551438331604
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 4.151469707489014
INFO:agents.father_agent:Step: 10, Training loss: 4.2161736488342285
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -170.94578552246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 82.0662612915039
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.21919584274292
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7906626506024096
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16015.1416015625
INFO:tools.evaluation_results_class:Current Best Return = -167.50149536132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8044776119402985
INFO:tools.evaluation_results_class:Average Episode Length = 352.44879518072287
INFO:tools.evaluation_results_class:Counted Episodes = 664
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -206.39431762695312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.77503204345703
INFO:tools.evaluation_results_class:Average Discounted Reward = -22.76192855834961
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8974042027194067
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24904.640625
INFO:tools.evaluation_results_class:Current Best Return = -206.39431762695312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8974042027194067
INFO:tools.evaluation_results_class:Average Episode Length = 313.25092707045735
INFO:tools.evaluation_results_class:Counted Episodes = 809
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.44985961914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 88.60028076171875
INFO:tools.evaluation_results_class:Average Discounted Reward = -8.279608726501465
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8064066852367688
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12453.6865234375
INFO:tools.evaluation_results_class:Current Best Return = -169.44985961914062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064066852367688
INFO:tools.evaluation_results_class:Average Episode Length = 355.2910863509749
INFO:tools.evaluation_results_class:Counted Episodes = 718
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 215.78587909022428
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 9 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -174.8861083984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 77.71919250488281
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.042167663574219
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7893915756630265
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15453.5869140625
INFO:tools.evaluation_results_class:Current Best Return = -167.50149536132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8044776119402985
INFO:tools.evaluation_results_class:Average Episode Length = 355.73010920436815
INFO:tools.evaluation_results_class:Counted Episodes = 641
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.5522031784057617
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 3.886356830596924
INFO:agents.father_agent:Step: 10, Training loss: 4.129334449768066
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -173.97784423828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.18670654296875
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.313982009887695
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7911392405063291
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13604.8671875
INFO:tools.evaluation_results_class:Current Best Return = -167.50149536132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8044776119402985
INFO:tools.evaluation_results_class:Average Episode Length = 362.94620253164555
INFO:tools.evaluation_results_class:Counted Episodes = 632
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -211.8331298828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.51183319091797
INFO:tools.evaluation_results_class:Average Discounted Reward = -23.90208625793457
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8792029887920298
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29473.24609375
INFO:tools.evaluation_results_class:Current Best Return = -211.8331298828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8792029887920298
INFO:tools.evaluation_results_class:Average Episode Length = 320.22540473225405
INFO:tools.evaluation_results_class:Counted Episodes = 803
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.87600708007812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 86.15902709960938
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.196784019470215
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8032345013477089
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14663.64453125
INFO:tools.evaluation_results_class:Current Best Return = -170.87600708007812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8032345013477089
INFO:tools.evaluation_results_class:Average Episode Length = 354.80323450134773
INFO:tools.evaluation_results_class:Counted Episodes = 742
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 218.07369026222307
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 10 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -181.17198181152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.41401290893555
INFO:tools.evaluation_results_class:Average Discounted Reward = -17.046920776367188
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7643312101910829
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15615.1240234375
INFO:tools.evaluation_results_class:Current Best Return = -167.50149536132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8044776119402985
INFO:tools.evaluation_results_class:Average Episode Length = 377.8407643312102
INFO:tools.evaluation_results_class:Counted Episodes = 628
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.9851269721984863
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 4.274697780609131
INFO:agents.father_agent:Step: 10, Training loss: 5.139707088470459
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -174.1112823486328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 83.9375
INFO:tools.evaluation_results_class:Average Discounted Reward = -8.537703514099121
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15072.703125
INFO:tools.evaluation_results_class:Current Best Return = -167.50149536132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 354.4771341463415
INFO:tools.evaluation_results_class:Counted Episodes = 656
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -204.71705627441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.68440246582031
INFO:tools.evaluation_results_class:Average Discounted Reward = -15.03853702545166
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8887545344619106
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29770.623046875
INFO:tools.evaluation_results_class:Current Best Return = -204.71705627441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8887545344619106
INFO:tools.evaluation_results_class:Average Episode Length = 307.9854897218863
INFO:tools.evaluation_results_class:Counted Episodes = 827
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.863037109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 99.24335479736328
INFO:tools.evaluation_results_class:Average Discounted Reward = -1.5665884017944336
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8284574468085106
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12401.7880859375
INFO:tools.evaluation_results_class:Current Best Return = -165.863037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8284574468085106
INFO:tools.evaluation_results_class:Average Episode Length = 341.405585106383
INFO:tools.evaluation_results_class:Counted Episodes = 752
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 205.9820673399837
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 11 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.97659301757812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 81.12480163574219
INFO:tools.evaluation_results_class:Average Discounted Reward = -9.716085433959961
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.797191887675507
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15848.8359375
INFO:tools.evaluation_results_class:Current Best Return = -167.50149536132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 357.6661466458658
INFO:tools.evaluation_results_class:Counted Episodes = 641
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.921103000640869
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 4.1460676193237305
INFO:agents.father_agent:Step: 10, Training loss: 3.1996517181396484
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -175.19427490234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.90127563476562
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.592270851135254
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7659235668789809
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15140.673828125
INFO:tools.evaluation_results_class:Current Best Return = -167.50149536132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 374.71337579617835
INFO:tools.evaluation_results_class:Counted Episodes = 628
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -214.83729553222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 61.14153289794922
INFO:tools.evaluation_results_class:Average Discounted Reward = -24.48065948486328
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8624338624338624
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28703.2109375
INFO:tools.evaluation_results_class:Current Best Return = -214.83729553222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8624338624338624
INFO:tools.evaluation_results_class:Average Episode Length = 329.16269841269843
INFO:tools.evaluation_results_class:Counted Episodes = 756
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.61581420898438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 89.36723327636719
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.049161434173584
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.809322033898305
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12327.78125
INFO:tools.evaluation_results_class:Current Best Return = -169.61581420898438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.809322033898305
INFO:tools.evaluation_results_class:Average Episode Length = 357.01977401129943
INFO:tools.evaluation_results_class:Counted Episodes = 708
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 225.14881193765666
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.1750030517578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 153.4250030517578
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.110136032104492
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.905
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8864.5751953125
INFO:tools.evaluation_results_class:Current Best Return = -136.1750030517578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.905
INFO:tools.evaluation_results_class:Average Episode Length = 287.075
INFO:tools.evaluation_results_class:Counted Episodes = 400
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.50389099121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 127.093505859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.589910507202148
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8831168831168831
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12330.8671875
INFO:tools.evaluation_results_class:Current Best Return = -155.50389099121094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8831168831168831
INFO:tools.evaluation_results_class:Average Episode Length = 292.0935064935065
INFO:tools.evaluation_results_class:Counted Episodes = 385
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.00259399414062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 147.41299438476562
INFO:tools.evaluation_results_class:Average Discounted Reward = 25.373929977416992
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9012987012987013
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8777.857421875
INFO:tools.evaluation_results_class:Current Best Return = -141.00259399414062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9012987012987013
INFO:tools.evaluation_results_class:Average Episode Length = 293.74805194805197
INFO:tools.evaluation_results_class:Counted Episodes = 385
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -132.7202911376953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 155.59652709960938
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.876121520996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.900990099009901
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7981.458984375
INFO:tools.evaluation_results_class:Current Best Return = -132.7202911376953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.900990099009901
INFO:tools.evaluation_results_class:Average Episode Length = 277.1707920792079
INFO:tools.evaluation_results_class:Counted Episodes = 404
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -132.82984924316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 149.47381591796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.172683715820312
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8821989528795812
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8485.4345703125
INFO:tools.evaluation_results_class:Current Best Return = -132.82984924316406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8821989528795812
INFO:tools.evaluation_results_class:Average Episode Length = 298.8874345549738
INFO:tools.evaluation_results_class:Counted Episodes = 382
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -208.8169708251953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 78.92838287353516
INFO:tools.evaluation_results_class:Average Discounted Reward = -15.423321723937988
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8992042440318302
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28106.0078125
INFO:tools.evaluation_results_class:Current Best Return = -208.8169708251953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8992042440318302
INFO:tools.evaluation_results_class:Average Episode Length = 299.50928381962865
INFO:tools.evaluation_results_class:Counted Episodes = 377
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -222.91351318359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.57297134399414
INFO:tools.evaluation_results_class:Average Discounted Reward = -28.115915298461914
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8702702702702703
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32654.962890625
INFO:tools.evaluation_results_class:Current Best Return = -222.91351318359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8702702702702703
INFO:tools.evaluation_results_class:Average Episode Length = 317.84864864864863
INFO:tools.evaluation_results_class:Counted Episodes = 370
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -208.73513793945312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 68.88648986816406
INFO:tools.evaluation_results_class:Average Discounted Reward = -16.523517608642578
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8675675675675676
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31533.994140625
INFO:tools.evaluation_results_class:Current Best Return = -208.73513793945312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8675675675675676
INFO:tools.evaluation_results_class:Average Episode Length = 306.94054054054055
INFO:tools.evaluation_results_class:Counted Episodes = 370
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -218.87123107910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 66.06027221679688
INFO:tools.evaluation_results_class:Average Discounted Reward = -24.414653778076172
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8904109589041096
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27342.130859375
INFO:tools.evaluation_results_class:Current Best Return = -218.87123107910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8904109589041096
INFO:tools.evaluation_results_class:Average Episode Length = 307.46027397260275
INFO:tools.evaluation_results_class:Counted Episodes = 365
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -211.28912353515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.70026397705078
INFO:tools.evaluation_results_class:Average Discounted Reward = -21.106483459472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9124668435013262
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27478.19140625
INFO:tools.evaluation_results_class:Current Best Return = -211.28912353515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9124668435013262
INFO:tools.evaluation_results_class:Average Episode Length = 293.3713527851459
INFO:tools.evaluation_results_class:Counted Episodes = 377
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -213.9220428466797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 73.38978576660156
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.76329803466797
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8978494623655914
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30812.064453125
INFO:tools.evaluation_results_class:Current Best Return = -213.9220428466797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8978494623655914
INFO:tools.evaluation_results_class:Average Episode Length = 303.18548387096774
INFO:tools.evaluation_results_class:Counted Episodes = 372
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -230.28338623046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 53.96730422973633
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.57973289489746
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.888283378746594
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38468.88671875
INFO:tools.evaluation_results_class:Current Best Return = -230.28338623046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.888283378746594
INFO:tools.evaluation_results_class:Average Episode Length = 309.267029972752
INFO:tools.evaluation_results_class:Counted Episodes = 367
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -217.65394592285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 73.57220458984375
INFO:tools.evaluation_results_class:Average Discounted Reward = -21.350025177001953
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9100817438692098
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34190.85546875
INFO:tools.evaluation_results_class:Current Best Return = -217.65394592285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9100817438692098
INFO:tools.evaluation_results_class:Average Episode Length = 306.4850136239782
INFO:tools.evaluation_results_class:Counted Episodes = 367
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -211.19667053222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.14127349853516
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.758743286132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8698060941828255
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24892.4375
INFO:tools.evaluation_results_class:Current Best Return = -211.19667053222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8698060941828255
INFO:tools.evaluation_results_class:Average Episode Length = 306.3795013850416
INFO:tools.evaluation_results_class:Counted Episodes = 361
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -214.31016540527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.46524047851562
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.175352096557617
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.893048128342246
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29796.6953125
INFO:tools.evaluation_results_class:Current Best Return = -214.31016540527344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.893048128342246
INFO:tools.evaluation_results_class:Average Episode Length = 301.3529411764706
INFO:tools.evaluation_results_class:Counted Episodes = 374
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -205.55584716796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 83.69091033935547
INFO:tools.evaluation_results_class:Average Discounted Reward = -9.466135025024414
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9038961038961039
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27860.26171875
INFO:tools.evaluation_results_class:Current Best Return = -205.55584716796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9038961038961039
INFO:tools.evaluation_results_class:Average Episode Length = 294.93506493506493
INFO:tools.evaluation_results_class:Counted Episodes = 385
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.09173583984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 113.49235534667969
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.207582473754883
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7737003058103975
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4512.92138671875
INFO:tools.evaluation_results_class:Current Best Return = -134.09173583984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7737003058103975
INFO:tools.evaluation_results_class:Average Episode Length = 357.8256880733945
INFO:tools.evaluation_results_class:Counted Episodes = 327
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.01795959472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 124.58084106445312
INFO:tools.evaluation_results_class:Average Discounted Reward = 15.925586700439453
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8143712574850299
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5570.21484375
INFO:tools.evaluation_results_class:Current Best Return = -136.01795959472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8143712574850299
INFO:tools.evaluation_results_class:Average Episode Length = 346.47904191616766
INFO:tools.evaluation_results_class:Counted Episodes = 334
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -128.55043029785156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 136.11814880371094
INFO:tools.evaluation_results_class:Average Discounted Reward = 27.9827880859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8270893371757925
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5610.4033203125
INFO:tools.evaluation_results_class:Current Best Return = -128.55043029785156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8270893371757925
INFO:tools.evaluation_results_class:Average Episode Length = 322.0115273775216
INFO:tools.evaluation_results_class:Counted Episodes = 347
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -126.0171890258789
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 132.55014038085938
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.754175186157227
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8080229226361032
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4584.7275390625
INFO:tools.evaluation_results_class:Current Best Return = -126.0171890258789
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8080229226361032
INFO:tools.evaluation_results_class:Average Episode Length = 336.3008595988539
INFO:tools.evaluation_results_class:Counted Episodes = 349
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.00308227539062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 114.88580322265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.371489524841309
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7777777777777778
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3834.31787109375
INFO:tools.evaluation_results_class:Current Best Return = -134.00308227539062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7777777777777778
INFO:tools.evaluation_results_class:Average Episode Length = 366.40432098765433
INFO:tools.evaluation_results_class:Counted Episodes = 324
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -172.555908203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 74.89751434326172
INFO:tools.evaluation_results_class:Average Discounted Reward = -3.485694169998169
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7732919254658385
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16304.0732421875
INFO:tools.evaluation_results_class:Current Best Return = -172.555908203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7732919254658385
INFO:tools.evaluation_results_class:Average Episode Length = 362.5993788819876
INFO:tools.evaluation_results_class:Counted Episodes = 322
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.99685668945312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 77.57546997070312
INFO:tools.evaluation_results_class:Average Discounted Reward = -11.830013275146484
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7861635220125787
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13602.8330078125
INFO:tools.evaluation_results_class:Current Best Return = -173.99685668945312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7861635220125787
INFO:tools.evaluation_results_class:Average Episode Length = 356.4622641509434
INFO:tools.evaluation_results_class:Counted Episodes = 318
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -184.6952362060547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.17777633666992
INFO:tools.evaluation_results_class:Average Discounted Reward = -14.180591583251953
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7746031746031746
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15976.21875
INFO:tools.evaluation_results_class:Current Best Return = -184.6952362060547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7746031746031746
INFO:tools.evaluation_results_class:Average Episode Length = 361.2952380952381
INFO:tools.evaluation_results_class:Counted Episodes = 315
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.3560333251953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.26625061035156
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.586176872253418
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7925696594427245
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13934.8486328125
INFO:tools.evaluation_results_class:Current Best Return = -173.3560333251953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7925696594427245
INFO:tools.evaluation_results_class:Average Episode Length = 349.6501547987616
INFO:tools.evaluation_results_class:Counted Episodes = 323
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.10975646972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 73.71951293945312
INFO:tools.evaluation_results_class:Average Discounted Reward = -5.264323711395264
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7713414634146342
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14029.109375
INFO:tools.evaluation_results_class:Current Best Return = -173.10975646972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7713414634146342
INFO:tools.evaluation_results_class:Average Episode Length = 361.6036585365854
INFO:tools.evaluation_results_class:Counted Episodes = 328
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -179.54354858398438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.22822570800781
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.620850563049316
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7867867867867868
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15927.681640625
INFO:tools.evaluation_results_class:Current Best Return = -179.54354858398438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7867867867867868
INFO:tools.evaluation_results_class:Average Episode Length = 353.5045045045045
INFO:tools.evaluation_results_class:Counted Episodes = 333
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.52818298339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 78.80118560791016
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.019258975982666
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8041543026706232
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13737.98828125
INFO:tools.evaluation_results_class:Current Best Return = -178.52818298339844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8041543026706232
INFO:tools.evaluation_results_class:Average Episode Length = 349.94362017804156
INFO:tools.evaluation_results_class:Counted Episodes = 337
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -179.93939208984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 84.78787994384766
INFO:tools.evaluation_results_class:Average Discounted Reward = -8.516287803649902
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8272727272727273
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17070.015625
INFO:tools.evaluation_results_class:Current Best Return = -179.93939208984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8272727272727273
INFO:tools.evaluation_results_class:Average Episode Length = 342.93333333333334
INFO:tools.evaluation_results_class:Counted Episodes = 330
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.00621032714844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 62.4161491394043
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.713765144348145
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7888198757763976
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17803.86328125
INFO:tools.evaluation_results_class:Current Best Return = -190.00621032714844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7888198757763976
INFO:tools.evaluation_results_class:Average Episode Length = 355.2919254658385
INFO:tools.evaluation_results_class:Counted Episodes = 322
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -167.56155395507812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 91.89789581298828
INFO:tools.evaluation_results_class:Average Discounted Reward = -0.49606406688690186
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8108108108108109
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13816.1376953125
INFO:tools.evaluation_results_class:Current Best Return = -167.56155395507812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8108108108108109
INFO:tools.evaluation_results_class:Average Episode Length = 342.7357357357357
INFO:tools.evaluation_results_class:Counted Episodes = 333
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -186.15383911132812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.871795654296875
INFO:tools.evaluation_results_class:Average Discounted Reward = -14.605688095092773
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7532051282051282
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15171.373046875
INFO:tools.evaluation_results_class:Current Best Return = -186.15383911132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7532051282051282
INFO:tools.evaluation_results_class:Average Episode Length = 370.59615384615387
INFO:tools.evaluation_results_class:Counted Episodes = 312
INFO:robust_rl.robust_rl_trainer:Iteration 12 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -181.50572204589844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 58.363338470458984
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.728376388549805
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7495908346972177
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14580.7529296875
INFO:tools.evaluation_results_class:Current Best Return = -167.50149536132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 384.0965630114566
INFO:tools.evaluation_results_class:Counted Episodes = 611
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.495373249053955
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 3.109898090362549
INFO:agents.father_agent:Step: 10, Training loss: 2.605158805847168
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -165.29159545898438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 78.13153839111328
INFO:tools.evaluation_results_class:Average Discounted Reward = -2.8803350925445557
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7606973058637084
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11403.2880859375
INFO:tools.evaluation_results_class:Current Best Return = -165.29159545898438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 367.12519809825676
INFO:tools.evaluation_results_class:Counted Episodes = 631
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -216.0940704345703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.30799102783203
INFO:tools.evaluation_results_class:Average Discounted Reward = -23.62795066833496
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.854381443298969
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30251.505859375
INFO:tools.evaluation_results_class:Current Best Return = -216.0940704345703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.854381443298969
INFO:tools.evaluation_results_class:Average Episode Length = 325.20747422680415
INFO:tools.evaluation_results_class:Counted Episodes = 776
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.30894470214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 103.75609588623047
INFO:tools.evaluation_results_class:Average Discounted Reward = 3.982111930847168
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8252032520325203
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11686.1259765625
INFO:tools.evaluation_results_class:Current Best Return = -160.30894470214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8252032520325203
INFO:tools.evaluation_results_class:Average Episode Length = 347.81571815718155
INFO:tools.evaluation_results_class:Counted Episodes = 738
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 207.340307560245
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 13 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -168.75425720214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.11128234863281
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.86508321762085
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7527047913446677
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12879.603515625
INFO:tools.evaluation_results_class:Current Best Return = -165.29159545898438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 366.3261205564142
INFO:tools.evaluation_results_class:Counted Episodes = 647
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.589272975921631
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 3.119802951812744
INFO:agents.father_agent:Step: 10, Training loss: 3.265660047531128
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -167.77635192871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 64.3003158569336
INFO:tools.evaluation_results_class:Average Discounted Reward = -5.639467239379883
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7252396166134185
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11682.650390625
INFO:tools.evaluation_results_class:Current Best Return = -165.29159545898438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 371.2300319488818
INFO:tools.evaluation_results_class:Counted Episodes = 626
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -204.23057556152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 68.85212707519531
INFO:tools.evaluation_results_class:Average Discounted Reward = -8.033565521240234
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8533834586466166
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29643.646484375
INFO:tools.evaluation_results_class:Current Best Return = -204.23057556152344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8533834586466166
INFO:tools.evaluation_results_class:Average Episode Length = 314.671679197995
INFO:tools.evaluation_results_class:Counted Episodes = 798
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.8082733154297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 81.68827819824219
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.06783320754766464
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.776551724137931
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12910.21484375
INFO:tools.evaluation_results_class:Current Best Return = -166.8082733154297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.776551724137931
INFO:tools.evaluation_results_class:Average Episode Length = 351.38068965517243
INFO:tools.evaluation_results_class:Counted Episodes = 725
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 219.84367292500036
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 14 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.14031982421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 64.6983871459961
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.4207048416137695
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7338709677419355
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12398.7626953125
INFO:tools.evaluation_results_class:Current Best Return = -165.29159545898438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 378.48225806451615
INFO:tools.evaluation_results_class:Counted Episodes = 620
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.4543068408966064
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 2.857412099838257
INFO:agents.father_agent:Step: 10, Training loss: 3.7970757484436035
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -160.39779663085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 73.06131744384766
INFO:tools.evaluation_results_class:Average Discounted Reward = 4.058352470397949
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7295597484276729
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11676.84375
INFO:tools.evaluation_results_class:Current Best Return = -160.39779663085938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 360.58962264150944
INFO:tools.evaluation_results_class:Counted Episodes = 636
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -198.2279815673828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.0832290649414
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.4351224899292
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8697225572979493
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23855.181640625
INFO:tools.evaluation_results_class:Current Best Return = -198.2279815673828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8697225572979493
INFO:tools.evaluation_results_class:Average Episode Length = 310.64414957780457
INFO:tools.evaluation_results_class:Counted Episodes = 829
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -161.50421142578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 78.49578857421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 2.9740543365478516
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.75
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10469.7138671875
INFO:tools.evaluation_results_class:Current Best Return = -161.50421142578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.75
INFO:tools.evaluation_results_class:Average Episode Length = 358.93398876404495
INFO:tools.evaluation_results_class:Counted Episodes = 712
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 225.6143059752593
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 15 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.39999389648438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.13968276977539
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.013523101806641
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.707936507936508
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10441.671875
INFO:tools.evaluation_results_class:Current Best Return = -160.39779663085938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 378.87619047619046
INFO:tools.evaluation_results_class:Counted Episodes = 630
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.0572755336761475
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 2.9736671447753906
INFO:agents.father_agent:Step: 10, Training loss: 4.192060470581055
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -153.7731475830078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.93055725097656
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.584402084350586
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6990740740740741
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10825.447265625
INFO:tools.evaluation_results_class:Current Best Return = -153.7731475830078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 359.20524691358025
INFO:tools.evaluation_results_class:Counted Episodes = 648
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -187.8694610595703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 93.26560974121094
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.976293087005615
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8785471055618616
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26055.4921875
INFO:tools.evaluation_results_class:Current Best Return = -187.8694610595703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8785471055618616
INFO:tools.evaluation_results_class:Average Episode Length = 290.14301929625424
INFO:tools.evaluation_results_class:Counted Episodes = 881
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.8326416015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 81.81590270996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.178732872009277
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7364016736401674
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10458.8623046875
INFO:tools.evaluation_results_class:Current Best Return = -153.8326416015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7364016736401674
INFO:tools.evaluation_results_class:Average Episode Length = 353.13668061366803
INFO:tools.evaluation_results_class:Counted Episodes = 717
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 238.30137459168174
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 16 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -161.4199981689453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 65.53384399414062
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.916914939880371
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7092307692307692
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11511.3916015625
INFO:tools.evaluation_results_class:Current Best Return = -153.7731475830078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 361.66923076923075
INFO:tools.evaluation_results_class:Counted Episodes = 650
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.469839334487915
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 3.064229965209961
INFO:agents.father_agent:Step: 10, Training loss: 2.8555519580841064
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -156.4580078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.48091506958008
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.360054969787598
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6748091603053435
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10571.3896484375
INFO:tools.evaluation_results_class:Current Best Return = -153.7731475830078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 358.7770992366412
INFO:tools.evaluation_results_class:Counted Episodes = 655
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -179.6234588623047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 98.90167236328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 16.954599380493164
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.870391061452514
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23831.181640625
INFO:tools.evaluation_results_class:Current Best Return = -179.6234588623047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.870391061452514
INFO:tools.evaluation_results_class:Average Episode Length = 281.75418994413405
INFO:tools.evaluation_results_class:Counted Episodes = 895
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.3988800048828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.07303237915039
INFO:tools.evaluation_results_class:Average Discounted Reward = 11.836798667907715
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6952247191011236
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10520.7900390625
INFO:tools.evaluation_results_class:Current Best Return = -159.3988800048828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6952247191011236
INFO:tools.evaluation_results_class:Average Episode Length = 362.83988764044943
INFO:tools.evaluation_results_class:Counted Episodes = 712
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 249.23905122660236
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -108.85877227783203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 180.5261993408203
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.16496276855469
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9043280182232346
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5960.46337890625
INFO:tools.evaluation_results_class:Current Best Return = -108.85877227783203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9043280182232346
INFO:tools.evaluation_results_class:Average Episode Length = 251.38041002277905
INFO:tools.evaluation_results_class:Counted Episodes = 439
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -118.26280975341797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 180.35635375976562
INFO:tools.evaluation_results_class:Average Discounted Reward = 53.80277633666992
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.933184855233853
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9252.8037109375
INFO:tools.evaluation_results_class:Current Best Return = -118.26280975341797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.933184855233853
INFO:tools.evaluation_results_class:Average Episode Length = 243.10690423162583
INFO:tools.evaluation_results_class:Counted Episodes = 449
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -113.22898864746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 186.60293579101562
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.61898040771484
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9369747899159664
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6616.76904296875
INFO:tools.evaluation_results_class:Current Best Return = -113.22898864746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9369747899159664
INFO:tools.evaluation_results_class:Average Episode Length = 235.5441176470588
INFO:tools.evaluation_results_class:Counted Episodes = 476
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -114.03520965576172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 179.6737060546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 50.94422149658203
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9178403755868545
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6431.865234375
INFO:tools.evaluation_results_class:Current Best Return = -114.03520965576172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9178403755868545
INFO:tools.evaluation_results_class:Average Episode Length = 264.61267605633805
INFO:tools.evaluation_results_class:Counted Episodes = 426
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -112.47884368896484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 181.8641357421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 55.09477996826172
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9198218262806236
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6922.83740234375
INFO:tools.evaluation_results_class:Current Best Return = -112.47884368896484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9198218262806236
INFO:tools.evaluation_results_class:Average Episode Length = 255.18040089086858
INFO:tools.evaluation_results_class:Counted Episodes = 449
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -187.23960876464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 100.6821517944336
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.498797416687012
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8997555012224939
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25343.0859375
INFO:tools.evaluation_results_class:Current Best Return = -187.23960876464844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8997555012224939
INFO:tools.evaluation_results_class:Average Episode Length = 258.09290953545235
INFO:tools.evaluation_results_class:Counted Episodes = 409
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -172.41949462890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.12245178222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.517751693725586
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8798185941043084
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21501.04296875
INFO:tools.evaluation_results_class:Current Best Return = -172.41949462890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8798185941043084
INFO:tools.evaluation_results_class:Average Episode Length = 260.13605442176873
INFO:tools.evaluation_results_class:Counted Episodes = 441
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.03248596191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 133.4663543701172
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.295379638671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9234338747099768
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19167.029296875
INFO:tools.evaluation_results_class:Current Best Return = -162.03248596191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9234338747099768
INFO:tools.evaluation_results_class:Average Episode Length = 250.0858468677494
INFO:tools.evaluation_results_class:Counted Episodes = 431
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.9197235107422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 132.46559143066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 27.100858688354492
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.944954128440367
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22024.880859375
INFO:tools.evaluation_results_class:Current Best Return = -169.9197235107422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.944954128440367
INFO:tools.evaluation_results_class:Average Episode Length = 241.30963302752295
INFO:tools.evaluation_results_class:Counted Episodes = 436
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -174.78700256347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 117.94843292236328
INFO:tools.evaluation_results_class:Average Discounted Reward = 22.445972442626953
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9147982062780269
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22773.2890625
INFO:tools.evaluation_results_class:Current Best Return = -174.78700256347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9147982062780269
INFO:tools.evaluation_results_class:Average Episode Length = 251.2085201793722
INFO:tools.evaluation_results_class:Counted Episodes = 446
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -175.779541015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 112.9477310180664
INFO:tools.evaluation_results_class:Average Discounted Reward = 22.723236083984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9022727272727272
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23227.419921875
INFO:tools.evaluation_results_class:Current Best Return = -175.779541015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9022727272727272
INFO:tools.evaluation_results_class:Average Episode Length = 260.04318181818184
INFO:tools.evaluation_results_class:Counted Episodes = 440
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -174.54794311523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 118.42008972167969
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.946094512939453
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9155251141552512
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25782.447265625
INFO:tools.evaluation_results_class:Current Best Return = -174.54794311523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9155251141552512
INFO:tools.evaluation_results_class:Average Episode Length = 257.8447488584475
INFO:tools.evaluation_results_class:Counted Episodes = 438
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -176.71493530273438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 118.2735595703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.087068557739258
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9218390804597701
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23206.619140625
INFO:tools.evaluation_results_class:Current Best Return = -176.71493530273438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9218390804597701
INFO:tools.evaluation_results_class:Average Episode Length = 254.91954022988506
INFO:tools.evaluation_results_class:Counted Episodes = 435
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -184.90164184570312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 100.62528991699219
INFO:tools.evaluation_results_class:Average Discounted Reward = 17.957714080810547
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.892271662763466
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28087.125
INFO:tools.evaluation_results_class:Current Best Return = -184.90164184570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.892271662763466
INFO:tools.evaluation_results_class:Average Episode Length = 265.26463700234194
INFO:tools.evaluation_results_class:Counted Episodes = 427
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -189.69534301757812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 107.2348861694336
INFO:tools.evaluation_results_class:Average Discounted Reward = 10.92975902557373
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9279069767441861
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26210.3203125
INFO:tools.evaluation_results_class:Current Best Return = -189.69534301757812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9279069767441861
INFO:tools.evaluation_results_class:Average Episode Length = 256.4720930232558
INFO:tools.evaluation_results_class:Counted Episodes = 430
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -184.70449829101562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 106.5484619140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.205520629882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9101654846335697
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23833.630859375
INFO:tools.evaluation_results_class:Current Best Return = -184.70449829101562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9101654846335697
INFO:tools.evaluation_results_class:Average Episode Length = 260.48936170212767
INFO:tools.evaluation_results_class:Counted Episodes = 423
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.785888671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 120.60235595703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 22.624591827392578
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9105882352941177
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21022.072265625
INFO:tools.evaluation_results_class:Current Best Return = -170.785888671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9105882352941177
INFO:tools.evaluation_results_class:Average Episode Length = 255.40470588235294
INFO:tools.evaluation_results_class:Counted Episodes = 425
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -172.82701110839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 121.39099884033203
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.40160369873047
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.919431279620853
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22203.04296875
INFO:tools.evaluation_results_class:Current Best Return = -172.82701110839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.919431279620853
INFO:tools.evaluation_results_class:Average Episode Length = 261.8222748815166
INFO:tools.evaluation_results_class:Counted Episodes = 422
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -186.17169189453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 104.87239074707031
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.92314624786377
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9095127610208816
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23639.078125
INFO:tools.evaluation_results_class:Current Best Return = -186.17169189453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9095127610208816
INFO:tools.evaluation_results_class:Average Episode Length = 266.1508120649652
INFO:tools.evaluation_results_class:Counted Episodes = 431
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -154.72000122070312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 143.94667053222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.29070281982422
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9333333333333333
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19107.1015625
INFO:tools.evaluation_results_class:Current Best Return = -154.72000122070312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9333333333333333
INFO:tools.evaluation_results_class:Average Episode Length = 235.39111111111112
INFO:tools.evaluation_results_class:Counted Episodes = 450
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.8954620361328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 123.9227294921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.309978485107422
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9181818181818182
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21376.359375
INFO:tools.evaluation_results_class:Current Best Return = -169.8954620361328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9181818181818182
INFO:tools.evaluation_results_class:Average Episode Length = 254.63181818181818
INFO:tools.evaluation_results_class:Counted Episodes = 440
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -126.9228515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 107.6172103881836
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.550168991088867
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7329376854599406
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4745.65576171875
INFO:tools.evaluation_results_class:Current Best Return = -126.9228515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7329376854599406
INFO:tools.evaluation_results_class:Average Episode Length = 334.1424332344214
INFO:tools.evaluation_results_class:Counted Episodes = 337
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -135.25856018066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 97.01557922363281
INFO:tools.evaluation_results_class:Average Discounted Reward = 22.476308822631836
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7258566978193146
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5904.91455078125
INFO:tools.evaluation_results_class:Current Best Return = -135.25856018066406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7258566978193146
INFO:tools.evaluation_results_class:Average Episode Length = 355.9283489096573
INFO:tools.evaluation_results_class:Counted Episodes = 321
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -126.93785095214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 107.18643951416016
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.78728103637695
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.731638418079096
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5368.25048828125
INFO:tools.evaluation_results_class:Current Best Return = -126.93785095214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.731638418079096
INFO:tools.evaluation_results_class:Average Episode Length = 329.6271186440678
INFO:tools.evaluation_results_class:Counted Episodes = 354
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -126.10144805908203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 120.6231918334961
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.78511047363281
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7710144927536232
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4797.7607421875
INFO:tools.evaluation_results_class:Current Best Return = -126.10144805908203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7710144927536232
INFO:tools.evaluation_results_class:Average Episode Length = 330.93333333333334
INFO:tools.evaluation_results_class:Counted Episodes = 345
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -124.18130493164062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 121.48442077636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.388999938964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7677053824362606
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5769.0439453125
INFO:tools.evaluation_results_class:Current Best Return = -124.18130493164062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7677053824362606
INFO:tools.evaluation_results_class:Average Episode Length = 320.2917847025496
INFO:tools.evaluation_results_class:Counted Episodes = 353
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -151.5830841064453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 82.37462615966797
INFO:tools.evaluation_results_class:Average Discounted Reward = 15.02753734588623
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7311178247734139
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8565.05859375
INFO:tools.evaluation_results_class:Current Best Return = -151.5830841064453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7311178247734139
INFO:tools.evaluation_results_class:Average Episode Length = 353.81570996978854
INFO:tools.evaluation_results_class:Counted Episodes = 331
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.67391967773438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 60.92856979370117
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.255729675292969
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6987577639751553
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10654.41796875
INFO:tools.evaluation_results_class:Current Best Return = -162.67391967773438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6987577639751553
INFO:tools.evaluation_results_class:Average Episode Length = 369.4378881987578
INFO:tools.evaluation_results_class:Counted Episodes = 322
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -146.4840545654297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 95.60289764404297
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.528657913208008
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7565217391304347
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9480.8232421875
INFO:tools.evaluation_results_class:Current Best Return = -146.4840545654297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7565217391304347
INFO:tools.evaluation_results_class:Average Episode Length = 335.5884057971015
INFO:tools.evaluation_results_class:Counted Episodes = 345
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.75
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.80813598632812
INFO:tools.evaluation_results_class:Average Discounted Reward = 11.9232177734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7267441860465116
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11741.9375
INFO:tools.evaluation_results_class:Current Best Return = -160.75
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7267441860465116
INFO:tools.evaluation_results_class:Average Episode Length = 343.26162790697674
INFO:tools.evaluation_results_class:Counted Episodes = 344
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -157.240966796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 81.79518127441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.924590110778809
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7469879518072289
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11520.779296875
INFO:tools.evaluation_results_class:Current Best Return = -157.240966796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7469879518072289
INFO:tools.evaluation_results_class:Average Episode Length = 343.95180722891564
INFO:tools.evaluation_results_class:Counted Episodes = 332
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -163.42201232910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 51.868499755859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.319486618041992
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.672782874617737
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11607.076171875
INFO:tools.evaluation_results_class:Current Best Return = -163.42201232910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.672782874617737
INFO:tools.evaluation_results_class:Average Episode Length = 366.20183486238534
INFO:tools.evaluation_results_class:Counted Episodes = 327
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -148.87896728515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 93.65705871582031
INFO:tools.evaluation_results_class:Average Discounted Reward = 22.058252334594727
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7579250720461095
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10046.0029296875
INFO:tools.evaluation_results_class:Current Best Return = -148.87896728515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7579250720461095
INFO:tools.evaluation_results_class:Average Episode Length = 327.80403458213254
INFO:tools.evaluation_results_class:Counted Episodes = 347
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.80923461914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.667694091796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 8.21068286895752
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7046153846153846
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12676.9052734375
INFO:tools.evaluation_results_class:Current Best Return = -165.80923461914062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7046153846153846
INFO:tools.evaluation_results_class:Average Episode Length = 355.0
INFO:tools.evaluation_results_class:Counted Episodes = 325
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.9519500732422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 77.52252197265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.297025680541992
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7327327327327328
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10388.7724609375
INFO:tools.evaluation_results_class:Current Best Return = -156.9519500732422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7327327327327328
INFO:tools.evaluation_results_class:Average Episode Length = 357.009009009009
INFO:tools.evaluation_results_class:Counted Episodes = 333
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.6041717529297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 85.20536041259766
INFO:tools.evaluation_results_class:Average Discounted Reward = 17.75813865661621
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7619047619047619
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11799.4892578125
INFO:tools.evaluation_results_class:Current Best Return = -158.6041717529297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7619047619047619
INFO:tools.evaluation_results_class:Average Episode Length = 331.66964285714283
INFO:tools.evaluation_results_class:Counted Episodes = 336
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -148.12078857421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 105.36235809326172
INFO:tools.evaluation_results_class:Average Discounted Reward = 27.947175979614258
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7921348314606742
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11808.826171875
INFO:tools.evaluation_results_class:Current Best Return = -148.12078857421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7921348314606742
INFO:tools.evaluation_results_class:Average Episode Length = 318.33988764044943
INFO:tools.evaluation_results_class:Counted Episodes = 356
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -150.09939575195312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 87.97289276123047
INFO:tools.evaluation_results_class:Average Discounted Reward = 24.235309600830078
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7439759036144579
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11081.2880859375
INFO:tools.evaluation_results_class:Current Best Return = -150.09939575195312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7439759036144579
INFO:tools.evaluation_results_class:Average Episode Length = 327.9548192771084
INFO:tools.evaluation_results_class:Counted Episodes = 332
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -150.24928283691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 90.8968505859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.343231201171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7535816618911175
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10075.15625
INFO:tools.evaluation_results_class:Current Best Return = -150.24928283691406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7535816618911175
INFO:tools.evaluation_results_class:Average Episode Length = 341.1977077363897
INFO:tools.evaluation_results_class:Counted Episodes = 349
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.43695068359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 77.2287368774414
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.890862464904785
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7302052785923754
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11118.498046875
INFO:tools.evaluation_results_class:Current Best Return = -156.43695068359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7302052785923754
INFO:tools.evaluation_results_class:Average Episode Length = 338.7741935483871
INFO:tools.evaluation_results_class:Counted Episodes = 341
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.80531311035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 78.23893737792969
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.736434936523438
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7345132743362832
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11211.419921875
INFO:tools.evaluation_results_class:Current Best Return = -156.80531311035156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7345132743362832
INFO:tools.evaluation_results_class:Average Episode Length = 352.37758112094394
INFO:tools.evaluation_results_class:Counted Episodes = 339
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.5454559326172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 76.18181610107422
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.418811798095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7272727272727273
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9657.11328125
INFO:tools.evaluation_results_class:Current Best Return = -156.5454559326172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7272727272727273
INFO:tools.evaluation_results_class:Average Episode Length = 352.04242424242426
INFO:tools.evaluation_results_class:Counted Episodes = 330
INFO:robust_rl.robust_rl_trainer:Iteration 17 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -161.92283630371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.23464584350586
INFO:tools.evaluation_results_class:Average Discounted Reward = 10.506584167480469
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.662992125984252
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11116.0244140625
INFO:tools.evaluation_results_class:Current Best Return = -153.7731475830078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 366.9984251968504
INFO:tools.evaluation_results_class:Counted Episodes = 635
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.966405153274536
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 3.414354085922241
INFO:agents.father_agent:Step: 10, Training loss: 3.391152858734131
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -153.58450317382812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 60.24496078491211
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.678336143493652
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6682170542635659
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8973.3876953125
INFO:tools.evaluation_results_class:Current Best Return = -153.58450317382812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 367.5302325581395
INFO:tools.evaluation_results_class:Counted Episodes = 645
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -174.3971405029297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 108.63916015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.48358917236328
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8844884488448845
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22852.576171875
INFO:tools.evaluation_results_class:Current Best Return = -174.3971405029297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8844884488448845
INFO:tools.evaluation_results_class:Average Episode Length = 281.87678767876787
INFO:tools.evaluation_results_class:Counted Episodes = 909
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.8898162841797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.8661117553711
INFO:tools.evaluation_results_class:Average Discounted Reward = 15.010021209716797
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7336122733612274
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10437.888671875
INFO:tools.evaluation_results_class:Current Best Return = -153.8898162841797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7336122733612274
INFO:tools.evaluation_results_class:Average Episode Length = 347.4560669456067
INFO:tools.evaluation_results_class:Counted Episodes = 717
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 252.30617391499933
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 18 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.38755798339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 30.082935333251953
INFO:tools.evaluation_results_class:Average Discounted Reward = 2.55598521232605
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6108452950558214
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10835.994140625
INFO:tools.evaluation_results_class:Current Best Return = -153.58450317382812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 392.2535885167464
INFO:tools.evaluation_results_class:Counted Episodes = 627
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.987394094467163
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 3.7176291942596436
INFO:agents.father_agent:Step: 10, Training loss: 2.254443645477295
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -144.4921112060547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 86.44046020507812
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.96158790588379
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7216642754662841
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9425.5068359375
INFO:tools.evaluation_results_class:Current Best Return = -144.4921112060547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 336.31563845050215
INFO:tools.evaluation_results_class:Counted Episodes = 697
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.82656860351562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 115.34178161621094
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.939102172851562
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9036511156186613
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23046.83203125
INFO:tools.evaluation_results_class:Current Best Return = -173.82656860351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9036511156186613
INFO:tools.evaluation_results_class:Average Episode Length = 261.7920892494929
INFO:tools.evaluation_results_class:Counted Episodes = 986
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.3631591796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 100.16315460205078
INFO:tools.evaluation_results_class:Average Discounted Reward = 27.543987274169922
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7578947368421053
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9115.1181640625
INFO:tools.evaluation_results_class:Current Best Return = -142.3631591796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7578947368421053
INFO:tools.evaluation_results_class:Average Episode Length = 329.92105263157896
INFO:tools.evaluation_results_class:Counted Episodes = 760
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 224.63834344820532
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 19 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -148.4158172607422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 83.502197265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.47600555419922
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7247437774524158
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9988.0361328125
INFO:tools.evaluation_results_class:Current Best Return = -144.4921112060547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8064024390243902
INFO:tools.evaluation_results_class:Average Episode Length = 335.99560761347
INFO:tools.evaluation_results_class:Counted Episodes = 683
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.104943752288818
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 2.9884910583496094
INFO:agents.father_agent:Step: 10, Training loss: 3.494720458984375
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -128.04029846191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 149.37118530273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.25539779663086
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8669108669108669
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9220.5888671875
INFO:tools.evaluation_results_class:Current Best Return = -128.04029846191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8669108669108669
INFO:tools.evaluation_results_class:Average Episode Length = 271.4065934065934
INFO:tools.evaluation_results_class:Counted Episodes = 819
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -155.80642700195312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 145.6387176513672
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.694435119628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9420160570918823
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18951.603515625
INFO:tools.evaluation_results_class:Current Best Return = -155.80642700195312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9420160570918823
INFO:tools.evaluation_results_class:Average Episode Length = 229.73238180196253
INFO:tools.evaluation_results_class:Counted Episodes = 1121
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -132.6095428466797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 142.61822509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.18048095703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8600867678958786
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10189.474609375
INFO:tools.evaluation_results_class:Current Best Return = -132.6095428466797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8600867678958786
INFO:tools.evaluation_results_class:Average Episode Length = 281.32104121475055
INFO:tools.evaluation_results_class:Counted Episodes = 922
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 179.6564382103333
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 20 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -132.56265258789062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 141.32205200195312
INFO:tools.evaluation_results_class:Average Discounted Reward = 46.41728210449219
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8558897243107769
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10817.447265625
INFO:tools.evaluation_results_class:Current Best Return = -128.04029846191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8669108669108669
INFO:tools.evaluation_results_class:Average Episode Length = 273.40225563909775
INFO:tools.evaluation_results_class:Counted Episodes = 798
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.698101758956909
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 4.970721244812012
INFO:agents.father_agent:Step: 10, Training loss: 3.961536169052124
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -111.12188720703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 192.62696838378906
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.36354064941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9492151431209603
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9076.0595703125
INFO:tools.evaluation_results_class:Current Best Return = -111.12188720703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9492151431209603
INFO:tools.evaluation_results_class:Average Episode Length = 211.49584487534625
INFO:tools.evaluation_results_class:Counted Episodes = 1083
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.47164916992188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 171.86647033691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 58.576744079589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9791816223977028
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18604.955078125
INFO:tools.evaluation_results_class:Current Best Return = -141.47164916992188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9791816223977028
INFO:tools.evaluation_results_class:Average Episode Length = 192.40416367552046
INFO:tools.evaluation_results_class:Counted Episodes = 1393
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -111.20655822753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 192.26885986328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 74.19449615478516
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9483606557377049
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9254.2607421875
INFO:tools.evaluation_results_class:Current Best Return = -111.20655822753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9483606557377049
INFO:tools.evaluation_results_class:Average Episode Length = 216.6934426229508
INFO:tools.evaluation_results_class:Counted Episodes = 1220
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 132.97092349140044
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 21 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -114.08514404296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 185.02554321289062
INFO:tools.evaluation_results_class:Average Discounted Reward = 73.13043975830078
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.934720908230842
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9906.9609375
INFO:tools.evaluation_results_class:Current Best Return = -111.12188720703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9492151431209603
INFO:tools.evaluation_results_class:Average Episode Length = 214.60075685903502
INFO:tools.evaluation_results_class:Counted Episodes = 1057
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.365945339202881
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 5.127066612243652
INFO:agents.father_agent:Step: 10, Training loss: 5.385538101196289
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -93.25944519042969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 225.3535614013672
INFO:tools.evaluation_results_class:Average Discounted Reward = 103.1578140258789
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9956656346749226
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7024.48046875
INFO:tools.evaluation_results_class:Current Best Return = -93.25944519042969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9956656346749226
INFO:tools.evaluation_results_class:Average Episode Length = 152.89721362229102
INFO:tools.evaluation_results_class:Counted Episodes = 1615
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -125.43939971923828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 193.67515563964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.57927703857422
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9972329828444937
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14808.34765625
INFO:tools.evaluation_results_class:Current Best Return = -125.43939971923828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9972329828444937
INFO:tools.evaluation_results_class:Average Episode Length = 154.37410071942446
INFO:tools.evaluation_results_class:Counted Episodes = 1807
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -94.23008728027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 224.16546630859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 102.12679290771484
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9949860724233983
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7453.361328125
INFO:tools.evaluation_results_class:Current Best Return = -94.23008728027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9949860724233983
INFO:tools.evaluation_results_class:Average Episode Length = 155.03955431754875
INFO:tools.evaluation_results_class:Counted Episodes = 1795
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=3, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 102.70861369691282
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=3, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.76190185546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.88372802734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 119.235107421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988925802879292
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3116.478515625
INFO:tools.evaluation_results_class:Current Best Return = -73.76190185546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988925802879292
INFO:tools.evaluation_results_class:Average Episode Length = 141.26688815060908
INFO:tools.evaluation_results_class:Counted Episodes = 903
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.55400848388672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.3310089111328
INFO:tools.evaluation_results_class:Average Discounted Reward = 110.03807067871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9965156794425087
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6450.75146484375
INFO:tools.evaluation_results_class:Current Best Return = -84.55400848388672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9965156794425087
INFO:tools.evaluation_results_class:Average Episode Length = 143.31242740998837
INFO:tools.evaluation_results_class:Counted Episodes = 861
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.2288589477539
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.77114868164062
INFO:tools.evaluation_results_class:Average Discounted Reward = 128.66441345214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4029.923828125
INFO:tools.evaluation_results_class:Current Best Return = -73.2288589477539
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 126.49154228855721
INFO:tools.evaluation_results_class:Counted Episodes = 1005
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.02340698242188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.97659301757812
INFO:tools.evaluation_results_class:Average Discounted Reward = 122.83863067626953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3297.252685546875
INFO:tools.evaluation_results_class:Current Best Return = -71.02340698242188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.1595744680851
INFO:tools.evaluation_results_class:Counted Episodes = 940
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.13528442382812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.86471557617188
INFO:tools.evaluation_results_class:Average Discounted Reward = 126.1793441772461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2791.283935546875
INFO:tools.evaluation_results_class:Current Best Return = -68.13528442382812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 135.59632034632034
INFO:tools.evaluation_results_class:Counted Episodes = 924
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -108.95751953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 211.04248046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 94.305908203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11542.2021484375
INFO:tools.evaluation_results_class:Current Best Return = -108.95751953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 139.58553386911595
INFO:tools.evaluation_results_class:Counted Episodes = 871
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -112.66065216064453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 207.33934020996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 90.66593933105469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11996.892578125
INFO:tools.evaluation_results_class:Current Best Return = -112.66065216064453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 142.217587373168
INFO:tools.evaluation_results_class:Counted Episodes = 887
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -114.8922348022461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 205.10775756835938
INFO:tools.evaluation_results_class:Average Discounted Reward = 90.43463897705078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14653.3017578125
INFO:tools.evaluation_results_class:Current Best Return = -114.8922348022461
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 144.5388180764774
INFO:tools.evaluation_results_class:Counted Episodes = 863
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -113.8250503540039
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 205.82937622070312
INFO:tools.evaluation_results_class:Average Discounted Reward = 91.50366973876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9989200863930886
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11960.833984375
INFO:tools.evaluation_results_class:Current Best Return = -113.8250503540039
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9989200863930886
INFO:tools.evaluation_results_class:Average Episode Length = 138.6889848812095
INFO:tools.evaluation_results_class:Counted Episodes = 926
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -124.41061401367188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 194.113037109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 81.61312103271484
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9953863898500577
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13858.123046875
INFO:tools.evaluation_results_class:Current Best Return = -124.41061401367188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9953863898500577
INFO:tools.evaluation_results_class:Average Episode Length = 144.61014994232988
INFO:tools.evaluation_results_class:Counted Episodes = 867
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -119.34368133544922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 200.6563262939453
INFO:tools.evaluation_results_class:Average Discounted Reward = 87.64071655273438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12552.296875
INFO:tools.evaluation_results_class:Current Best Return = -119.34368133544922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 140.87471264367815
INFO:tools.evaluation_results_class:Counted Episodes = 870
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -114.61965942382812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 204.6965789794922
INFO:tools.evaluation_results_class:Average Discounted Reward = 93.09803771972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9978632478632479
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12589.1962890625
INFO:tools.evaluation_results_class:Current Best Return = -114.61965942382812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978632478632479
INFO:tools.evaluation_results_class:Average Episode Length = 135.53846153846155
INFO:tools.evaluation_results_class:Counted Episodes = 936
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -121.97772216796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 197.64712524414062
INFO:tools.evaluation_results_class:Average Discounted Reward = 83.55895233154297
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988276670574443
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14828.9638671875
INFO:tools.evaluation_results_class:Current Best Return = -121.97772216796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988276670574443
INFO:tools.evaluation_results_class:Average Episode Length = 144.11957796014067
INFO:tools.evaluation_results_class:Counted Episodes = 853
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -116.38084411621094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 202.90646362304688
INFO:tools.evaluation_results_class:Average Discounted Reward = 89.92111206054688
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9977728285077951
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12899.9892578125
INFO:tools.evaluation_results_class:Current Best Return = -116.38084411621094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9977728285077951
INFO:tools.evaluation_results_class:Average Episode Length = 140.7349665924276
INFO:tools.evaluation_results_class:Counted Episodes = 898
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -118.55858612060547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 201.44140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 86.74931335449219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13104.0732421875
INFO:tools.evaluation_results_class:Current Best Return = -118.55858612060547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 144.82821387940842
INFO:tools.evaluation_results_class:Counted Episodes = 879
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -118.95454406738281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 200.33592224121094
INFO:tools.evaluation_results_class:Average Discounted Reward = 88.94084930419922
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9977827050997783
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14418.232421875
INFO:tools.evaluation_results_class:Current Best Return = -118.95454406738281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9977827050997783
INFO:tools.evaluation_results_class:Average Episode Length = 138.40022172949003
INFO:tools.evaluation_results_class:Counted Episodes = 902
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -117.88664245605469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 202.1133575439453
INFO:tools.evaluation_results_class:Average Discounted Reward = 89.90056610107422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13599.1884765625
INFO:tools.evaluation_results_class:Current Best Return = -117.88664245605469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 140.25140291806957
INFO:tools.evaluation_results_class:Counted Episodes = 891
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -118.65731048583984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 201.3426971435547
INFO:tools.evaluation_results_class:Average Discounted Reward = 85.28726959228516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12037.8525390625
INFO:tools.evaluation_results_class:Current Best Return = -118.65731048583984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 144.84795321637426
INFO:tools.evaluation_results_class:Counted Episodes = 855
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -112.80506896972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 206.84251403808594
INFO:tools.evaluation_results_class:Average Discounted Reward = 94.0479736328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.998898678414097
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11753.6318359375
INFO:tools.evaluation_results_class:Current Best Return = -112.80506896972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.998898678414097
INFO:tools.evaluation_results_class:Average Episode Length = 139.22797356828193
INFO:tools.evaluation_results_class:Counted Episodes = 908
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -120.69283294677734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 199.3071746826172
INFO:tools.evaluation_results_class:Average Discounted Reward = 85.48953247070312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13866.0029296875
INFO:tools.evaluation_results_class:Current Best Return = -120.69283294677734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 140.99203640500568
INFO:tools.evaluation_results_class:Counted Episodes = 879
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -122.68035888671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 196.9644775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 85.8552017211914
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988901220865705
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15305.6337890625
INFO:tools.evaluation_results_class:Current Best Return = -122.68035888671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988901220865705
INFO:tools.evaluation_results_class:Average Episode Length = 141.67591564927858
INFO:tools.evaluation_results_class:Counted Episodes = 901
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -114.92720794677734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 204.71444702148438
INFO:tools.evaluation_results_class:Average Discounted Reward = 90.2423324584961
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988801791713325
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11296.4990234375
INFO:tools.evaluation_results_class:Current Best Return = -114.92720794677734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988801791713325
INFO:tools.evaluation_results_class:Average Episode Length = 139.9137737961926
INFO:tools.evaluation_results_class:Counted Episodes = 893
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -109.32846069335938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 209.95162963867188
INFO:tools.evaluation_results_class:Average Discounted Reward = 95.68094635009766
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9977502812148481
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11501.9775390625
INFO:tools.evaluation_results_class:Current Best Return = -109.32846069335938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9977502812148481
INFO:tools.evaluation_results_class:Average Episode Length = 137.72553430821148
INFO:tools.evaluation_results_class:Counted Episodes = 889
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -115.49150848388672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 203.42129516601562
INFO:tools.evaluation_results_class:Average Discounted Reward = 90.82147216796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9966024915062288
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12641.31640625
INFO:tools.evaluation_results_class:Current Best Return = -115.49150848388672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9966024915062288
INFO:tools.evaluation_results_class:Average Episode Length = 141.00113250283127
INFO:tools.evaluation_results_class:Counted Episodes = 883
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -121.10213470458984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 198.53872680664062
INFO:tools.evaluation_results_class:Average Discounted Reward = 87.37494659423828
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988776655443322
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13451.62109375
INFO:tools.evaluation_results_class:Current Best Return = -121.10213470458984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988776655443322
INFO:tools.evaluation_results_class:Average Episode Length = 140.47250280583614
INFO:tools.evaluation_results_class:Counted Episodes = 891
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -115.54292297363281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 204.100341796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 89.68549346923828
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988851727982163
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12598.919921875
INFO:tools.evaluation_results_class:Current Best Return = -115.54292297363281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988851727982163
INFO:tools.evaluation_results_class:Average Episode Length = 140.99777034559642
INFO:tools.evaluation_results_class:Counted Episodes = 897
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.54515075683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.0980987548828
INFO:tools.evaluation_results_class:Average Discounted Reward = 127.43647766113281
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988851727982163
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2284.89453125
INFO:tools.evaluation_results_class:Current Best Return = -64.54515075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988851727982163
INFO:tools.evaluation_results_class:Average Episode Length = 139.50947603121517
INFO:tools.evaluation_results_class:Counted Episodes = 897
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.89405059814453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.72500610351562
INFO:tools.evaluation_results_class:Average Discounted Reward = 116.15225219726562
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988095238095238
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3550.0947265625
INFO:tools.evaluation_results_class:Current Best Return = -74.89405059814453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988095238095238
INFO:tools.evaluation_results_class:Average Episode Length = 148.3797619047619
INFO:tools.evaluation_results_class:Counted Episodes = 840
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.30257415771484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.01072692871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 129.43685913085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9978540772532188
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3069.08642578125
INFO:tools.evaluation_results_class:Current Best Return = -69.30257415771484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978540772532188
INFO:tools.evaluation_results_class:Average Episode Length = 133.89055793991417
INFO:tools.evaluation_results_class:Counted Episodes = 932
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -63.28839874267578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.3580017089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 129.646240234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988950276243094
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2184.8994140625
INFO:tools.evaluation_results_class:Current Best Return = -63.28839874267578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988950276243094
INFO:tools.evaluation_results_class:Average Episode Length = 138.86077348066297
INFO:tools.evaluation_results_class:Counted Episodes = 905
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.94279479980469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.32493591308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 125.4149169921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9977116704805492
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2216.896240234375
INFO:tools.evaluation_results_class:Current Best Return = -64.94279479980469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9977116704805492
INFO:tools.evaluation_results_class:Average Episode Length = 141.36613272311212
INFO:tools.evaluation_results_class:Counted Episodes = 874
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -87.67476654052734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 231.95486450195312
INFO:tools.evaluation_results_class:Average Discounted Reward = 110.70825958251953
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988425925925926
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6855.5126953125
INFO:tools.evaluation_results_class:Current Best Return = -87.67476654052734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988425925925926
INFO:tools.evaluation_results_class:Average Episode Length = 142.28587962962962
INFO:tools.evaluation_results_class:Counted Episodes = 864
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -94.8159408569336
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 224.05862426757812
INFO:tools.evaluation_results_class:Average Discounted Reward = 102.59794616699219
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9964830011723329
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7436.06103515625
INFO:tools.evaluation_results_class:Current Best Return = -94.8159408569336
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9964830011723329
INFO:tools.evaluation_results_class:Average Episode Length = 148.80539273153576
INFO:tools.evaluation_results_class:Counted Episodes = 853
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -89.87641906738281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.76077270507812
INFO:tools.evaluation_results_class:Average Discounted Reward = 106.3843765258789
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988662131519275
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6595.41455078125
INFO:tools.evaluation_results_class:Current Best Return = -89.87641906738281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988662131519275
INFO:tools.evaluation_results_class:Average Episode Length = 143.17120181405895
INFO:tools.evaluation_results_class:Counted Episodes = 882
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -89.35651397705078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 230.290283203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 109.58622741699219
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988962472406181
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6486.5185546875
INFO:tools.evaluation_results_class:Current Best Return = -89.35651397705078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988962472406181
INFO:tools.evaluation_results_class:Average Episode Length = 138.9746136865342
INFO:tools.evaluation_results_class:Counted Episodes = 906
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -89.39408111572266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.8769989013672
INFO:tools.evaluation_results_class:Average Discounted Reward = 109.08570861816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9977220956719818
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6416.3505859375
INFO:tools.evaluation_results_class:Current Best Return = -89.39408111572266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9977220956719818
INFO:tools.evaluation_results_class:Average Episode Length = 141.8382687927107
INFO:tools.evaluation_results_class:Counted Episodes = 878
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -86.28325653076172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.3497772216797
INFO:tools.evaluation_results_class:Average Discounted Reward = 111.5309066772461
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988532110091743
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6928.78564453125
INFO:tools.evaluation_results_class:Current Best Return = -86.28325653076172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988532110091743
INFO:tools.evaluation_results_class:Average Episode Length = 142.99426605504587
INFO:tools.evaluation_results_class:Counted Episodes = 872
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -90.01811981201172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.61947631835938
INFO:tools.evaluation_results_class:Average Discounted Reward = 108.40504455566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988674971687429
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6205.59912109375
INFO:tools.evaluation_results_class:Current Best Return = -90.01811981201172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988674971687429
INFO:tools.evaluation_results_class:Average Episode Length = 141.02038505096263
INFO:tools.evaluation_results_class:Counted Episodes = 883
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -86.46253967285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.1900177001953
INFO:tools.evaluation_results_class:Average Discounted Reward = 112.37474822998047
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.998914223669924
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5817.638671875
INFO:tools.evaluation_results_class:Current Best Return = -86.46253967285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.998914223669924
INFO:tools.evaluation_results_class:Average Episode Length = 138.24212812160695
INFO:tools.evaluation_results_class:Counted Episodes = 921
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -89.64752960205078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.98507690429688
INFO:tools.evaluation_results_class:Average Discounted Reward = 108.86003875732422
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988518943742825
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7802.830078125
INFO:tools.evaluation_results_class:Current Best Return = -89.64752960205078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988518943742825
INFO:tools.evaluation_results_class:Average Episode Length = 142.6222732491389
INFO:tools.evaluation_results_class:Counted Episodes = 871
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -90.7977523803711
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.20223999023438
INFO:tools.evaluation_results_class:Average Discounted Reward = 108.75791931152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6513.4912109375
INFO:tools.evaluation_results_class:Current Best Return = -90.7977523803711
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 140.9932584269663
INFO:tools.evaluation_results_class:Counted Episodes = 890
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -90.08944702148438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 228.80963134765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 109.22328186035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9965596330275229
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6647.296875
INFO:tools.evaluation_results_class:Current Best Return = -90.08944702148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9965596330275229
INFO:tools.evaluation_results_class:Average Episode Length = 141.6880733944954
INFO:tools.evaluation_results_class:Counted Episodes = 872
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -89.50113677978516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 230.4988555908203
INFO:tools.evaluation_results_class:Average Discounted Reward = 108.12511444091797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6843.587890625
INFO:tools.evaluation_results_class:Current Best Return = -89.50113677978516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 141.8132118451025
INFO:tools.evaluation_results_class:Counted Episodes = 878
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -93.46666717529297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 226.1590576171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 105.3920669555664
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988304093567252
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6138.123046875
INFO:tools.evaluation_results_class:Current Best Return = -93.46666717529297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988304093567252
INFO:tools.evaluation_results_class:Average Episode Length = 142.2608187134503
INFO:tools.evaluation_results_class:Counted Episodes = 855
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -86.09966278076172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.18365478515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 112.74783325195312
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9977603583426652
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5411.0146484375
INFO:tools.evaluation_results_class:Current Best Return = -86.09966278076172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9977603583426652
INFO:tools.evaluation_results_class:Average Episode Length = 140.4434490481523
INFO:tools.evaluation_results_class:Counted Episodes = 893
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -95.7811050415039
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 223.85023498535156
INFO:tools.evaluation_results_class:Average Discounted Reward = 102.66217041015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988479262672811
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7130.85302734375
INFO:tools.evaluation_results_class:Current Best Return = -95.7811050415039
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988479262672811
INFO:tools.evaluation_results_class:Average Episode Length = 145.036866359447
INFO:tools.evaluation_results_class:Counted Episodes = 868
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -92.44393920898438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 227.55606079101562
INFO:tools.evaluation_results_class:Average Discounted Reward = 108.92874145507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7708.0634765625
INFO:tools.evaluation_results_class:Current Best Return = -92.44393920898438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 142.51487414187642
INFO:tools.evaluation_results_class:Counted Episodes = 874
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -96.28653717041016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 222.97698974609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 104.3348159790039
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.997698504027618
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9272.8125
INFO:tools.evaluation_results_class:Current Best Return = -96.28653717041016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.997698504027618
INFO:tools.evaluation_results_class:Average Episode Length = 144.64672036823936
INFO:tools.evaluation_results_class:Counted Episodes = 869
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -87.5298843383789
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 231.73448181152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 109.95613098144531
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9977011494252873
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6187.8837890625
INFO:tools.evaluation_results_class:Current Best Return = -87.5298843383789
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9977011494252873
INFO:tools.evaluation_results_class:Average Episode Length = 144.05862068965519
INFO:tools.evaluation_results_class:Counted Episodes = 870
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -93.38782501220703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 225.46658325195312
INFO:tools.evaluation_results_class:Average Discounted Reward = 105.27094268798828
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9964200477326969
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6920.26611328125
INFO:tools.evaluation_results_class:Current Best Return = -93.38782501220703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9964200477326969
INFO:tools.evaluation_results_class:Average Episode Length = 146.76491646778044
INFO:tools.evaluation_results_class:Counted Episodes = 838
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -99.96818542480469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 218.5772705078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 100.66193389892578
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9954545454545455
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10113.265625
INFO:tools.evaluation_results_class:Current Best Return = -99.96818542480469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9954545454545455
INFO:tools.evaluation_results_class:Average Episode Length = 146.5772727272727
INFO:tools.evaluation_results_class:Counted Episodes = 880
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -95.58382415771484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 224.4161834716797
INFO:tools.evaluation_results_class:Average Discounted Reward = 101.54195404052734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8285.83984375
INFO:tools.evaluation_results_class:Current Best Return = -95.58382415771484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 148.64009378663542
INFO:tools.evaluation_results_class:Counted Episodes = 853
INFO:robust_rl.robust_rl_trainer:Iteration 22 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -93.828369140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 224.64601135253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 105.31458282470703
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9952324195470799
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7267.79736328125
INFO:tools.evaluation_results_class:Current Best Return = -93.25944519042969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9956656346749226
INFO:tools.evaluation_results_class:Average Episode Length = 148.2467222884386
INFO:tools.evaluation_results_class:Counted Episodes = 1678
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.4628753662109375
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 6.15547513961792
INFO:agents.father_agent:Step: 10, Training loss: 7.080652236938477
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -74.04573822021484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.95425415039062
INFO:tools.evaluation_results_class:Average Discounted Reward = 139.1153106689453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5244.7294921875
INFO:tools.evaluation_results_class:Current Best Return = -74.04573822021484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.133506879881
INFO:tools.evaluation_results_class:Counted Episodes = 2689
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -105.06339263916016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 214.9365997314453
INFO:tools.evaluation_results_class:Average Discounted Reward = 105.62291717529297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11157.296875
INFO:tools.evaluation_results_class:Current Best Return = -105.06339263916016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 118.3799591002045
INFO:tools.evaluation_results_class:Counted Episodes = 2445
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.440185546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.559814453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 138.404052734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4989.80224609375
INFO:tools.evaluation_results_class:Current Best Return = -73.440185546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.57987106017193
INFO:tools.evaluation_results_class:Counted Episodes = 2792
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 84.60203846359475
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 23 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.62885284423828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.37115478515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 140.5762176513672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5030.9462890625
INFO:tools.evaluation_results_class:Current Best Return = -72.62885284423828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.0954169797145
INFO:tools.evaluation_results_class:Counted Episodes = 2662
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.209751605987549
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 5.9335713386535645
INFO:agents.father_agent:Step: 10, Training loss: 5.846179485321045
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -64.07132720947266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.9286651611328
INFO:tools.evaluation_results_class:Average Discounted Reward = 161.5421600341797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4543.111328125
INFO:tools.evaluation_results_class:Current Best Return = -64.07132720947266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.04493119910137
INFO:tools.evaluation_results_class:Counted Episodes = 3561
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.70015716552734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.2998504638672
INFO:tools.evaluation_results_class:Average Discounted Reward = 136.0041961669922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7253.888671875
INFO:tools.evaluation_results_class:Current Best Return = -83.70015716552734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.00282574568288
INFO:tools.evaluation_results_class:Counted Episodes = 3185
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.53123474121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.46876525878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 160.7591094970703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4477.93798828125
INFO:tools.evaluation_results_class:Current Best Return = -64.53123474121094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.56465753424658
INFO:tools.evaluation_results_class:Counted Episodes = 3650
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 75.36743905738331
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 24 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -60.69315719604492
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.3068542480469
INFO:tools.evaluation_results_class:Average Discounted Reward = 165.8130645751953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3911.211181640625
INFO:tools.evaluation_results_class:Current Best Return = -60.69315719604492
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.36534216335541
INFO:tools.evaluation_results_class:Counted Episodes = 3624
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.145014762878418
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 7.95371150970459
INFO:agents.father_agent:Step: 10, Training loss: 9.16622543334961
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -56.8162956237793
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.1837158203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.38980102539062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3635.546142578125
INFO:tools.evaluation_results_class:Current Best Return = -56.8162956237793
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.51973684210526
INFO:tools.evaluation_results_class:Counted Episodes = 3952
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.66075134277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.33924865722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 151.90931701660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5224.89453125
INFO:tools.evaluation_results_class:Current Best Return = -72.66075134277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.12640765765765
INFO:tools.evaluation_results_class:Counted Episodes = 3552
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.22927474975586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.7707214355469
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.72601318359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3424.139892578125
INFO:tools.evaluation_results_class:Current Best Return = -58.22927474975586
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.40344403444034
INFO:tools.evaluation_results_class:Counted Episodes = 4065
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 71.27791206411469
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 25 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.15869903564453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.84130859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.15061950683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3717.853271484375
INFO:tools.evaluation_results_class:Current Best Return = -56.8162956237793
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.8685857321652
INFO:tools.evaluation_results_class:Counted Episodes = 3995
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.27530574798584
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 7.661364555358887
INFO:agents.father_agent:Step: 10, Training loss: 8.935089111328125
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -53.877418518066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.1225891113281
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.507568359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3050.549560546875
INFO:tools.evaluation_results_class:Current Best Return = -53.877418518066406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.93770409444863
INFO:tools.evaluation_results_class:Counted Episodes = 3981
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.8427963256836
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.15719604492188
INFO:tools.evaluation_results_class:Average Discounted Reward = 151.299560546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5261.94189453125
INFO:tools.evaluation_results_class:Current Best Return = -70.8427963256836
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.03499713138267
INFO:tools.evaluation_results_class:Counted Episodes = 3486
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.136131286621094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.8638610839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.34571838378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3363.873046875
INFO:tools.evaluation_results_class:Current Best Return = -56.136131286621094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.23227863625215
INFO:tools.evaluation_results_class:Counted Episodes = 4077
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 69.25565096374685
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 26 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.22008514404297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.7799072265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 176.5407257080078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3084.389892578125
INFO:tools.evaluation_results_class:Current Best Return = -53.877418518066406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.64901324006995
INFO:tools.evaluation_results_class:Counted Episodes = 4003
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.982658863067627
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 7.961490631103516
INFO:agents.father_agent:Step: 10, Training loss: 7.699216842651367
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -52.226497650146484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.77349853515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.19908142089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2811.237060546875
INFO:tools.evaluation_results_class:Current Best Return = -52.226497650146484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.91953400162558
INFO:tools.evaluation_results_class:Counted Episodes = 3691
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.45829772949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.5417022705078
INFO:tools.evaluation_results_class:Average Discounted Reward = 144.67092895507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4891.255859375
INFO:tools.evaluation_results_class:Current Best Return = -72.45829772949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.11496407372697
INFO:tools.evaluation_results_class:Counted Episodes = 3201
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.7227783203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.2772216796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 169.79006958007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3097.6279296875
INFO:tools.evaluation_results_class:Current Best Return = -55.7227783203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.65354122621565
INFO:tools.evaluation_results_class:Counted Episodes = 3784
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 69.78024838614546
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -47.107051849365234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.8929443359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 168.05699157714844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1803.71630859375
INFO:tools.evaluation_results_class:Current Best Return = -47.107051849365234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.53022670025189
INFO:tools.evaluation_results_class:Counted Episodes = 1588
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.80076599121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.19921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 161.58168029785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2670.4111328125
INFO:tools.evaluation_results_class:Current Best Return = -53.80076599121094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.12388250319285
INFO:tools.evaluation_results_class:Counted Episodes = 1566
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.52165985107422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.47833251953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 162.052978515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3072.076171875
INFO:tools.evaluation_results_class:Current Best Return = -56.52165985107422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.65160891089108
INFO:tools.evaluation_results_class:Counted Episodes = 1616
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -47.79462432861328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.20538330078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.79742431640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1930.83251953125
INFO:tools.evaluation_results_class:Current Best Return = -47.79462432861328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.8208573256558
INFO:tools.evaluation_results_class:Counted Episodes = 1563
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.99174499511719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.00823974609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 166.4326629638672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2026.7891845703125
INFO:tools.evaluation_results_class:Current Best Return = -48.99174499511719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.05079365079365
INFO:tools.evaluation_results_class:Counted Episodes = 1575
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.34088134765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.65911865234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 147.28524780273438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5004.5205078125
INFO:tools.evaluation_results_class:Current Best Return = -69.34088134765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.00970245795601
INFO:tools.evaluation_results_class:Counted Episodes = 1546
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.25177001953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.74822998046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 150.2718505859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4510.111328125
INFO:tools.evaluation_results_class:Current Best Return = -66.25177001953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.0398201669878
INFO:tools.evaluation_results_class:Counted Episodes = 1557
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.596923828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.403076171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 146.73028564453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4648.80029296875
INFO:tools.evaluation_results_class:Current Best Return = -67.596923828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.93248663101605
INFO:tools.evaluation_results_class:Counted Episodes = 1496
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.8703842163086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.12962341308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.943359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4976.75634765625
INFO:tools.evaluation_results_class:Current Best Return = -70.8703842163086
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.28969539857421
INFO:tools.evaluation_results_class:Counted Episodes = 1543
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.0361099243164
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.96388244628906
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.79031372070312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4641.65576171875
INFO:tools.evaluation_results_class:Current Best Return = -70.0361099243164
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.5252790544977
INFO:tools.evaluation_results_class:Counted Episodes = 1523
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.50749206542969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.4925079345703
INFO:tools.evaluation_results_class:Average Discounted Reward = 148.78103637695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4044.376220703125
INFO:tools.evaluation_results_class:Current Best Return = -67.50749206542969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.32312703583062
INFO:tools.evaluation_results_class:Counted Episodes = 1535
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.96389770507812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.03610229492188
INFO:tools.evaluation_results_class:Average Discounted Reward = 147.16943359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4688.84716796875
INFO:tools.evaluation_results_class:Current Best Return = -70.96389770507812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.25660863958737
INFO:tools.evaluation_results_class:Counted Episodes = 1551
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.44993591308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.55006408691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 148.5714874267578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4709.666015625
INFO:tools.evaluation_results_class:Current Best Return = -68.44993591308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.77086007702182
INFO:tools.evaluation_results_class:Counted Episodes = 1558
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.51087951660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.48912048339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 149.43333435058594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4280.796875
INFO:tools.evaluation_results_class:Current Best Return = -66.51087951660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.96967699406724
INFO:tools.evaluation_results_class:Counted Episodes = 1517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -73.66514587402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.33485412597656
INFO:tools.evaluation_results_class:Average Discounted Reward = 143.2185516357422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4902.24755859375
INFO:tools.evaluation_results_class:Current Best Return = -73.66514587402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.34791122715404
INFO:tools.evaluation_results_class:Counted Episodes = 1532
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.75276184082031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.2472381591797
INFO:tools.evaluation_results_class:Average Discounted Reward = 146.36643981933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4562.23046875
INFO:tools.evaluation_results_class:Current Best Return = -69.75276184082031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.1515940143136
INFO:tools.evaluation_results_class:Counted Episodes = 1537
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.97521209716797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.02479553222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 148.43399047851562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4010.581298828125
INFO:tools.evaluation_results_class:Current Best Return = -67.97521209716797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.40834964122635
INFO:tools.evaluation_results_class:Counted Episodes = 1533
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.49739074707031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.5026092529297
INFO:tools.evaluation_results_class:Average Discounted Reward = 147.45574951171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4670.42529296875
INFO:tools.evaluation_results_class:Current Best Return = -68.49739074707031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.31462140992167
INFO:tools.evaluation_results_class:Counted Episodes = 1532
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.75971221923828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.24029541015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 146.3904266357422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4281.73876953125
INFO:tools.evaluation_results_class:Current Best Return = -69.75971221923828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.17906517445688
INFO:tools.evaluation_results_class:Counted Episodes = 1519
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.4435043334961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.55648803710938
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.93606567382812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5322.35400390625
INFO:tools.evaluation_results_class:Current Best Return = -71.4435043334961
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.60584415584415
INFO:tools.evaluation_results_class:Counted Episodes = 1540
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.4562759399414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.54371643066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 146.94493103027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4551.388671875
INFO:tools.evaluation_results_class:Current Best Return = -69.4562759399414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.01314924391848
INFO:tools.evaluation_results_class:Counted Episodes = 1521
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.87928771972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.12071228027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.9574432373047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4689.70361328125
INFO:tools.evaluation_results_class:Current Best Return = -69.87928771972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.70382585751979
INFO:tools.evaluation_results_class:Counted Episodes = 1516
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.40444946289062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.59555053710938
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.4884796142578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4855.0966796875
INFO:tools.evaluation_results_class:Current Best Return = -71.40444946289062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.10340314136126
INFO:tools.evaluation_results_class:Counted Episodes = 1528
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.501953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.498046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 143.98313903808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5305.86181640625
INFO:tools.evaluation_results_class:Current Best Return = -72.501953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.34941329856584
INFO:tools.evaluation_results_class:Counted Episodes = 1534
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.47174835205078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.52825927734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.090576171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4389.8720703125
INFO:tools.evaluation_results_class:Current Best Return = -70.47174835205078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.96714848883049
INFO:tools.evaluation_results_class:Counted Episodes = 1522
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.31114196777344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.68885803222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 147.29391479492188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4410.115234375
INFO:tools.evaluation_results_class:Current Best Return = -68.31114196777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.8220171390903
INFO:tools.evaluation_results_class:Counted Episodes = 1517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.47683715820312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.52316284179688
INFO:tools.evaluation_results_class:Average Discounted Reward = 141.0841522216797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6065.86669921875
INFO:tools.evaluation_results_class:Current Best Return = -82.47683715820312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.10006176652254
INFO:tools.evaluation_results_class:Counted Episodes = 1619
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.73713684082031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.2628631591797
INFO:tools.evaluation_results_class:Average Discounted Reward = 139.7301788330078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5893.73779296875
INFO:tools.evaluation_results_class:Current Best Return = -83.73713684082031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.49845009299442
INFO:tools.evaluation_results_class:Counted Episodes = 1613
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.48546600341797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.5145263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 142.5104217529297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5791.2421875
INFO:tools.evaluation_results_class:Current Best Return = -81.48546600341797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.52628324056896
INFO:tools.evaluation_results_class:Counted Episodes = 1617
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.29975128173828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.7002410888672
INFO:tools.evaluation_results_class:Average Discounted Reward = 142.44935607910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5875.82666015625
INFO:tools.evaluation_results_class:Current Best Return = -81.29975128173828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.21375921375922
INFO:tools.evaluation_results_class:Counted Episodes = 1628
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.06049346923828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.9394989013672
INFO:tools.evaluation_results_class:Average Discounted Reward = 143.44284057617188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5255.85791015625
INFO:tools.evaluation_results_class:Current Best Return = -79.06049346923828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.69012345679012
INFO:tools.evaluation_results_class:Counted Episodes = 1620
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -40.06638717651367
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.9336242675781
INFO:tools.evaluation_results_class:Average Discounted Reward = 186.13702392578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1322.2388916015625
INFO:tools.evaluation_results_class:Current Best Return = -40.06638717651367
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 75.91359325605902
INFO:tools.evaluation_results_class:Counted Episodes = 1898
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -41.768436431884766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.2315673828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.87847900390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1732.838623046875
INFO:tools.evaluation_results_class:Current Best Return = -41.768436431884766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.61334056399133
INFO:tools.evaluation_results_class:Counted Episodes = 1844
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -44.630516052246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.3694763183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 181.8121337890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1757.0750732421875
INFO:tools.evaluation_results_class:Current Best Return = -44.630516052246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 76.7745879851143
INFO:tools.evaluation_results_class:Counted Episodes = 1881
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -39.75146484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.24853515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.47979736328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1372.145263671875
INFO:tools.evaluation_results_class:Current Best Return = -39.75146484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 76.76636508781267
INFO:tools.evaluation_results_class:Counted Episodes = 1879
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -40.24445724487305
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.75555419921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 185.88169860839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1388.2109375
INFO:tools.evaluation_results_class:Current Best Return = -40.24445724487305
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 75.56441393875396
INFO:tools.evaluation_results_class:Counted Episodes = 1894
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.33296203613281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.66705322265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 175.27734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2278.09033203125
INFO:tools.evaluation_results_class:Current Best Return = -48.33296203613281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.6659217877095
INFO:tools.evaluation_results_class:Counted Episodes = 1790
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.7889518737793
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.2110595703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.37603759765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2508.9541015625
INFO:tools.evaluation_results_class:Current Best Return = -50.7889518737793
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.29171270718233
INFO:tools.evaluation_results_class:Counted Episodes = 1810
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.8777961730957
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.1221923828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.99778747558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2431.95361328125
INFO:tools.evaluation_results_class:Current Best Return = -49.8777961730957
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.44462629569013
INFO:tools.evaluation_results_class:Counted Episodes = 1833
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.543216705322266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.456787109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.55215454101562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2763.499755859375
INFO:tools.evaluation_results_class:Current Best Return = -52.543216705322266
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.86378555798687
INFO:tools.evaluation_results_class:Counted Episodes = 1828
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.09526443481445
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.90472412109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.707763671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2762.9892578125
INFO:tools.evaluation_results_class:Current Best Return = -52.09526443481445
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.82323788546256
INFO:tools.evaluation_results_class:Counted Episodes = 1816
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.13793182373047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.862060546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.73814392089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2944.62353515625
INFO:tools.evaluation_results_class:Current Best Return = -52.13793182373047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.75478927203065
INFO:tools.evaluation_results_class:Counted Episodes = 1827
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.23532485961914
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.7646789550781
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.03627014160156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3098.796875
INFO:tools.evaluation_results_class:Current Best Return = -54.23532485961914
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.50442967884828
INFO:tools.evaluation_results_class:Counted Episodes = 1806
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.69369888305664
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.3063049316406
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.28639221191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2777.091552734375
INFO:tools.evaluation_results_class:Current Best Return = -52.69369888305664
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.69424657534246
INFO:tools.evaluation_results_class:Counted Episodes = 1825
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.200435638427734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.799560546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.94345092773438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2625.2919921875
INFO:tools.evaluation_results_class:Current Best Return = -52.200435638427734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.33769063180829
INFO:tools.evaluation_results_class:Counted Episodes = 1836
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.550025939941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.4499816894531
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.0323486328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3162.729736328125
INFO:tools.evaluation_results_class:Current Best Return = -54.550025939941406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.25815367606413
INFO:tools.evaluation_results_class:Counted Episodes = 1809
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.121910095214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.8780822753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.87425231933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2923.13720703125
INFO:tools.evaluation_results_class:Current Best Return = -53.121910095214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.81823174080176
INFO:tools.evaluation_results_class:Counted Episodes = 1821
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.839935302734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.1600646972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.5441436767578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2646.044189453125
INFO:tools.evaluation_results_class:Current Best Return = -51.839935302734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.46149614961496
INFO:tools.evaluation_results_class:Counted Episodes = 1818
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.119384765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.880615234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.67127990722656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2841.800537109375
INFO:tools.evaluation_results_class:Current Best Return = -53.119384765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.02847754654984
INFO:tools.evaluation_results_class:Counted Episodes = 1826
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.09601593017578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.90399169921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.49227905273438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2835.989990234375
INFO:tools.evaluation_results_class:Current Best Return = -54.09601593017578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.33333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1833
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.310577392578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.6894226074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.8669891357422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2511.755126953125
INFO:tools.evaluation_results_class:Current Best Return = -52.310577392578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.57134862898712
INFO:tools.evaluation_results_class:Counted Episodes = 1787
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.515933990478516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.48406982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 173.47364807128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2924.5224609375
INFO:tools.evaluation_results_class:Current Best Return = -51.515933990478516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.57197802197803
INFO:tools.evaluation_results_class:Counted Episodes = 1820
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.876651763916016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.12335205078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 170.32801818847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2865.510498046875
INFO:tools.evaluation_results_class:Current Best Return = -54.876651763916016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.90748898678414
INFO:tools.evaluation_results_class:Counted Episodes = 1816
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.314361572265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.6856384277344
INFO:tools.evaluation_results_class:Average Discounted Reward = 174.37391662597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2575.902099609375
INFO:tools.evaluation_results_class:Current Best Return = -51.314361572265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.84498644986449
INFO:tools.evaluation_results_class:Counted Episodes = 1845
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.340110778808594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.6598815917969
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.78640747070312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2672.798095703125
INFO:tools.evaluation_results_class:Current Best Return = -52.340110778808594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.57197802197803
INFO:tools.evaluation_results_class:Counted Episodes = 1820
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.70033264160156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.2996826171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.77955627441406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2567.27294921875
INFO:tools.evaluation_results_class:Current Best Return = -51.70033264160156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.95971302428256
INFO:tools.evaluation_results_class:Counted Episodes = 1812
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.71483612060547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.28515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 172.44357299804688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2931.462158203125
INFO:tools.evaluation_results_class:Current Best Return = -52.71483612060547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.70714285714286
INFO:tools.evaluation_results_class:Counted Episodes = 1820
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.01809692382812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.98190307617188
INFO:tools.evaluation_results_class:Average Discounted Reward = 161.38540649414062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4345.1376953125
INFO:tools.evaluation_results_class:Current Best Return = -70.01809692382812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 74.5889348500517
INFO:tools.evaluation_results_class:Counted Episodes = 1934
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.57454681396484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.4254608154297
INFO:tools.evaluation_results_class:Average Discounted Reward = 161.8264923095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4242.6630859375
INFO:tools.evaluation_results_class:Current Best Return = -69.57454681396484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 74.65558441558441
INFO:tools.evaluation_results_class:Counted Episodes = 1925
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -71.58436584472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.41563415527344
INFO:tools.evaluation_results_class:Average Discounted Reward = 159.94830322265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4092.5263671875
INFO:tools.evaluation_results_class:Current Best Return = -71.58436584472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 74.33488612836439
INFO:tools.evaluation_results_class:Counted Episodes = 1932
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -69.01614379882812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.98385620117188
INFO:tools.evaluation_results_class:Average Discounted Reward = 161.55178833007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4176.67041015625
INFO:tools.evaluation_results_class:Current Best Return = -69.01614379882812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 74.97760416666667
INFO:tools.evaluation_results_class:Counted Episodes = 1920
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.30506896972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.69493103027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 162.58184814453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3979.896484375
INFO:tools.evaluation_results_class:Current Best Return = -68.30506896972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 74.72388831437435
INFO:tools.evaluation_results_class:Counted Episodes = 1934
INFO:robust_rl.robust_rl_trainer:Iteration 27 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.255985260009766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.7440185546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 171.88035583496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2721.15673828125
INFO:tools.evaluation_results_class:Current Best Return = -52.226497650146484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.56882480957563
INFO:tools.evaluation_results_class:Counted Episodes = 3676
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.784462928771973
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 6.829418182373047
INFO:agents.father_agent:Step: 10, Training loss: 7.030540466308594
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -52.88938903808594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.1106262207031
INFO:tools.evaluation_results_class:Average Discounted Reward = 165.70907592773438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2650.07763671875
INFO:tools.evaluation_results_class:Current Best Return = -52.226497650146484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.03527653213752
INFO:tools.evaluation_results_class:Counted Episodes = 3345
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.76593017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.23406982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 132.96278381347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5235.03076171875
INFO:tools.evaluation_results_class:Current Best Return = -76.76593017578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.12530799014432
INFO:tools.evaluation_results_class:Counted Episodes = 2841
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.51246643066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.4875183105469
INFO:tools.evaluation_results_class:Average Discounted Reward = 161.5345001220703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3178.272705078125
INFO:tools.evaluation_results_class:Current Best Return = -57.51246643066406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.62804341449106
INFO:tools.evaluation_results_class:Counted Episodes = 3409
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 71.02226339704382
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 28 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.659698486328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.3403015136719
INFO:tools.evaluation_results_class:Average Discounted Reward = 167.3934783935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2646.8818359375
INFO:tools.evaluation_results_class:Current Best Return = -52.226497650146484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.23491315866941
INFO:tools.evaluation_results_class:Counted Episodes = 3397
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.957721710205078
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 6.187954902648926
INFO:agents.father_agent:Step: 10, Training loss: 5.656558990478516
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -53.35616683959961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.6438293457031
INFO:tools.evaluation_results_class:Average Discounted Reward = 161.6463165283203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2561.653076171875
INFO:tools.evaluation_results_class:Current Best Return = -52.226497650146484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.59023152553124
INFO:tools.evaluation_results_class:Counted Episodes = 3153
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.00261688232422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.99737548828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 129.46487426757812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5038.77294921875
INFO:tools.evaluation_results_class:Current Best Return = -76.00261688232422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.40366629255519
INFO:tools.evaluation_results_class:Counted Episodes = 2673
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.334163665771484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.66583251953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 158.49758911132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2768.364990234375
INFO:tools.evaluation_results_class:Current Best Return = -56.334163665771484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.1519775770788
INFO:tools.evaluation_results_class:Counted Episodes = 3211
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 71.4270815197153
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 29 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.68122100830078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.31878662109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 161.5450897216797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2531.302978515625
INFO:tools.evaluation_results_class:Current Best Return = -52.226497650146484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.48603531300161
INFO:tools.evaluation_results_class:Counted Episodes = 3115
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.557435035705566
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 39.06%
INFO:agents.father_agent:Step: 5, Training loss: 5.833512306213379
INFO:agents.father_agent:Step: 10, Training loss: 6.10105037689209
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -51.51680374145508
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.4831848144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 162.19300842285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2308.717529296875
INFO:tools.evaluation_results_class:Current Best Return = -51.51680374145508
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.11092985318108
INFO:tools.evaluation_results_class:Counted Episodes = 3065
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.68351745605469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.3164825439453
INFO:tools.evaluation_results_class:Average Discounted Reward = 123.99877166748047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5442.65478515625
INFO:tools.evaluation_results_class:Current Best Return = -79.68351745605469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 113.14541213063764
INFO:tools.evaluation_results_class:Counted Episodes = 2572
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.696990966796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.3030090332031
INFO:tools.evaluation_results_class:Average Discounted Reward = 152.84254455566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2952.737060546875
INFO:tools.evaluation_results_class:Current Best Return = -58.696990966796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.10353953026795
INFO:tools.evaluation_results_class:Counted Episodes = 3023
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 69.93956456139362
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 30 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.413814544677734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.586181640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 162.40560913085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2445.460693359375
INFO:tools.evaluation_results_class:Current Best Return = -51.413814544677734
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.87540348612008
INFO:tools.evaluation_results_class:Counted Episodes = 3098
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.029604911804199
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 39.06%
INFO:agents.father_agent:Step: 5, Training loss: 5.177261829376221
INFO:agents.father_agent:Step: 10, Training loss: 5.870903968811035
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -50.170318603515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.8296813964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 162.96888732910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2435.06005859375
INFO:tools.evaluation_results_class:Current Best Return = -50.170318603515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.46224256292906
INFO:tools.evaluation_results_class:Counted Episodes = 3059
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -74.85763549804688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.14236450195312
INFO:tools.evaluation_results_class:Average Discounted Reward = 129.1498565673828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5238.85791015625
INFO:tools.evaluation_results_class:Current Best Return = -74.85763549804688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.99387676999618
INFO:tools.evaluation_results_class:Counted Episodes = 2613
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.7485466003418
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.25146484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 153.6099853515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3140.094482421875
INFO:tools.evaluation_results_class:Current Best Return = -59.7485466003418
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.96961861667744
INFO:tools.evaluation_results_class:Counted Episodes = 3094
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 71.80885423991474
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 31 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.36437225341797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.6356201171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 163.2109832763672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2207.35205078125
INFO:tools.evaluation_results_class:Current Best Return = -50.170318603515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.82864532816036
INFO:tools.evaluation_results_class:Counted Episodes = 3093
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.740549564361572
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 40.62%
INFO:agents.father_agent:Step: 5, Training loss: 5.418034076690674
INFO:agents.father_agent:Step: 10, Training loss: 4.805630207061768
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -47.92277908325195
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.07720947265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 165.03208923339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1919.2149658203125
INFO:tools.evaluation_results_class:Current Best Return = -47.92277908325195
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.73487773487774
INFO:tools.evaluation_results_class:Counted Episodes = 3108
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.57083129882812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.42916870117188
INFO:tools.evaluation_results_class:Average Discounted Reward = 132.76834106445312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4666.9013671875
INFO:tools.evaluation_results_class:Current Best Return = -72.57083129882812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.51341140914242
INFO:tools.evaluation_results_class:Counted Episodes = 2647
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.286376953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.713623046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 154.4892120361328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2741.412841796875
INFO:tools.evaluation_results_class:Current Best Return = -56.286376953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.47265495525356
INFO:tools.evaluation_results_class:Counted Episodes = 3017
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 65.90744796580172
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.97404479980469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.02593994140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 153.3138885498047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1650.5595703125
INFO:tools.evaluation_results_class:Current Best Return = -49.97404479980469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.90229007633587
INFO:tools.evaluation_results_class:Counted Episodes = 1310
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -51.02572250366211
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.9742736816406
INFO:tools.evaluation_results_class:Average Discounted Reward = 148.34535217285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1801.450927734375
INFO:tools.evaluation_results_class:Current Best Return = -51.02572250366211
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.90996784565917
INFO:tools.evaluation_results_class:Counted Episodes = 1244
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.51559066772461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.4844055175781
INFO:tools.evaluation_results_class:Average Discounted Reward = 149.43417358398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2593.59423828125
INFO:tools.evaluation_results_class:Current Best Return = -56.51559066772461
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.15817490494297
INFO:tools.evaluation_results_class:Counted Episodes = 1315
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.25208282470703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.7479248046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 151.74913024902344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2157.605712890625
INFO:tools.evaluation_results_class:Current Best Return = -52.25208282470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.46479939439818
INFO:tools.evaluation_results_class:Counted Episodes = 1321
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.535518646240234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.4644775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 151.4429931640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2224.401611328125
INFO:tools.evaluation_results_class:Current Best Return = -50.535518646240234
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.08118657298985
INFO:tools.evaluation_results_class:Counted Episodes = 1281
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.34674072265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.65325927734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 137.54185485839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3772.072021484375
INFO:tools.evaluation_results_class:Current Best Return = -64.34674072265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.49879324215607
INFO:tools.evaluation_results_class:Counted Episodes = 1243
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.62540435791016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.3745880126953
INFO:tools.evaluation_results_class:Average Discounted Reward = 136.3464813232422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3316.7314453125
INFO:tools.evaluation_results_class:Current Best Return = -64.62540435791016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.3584142394822
INFO:tools.evaluation_results_class:Counted Episodes = 1236
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -65.50357818603516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.4964141845703
INFO:tools.evaluation_results_class:Average Discounted Reward = 136.61550903320312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3528.807373046875
INFO:tools.evaluation_results_class:Current Best Return = -65.50357818603516
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.6268894192522
INFO:tools.evaluation_results_class:Counted Episodes = 1257
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.02629852294922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.9737091064453
INFO:tools.evaluation_results_class:Average Discounted Reward = 137.01559448242188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3359.732177734375
INFO:tools.evaluation_results_class:Current Best Return = -66.02629852294922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.10199203187251
INFO:tools.evaluation_results_class:Counted Episodes = 1255
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.51754760742188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.48245239257812
INFO:tools.evaluation_results_class:Average Discounted Reward = 136.5889129638672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3418.31494140625
INFO:tools.evaluation_results_class:Current Best Return = -66.51754760742188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.07097288676236
INFO:tools.evaluation_results_class:Counted Episodes = 1254
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.6863021850586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.31369018554688
INFO:tools.evaluation_results_class:Average Discounted Reward = 135.9071807861328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3335.371337890625
INFO:tools.evaluation_results_class:Current Best Return = -66.6863021850586
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.47929936305732
INFO:tools.evaluation_results_class:Counted Episodes = 1256
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.671142578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.328857421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 135.020751953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4162.20947265625
INFO:tools.evaluation_results_class:Current Best Return = -68.671142578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.23794466403162
INFO:tools.evaluation_results_class:Counted Episodes = 1265
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.30078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.69921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 138.7826385498047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3855.48681640625
INFO:tools.evaluation_results_class:Current Best Return = -66.30078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.70234375
INFO:tools.evaluation_results_class:Counted Episodes = 1280
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -68.46380615234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.53619384765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.91798400878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3944.380615234375
INFO:tools.evaluation_results_class:Current Best Return = -68.46380615234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.67700875099443
INFO:tools.evaluation_results_class:Counted Episodes = 1257
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.04299926757812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.95700073242188
INFO:tools.evaluation_results_class:Average Discounted Reward = 136.80552673339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3695.83349609375
INFO:tools.evaluation_results_class:Current Best Return = -67.04299926757812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.45113369820172
INFO:tools.evaluation_results_class:Counted Episodes = 1279
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.66829681396484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.3317108154297
INFO:tools.evaluation_results_class:Average Discounted Reward = 136.76820373535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3045.312255859375
INFO:tools.evaluation_results_class:Current Best Return = -64.66829681396484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.62265688671556
INFO:tools.evaluation_results_class:Counted Episodes = 1227
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.88372039794922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.11627197265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.51609802246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4035.834716796875
INFO:tools.evaluation_results_class:Current Best Return = -67.88372039794922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.06415396952687
INFO:tools.evaluation_results_class:Counted Episodes = 1247
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.8341293334961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.16586303710938
INFO:tools.evaluation_results_class:Average Discounted Reward = 135.19308471679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3711.7568359375
INFO:tools.evaluation_results_class:Current Best Return = -67.8341293334961
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.43221690590111
INFO:tools.evaluation_results_class:Counted Episodes = 1254
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.21422576904297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.7857666015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 136.40000915527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3358.464111328125
INFO:tools.evaluation_results_class:Current Best Return = -64.21422576904297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.95666394112837
INFO:tools.evaluation_results_class:Counted Episodes = 1223
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -67.38719940185547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.61279296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 136.0074462890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3586.66162109375
INFO:tools.evaluation_results_class:Current Best Return = -67.38719940185547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.5168
INFO:tools.evaluation_results_class:Counted Episodes = 1250
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.17755889892578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.8224334716797
INFO:tools.evaluation_results_class:Average Discounted Reward = 136.41026306152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3576.27685546875
INFO:tools.evaluation_results_class:Current Best Return = -66.17755889892578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.64326069410815
INFO:tools.evaluation_results_class:Counted Episodes = 1239
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -66.66639709472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.33360290527344
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.82432556152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3709.823486328125
INFO:tools.evaluation_results_class:Current Best Return = -66.66639709472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.95421095666394
INFO:tools.evaluation_results_class:Counted Episodes = 1223
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
2025-08-20 00:49:05.345255: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-20 00:49:05.347165: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 00:49:05.377554: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-20 00:49:05.377600: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-20 00:49:05.378864: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-20 00:49:05.384526: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 00:49:05.384733: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-20 00:49:05.925431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/obstacles-8-5/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/obstacles-8-5/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"penalty"}min=? [F ((x = 8) & (y = 8))] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 468 states and 975 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"rewardmodel_penalty"}min=? [F ((x = 8) & (y = 8))] 
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1219.537841796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1155.537841796875
INFO:tools.evaluation_results_class:Average Discounted Reward = -454.28485107421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.2
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 387004.96875
INFO:tools.evaluation_results_class:Current Best Return = -1219.537841796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.2
INFO:tools.evaluation_results_class:Average Episode Length = 557.4466019417475
INFO:tools.evaluation_results_class:Counted Episodes = 515
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 343.7737731933594
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Step: 5, Training loss: 7.961475849151611
INFO:agents.father_agent:Step: 10, Training loss: 7.786648750305176
INFO:agents.father_agent:Step: 15, Training loss: 7.343858242034912
INFO:agents.father_agent:Step: 20, Training loss: 6.732021808624268
INFO:agents.father_agent:Step: 25, Training loss: 11.171517372131348
INFO:agents.father_agent:Step: 30, Training loss: 7.0452165603637695
INFO:agents.father_agent:Step: 35, Training loss: 6.873876094818115
INFO:agents.father_agent:Step: 40, Training loss: 7.947027683258057
INFO:agents.father_agent:Step: 45, Training loss: 6.77626371383667
INFO:agents.father_agent:Step: 50, Training loss: 6.840181350708008
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 39.06%
INFO:agents.father_agent:Step: 55, Training loss: 11.294228553771973
INFO:agents.father_agent:Step: 60, Training loss: 7.90299129486084
INFO:agents.father_agent:Step: 65, Training loss: 6.326085090637207
INFO:agents.father_agent:Step: 70, Training loss: 5.972774505615234
INFO:agents.father_agent:Step: 75, Training loss: 6.679168224334717
INFO:agents.father_agent:Step: 80, Training loss: 7.191692352294922
INFO:agents.father_agent:Step: 85, Training loss: 8.90346622467041
INFO:agents.father_agent:Step: 90, Training loss: 7.037497043609619
INFO:agents.father_agent:Step: 95, Training loss: 7.770709991455078
INFO:agents.father_agent:Step: 100, Training loss: 5.1002984046936035
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -502.7544250488281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -235.46029663085938
INFO:tools.evaluation_results_class:Average Discounted Reward = -202.5425262451172
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8352941176470589
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 114496.9296875
INFO:tools.evaluation_results_class:Current Best Return = -502.7544250488281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8352941176470589
INFO:tools.evaluation_results_class:Average Episode Length = 348.7147058823529
INFO:tools.evaluation_results_class:Counted Episodes = 680
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 43.75%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -513.2682495117188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -244.29656982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = -209.76904296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8405365126676602
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 125013.171875
INFO:tools.evaluation_results_class:Current Best Return = -502.7544250488281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8405365126676602
INFO:tools.evaluation_results_class:Average Episode Length = 344.7853949329359
INFO:tools.evaluation_results_class:Counted Episodes = 671
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -788.26953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -621.4609985351562
INFO:tools.evaluation_results_class:Average Discounted Reward = -328.12994384765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.5212765957446809
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 222224.25
INFO:tools.evaluation_results_class:Current Best Return = -788.26953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.5212765957446809
INFO:tools.evaluation_results_class:Average Episode Length = 467.78014184397165
INFO:tools.evaluation_results_class:Counted Episodes = 564
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -500.3711853027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -220.3711700439453
INFO:tools.evaluation_results_class:Average Discounted Reward = -203.46588134765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.875
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 123140.0234375
INFO:tools.evaluation_results_class:Current Best Return = -500.3711853027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.875
INFO:tools.evaluation_results_class:Average Episode Length = 333.63520408163265
INFO:tools.evaluation_results_class:Counted Episodes = 784
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 790.8678633329217
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -814.6029663085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -644.0147094726562
INFO:tools.evaluation_results_class:Average Discounted Reward = -309.428955078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.5330882352941176
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 204520.03125
INFO:tools.evaluation_results_class:Current Best Return = -814.6029663085938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.5330882352941176
INFO:tools.evaluation_results_class:Average Episode Length = 467.70588235294116
INFO:tools.evaluation_results_class:Counted Episodes = 272
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -767.5447998046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -593.2164306640625
INFO:tools.evaluation_results_class:Average Discounted Reward = -319.9006042480469
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.5447761194029851
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 225494.09375
INFO:tools.evaluation_results_class:Current Best Return = -767.5447998046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.5447761194029851
INFO:tools.evaluation_results_class:Average Episode Length = 470.95522388059703
INFO:tools.evaluation_results_class:Counted Episodes = 268
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -797.7122192382812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -630.8057250976562
INFO:tools.evaluation_results_class:Average Discounted Reward = -308.917724609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.5215827338129496
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 203849.265625
INFO:tools.evaluation_results_class:Current Best Return = -797.7122192382812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.5215827338129496
INFO:tools.evaluation_results_class:Average Episode Length = 465.9640287769784
INFO:tools.evaluation_results_class:Counted Episodes = 278
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -756.6317749023438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -572.949462890625
INFO:tools.evaluation_results_class:Average Discounted Reward = -288.9869689941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.5740072202166066
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 208014.15625
INFO:tools.evaluation_results_class:Current Best Return = -756.6317749023438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.5740072202166066
INFO:tools.evaluation_results_class:Average Episode Length = 455.16606498194943
INFO:tools.evaluation_results_class:Counted Episodes = 277
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -764.0364990234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -594.6934204101562
INFO:tools.evaluation_results_class:Average Discounted Reward = -301.44427490234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.5291970802919708
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 212627.265625
INFO:tools.evaluation_results_class:Current Best Return = -764.0364990234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.5291970802919708
INFO:tools.evaluation_results_class:Average Episode Length = 463.6861313868613
INFO:tools.evaluation_results_class:Counted Episodes = 274
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -989.6749267578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -822.3250732421875
INFO:tools.evaluation_results_class:Average Discounted Reward = -442.03125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.5229681978798587
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 314177.3125
INFO:tools.evaluation_results_class:Current Best Return = -989.6749267578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.5229681978798587
INFO:tools.evaluation_results_class:Average Episode Length = 464.30035335689047
INFO:tools.evaluation_results_class:Counted Episodes = 283
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -572.566650390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -294.2636413574219
INFO:tools.evaluation_results_class:Average Discounted Reward = -213.7516632080078
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8696969696969697
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 161953.984375
INFO:tools.evaluation_results_class:Current Best Return = -572.566650390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8696969696969697
INFO:tools.evaluation_results_class:Average Episode Length = 333.26060606060605
INFO:tools.evaluation_results_class:Counted Episodes = 330
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -456.77874755859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -180.20059204101562
INFO:tools.evaluation_results_class:Average Discounted Reward = -179.84994506835938
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8643067846607669
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 101790.4453125
INFO:tools.evaluation_results_class:Current Best Return = -456.77874755859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8643067846607669
INFO:tools.evaluation_results_class:Average Episode Length = 329.60766961651916
INFO:tools.evaluation_results_class:Counted Episodes = 339
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -543.612060546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -267.75
INFO:tools.evaluation_results_class:Average Discounted Reward = -220.00320434570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8620689655172413
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 153892.09375
INFO:tools.evaluation_results_class:Current Best Return = -543.612060546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8620689655172413
INFO:tools.evaluation_results_class:Average Episode Length = 333.52298850574715
INFO:tools.evaluation_results_class:Counted Episodes = 348
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -495.15789794921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -208.84210205078125
INFO:tools.evaluation_results_class:Average Discounted Reward = -185.32513427734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8947368421052632
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 154459.375
INFO:tools.evaluation_results_class:Current Best Return = -495.15789794921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8947368421052632
INFO:tools.evaluation_results_class:Average Episode Length = 317.5927977839335
INFO:tools.evaluation_results_class:Counted Episodes = 361
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -449.38067626953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -167.5625
INFO:tools.evaluation_results_class:Average Discounted Reward = -178.7560272216797
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8806818181818182
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 72417.953125
INFO:tools.evaluation_results_class:Current Best Return = -449.38067626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8806818181818182
INFO:tools.evaluation_results_class:Average Episode Length = 317.82954545454544
INFO:tools.evaluation_results_class:Counted Episodes = 352
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -647.4570922851562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -361.1412658691406
INFO:tools.evaluation_results_class:Average Discounted Reward = -288.5177917480469
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8947368421052632
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 171024.1875
INFO:tools.evaluation_results_class:Current Best Return = -647.4570922851562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8947368421052632
INFO:tools.evaluation_results_class:Average Episode Length = 313.93628808864264
INFO:tools.evaluation_results_class:Counted Episodes = 361
INFO:robust_rl.robust_rl_trainer:Iteration 2 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -540.4938354492188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -284.69134521484375
INFO:tools.evaluation_results_class:Average Discounted Reward = -219.66485595703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7993827160493827
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 154575.5625
INFO:tools.evaluation_results_class:Current Best Return = -502.7544250488281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8405365126676602
INFO:tools.evaluation_results_class:Average Episode Length = 351.98765432098764
INFO:tools.evaluation_results_class:Counted Episodes = 648
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.5444464683532715
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 39.06%
INFO:agents.father_agent:Step: 5, Training loss: 8.319490432739258
INFO:agents.father_agent:Step: 10, Training loss: 7.026744365692139
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -508.4460144042969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -223.90182495117188
INFO:tools.evaluation_results_class:Average Discounted Reward = -208.66322326660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8892005610098177
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 128318.015625
INFO:tools.evaluation_results_class:Current Best Return = -502.7544250488281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8892005610098177
INFO:tools.evaluation_results_class:Average Episode Length = 320.9747545582048
INFO:tools.evaluation_results_class:Counted Episodes = 713
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -786.7325439453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -599.2026977539062
INFO:tools.evaluation_results_class:Average Discounted Reward = -330.39886474609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.58603066439523
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 234400.921875
INFO:tools.evaluation_results_class:Current Best Return = -786.7325439453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.58603066439523
INFO:tools.evaluation_results_class:Average Episode Length = 447.4974446337308
INFO:tools.evaluation_results_class:Counted Episodes = 587
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -464.31561279296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -171.9279022216797
INFO:tools.evaluation_results_class:Average Discounted Reward = -181.68057250976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9137115839243499
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 116133.9375
INFO:tools.evaluation_results_class:Current Best Return = -464.31561279296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9137115839243499
INFO:tools.evaluation_results_class:Average Episode Length = 307.54137115839245
INFO:tools.evaluation_results_class:Counted Episodes = 846
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 728.8382239520843
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 3 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -544.9281005859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -263.87689208984375
INFO:tools.evaluation_results_class:Average Discounted Reward = -223.9488525390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8782849239280774
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 163361.453125
INFO:tools.evaluation_results_class:Current Best Return = -502.7544250488281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8892005610098177
INFO:tools.evaluation_results_class:Average Episode Length = 315.31258644536655
INFO:tools.evaluation_results_class:Counted Episodes = 723
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.027258396148682
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Step: 5, Training loss: 8.446663856506348
INFO:agents.father_agent:Step: 10, Training loss: 6.65123176574707
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -475.11041259765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -178.11424255371094
INFO:tools.evaluation_results_class:Average Discounted Reward = -186.14874267578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9281129653401797
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 121741.2890625
INFO:tools.evaluation_results_class:Current Best Return = -475.11041259765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9281129653401797
INFO:tools.evaluation_results_class:Average Episode Length = 288.65083440308086
INFO:tools.evaluation_results_class:Counted Episodes = 779
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -780.3965454101562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -580.6724243164062
INFO:tools.evaluation_results_class:Average Discounted Reward = -336.9665222167969
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6241379310344828
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 225727.640625
INFO:tools.evaluation_results_class:Current Best Return = -780.3965454101562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6241379310344828
INFO:tools.evaluation_results_class:Average Episode Length = 441.7655172413793
INFO:tools.evaluation_results_class:Counted Episodes = 580
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -500.0605163574219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -204.9006805419922
INFO:tools.evaluation_results_class:Average Discounted Reward = -211.2650909423828
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9223744292237442
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 124377.671875
INFO:tools.evaluation_results_class:Current Best Return = -500.0605163574219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9223744292237442
INFO:tools.evaluation_results_class:Average Episode Length = 297.1255707762557
INFO:tools.evaluation_results_class:Counted Episodes = 876
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 708.9464552219953
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 4 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -534.1417236328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -244.94384765625
INFO:tools.evaluation_results_class:Average Discounted Reward = -217.13014221191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9037433155080213
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 158643.875
INFO:tools.evaluation_results_class:Current Best Return = -475.11041259765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9281129653401797
INFO:tools.evaluation_results_class:Average Episode Length = 307.45454545454544
INFO:tools.evaluation_results_class:Counted Episodes = 748
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.737281322479248
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Step: 5, Training loss: 10.860891342163086
INFO:agents.father_agent:Step: 10, Training loss: 7.628692150115967
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -479.7242736816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -175.8026885986328
INFO:tools.evaluation_results_class:Average Discounted Reward = -190.82723999023438
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9497549019607843
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 120270.9375
INFO:tools.evaluation_results_class:Current Best Return = -475.11041259765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9497549019607843
INFO:tools.evaluation_results_class:Average Episode Length = 275.6593137254902
INFO:tools.evaluation_results_class:Counted Episodes = 816
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -738.885498046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -526.7564697265625
INFO:tools.evaluation_results_class:Average Discounted Reward = -311.7863464355469
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6629032258064517
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 211275.96875
INFO:tools.evaluation_results_class:Current Best Return = -738.885498046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6629032258064517
INFO:tools.evaluation_results_class:Average Episode Length = 420.48064516129034
INFO:tools.evaluation_results_class:Counted Episodes = 620
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -445.9215393066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -133.86593627929688
INFO:tools.evaluation_results_class:Average Discounted Reward = -171.10678100585938
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9751737835153923
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 116159.8515625
INFO:tools.evaluation_results_class:Current Best Return = -445.9215393066406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9751737835153923
INFO:tools.evaluation_results_class:Average Episode Length = 264.3167825223436
INFO:tools.evaluation_results_class:Counted Episodes = 1007
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 638.4220642753395
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 5 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -494.1970520019531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -194.9559326171875
INFO:tools.evaluation_results_class:Average Discounted Reward = -202.41336059570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9351285189718482
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 140321.015625
INFO:tools.evaluation_results_class:Current Best Return = -475.11041259765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9497549019607843
INFO:tools.evaluation_results_class:Average Episode Length = 278.2827417380661
INFO:tools.evaluation_results_class:Counted Episodes = 817
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.247669696807861
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 39.06%
INFO:agents.father_agent:Step: 5, Training loss: 11.027739524841309
INFO:agents.father_agent:Step: 10, Training loss: 7.931899547576904
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -473.25567626953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -164.16477966308594
INFO:tools.evaluation_results_class:Average Discounted Reward = -186.4612274169922
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9659090909090909
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 130158.0546875
INFO:tools.evaluation_results_class:Current Best Return = -473.25567626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9659090909090909
INFO:tools.evaluation_results_class:Average Episode Length = 260.08863636363634
INFO:tools.evaluation_results_class:Counted Episodes = 880
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -680.2568969726562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -441.4877014160156
INFO:tools.evaluation_results_class:Average Discounted Reward = -290.9815979003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7461538461538462
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 177688.09375
INFO:tools.evaluation_results_class:Current Best Return = -680.2568969726562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7461538461538462
INFO:tools.evaluation_results_class:Average Episode Length = 395.84
INFO:tools.evaluation_results_class:Counted Episodes = 650
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -449.3406982421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -134.64456176757812
INFO:tools.evaluation_results_class:Average Discounted Reward = -175.1721649169922
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9834254143646409
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 115300.625
INFO:tools.evaluation_results_class:Current Best Return = -449.3406982421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9834254143646409
INFO:tools.evaluation_results_class:Average Episode Length = 245.50460405156537
INFO:tools.evaluation_results_class:Counted Episodes = 1086
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 593.7417806740295
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 6 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -493.6148681640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -186.40066528320312
INFO:tools.evaluation_results_class:Average Discounted Reward = -192.8514862060547
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9600443951165372
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 136467.4375
INFO:tools.evaluation_results_class:Current Best Return = -473.25567626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9659090909090909
INFO:tools.evaluation_results_class:Average Episode Length = 257.1198668146504
INFO:tools.evaluation_results_class:Counted Episodes = 901
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.131959915161133
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 40.62%
INFO:agents.father_agent:Step: 5, Training loss: 11.580656051635742
INFO:agents.father_agent:Step: 10, Training loss: 8.57740306854248
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -425.8925476074219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -110.5392074584961
INFO:tools.evaluation_results_class:Average Discounted Reward = -155.20265197753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9854791868344628
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 110738.2578125
INFO:tools.evaluation_results_class:Current Best Return = -425.8925476074219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9854791868344628
INFO:tools.evaluation_results_class:Average Episode Length = 224.78702807357212
INFO:tools.evaluation_results_class:Counted Episodes = 1033
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -700.590087890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -445.9354248046875
INFO:tools.evaluation_results_class:Average Discounted Reward = -300.86053466796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7957957957957958
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 196965.59375
INFO:tools.evaluation_results_class:Current Best Return = -700.590087890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7957957957957958
INFO:tools.evaluation_results_class:Average Episode Length = 375.6336336336336
INFO:tools.evaluation_results_class:Counted Episodes = 666
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -414.78680419921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -99.0677261352539
INFO:tools.evaluation_results_class:Average Discounted Reward = -149.76319885253906
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9866220735785953
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 93132.9140625
INFO:tools.evaluation_results_class:Current Best Return = -414.78680419921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9866220735785953
INFO:tools.evaluation_results_class:Average Episode Length = 228.48996655518394
INFO:tools.evaluation_results_class:Counted Episodes = 1196
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 538.346020646949
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -635.5016479492188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -391.1003112792969
INFO:tools.evaluation_results_class:Average Discounted Reward = -241.84744262695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7637540453074434
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 167481.84375
INFO:tools.evaluation_results_class:Current Best Return = -635.5016479492188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7637540453074434
INFO:tools.evaluation_results_class:Average Episode Length = 383.8802588996764
INFO:tools.evaluation_results_class:Counted Episodes = 309
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -540.2222290039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -288.370361328125
INFO:tools.evaluation_results_class:Average Discounted Reward = -219.06822204589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7870370370370371
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 156938.796875
INFO:tools.evaluation_results_class:Current Best Return = -540.2222290039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7870370370370371
INFO:tools.evaluation_results_class:Average Episode Length = 361.4691358024691
INFO:tools.evaluation_results_class:Counted Episodes = 324
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -708.7583618164062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -470.369140625
INFO:tools.evaluation_results_class:Average Discounted Reward = -291.59478759765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7449664429530202
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 200547.59375
INFO:tools.evaluation_results_class:Current Best Return = -708.7583618164062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7449664429530202
INFO:tools.evaluation_results_class:Average Episode Length = 393.0
INFO:tools.evaluation_results_class:Counted Episodes = 298
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -604.5498657226562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -347.31512451171875
INFO:tools.evaluation_results_class:Average Discounted Reward = -226.6304473876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8038585209003215
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 205694.8125
INFO:tools.evaluation_results_class:Current Best Return = -604.5498657226562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8038585209003215
INFO:tools.evaluation_results_class:Average Episode Length = 372.60128617363347
INFO:tools.evaluation_results_class:Counted Episodes = 311
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -540.08251953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -281.034912109375
INFO:tools.evaluation_results_class:Average Discounted Reward = -215.1547088623047
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8095238095238095
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 110357.484375
INFO:tools.evaluation_results_class:Current Best Return = -540.08251953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8095238095238095
INFO:tools.evaluation_results_class:Average Episode Length = 369.92063492063494
INFO:tools.evaluation_results_class:Counted Episodes = 315
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -757.50634765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -498.6528625488281
INFO:tools.evaluation_results_class:Average Discounted Reward = -344.4204406738281
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8089171974522293
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 210406.796875
INFO:tools.evaluation_results_class:Current Best Return = -757.50634765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8089171974522293
INFO:tools.evaluation_results_class:Average Episode Length = 365.97452229299364
INFO:tools.evaluation_results_class:Counted Episodes = 314
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -682.1464233398438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -428.93768310546875
INFO:tools.evaluation_results_class:Average Discounted Reward = -296.936767578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7912772585669782
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 195684.90625
INFO:tools.evaluation_results_class:Current Best Return = -682.1464233398438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7912772585669782
INFO:tools.evaluation_results_class:Average Episode Length = 356.2398753894081
INFO:tools.evaluation_results_class:Counted Episodes = 321
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -770.354248046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -510.5423278808594
INFO:tools.evaluation_results_class:Average Discounted Reward = -324.6932373046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8119122257053292
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 246587.6875
INFO:tools.evaluation_results_class:Current Best Return = -770.354248046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8119122257053292
INFO:tools.evaluation_results_class:Average Episode Length = 369.30094043887146
INFO:tools.evaluation_results_class:Counted Episodes = 319
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -783.579833984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -529.24755859375
INFO:tools.evaluation_results_class:Average Discounted Reward = -352.3501281738281
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7947882736156352
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 238013.234375
INFO:tools.evaluation_results_class:Current Best Return = -783.579833984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7947882736156352
INFO:tools.evaluation_results_class:Average Episode Length = 368.60912052117266
INFO:tools.evaluation_results_class:Counted Episodes = 307
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -766.3819580078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -507.00311279296875
INFO:tools.evaluation_results_class:Average Discounted Reward = -329.6735534667969
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8105590062111802
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 247332.671875
INFO:tools.evaluation_results_class:Current Best Return = -766.3819580078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8105590062111802
INFO:tools.evaluation_results_class:Average Episode Length = 358.95652173913044
INFO:tools.evaluation_results_class:Counted Episodes = 322
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -719.3694458007812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -463.5732421875
INFO:tools.evaluation_results_class:Average Discounted Reward = -319.3735046386719
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7993630573248408
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 204565.953125
INFO:tools.evaluation_results_class:Current Best Return = -719.3694458007812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7993630573248408
INFO:tools.evaluation_results_class:Average Episode Length = 360.71974522292993
INFO:tools.evaluation_results_class:Counted Episodes = 314
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -357.6044006347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -41.120880126953125
INFO:tools.evaluation_results_class:Average Discounted Reward = -106.45726776123047
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.989010989010989
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 79047.2109375
INFO:tools.evaluation_results_class:Current Best Return = -357.6044006347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.989010989010989
INFO:tools.evaluation_results_class:Average Episode Length = 219.2783882783883
INFO:tools.evaluation_results_class:Counted Episodes = 546
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -263.2257995605469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.05376434326172
INFO:tools.evaluation_results_class:Average Discounted Reward = -63.49004364013672
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9946236559139785
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31691.26953125
INFO:tools.evaluation_results_class:Current Best Return = -263.2257995605469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9946236559139785
INFO:tools.evaluation_results_class:Average Episode Length = 209.37992831541217
INFO:tools.evaluation_results_class:Counted Episodes = 558
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -353.270751953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -33.8483772277832
INFO:tools.evaluation_results_class:Average Discounted Reward = -117.01091003417969
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9981949458483754
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 75582.921875
INFO:tools.evaluation_results_class:Current Best Return = -353.270751953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9981949458483754
INFO:tools.evaluation_results_class:Average Episode Length = 211.27436823104694
INFO:tools.evaluation_results_class:Counted Episodes = 554
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -304.6389465332031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.661947250366211
INFO:tools.evaluation_results_class:Average Discounted Reward = -77.27169799804688
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9946902654867257
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 78670.65625
INFO:tools.evaluation_results_class:Current Best Return = -304.6389465332031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9946902654867257
INFO:tools.evaluation_results_class:Average Episode Length = 208.2212389380531
INFO:tools.evaluation_results_class:Counted Episodes = 565
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -284.1819763183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.51171112060547
INFO:tools.evaluation_results_class:Average Discounted Reward = -73.16620635986328
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9927927927927928
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38980.73046875
INFO:tools.evaluation_results_class:Current Best Return = -284.1819763183594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9927927927927928
INFO:tools.evaluation_results_class:Average Episode Length = 212.06306306306305
INFO:tools.evaluation_results_class:Counted Episodes = 555
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -495.64642333984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -176.23573303222656
INFO:tools.evaluation_results_class:Average Discounted Reward = -192.3186798095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.998158379373849
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 115259.8671875
INFO:tools.evaluation_results_class:Current Best Return = -495.64642333984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.998158379373849
INFO:tools.evaluation_results_class:Average Episode Length = 210.39594843462245
INFO:tools.evaluation_results_class:Counted Episodes = 543
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -502.0108642578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -186.06884765625
INFO:tools.evaluation_results_class:Average Discounted Reward = -196.44117736816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9873188405797102
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 126985.1484375
INFO:tools.evaluation_results_class:Current Best Return = -502.0108642578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9873188405797102
INFO:tools.evaluation_results_class:Average Episode Length = 215.29710144927537
INFO:tools.evaluation_results_class:Counted Episodes = 552
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -505.35662841796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -187.12132263183594
INFO:tools.evaluation_results_class:Average Discounted Reward = -199.18753051757812
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9944852941176471
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 134539.125
INFO:tools.evaluation_results_class:Current Best Return = -505.35662841796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9944852941176471
INFO:tools.evaluation_results_class:Average Episode Length = 218.5808823529412
INFO:tools.evaluation_results_class:Counted Episodes = 544
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -482.6434631347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -163.7565155029297
INFO:tools.evaluation_results_class:Average Discounted Reward = -184.43504333496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9965217391304347
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 128893.0234375
INFO:tools.evaluation_results_class:Current Best Return = -482.6434631347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9965217391304347
INFO:tools.evaluation_results_class:Average Episode Length = 207.63304347826087
INFO:tools.evaluation_results_class:Counted Episodes = 575
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -484.3704528808594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -166.70620727539062
INFO:tools.evaluation_results_class:Average Discounted Reward = -181.33270263671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9927007299270073
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 109494.9765625
INFO:tools.evaluation_results_class:Current Best Return = -484.3704528808594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9927007299270073
INFO:tools.evaluation_results_class:Average Episode Length = 214.9890510948905
INFO:tools.evaluation_results_class:Counted Episodes = 548
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -496.7274475097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -181.25840759277344
INFO:tools.evaluation_results_class:Average Discounted Reward = -195.7888946533203
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9858407079646018
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 117973.3203125
INFO:tools.evaluation_results_class:Current Best Return = -496.7274475097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9858407079646018
INFO:tools.evaluation_results_class:Average Episode Length = 213.8495575221239
INFO:tools.evaluation_results_class:Counted Episodes = 565
INFO:robust_rl.robust_rl_trainer:Iteration 7 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -430.60076904296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -117.1124038696289
INFO:tools.evaluation_results_class:Average Discounted Reward = -161.8601531982422
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9796511627906976
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 96553.2421875
INFO:tools.evaluation_results_class:Current Best Return = -425.8925476074219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9854791868344628
INFO:tools.evaluation_results_class:Average Episode Length = 228.10852713178295
INFO:tools.evaluation_results_class:Counted Episodes = 1032
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.661331653594971
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 39.06%
INFO:agents.father_agent:Step: 5, Training loss: 8.938255310058594
INFO:agents.father_agent:Step: 10, Training loss: 8.77021598815918
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -402.7760009765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -83.58748626708984
INFO:tools.evaluation_results_class:Average Discounted Reward = -137.52687072753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9974640743871513
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 101603.3671875
INFO:tools.evaluation_results_class:Current Best Return = -402.7760009765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9974640743871513
INFO:tools.evaluation_results_class:Average Episode Length = 202.93913778529162
INFO:tools.evaluation_results_class:Counted Episodes = 1183
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -632.903564453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -367.11846923828125
INFO:tools.evaluation_results_class:Average Discounted Reward = -271.8484802246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8305785123966942
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 183296.859375
INFO:tools.evaluation_results_class:Current Best Return = -632.903564453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8305785123966942
INFO:tools.evaluation_results_class:Average Episode Length = 351.57300275482095
INFO:tools.evaluation_results_class:Counted Episodes = 726
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -404.97003173828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -85.9531478881836
INFO:tools.evaluation_results_class:Average Discounted Reward = -143.87278747558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9969278033794163
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 94441.6171875
INFO:tools.evaluation_results_class:Current Best Return = -404.97003173828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9969278033794163
INFO:tools.evaluation_results_class:Average Episode Length = 209.4162826420891
INFO:tools.evaluation_results_class:Counted Episodes = 1302
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 512.2857338529843
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 8 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -418.0414733886719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -99.14779663085938
INFO:tools.evaluation_results_class:Average Discounted Reward = -152.37510681152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.996542783059637
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 92526.078125
INFO:tools.evaluation_results_class:Current Best Return = -402.7760009765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9974640743871513
INFO:tools.evaluation_results_class:Average Episode Length = 208.5972342264477
INFO:tools.evaluation_results_class:Counted Episodes = 1157
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.931270599365234
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Step: 5, Training loss: 11.340252876281738
INFO:agents.father_agent:Step: 10, Training loss: 8.53188705444336
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -380.4914855957031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -61.23338317871094
INFO:tools.evaluation_results_class:Average Discounted Reward = -125.52265167236328
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9976816074188563
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 77641.0234375
INFO:tools.evaluation_results_class:Current Best Return = -380.4914855957031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9976816074188563
INFO:tools.evaluation_results_class:Average Episode Length = 191.9273570324575
INFO:tools.evaluation_results_class:Counted Episodes = 1294
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -571.7680053710938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -282.7357177734375
INFO:tools.evaluation_results_class:Average Discounted Reward = -244.79055786132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9032258064516129
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 140474.578125
INFO:tools.evaluation_results_class:Current Best Return = -571.7680053710938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9032258064516129
INFO:tools.evaluation_results_class:Average Episode Length = 315.1166253101737
INFO:tools.evaluation_results_class:Counted Episodes = 806
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -373.5465087890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -54.19823455810547
INFO:tools.evaluation_results_class:Average Discounted Reward = -119.21105194091797
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9979633401221996
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 81288.4375
INFO:tools.evaluation_results_class:Current Best Return = -373.5465087890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9979633401221996
INFO:tools.evaluation_results_class:Average Episode Length = 189.2620502376103
INFO:tools.evaluation_results_class:Counted Episodes = 1473
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 466.99757891484927
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 9 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -389.4686584472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -69.71943664550781
INFO:tools.evaluation_results_class:Average Discounted Reward = -130.2977294921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9992163009404389
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 85361.265625
INFO:tools.evaluation_results_class:Current Best Return = -380.4914855957031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9992163009404389
INFO:tools.evaluation_results_class:Average Episode Length = 190.50156739811914
INFO:tools.evaluation_results_class:Counted Episodes = 1276
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.854122161865234
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Step: 5, Training loss: 14.437159538269043
INFO:agents.father_agent:Step: 10, Training loss: 13.005873680114746
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -343.7777099609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -23.77770233154297
INFO:tools.evaluation_results_class:Average Discounted Reward = -98.31165313720703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 70944.7265625
INFO:tools.evaluation_results_class:Current Best Return = -343.7777099609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 172.18513513513514
INFO:tools.evaluation_results_class:Counted Episodes = 1480
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -564.5972900390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -264.6420593261719
INFO:tools.evaluation_results_class:Average Discounted Reward = -240.1292266845703
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9373601789709173
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 160107.84375
INFO:tools.evaluation_results_class:Current Best Return = -564.5972900390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9373601789709173
INFO:tools.evaluation_results_class:Average Episode Length = 292.3646532438479
INFO:tools.evaluation_results_class:Counted Episodes = 894
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -356.95819091796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -37.56618118286133
INFO:tools.evaluation_results_class:Average Discounted Reward = -107.83930969238281
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9981000633312223
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 71890.21875
INFO:tools.evaluation_results_class:Current Best Return = -356.95819091796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9981000633312223
INFO:tools.evaluation_results_class:Average Episode Length = 178.51234958834706
INFO:tools.evaluation_results_class:Counted Episodes = 1579
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 422.9483498383586
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 10 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -362.2745666503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -42.27456283569336
INFO:tools.evaluation_results_class:Average Discounted Reward = -108.78892517089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 72346.890625
INFO:tools.evaluation_results_class:Current Best Return = -343.7777099609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 172.07133243606998
INFO:tools.evaluation_results_class:Counted Episodes = 1486
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.475543022155762
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Step: 5, Training loss: 13.481897354125977
INFO:agents.father_agent:Step: 10, Training loss: 9.67802619934082
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -334.8999938964844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -14.899999618530273
INFO:tools.evaluation_results_class:Average Discounted Reward = -90.92381286621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 55113.671875
INFO:tools.evaluation_results_class:Current Best Return = -334.8999938964844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 160.65030674846625
INFO:tools.evaluation_results_class:Counted Episodes = 1630
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -538.4688110351562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -227.63970947265625
INFO:tools.evaluation_results_class:Average Discounted Reward = -219.2317352294922
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9713408393039918
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 126466.9921875
INFO:tools.evaluation_results_class:Current Best Return = -538.4688110351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9713408393039918
INFO:tools.evaluation_results_class:Average Episode Length = 269.15353121801434
INFO:tools.evaluation_results_class:Counted Episodes = 977
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -343.6007995605469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -23.783029556274414
INFO:tools.evaluation_results_class:Average Discounted Reward = -95.92119598388672
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9994305239179955
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 65585.1953125
INFO:tools.evaluation_results_class:Current Best Return = -343.6007995605469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9994305239179955
INFO:tools.evaluation_results_class:Average Episode Length = 161.47038724373576
INFO:tools.evaluation_results_class:Counted Episodes = 1756
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 401.8990274046901
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 11 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -348.2464294433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -28.24643898010254
INFO:tools.evaluation_results_class:Average Discounted Reward = -97.4085922241211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 70000.2265625
INFO:tools.evaluation_results_class:Current Best Return = -334.8999938964844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 160.44643962848298
INFO:tools.evaluation_results_class:Counted Episodes = 1615
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.424856185913086
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 5, Training loss: 12.02507209777832
INFO:agents.father_agent:Step: 10, Training loss: 12.979387283325195
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -312.8291931152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 7.170797348022461
INFO:tools.evaluation_results_class:Average Discounted Reward = -67.88180541992188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 53278.9609375
INFO:tools.evaluation_results_class:Current Best Return = -312.8291931152344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 142.82004310344828
INFO:tools.evaluation_results_class:Counted Episodes = 1856
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -487.92779541015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -173.6319122314453
INFO:tools.evaluation_results_class:Average Discounted Reward = -196.19395446777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.982174688057041
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 122996.6796875
INFO:tools.evaluation_results_class:Current Best Return = -487.92779541015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.982174688057041
INFO:tools.evaluation_results_class:Average Episode Length = 241.05525846702318
INFO:tools.evaluation_results_class:Counted Episodes = 1122
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -321.11871337890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1.2860878705978394
INFO:tools.evaluation_results_class:Average Discounted Reward = -76.72808837890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9994769874476988
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 56700.5703125
INFO:tools.evaluation_results_class:Current Best Return = -321.11871337890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9994769874476988
INFO:tools.evaluation_results_class:Average Episode Length = 149.82112970711296
INFO:tools.evaluation_results_class:Counted Episodes = 1912
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 362.6680865257141
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -398.3730163574219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -79.64286041259766
INFO:tools.evaluation_results_class:Average Discounted Reward = -137.00289916992188
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.996031746031746
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 82789.65625
INFO:tools.evaluation_results_class:Current Best Return = -398.3730163574219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.996031746031746
INFO:tools.evaluation_results_class:Average Episode Length = 231.15873015873015
INFO:tools.evaluation_results_class:Counted Episodes = 504
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -345.4686279296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -29.233333587646484
INFO:tools.evaluation_results_class:Average Discounted Reward = -126.09097290039062
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9882352941176471
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 65710.65625
INFO:tools.evaluation_results_class:Current Best Return = -345.4686279296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9882352941176471
INFO:tools.evaluation_results_class:Average Episode Length = 222.60392156862744
INFO:tools.evaluation_results_class:Counted Episodes = 510
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -394.47369384765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -82.89473724365234
INFO:tools.evaluation_results_class:Average Discounted Reward = -151.29507446289062
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9736842105263158
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 78623.875
INFO:tools.evaluation_results_class:Current Best Return = -394.47369384765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9736842105263158
INFO:tools.evaluation_results_class:Average Episode Length = 239.46153846153845
INFO:tools.evaluation_results_class:Counted Episodes = 494
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -341.2641906738281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -25.021526336669922
INFO:tools.evaluation_results_class:Average Discounted Reward = -108.35163116455078
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9882583170254403
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 80996.1953125
INFO:tools.evaluation_results_class:Current Best Return = -341.2641906738281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9882583170254403
INFO:tools.evaluation_results_class:Average Episode Length = 232.6908023483366
INFO:tools.evaluation_results_class:Counted Episodes = 511
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -343.045166015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -28.301847457885742
INFO:tools.evaluation_results_class:Average Discounted Reward = -113.24356842041016
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9835728952772074
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 57720.04296875
INFO:tools.evaluation_results_class:Current Best Return = -343.045166015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9835728952772074
INFO:tools.evaluation_results_class:Average Episode Length = 231.80082135523614
INFO:tools.evaluation_results_class:Counted Episodes = 487
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -530.2868041992188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -215.24806213378906
INFO:tools.evaluation_results_class:Average Discounted Reward = -223.91473388671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9844961240310077
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 114334.34375
INFO:tools.evaluation_results_class:Current Best Return = -530.2868041992188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9844961240310077
INFO:tools.evaluation_results_class:Average Episode Length = 229.71317829457365
INFO:tools.evaluation_results_class:Counted Episodes = 516
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -497.2832946777344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -178.4840545654297
INFO:tools.evaluation_results_class:Average Discounted Reward = -202.9300994873047
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9962476547842402
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 110374.5078125
INFO:tools.evaluation_results_class:Current Best Return = -497.2832946777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9962476547842402
INFO:tools.evaluation_results_class:Average Episode Length = 216.8686679174484
INFO:tools.evaluation_results_class:Counted Episodes = 533
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -531.4755249023438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -213.3542022705078
INFO:tools.evaluation_results_class:Average Discounted Reward = -214.12411499023438
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9941291585127201
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 157210.703125
INFO:tools.evaluation_results_class:Current Best Return = -531.4755249023438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9941291585127201
INFO:tools.evaluation_results_class:Average Episode Length = 233.84148727984345
INFO:tools.evaluation_results_class:Counted Episodes = 511
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -520.2633666992188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -209.13465881347656
INFO:tools.evaluation_results_class:Average Discounted Reward = -210.4467010498047
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9722772277227723
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 131728.171875
INFO:tools.evaluation_results_class:Current Best Return = -520.2633666992188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9722772277227723
INFO:tools.evaluation_results_class:Average Episode Length = 234.8851485148515
INFO:tools.evaluation_results_class:Counted Episodes = 505
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -527.962646484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -209.7570037841797
INFO:tools.evaluation_results_class:Average Discounted Reward = -211.39956665039062
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.994392523364486
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 119180.2265625
INFO:tools.evaluation_results_class:Current Best Return = -527.962646484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.994392523364486
INFO:tools.evaluation_results_class:Average Episode Length = 226.22242990654206
INFO:tools.evaluation_results_class:Counted Episodes = 535
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -524.8767700195312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -206.78529357910156
INFO:tools.evaluation_results_class:Average Discounted Reward = -210.78688049316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9940357852882704
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 146273.3125
INFO:tools.evaluation_results_class:Current Best Return = -524.8767700195312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9940357852882704
INFO:tools.evaluation_results_class:Average Episode Length = 226.0576540755467
INFO:tools.evaluation_results_class:Counted Episodes = 503
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -525.0323486328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -211.49696350097656
INFO:tools.evaluation_results_class:Average Discounted Reward = -209.37498474121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9797979797979798
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 133123.671875
INFO:tools.evaluation_results_class:Current Best Return = -525.0323486328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9797979797979798
INFO:tools.evaluation_results_class:Average Episode Length = 231.94949494949495
INFO:tools.evaluation_results_class:Counted Episodes = 495
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -528.6712036132812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -211.80235290527344
INFO:tools.evaluation_results_class:Average Discounted Reward = -213.6461944580078
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9902152641878669
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 116655.515625
INFO:tools.evaluation_results_class:Current Best Return = -528.6712036132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9902152641878669
INFO:tools.evaluation_results_class:Average Episode Length = 232.279843444227
INFO:tools.evaluation_results_class:Counted Episodes = 511
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -506.27203369140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -189.337158203125
INFO:tools.evaluation_results_class:Average Discounted Reward = -201.5498504638672
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9904214559386973
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 119728.859375
INFO:tools.evaluation_results_class:Current Best Return = -506.27203369140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9904214559386973
INFO:tools.evaluation_results_class:Average Episode Length = 224.095785440613
INFO:tools.evaluation_results_class:Counted Episodes = 522
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -516.82666015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -198.65524291992188
INFO:tools.evaluation_results_class:Average Discounted Reward = -210.1566925048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9942857142857143
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 122243.0625
INFO:tools.evaluation_results_class:Current Best Return = -516.82666015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9942857142857143
INFO:tools.evaluation_results_class:Average Episode Length = 224.61523809523808
INFO:tools.evaluation_results_class:Counted Episodes = 525
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -539.0020141601562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -224.72763061523438
INFO:tools.evaluation_results_class:Average Discounted Reward = -222.696533203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9821073558648111
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 138762.625
INFO:tools.evaluation_results_class:Current Best Return = -539.0020141601562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9821073558648111
INFO:tools.evaluation_results_class:Average Episode Length = 238.04572564612326
INFO:tools.evaluation_results_class:Counted Episodes = 503
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -258.30999755859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 61.689998626708984
INFO:tools.evaluation_results_class:Average Discounted Reward = -29.4185848236084
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44532.0546875
INFO:tools.evaluation_results_class:Current Best Return = -258.30999755859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 146.57555555555555
INFO:tools.evaluation_results_class:Counted Episodes = 900
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -194.97616577148438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 125.02383422851562
INFO:tools.evaluation_results_class:Average Discounted Reward = 3.159529447555542
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20426.27734375
INFO:tools.evaluation_results_class:Current Best Return = -194.97616577148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 145.94474539544962
INFO:tools.evaluation_results_class:Counted Episodes = 923
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -242.01278686523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 77.98722076416016
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.80712890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37085.90234375
INFO:tools.evaluation_results_class:Current Best Return = -242.01278686523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 142.6506922257721
INFO:tools.evaluation_results_class:Counted Episodes = 939
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -210.97909545898438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.02090454101562
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.943697988986969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35800.92578125
INFO:tools.evaluation_results_class:Current Best Return = -210.97909545898438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 147.72167216721672
INFO:tools.evaluation_results_class:Counted Episodes = 909
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -209.61846923828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 110.38153839111328
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.357805252075195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20351.96875
INFO:tools.evaluation_results_class:Current Best Return = -209.61846923828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 147.1979977753059
INFO:tools.evaluation_results_class:Counted Episodes = 899
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -328.0658874511719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -8.065875053405762
INFO:tools.evaluation_results_class:Average Discounted Reward = -76.81111145019531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 50662.07421875
INFO:tools.evaluation_results_class:Current Best Return = -328.0658874511719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 144.32829373650108
INFO:tools.evaluation_results_class:Counted Episodes = 926
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -357.5220642089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -37.52207565307617
INFO:tools.evaluation_results_class:Average Discounted Reward = -98.4260482788086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 59682.0625
INFO:tools.evaluation_results_class:Current Best Return = -357.5220642089844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 145.8101545253863
INFO:tools.evaluation_results_class:Counted Episodes = 906
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -354.06365966796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -34.06366729736328
INFO:tools.evaluation_results_class:Average Discounted Reward = -96.22386169433594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 61078.87890625
INFO:tools.evaluation_results_class:Current Best Return = -354.06365966796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 145.22612513721185
INFO:tools.evaluation_results_class:Counted Episodes = 911
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -354.236083984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -34.236080169677734
INFO:tools.evaluation_results_class:Average Discounted Reward = -96.02694702148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 64521.1328125
INFO:tools.evaluation_results_class:Current Best Return = -354.236083984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 147.0801781737194
INFO:tools.evaluation_results_class:Counted Episodes = 898
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -351.0524597167969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -31.052459716796875
INFO:tools.evaluation_results_class:Average Discounted Reward = -92.63673400878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 56869.99609375
INFO:tools.evaluation_results_class:Current Best Return = -351.0524597167969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 143.65792349726775
INFO:tools.evaluation_results_class:Counted Episodes = 915
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -347.1348571777344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -27.134868621826172
INFO:tools.evaluation_results_class:Average Discounted Reward = -93.33064270019531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 53824.4140625
INFO:tools.evaluation_results_class:Current Best Return = -347.1348571777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 144.63377192982455
INFO:tools.evaluation_results_class:Counted Episodes = 912
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -355.90045166015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -35.90044403076172
INFO:tools.evaluation_results_class:Average Discounted Reward = -99.21488189697266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 57098.71875
INFO:tools.evaluation_results_class:Current Best Return = -355.90045166015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 145.60619469026548
INFO:tools.evaluation_results_class:Counted Episodes = 904
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -364.6506652832031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -44.65065383911133
INFO:tools.evaluation_results_class:Average Discounted Reward = -100.82181549072266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 67304.65625
INFO:tools.evaluation_results_class:Current Best Return = -364.6506652832031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 144.85152838427948
INFO:tools.evaluation_results_class:Counted Episodes = 916
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -358.5146789550781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -38.51469039916992
INFO:tools.evaluation_results_class:Average Discounted Reward = -99.52013397216797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 60240.06640625
INFO:tools.evaluation_results_class:Current Best Return = -358.5146789550781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 145.37431991294886
INFO:tools.evaluation_results_class:Counted Episodes = 919
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -353.5195617675781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -33.51956558227539
INFO:tools.evaluation_results_class:Average Discounted Reward = -97.44337463378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 51560.58203125
INFO:tools.evaluation_results_class:Current Best Return = -353.5195617675781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 144.31739130434784
INFO:tools.evaluation_results_class:Counted Episodes = 920
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -353.0469055175781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -33.04690933227539
INFO:tools.evaluation_results_class:Average Discounted Reward = -94.90687561035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 54672.83203125
INFO:tools.evaluation_results_class:Current Best Return = -353.0469055175781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 143.49893390191897
INFO:tools.evaluation_results_class:Counted Episodes = 938
INFO:robust_rl.robust_rl_trainer:Iteration 12 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -309.7337341308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 10.26626205444336
INFO:tools.evaluation_results_class:Average Discounted Reward = -70.77236938476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 51351.35546875
INFO:tools.evaluation_results_class:Current Best Return = -309.7337341308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 145.07166482910694
INFO:tools.evaluation_results_class:Counted Episodes = 1814
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.727832794189453
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 13.602117538452148
INFO:agents.father_agent:Step: 10, Training loss: 11.229789733886719
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -288.2909851074219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 31.70901107788086
INFO:tools.evaluation_results_class:Average Discounted Reward = -49.736351013183594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 43598.38671875
INFO:tools.evaluation_results_class:Current Best Return = -288.2909851074219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 133.6518956179222
INFO:tools.evaluation_results_class:Counted Episodes = 2031
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -468.8802490234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -151.7281494140625
INFO:tools.evaluation_results_class:Average Discounted Reward = -183.0469512939453
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9911003236245954
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 114726.6640625
INFO:tools.evaluation_results_class:Current Best Return = -468.8802490234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9911003236245954
INFO:tools.evaluation_results_class:Average Episode Length = 221.66666666666666
INFO:tools.evaluation_results_class:Counted Episodes = 1236
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -303.4109802246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 16.58902931213379
INFO:tools.evaluation_results_class:Average Discounted Reward = -59.11996841430664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 50416.66015625
INFO:tools.evaluation_results_class:Current Best Return = -303.4109802246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 134.73314737331475
INFO:tools.evaluation_results_class:Counted Episodes = 2151
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 336.0843936458709
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 13 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -294.6358337402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 25.36417007446289
INFO:tools.evaluation_results_class:Average Discounted Reward = -50.67877960205078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 49926.53125
INFO:tools.evaluation_results_class:Current Best Return = -288.2909851074219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 132.60841899167892
INFO:tools.evaluation_results_class:Counted Episodes = 2043
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.168384552001953
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 12.794767379760742
INFO:agents.father_agent:Step: 10, Training loss: 14.684779167175293
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -274.4894714355469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.51053237915039
INFO:tools.evaluation_results_class:Average Discounted Reward = -33.49968338012695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41370.46875
INFO:tools.evaluation_results_class:Current Best Return = -274.4894714355469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 123.01613626176602
INFO:tools.evaluation_results_class:Counted Episodes = 2231
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -397.55780029296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -78.41801452636719
INFO:tools.evaluation_results_class:Average Discounted Reward = -136.19482421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9973118279569892
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 86473.21875
INFO:tools.evaluation_results_class:Current Best Return = -397.55780029296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9973118279569892
INFO:tools.evaluation_results_class:Average Episode Length = 187.43279569892474
INFO:tools.evaluation_results_class:Counted Episodes = 1488
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -272.460205078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 47.539791107177734
INFO:tools.evaluation_results_class:Average Discounted Reward = -32.93630599975586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42378.0703125
INFO:tools.evaluation_results_class:Current Best Return = -272.460205078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 122.7061052631579
INFO:tools.evaluation_results_class:Counted Episodes = 2375
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 308.62571806267766
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 14 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -271.6831359863281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.31684875488281
INFO:tools.evaluation_results_class:Average Discounted Reward = -34.03970718383789
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38415.96875
INFO:tools.evaluation_results_class:Current Best Return = -271.6831359863281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.13919413919415
INFO:tools.evaluation_results_class:Counted Episodes = 2184
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.35159683227539
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Step: 5, Training loss: 14.629412651062012
INFO:agents.father_agent:Step: 10, Training loss: 12.01431941986084
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -252.7460479736328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.25395202636719
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.301552772521973
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35327.53125
INFO:tools.evaluation_results_class:Current Best Return = -252.7460479736328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.64787018255578
INFO:tools.evaluation_results_class:Counted Episodes = 2465
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -377.6854248046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -57.6854248046875
INFO:tools.evaluation_results_class:Average Discounted Reward = -118.08776092529297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 75633.2734375
INFO:tools.evaluation_results_class:Current Best Return = -377.6854248046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 167.35900473933648
INFO:tools.evaluation_results_class:Counted Episodes = 1688
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -260.1461181640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.85387420654297
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.517274856567383
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37814.98046875
INFO:tools.evaluation_results_class:Current Best Return = -260.1461181640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 113.21937984496124
INFO:tools.evaluation_results_class:Counted Episodes = 2580
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 290.5715602064429
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 15 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -255.22842407226562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 64.7715835571289
INFO:tools.evaluation_results_class:Average Discounted Reward = -15.651650428771973
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34660.57421875
INFO:tools.evaluation_results_class:Current Best Return = -252.7460479736328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 113.1327361563518
INFO:tools.evaluation_results_class:Counted Episodes = 2456
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.830198287963867
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Step: 5, Training loss: 12.860105514526367
INFO:agents.father_agent:Step: 10, Training loss: 14.599628448486328
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -249.3565216064453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 70.64348602294922
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.7197771072387695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34484.359375
INFO:tools.evaluation_results_class:Current Best Return = -249.3565216064453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.75659151700421
INFO:tools.evaluation_results_class:Counted Episodes = 2617
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -343.8544616699219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -23.85445213317871
INFO:tools.evaluation_results_class:Average Discounted Reward = -90.11688995361328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 60792.140625
INFO:tools.evaluation_results_class:Current Best Return = -343.8544616699219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 147.15674300254452
INFO:tools.evaluation_results_class:Counted Episodes = 1965
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -239.4916229248047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.50837707519531
INFO:tools.evaluation_results_class:Average Discounted Reward = -0.9666140675544739
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28425.865234375
INFO:tools.evaluation_results_class:Current Best Return = -239.4916229248047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.87472687545521
INFO:tools.evaluation_results_class:Counted Episodes = 2746
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 271.87050063975454
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 16 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -244.32676696777344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 75.6732406616211
INFO:tools.evaluation_results_class:Average Discounted Reward = -3.353851795196533
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30427.412109375
INFO:tools.evaluation_results_class:Current Best Return = -244.32676696777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.10386656557999
INFO:tools.evaluation_results_class:Counted Episodes = 2638
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.401556015014648
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 13.730579376220703
INFO:agents.father_agent:Step: 10, Training loss: 11.552123069763184
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -239.09735107421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.90264892578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 2.490509271621704
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30798.009765625
INFO:tools.evaluation_results_class:Current Best Return = -239.09735107421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.31227038941954
INFO:tools.evaluation_results_class:Counted Episodes = 2722
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -305.8781433105469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.121858596801758
INFO:tools.evaluation_results_class:Average Discounted Reward = -60.501075744628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 50657.1875
INFO:tools.evaluation_results_class:Current Best Return = -305.8781433105469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.26410621147463
INFO:tools.evaluation_results_class:Counted Episodes = 2109
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -243.5236358642578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 76.47635650634766
INFO:tools.evaluation_results_class:Average Discounted Reward = -1.665095567703247
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29593.65625
INFO:tools.evaluation_results_class:Current Best Return = -243.5236358642578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.82851093860268
INFO:tools.evaluation_results_class:Counted Episodes = 2834
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 267.3235183461287
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -239.4270477294922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.57295989990234
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.950796127319336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37707.36328125
INFO:tools.evaluation_results_class:Current Best Return = -239.4270477294922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 132.09143968871595
INFO:tools.evaluation_results_class:Counted Episodes = 1028
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -208.78221130371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 111.21778106689453
INFO:tools.evaluation_results_class:Average Discounted Reward = -1.9948668479919434
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24436.626953125
INFO:tools.evaluation_results_class:Current Best Return = -208.78221130371094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 134.35064935064935
INFO:tools.evaluation_results_class:Counted Episodes = 1001
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -220.902099609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 99.097900390625
INFO:tools.evaluation_results_class:Average Discounted Reward = -9.197590827941895
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27953.603515625
INFO:tools.evaluation_results_class:Current Best Return = -220.902099609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 133.94305694305694
INFO:tools.evaluation_results_class:Counted Episodes = 1001
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -186.53334045410156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 133.46665954589844
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.37285041809082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28978.58984375
INFO:tools.evaluation_results_class:Current Best Return = -186.53334045410156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 132.1843137254902
INFO:tools.evaluation_results_class:Counted Episodes = 1020
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -207.01416015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 112.98584747314453
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.04140979424118996
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18792.962890625
INFO:tools.evaluation_results_class:Current Best Return = -207.01416015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 135.7138523761375
INFO:tools.evaluation_results_class:Counted Episodes = 989
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -319.15362548828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 0.846388578414917
INFO:tools.evaluation_results_class:Average Discounted Reward = -69.87601470947266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45862.140625
INFO:tools.evaluation_results_class:Current Best Return = -319.15362548828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 134.0762970498474
INFO:tools.evaluation_results_class:Counted Episodes = 983
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -334.1968078613281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -14.196819305419922
INFO:tools.evaluation_results_class:Average Discounted Reward = -78.28363037109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 54539.72265625
INFO:tools.evaluation_results_class:Current Best Return = -334.1968078613281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 135.51292246520876
INFO:tools.evaluation_results_class:Counted Episodes = 1006
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -325.3853759765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -5.385385513305664
INFO:tools.evaluation_results_class:Average Discounted Reward = -74.51803588867188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 53876.4765625
INFO:tools.evaluation_results_class:Current Best Return = -325.3853759765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 135.04404404404406
INFO:tools.evaluation_results_class:Counted Episodes = 999
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -324.007080078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -4.007070541381836
INFO:tools.evaluation_results_class:Average Discounted Reward = -72.27549743652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 50500.03515625
INFO:tools.evaluation_results_class:Current Best Return = -324.007080078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 135.14747474747475
INFO:tools.evaluation_results_class:Counted Episodes = 990
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -320.6000061035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -0.6000000238418579
INFO:tools.evaluation_results_class:Average Discounted Reward = -70.12336730957031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45161.25
INFO:tools.evaluation_results_class:Current Best Return = -320.6000061035156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 133.79207920792078
INFO:tools.evaluation_results_class:Counted Episodes = 1010
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -338.7259826660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -18.725971221923828
INFO:tools.evaluation_results_class:Average Discounted Reward = -82.98009490966797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 56923.63671875
INFO:tools.evaluation_results_class:Current Best Return = -338.7259826660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.2965235173824
INFO:tools.evaluation_results_class:Counted Episodes = 978
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -317.512451171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 2.487537384033203
INFO:tools.evaluation_results_class:Average Discounted Reward = -67.78413391113281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 50482.01171875
INFO:tools.evaluation_results_class:Current Best Return = -317.512451171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 134.51545363908275
INFO:tools.evaluation_results_class:Counted Episodes = 1003
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -319.6822204589844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 0.31777557730674744
INFO:tools.evaluation_results_class:Average Discounted Reward = -68.39067077636719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 49803.16796875
INFO:tools.evaluation_results_class:Current Best Return = -319.6822204589844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 133.69314796425024
INFO:tools.evaluation_results_class:Counted Episodes = 1007
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -323.1791687011719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -3.1791791915893555
INFO:tools.evaluation_results_class:Average Discounted Reward = -72.43525695800781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 48603.578125
INFO:tools.evaluation_results_class:Current Best Return = -323.1791687011719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 135.02002002002
INFO:tools.evaluation_results_class:Counted Episodes = 999
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -333.69989013671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -13.699898719787598
INFO:tools.evaluation_results_class:Average Discounted Reward = -79.23857879638672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 52489.94921875
INFO:tools.evaluation_results_class:Current Best Return = -333.69989013671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 135.90132248219734
INFO:tools.evaluation_results_class:Counted Episodes = 983
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -344.5299987792969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -24.530000686645508
INFO:tools.evaluation_results_class:Average Discounted Reward = -86.00968170166016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 56917.6640625
INFO:tools.evaluation_results_class:Current Best Return = -344.5299987792969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 134.724
INFO:tools.evaluation_results_class:Counted Episodes = 1000
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -335.15509033203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -15.155085563659668
INFO:tools.evaluation_results_class:Average Discounted Reward = -82.02105712890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 53093.87890625
INFO:tools.evaluation_results_class:Current Best Return = -335.15509033203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 134.37361530715006
INFO:tools.evaluation_results_class:Counted Episodes = 993
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -322.9247741699219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -2.924774408340454
INFO:tools.evaluation_results_class:Average Discounted Reward = -69.700927734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 47526.01953125
INFO:tools.evaluation_results_class:Current Best Return = -322.9247741699219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 134.69909729187563
INFO:tools.evaluation_results_class:Counted Episodes = 997
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -332.9324035644531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -12.932391166687012
INFO:tools.evaluation_results_class:Average Discounted Reward = -76.8026351928711
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 50158.046875
INFO:tools.evaluation_results_class:Current Best Return = -332.9324035644531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 134.8183652875883
INFO:tools.evaluation_results_class:Counted Episodes = 991
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -315.2243957519531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 4.775590419769287
INFO:tools.evaluation_results_class:Average Discounted Reward = -62.9990234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41727.48828125
INFO:tools.evaluation_results_class:Current Best Return = -315.2243957519531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 133.33858267716536
INFO:tools.evaluation_results_class:Counted Episodes = 1016
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -326.08209228515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -6.082097053527832
INFO:tools.evaluation_results_class:Average Discounted Reward = -72.51063537597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 49882.4140625
INFO:tools.evaluation_results_class:Current Best Return = -326.08209228515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 132.84174085064294
INFO:tools.evaluation_results_class:Counted Episodes = 1011
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -186.89273071289062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 133.10726928710938
INFO:tools.evaluation_results_class:Average Discounted Reward = 41.267356872558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23786.345703125
INFO:tools.evaluation_results_class:Current Best Return = -186.89273071289062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.53049228508449
INFO:tools.evaluation_results_class:Counted Episodes = 1361
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.1837615966797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 178.8162384033203
INFO:tools.evaluation_results_class:Average Discounted Reward = 70.19119262695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9878.125
INFO:tools.evaluation_results_class:Current Best Return = -141.1837615966797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 102.87306273062731
INFO:tools.evaluation_results_class:Counted Episodes = 1355
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -175.9409942626953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 144.0590057373047
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.510887145996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22183.03515625
INFO:tools.evaluation_results_class:Current Best Return = -175.9409942626953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.52501867064974
INFO:tools.evaluation_results_class:Counted Episodes = 1339
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.0483856201172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 181.9516143798828
INFO:tools.evaluation_results_class:Average Discounted Reward = 77.45914459228516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16529.73828125
INFO:tools.evaluation_results_class:Current Best Return = -138.0483856201172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.45454545454545
INFO:tools.evaluation_results_class:Counted Episodes = 1364
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.9109649658203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 174.0890350341797
INFO:tools.evaluation_results_class:Average Discounted Reward = 67.74231719970703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9829.6748046875
INFO:tools.evaluation_results_class:Current Best Return = -145.9109649658203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.38999264164828
INFO:tools.evaluation_results_class:Counted Episodes = 1359
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -239.5738525390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.4261474609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.9310173988342285
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29384.76171875
INFO:tools.evaluation_results_class:Current Best Return = -239.5738525390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.74593796159527
INFO:tools.evaluation_results_class:Counted Episodes = 1354
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -267.94873046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 52.0512809753418
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.213998794555664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33851.0859375
INFO:tools.evaluation_results_class:Current Best Return = -267.94873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.603663003663
INFO:tools.evaluation_results_class:Counted Episodes = 1365
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -265.68011474609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.319881439208984
INFO:tools.evaluation_results_class:Average Discounted Reward = -16.635738372802734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35473.69140625
INFO:tools.evaluation_results_class:Current Best Return = -265.68011474609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.1995597945708
INFO:tools.evaluation_results_class:Counted Episodes = 1363
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -251.8473358154297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 68.15266418457031
INFO:tools.evaluation_results_class:Average Discounted Reward = -5.562318325042725
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31776.54296875
INFO:tools.evaluation_results_class:Current Best Return = -251.8473358154297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 102.53688823959094
INFO:tools.evaluation_results_class:Counted Episodes = 1369
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -256.01019287109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.98981857299805
INFO:tools.evaluation_results_class:Average Discounted Reward = -9.048157691955566
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31397.166015625
INFO:tools.evaluation_results_class:Current Best Return = -256.01019287109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 102.22036363636364
INFO:tools.evaluation_results_class:Counted Episodes = 1375
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -254.91275024414062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 65.08724212646484
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.0371317863464355
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34270.359375
INFO:tools.evaluation_results_class:Current Best Return = -254.91275024414062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 102.58797653958945
INFO:tools.evaluation_results_class:Counted Episodes = 1364
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -256.26092529296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.73906707763672
INFO:tools.evaluation_results_class:Average Discounted Reward = -9.799822807312012
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32594.0703125
INFO:tools.evaluation_results_class:Current Best Return = -256.26092529296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.20681986656783
INFO:tools.evaluation_results_class:Counted Episodes = 1349
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -265.2926025390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.7074089050293
INFO:tools.evaluation_results_class:Average Discounted Reward = -15.775535583496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37199.69140625
INFO:tools.evaluation_results_class:Current Best Return = -265.2926025390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.45481481481481
INFO:tools.evaluation_results_class:Counted Episodes = 1350
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -253.127685546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 66.87230682373047
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.581661701202393
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32088.69140625
INFO:tools.evaluation_results_class:Current Best Return = -253.127685546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.7765404602821
INFO:tools.evaluation_results_class:Counted Episodes = 1347
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -262.40057373046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.59941482543945
INFO:tools.evaluation_results_class:Average Discounted Reward = -14.223520278930664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32382.990234375
INFO:tools.evaluation_results_class:Current Best Return = -262.40057373046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 102.4512105649303
INFO:tools.evaluation_results_class:Counted Episodes = 1363
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -252.4512176513672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.54878997802734
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.590895652770996
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28666.345703125
INFO:tools.evaluation_results_class:Current Best Return = -252.4512176513672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.0630961115187
INFO:tools.evaluation_results_class:Counted Episodes = 1363
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -255.78884887695312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 64.21114349365234
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.783482551574707
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36496.75390625
INFO:tools.evaluation_results_class:Current Best Return = -255.78884887695312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 102.56598240469208
INFO:tools.evaluation_results_class:Counted Episodes = 1364
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -256.93048095703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.06952667236328
INFO:tools.evaluation_results_class:Average Discounted Reward = -9.847501754760742
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27422.03515625
INFO:tools.evaluation_results_class:Current Best Return = -256.93048095703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.8491124260355
INFO:tools.evaluation_results_class:Counted Episodes = 1352
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -270.993408203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 49.0066032409668
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.835018157958984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33164.44921875
INFO:tools.evaluation_results_class:Current Best Return = -270.993408203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 102.94424064563464
INFO:tools.evaluation_results_class:Counted Episodes = 1363
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -262.7750244140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.22498321533203
INFO:tools.evaluation_results_class:Average Discounted Reward = -14.827756881713867
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28770.705078125
INFO:tools.evaluation_results_class:Current Best Return = -262.7750244140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 102.79985390796202
INFO:tools.evaluation_results_class:Counted Episodes = 1369
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -269.2760925292969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.723907470703125
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.203140258789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30160.87890625
INFO:tools.evaluation_results_class:Current Best Return = -269.2760925292969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.02738712065137
INFO:tools.evaluation_results_class:Counted Episodes = 1351
INFO:robust_rl.robust_rl_trainer:Iteration 17 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -246.12905883789062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 73.87094116210938
INFO:tools.evaluation_results_class:Average Discounted Reward = -2.914039134979248
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31642.681640625
INFO:tools.evaluation_results_class:Current Best Return = -239.09735107421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.18314061917195
INFO:tools.evaluation_results_class:Counted Episodes = 2681
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.761512756347656
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 12.910964012145996
INFO:agents.father_agent:Step: 10, Training loss: 12.118900299072266
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -232.84823608398438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 87.15176391601562
INFO:tools.evaluation_results_class:Average Discounted Reward = 9.138494491577148
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29473.578125
INFO:tools.evaluation_results_class:Current Best Return = -232.84823608398438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 101.06270039187746
INFO:tools.evaluation_results_class:Counted Episodes = 2807
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -289.9449462890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 30.055057525634766
INFO:tools.evaluation_results_class:Average Discounted Reward = -47.04844284057617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41016.0078125
INFO:tools.evaluation_results_class:Current Best Return = -289.9449462890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 129.27305282005372
INFO:tools.evaluation_results_class:Counted Episodes = 2234
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -239.1873321533203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.81267547607422
INFO:tools.evaluation_results_class:Average Discounted Reward = 3.8496243953704834
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31630.421875
INFO:tools.evaluation_results_class:Current Best Return = -239.1873321533203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 101.52341597796143
INFO:tools.evaluation_results_class:Counted Episodes = 2904
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 260.6042299710126
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 18 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -235.09356689453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 84.90643310546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.576612949371338
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29201.419921875
INFO:tools.evaluation_results_class:Current Best Return = -232.84823608398438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 101.98049132947978
INFO:tools.evaluation_results_class:Counted Episodes = 2768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.346549034118652
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 11.020090103149414
INFO:agents.father_agent:Step: 10, Training loss: 11.467761993408203
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -234.84934997558594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 85.1506576538086
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.217952251434326
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28322.103515625
INFO:tools.evaluation_results_class:Current Best Return = -232.84823608398438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 100.75965969514357
INFO:tools.evaluation_results_class:Counted Episodes = 2821
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -283.12762451171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.87238693237305
INFO:tools.evaluation_results_class:Average Discounted Reward = -39.80277633666992
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40671.58203125
INFO:tools.evaluation_results_class:Current Best Return = -283.12762451171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.13017498932992
INFO:tools.evaluation_results_class:Counted Episodes = 2343
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -236.78048706054688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 83.21952056884766
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.202803611755371
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28047.392578125
INFO:tools.evaluation_results_class:Current Best Return = -236.78048706054688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.89226612630868
INFO:tools.evaluation_results_class:Counted Episodes = 2961
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 255.41382710210146
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 19 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -228.37612915039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 91.62387084960938
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.398975372314453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26313.0859375
INFO:tools.evaluation_results_class:Current Best Return = -228.37612915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 98.48155880306193
INFO:tools.evaluation_results_class:Counted Episodes = 2874
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.741678237915039
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 35.94%
INFO:agents.father_agent:Step: 5, Training loss: 9.79572582244873
INFO:agents.father_agent:Step: 10, Training loss: 11.96277141571045
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -223.31748962402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 96.68251037597656
INFO:tools.evaluation_results_class:Average Discounted Reward = 18.028451919555664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26538.2265625
INFO:tools.evaluation_results_class:Current Best Return = -223.31748962402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 98.50242886884108
INFO:tools.evaluation_results_class:Counted Episodes = 2882
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -275.57421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.42578125
INFO:tools.evaluation_results_class:Average Discounted Reward = -33.679969787597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38479.953125
INFO:tools.evaluation_results_class:Current Best Return = -275.57421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 121.51975051975052
INFO:tools.evaluation_results_class:Counted Episodes = 2405
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -228.98524475097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 91.01475524902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.342130661010742
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25918.498046875
INFO:tools.evaluation_results_class:Current Best Return = -228.98524475097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.00670690811536
INFO:tools.evaluation_results_class:Counted Episodes = 2982
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 252.98315291500788
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 20 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -234.92510986328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 85.07489013671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 8.395663261413574
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28501.505859375
INFO:tools.evaluation_results_class:Current Best Return = -223.31748962402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.04319052594914
INFO:tools.evaluation_results_class:Counted Episodes = 2871
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.900741577148438
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Step: 5, Training loss: 10.261009216308594
INFO:agents.father_agent:Step: 10, Training loss: 8.444119453430176
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -226.3228759765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 93.6771240234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 16.6342716217041
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28204.1640625
INFO:tools.evaluation_results_class:Current Best Return = -223.31748962402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.91544493692464
INFO:tools.evaluation_results_class:Counted Episodes = 2933
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -269.7913513183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.208656311035156
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.5615177154541
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37058.9375
INFO:tools.evaluation_results_class:Current Best Return = -269.7913513183594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 119.15598203348306
INFO:tools.evaluation_results_class:Counted Episodes = 2449
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -228.8748779296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 91.1251220703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.04370403289795
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28363.439453125
INFO:tools.evaluation_results_class:Current Best Return = -228.8748779296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.45945051307514
INFO:tools.evaluation_results_class:Counted Episodes = 3021
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 246.73388509847203
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 21 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -223.141845703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 96.85816192626953
INFO:tools.evaluation_results_class:Average Discounted Reward = 18.54521369934082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25004.82421875
INFO:tools.evaluation_results_class:Current Best Return = -223.141845703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.82626399728538
INFO:tools.evaluation_results_class:Counted Episodes = 2947
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.544072151184082
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 39.06%
INFO:agents.father_agent:Step: 5, Training loss: 10.345301628112793
INFO:agents.father_agent:Step: 10, Training loss: 8.615777015686035
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -226.1207275390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 93.8792724609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 17.535667419433594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26413.736328125
INFO:tools.evaluation_results_class:Current Best Return = -223.141845703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.41582830315225
INFO:tools.evaluation_results_class:Counted Episodes = 2982
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -266.3895263671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 53.610477447509766
INFO:tools.evaluation_results_class:Average Discounted Reward = -23.002559661865234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34891.55859375
INFO:tools.evaluation_results_class:Current Best Return = -266.3895263671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.62268609688854
INFO:tools.evaluation_results_class:Counted Episodes = 2539
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -222.76524353027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 97.2347640991211
INFO:tools.evaluation_results_class:Average Discounted Reward = 20.032352447509766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26238.98828125
INFO:tools.evaluation_results_class:Current Best Return = -222.76524353027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.32699129313124
INFO:tools.evaluation_results_class:Counted Episodes = 3101
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 243.32940041847274
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -199.15093994140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 120.84906005859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 24.943885803222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25239.2265625
INFO:tools.evaluation_results_class:Current Best Return = -199.15093994140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.74733388022969
INFO:tools.evaluation_results_class:Counted Episodes = 1219
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.9925537109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 141.0074462890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.458370208740234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14076.6435546875
INFO:tools.evaluation_results_class:Current Best Return = -178.9925537109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 113.61092715231788
INFO:tools.evaluation_results_class:Counted Episodes = 1208
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -182.34971618652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 137.65028381347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.11925506591797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22667.267578125
INFO:tools.evaluation_results_class:Current Best Return = -182.34971618652344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.19733555370524
INFO:tools.evaluation_results_class:Counted Episodes = 1201
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -145.8430633544922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 174.1569366455078
INFO:tools.evaluation_results_class:Average Discounted Reward = 62.3829460144043
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17704.021484375
INFO:tools.evaluation_results_class:Current Best Return = -145.8430633544922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.05505341002466
INFO:tools.evaluation_results_class:Counted Episodes = 1217
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.29391479492188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 157.70608520507812
INFO:tools.evaluation_results_class:Average Discounted Reward = 47.21725845336914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12068.599609375
INFO:tools.evaluation_results_class:Current Best Return = -162.29391479492188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.26228143213989
INFO:tools.evaluation_results_class:Counted Episodes = 1201
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -257.993408203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 62.00658416748047
INFO:tools.evaluation_results_class:Average Discounted Reward = -14.030632972717285
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35616.7890625
INFO:tools.evaluation_results_class:Current Best Return = -257.993408203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 113.1201646090535
INFO:tools.evaluation_results_class:Counted Episodes = 1215
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -271.7774963378906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.22249984741211
INFO:tools.evaluation_results_class:Average Discounted Reward = -27.034940719604492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36039.5
INFO:tools.evaluation_results_class:Current Best Return = -271.7774963378906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.95166666666667
INFO:tools.evaluation_results_class:Counted Episodes = 1200
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -266.5466613769531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 53.45334243774414
INFO:tools.evaluation_results_class:Average Discounted Reward = -21.730464935302734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35343.55859375
INFO:tools.evaluation_results_class:Current Best Return = -266.5466613769531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 113.63914120561519
INFO:tools.evaluation_results_class:Counted Episodes = 1211
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -272.781982421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 47.21803283691406
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.384870529174805
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38726.97265625
INFO:tools.evaluation_results_class:Current Best Return = -272.781982421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 113.28524590163934
INFO:tools.evaluation_results_class:Counted Episodes = 1220
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -265.80181884765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.198177337646484
INFO:tools.evaluation_results_class:Average Discounted Reward = -20.739120483398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33892.140625
INFO:tools.evaluation_results_class:Current Best Return = -265.80181884765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.97180762852405
INFO:tools.evaluation_results_class:Counted Episodes = 1206
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -264.9884033203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.0116081237793
INFO:tools.evaluation_results_class:Average Discounted Reward = -22.8712158203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31915.34375
INFO:tools.evaluation_results_class:Current Best Return = -264.9884033203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.07131011608624
INFO:tools.evaluation_results_class:Counted Episodes = 1206
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -267.70892333984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 52.29108810424805
INFO:tools.evaluation_results_class:Average Discounted Reward = -22.664539337158203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36174.90625
INFO:tools.evaluation_results_class:Current Best Return = -267.70892333984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.57318070318888
INFO:tools.evaluation_results_class:Counted Episodes = 1223
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -276.9768981933594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.023101806640625
INFO:tools.evaluation_results_class:Average Discounted Reward = -29.07857894897461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 39225.4140625
INFO:tools.evaluation_results_class:Current Best Return = -276.9768981933594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.07590759075907
INFO:tools.evaluation_results_class:Counted Episodes = 1212
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -264.9883728027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.011619567871094
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.44580841064453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36814.3515625
INFO:tools.evaluation_results_class:Current Best Return = -264.9883728027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 113.34854771784232
INFO:tools.evaluation_results_class:Counted Episodes = 1205
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -266.2833557128906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 53.71664047241211
INFO:tools.evaluation_results_class:Average Discounted Reward = -23.485490798950195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34943.01953125
INFO:tools.evaluation_results_class:Current Best Return = -266.2833557128906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 113.24711696869852
INFO:tools.evaluation_results_class:Counted Episodes = 1214
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -276.922607421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.0773811340332
INFO:tools.evaluation_results_class:Average Discounted Reward = -30.417587280273438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41873.3984375
INFO:tools.evaluation_results_class:Current Best Return = -276.922607421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.50170068027211
INFO:tools.evaluation_results_class:Counted Episodes = 1176
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -275.01080322265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.98918533325195
INFO:tools.evaluation_results_class:Average Discounted Reward = -28.266651153564453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36784.8515625
INFO:tools.evaluation_results_class:Current Best Return = -275.01080322265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.9018302828619
INFO:tools.evaluation_results_class:Counted Episodes = 1202
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -274.7013854980469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.298606872558594
INFO:tools.evaluation_results_class:Average Discounted Reward = -27.569168090820312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33111.71875
INFO:tools.evaluation_results_class:Current Best Return = -274.7013854980469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.72764561115669
INFO:tools.evaluation_results_class:Counted Episodes = 1219
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -278.453857421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.54612731933594
INFO:tools.evaluation_results_class:Average Discounted Reward = -28.462173461914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38164.89453125
INFO:tools.evaluation_results_class:Current Best Return = -278.453857421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.11202635914333
INFO:tools.evaluation_results_class:Counted Episodes = 1214
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -279.2589416503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.74106979370117
INFO:tools.evaluation_results_class:Average Discounted Reward = -31.345779418945312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33698.45703125
INFO:tools.evaluation_results_class:Current Best Return = -279.2589416503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.39123376623377
INFO:tools.evaluation_results_class:Counted Episodes = 1232
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -277.37701416015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.622989654541016
INFO:tools.evaluation_results_class:Average Discounted Reward = -30.170787811279297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35203.35546875
INFO:tools.evaluation_results_class:Current Best Return = -277.37701416015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.30707395498392
INFO:tools.evaluation_results_class:Counted Episodes = 1244
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -265.6458435058594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.35416793823242
INFO:tools.evaluation_results_class:Average Discounted Reward = -21.790210723876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30144.712890625
INFO:tools.evaluation_results_class:Current Best Return = -265.6458435058594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.66166666666666
INFO:tools.evaluation_results_class:Counted Episodes = 1200
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -275.48553466796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.51448059082031
INFO:tools.evaluation_results_class:Average Discounted Reward = -29.718706130981445
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31633.28515625
INFO:tools.evaluation_results_class:Current Best Return = -275.48553466796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 117.03066439522998
INFO:tools.evaluation_results_class:Counted Episodes = 1174
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -274.3077697753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.69224166870117
INFO:tools.evaluation_results_class:Average Discounted Reward = -27.1708984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34256.1796875
INFO:tools.evaluation_results_class:Current Best Return = -274.3077697753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.86155129274395
INFO:tools.evaluation_results_class:Counted Episodes = 1199
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -271.6189270019531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.38106918334961
INFO:tools.evaluation_results_class:Average Discounted Reward = -24.120031356811523
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34107.484375
INFO:tools.evaluation_results_class:Current Best Return = -271.6189270019531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 113.88559670781893
INFO:tools.evaluation_results_class:Counted Episodes = 1215
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -272.5985107421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 47.401485443115234
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.524858474731445
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31605.40234375
INFO:tools.evaluation_results_class:Current Best Return = -272.5985107421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.82028029678483
INFO:tools.evaluation_results_class:Counted Episodes = 1213
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.744873046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 149.255126953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 56.08374786376953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19410.751953125
INFO:tools.evaluation_results_class:Current Best Return = -170.744873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.31190150478797
INFO:tools.evaluation_results_class:Counted Episodes = 1462
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.2081298828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 181.7918701171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 77.67054748535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9342.986328125
INFO:tools.evaluation_results_class:Current Best Return = -138.2081298828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.5328813559322
INFO:tools.evaluation_results_class:Counted Episodes = 1475
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -150.0337677001953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 169.9662322998047
INFO:tools.evaluation_results_class:Average Discounted Reward = 70.41339111328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14194.984375
INFO:tools.evaluation_results_class:Current Best Return = -150.0337677001953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.70492910195814
INFO:tools.evaluation_results_class:Counted Episodes = 1481
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -122.6487808227539
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 197.35122680664062
INFO:tools.evaluation_results_class:Average Discounted Reward = 93.86918640136719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15055.3369140625
INFO:tools.evaluation_results_class:Current Best Return = -122.6487808227539
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.47961956521739
INFO:tools.evaluation_results_class:Counted Episodes = 1472
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.904052734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 181.095947265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.37435150146484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8313.669921875
INFO:tools.evaluation_results_class:Current Best Return = -138.904052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.27837837837838
INFO:tools.evaluation_results_class:Counted Episodes = 1480
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -212.9032745361328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 107.09673309326172
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.581539154052734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27643.65625
INFO:tools.evaluation_results_class:Current Best Return = -212.9032745361328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.54223433242507
INFO:tools.evaluation_results_class:Counted Episodes = 1468
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -228.67327880859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 91.32672882080078
INFO:tools.evaluation_results_class:Average Discounted Reward = 16.038986206054688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27466.83203125
INFO:tools.evaluation_results_class:Current Best Return = -228.67327880859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.91455912508545
INFO:tools.evaluation_results_class:Counted Episodes = 1463
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -234.7814178466797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 85.21858215332031
INFO:tools.evaluation_results_class:Average Discounted Reward = 11.540802001953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27744.443359375
INFO:tools.evaluation_results_class:Current Best Return = -234.7814178466797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.72677595628416
INFO:tools.evaluation_results_class:Counted Episodes = 1464
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -234.06744384765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 85.93256378173828
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.609050750732422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30342.6640625
INFO:tools.evaluation_results_class:Current Best Return = -234.06744384765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.23297002724796
INFO:tools.evaluation_results_class:Counted Episodes = 1468
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -228.89889526367188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 91.1010971069336
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.993932723999023
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27352.197265625
INFO:tools.evaluation_results_class:Current Best Return = -228.89889526367188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.828060522696
INFO:tools.evaluation_results_class:Counted Episodes = 1454
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -230.5962371826172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 89.40377044677734
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.336552619934082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24763.076171875
INFO:tools.evaluation_results_class:Current Best Return = -230.5962371826172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.63257065948856
INFO:tools.evaluation_results_class:Counted Episodes = 1486
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -228.5294952392578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 91.47050476074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 16.40452003479004
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28764.74609375
INFO:tools.evaluation_results_class:Current Best Return = -228.5294952392578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.63050847457627
INFO:tools.evaluation_results_class:Counted Episodes = 1475
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -235.4654998779297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 84.53450775146484
INFO:tools.evaluation_results_class:Average Discounted Reward = 10.543634414672852
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28139.76171875
INFO:tools.evaluation_results_class:Current Best Return = -235.4654998779297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.5764546684709
INFO:tools.evaluation_results_class:Counted Episodes = 1478
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -230.71188354492188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 89.28811645507812
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.151159286499023
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26851.4921875
INFO:tools.evaluation_results_class:Current Best Return = -230.71188354492188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.28475486903963
INFO:tools.evaluation_results_class:Counted Episodes = 1489
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -240.58932495117188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.41067504882812
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.106579780578613
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28245.59375
INFO:tools.evaluation_results_class:Current Best Return = -240.58932495117188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.2662559890486
INFO:tools.evaluation_results_class:Counted Episodes = 1461
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -231.2786102294922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 88.72138977050781
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.841043472290039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28248.158203125
INFO:tools.evaluation_results_class:Current Best Return = -231.2786102294922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.84877384196186
INFO:tools.evaluation_results_class:Counted Episodes = 1468
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -232.74085998535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 87.2591323852539
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.625102996826172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26223.9453125
INFO:tools.evaluation_results_class:Current Best Return = -232.74085998535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.46684709066305
INFO:tools.evaluation_results_class:Counted Episodes = 1478
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -230.4684295654297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 89.53157043457031
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.355073928833008
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24292.453125
INFO:tools.evaluation_results_class:Current Best Return = -230.4684295654297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.76306856754923
INFO:tools.evaluation_results_class:Counted Episodes = 1473
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -231.98715209960938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 88.01284790039062
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.592001914978027
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23926.376953125
INFO:tools.evaluation_results_class:Current Best Return = -231.98715209960938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.47194050033806
INFO:tools.evaluation_results_class:Counted Episodes = 1479
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -239.1536865234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.8463134765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 8.128270149230957
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26940.4921875
INFO:tools.evaluation_results_class:Current Best Return = -239.1536865234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.55054644808743
INFO:tools.evaluation_results_class:Counted Episodes = 1464
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -240.00816345214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.99183654785156
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.847301006317139
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25053.580078125
INFO:tools.evaluation_results_class:Current Best Return = -240.00816345214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.09931972789116
INFO:tools.evaluation_results_class:Counted Episodes = 1470
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -239.2977752685547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.70222473144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 8.379919052124023
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27308.34765625
INFO:tools.evaluation_results_class:Current Best Return = -239.2977752685547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.89466576637408
INFO:tools.evaluation_results_class:Counted Episodes = 1481
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -238.4541778564453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 81.54582977294922
INFO:tools.evaluation_results_class:Average Discounted Reward = 8.079751014709473
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27139.353515625
INFO:tools.evaluation_results_class:Current Best Return = -238.4541778564453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.27496580027359
INFO:tools.evaluation_results_class:Counted Episodes = 1462
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -237.90591430664062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 82.09408569335938
INFO:tools.evaluation_results_class:Average Discounted Reward = 8.716819763183594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23944.109375
INFO:tools.evaluation_results_class:Current Best Return = -237.90591430664062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.84408602150538
INFO:tools.evaluation_results_class:Counted Episodes = 1488
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -240.92349243164062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.07649993896484
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.099170684814453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29132.9296875
INFO:tools.evaluation_results_class:Current Best Return = -240.92349243164062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.51366120218579
INFO:tools.evaluation_results_class:Counted Episodes = 1464
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -236.77423095703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 83.22576141357422
INFO:tools.evaluation_results_class:Average Discounted Reward = 8.991395950317383
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26944.96484375
INFO:tools.evaluation_results_class:Current Best Return = -236.77423095703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.47186440677966
INFO:tools.evaluation_results_class:Counted Episodes = 1475
INFO:robust_rl.robust_rl_trainer:Iteration 22 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -222.3155975341797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 97.68440246582031
INFO:tools.evaluation_results_class:Average Discounted Reward = 20.51737403869629
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26643.916015625
INFO:tools.evaluation_results_class:Current Best Return = -222.3155975341797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.98282249915796
INFO:tools.evaluation_results_class:Counted Episodes = 2969
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.583706855773926
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Step: 5, Training loss: 10.179784774780273
INFO:agents.father_agent:Step: 10, Training loss: 9.652420043945312
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -222.4886932373047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 97.51131439208984
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.838796615600586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26935.54296875
INFO:tools.evaluation_results_class:Current Best Return = -222.3155975341797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.63206919494344
INFO:tools.evaluation_results_class:Counted Episodes = 3006
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -260.517333984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.4826774597168
INFO:tools.evaluation_results_class:Average Discounted Reward = -17.249399185180664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34479.69140625
INFO:tools.evaluation_results_class:Current Best Return = -260.517333984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.97074672825251
INFO:tools.evaluation_results_class:Counted Episodes = 2598
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -224.08148193359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 95.91851806640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 17.95663833618164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23796.625
INFO:tools.evaluation_results_class:Current Best Return = -224.08148193359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.15523349436393
INFO:tools.evaluation_results_class:Counted Episodes = 3105
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 243.40427787839215
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 23 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -221.3341064453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 98.66588592529297
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.25176239013672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23733.5546875
INFO:tools.evaluation_results_class:Current Best Return = -221.3341064453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.99666221628839
INFO:tools.evaluation_results_class:Counted Episodes = 2996
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.815038681030273
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 40.62%
INFO:agents.father_agent:Step: 5, Training loss: 8.294910430908203
INFO:agents.father_agent:Step: 10, Training loss: 9.037208557128906
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -224.50213623046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 95.49785614013672
INFO:tools.evaluation_results_class:Average Discounted Reward = 20.13536262512207
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25933.58984375
INFO:tools.evaluation_results_class:Current Best Return = -221.3341064453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.2
INFO:tools.evaluation_results_class:Counted Episodes = 3035
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -253.916015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 66.08399200439453
INFO:tools.evaluation_results_class:Average Discounted Reward = -12.063915252685547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33214.50390625
INFO:tools.evaluation_results_class:Current Best Return = -253.916015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.0761154855643
INFO:tools.evaluation_results_class:Counted Episodes = 2667
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -225.70960998535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 94.2903823852539
INFO:tools.evaluation_results_class:Average Discounted Reward = 18.25438117980957
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26371.091796875
INFO:tools.evaluation_results_class:Current Best Return = -225.70960998535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.75256410256411
INFO:tools.evaluation_results_class:Counted Episodes = 3120
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 239.6163647714952
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 24 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -216.29908752441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 103.70091247558594
INFO:tools.evaluation_results_class:Average Discounted Reward = 25.8614444732666
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24865.375
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.41030658838878
INFO:tools.evaluation_results_class:Counted Episodes = 3066
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.71446418762207
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 39.06%
INFO:agents.father_agent:Step: 5, Training loss: 5.351184368133545
INFO:agents.father_agent:Step: 10, Training loss: 8.760764122009277
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -229.32009887695312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 90.6799087524414
INFO:tools.evaluation_results_class:Average Discounted Reward = 15.09711742401123
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28085.078125
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.44926568758345
INFO:tools.evaluation_results_class:Counted Episodes = 2996
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -250.27865600585938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.72134399414062
INFO:tools.evaluation_results_class:Average Discounted Reward = -8.990053176879883
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31512.34375
INFO:tools.evaluation_results_class:Current Best Return = -250.27865600585938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.6025641025641
INFO:tools.evaluation_results_class:Counted Episodes = 2652
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -227.83103942871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 92.16896057128906
INFO:tools.evaluation_results_class:Average Discounted Reward = 17.528650283813477
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28438.197265625
INFO:tools.evaluation_results_class:Current Best Return = -227.83103942871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.70432
INFO:tools.evaluation_results_class:Counted Episodes = 3125
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 242.2589866737038
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 25 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -227.3207550048828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 92.67925262451172
INFO:tools.evaluation_results_class:Average Discounted Reward = 16.202619552612305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25544.0234375
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.6060808553291
INFO:tools.evaluation_results_class:Counted Episodes = 2993
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.227019309997559
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 39.06%
INFO:agents.father_agent:Step: 5, Training loss: 9.286480903625488
INFO:agents.father_agent:Step: 10, Training loss: 8.520930290222168
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -221.0177764892578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 98.98221588134766
INFO:tools.evaluation_results_class:Average Discounted Reward = 22.559799194335938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26107.611328125
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.87191307211063
INFO:tools.evaluation_results_class:Counted Episodes = 3037
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -251.3948516845703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 68.60515594482422
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.449421882629395
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30600.82421875
INFO:tools.evaluation_results_class:Current Best Return = -251.3948516845703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.07209562943594
INFO:tools.evaluation_results_class:Counted Episodes = 2677
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -224.4446563720703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 95.55534362792969
INFO:tools.evaluation_results_class:Average Discounted Reward = 20.014772415161133
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28122.11328125
INFO:tools.evaluation_results_class:Current Best Return = -224.4446563720703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.50494417862839
INFO:tools.evaluation_results_class:Counted Episodes = 3135
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 241.141358350028
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 26 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -218.0488739013672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 101.95112609863281
INFO:tools.evaluation_results_class:Average Discounted Reward = 25.629261016845703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25719.9140625
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.960247637667
INFO:tools.evaluation_results_class:Counted Episodes = 3069
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.522515296936035
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 7.39730978012085
INFO:agents.father_agent:Step: 10, Training loss: 8.138712882995605
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -217.42880249023438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 102.57119750976562
INFO:tools.evaluation_results_class:Average Discounted Reward = 25.29920768737793
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23975.845703125
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.14988595633757
INFO:tools.evaluation_results_class:Counted Episodes = 3069
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -249.56228637695312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 70.43770599365234
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.819042682647705
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30661.62890625
INFO:tools.evaluation_results_class:Current Best Return = -249.56228637695312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.08776496838973
INFO:tools.evaluation_results_class:Counted Episodes = 2689
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -224.16383361816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 95.83617401123047
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.779674530029297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26502.08203125
INFO:tools.evaluation_results_class:Current Best Return = -224.16383361816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.41407002891101
INFO:tools.evaluation_results_class:Counted Episodes = 3113
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 235.90827311666075
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -189.468994140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 130.531005859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.6405029296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22923.33984375
INFO:tools.evaluation_results_class:Current Best Return = -189.468994140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.35968992248063
INFO:tools.evaluation_results_class:Counted Episodes = 1290
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -163.17501831054688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 156.82498168945312
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.50910568237305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14185.4091796875
INFO:tools.evaluation_results_class:Current Best Return = -163.17501831054688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.85427910562838
INFO:tools.evaluation_results_class:Counted Episodes = 1297
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -153.9767303466797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 166.0232696533203
INFO:tools.evaluation_results_class:Average Discounted Reward = 58.679622650146484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14081.83203125
INFO:tools.evaluation_results_class:Current Best Return = -153.9767303466797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.33514352211016
INFO:tools.evaluation_results_class:Counted Episodes = 1289
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.34298706054688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 181.65701293945312
INFO:tools.evaluation_results_class:Average Discounted Reward = 73.0116195678711
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16968.1328125
INFO:tools.evaluation_results_class:Current Best Return = -138.34298706054688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.33437744714173
INFO:tools.evaluation_results_class:Counted Episodes = 1277
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.932373046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 160.067626953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 54.89432907104492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10440.119140625
INFO:tools.evaluation_results_class:Current Best Return = -159.932373046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.919452887538
INFO:tools.evaluation_results_class:Counted Episodes = 1316
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -246.51568603515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 73.48431396484375
INFO:tools.evaluation_results_class:Average Discounted Reward = -2.1083760261535645
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31066.375
INFO:tools.evaluation_results_class:Current Best Return = -246.51568603515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.25707727620505
INFO:tools.evaluation_results_class:Counted Episodes = 1307
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -247.96519470214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.03480529785156
INFO:tools.evaluation_results_class:Average Discounted Reward = -4.234433650970459
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32250.3203125
INFO:tools.evaluation_results_class:Current Best Return = -247.96519470214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.11523588553752
INFO:tools.evaluation_results_class:Counted Episodes = 1293
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -239.1488037109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.8511962890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 1.3208683729171753
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31934.8984375
INFO:tools.evaluation_results_class:Current Best Return = -239.1488037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.5674633770239
INFO:tools.evaluation_results_class:Counted Episodes = 1297
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -252.4854278564453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.51457977294922
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.607267379760742
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34338.40234375
INFO:tools.evaluation_results_class:Current Best Return = -252.4854278564453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.23798266351457
INFO:tools.evaluation_results_class:Counted Episodes = 1269
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -252.5530242919922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.44697570800781
INFO:tools.evaluation_results_class:Average Discounted Reward = -9.962535858154297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35131.51171875
INFO:tools.evaluation_results_class:Current Best Return = -252.5530242919922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.86017282010998
INFO:tools.evaluation_results_class:Counted Episodes = 1273
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -247.1854705810547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.81452941894531
INFO:tools.evaluation_results_class:Average Discounted Reward = -5.357901096343994
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31216.04296875
INFO:tools.evaluation_results_class:Current Best Return = -247.1854705810547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.28129829984545
INFO:tools.evaluation_results_class:Counted Episodes = 1294
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -245.6457061767578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 74.35429382324219
INFO:tools.evaluation_results_class:Average Discounted Reward = -4.4461750984191895
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31534.607421875
INFO:tools.evaluation_results_class:Current Best Return = -245.6457061767578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.17484662576688
INFO:tools.evaluation_results_class:Counted Episodes = 1304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -249.75401306152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 70.24598693847656
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.700939178466797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30785.7421875
INFO:tools.evaluation_results_class:Current Best Return = -249.75401306152344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.31550802139037
INFO:tools.evaluation_results_class:Counted Episodes = 1309
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -249.25
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 70.75
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.392998218536377
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33815.078125
INFO:tools.evaluation_results_class:Current Best Return = -249.25
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.79411764705883
INFO:tools.evaluation_results_class:Counted Episodes = 1292
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -262.7879333496094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.212074279785156
INFO:tools.evaluation_results_class:Average Discounted Reward = -16.32433319091797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34294.99609375
INFO:tools.evaluation_results_class:Current Best Return = -262.7879333496094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.75232198142415
INFO:tools.evaluation_results_class:Counted Episodes = 1292
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -255.107421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 64.892578125
INFO:tools.evaluation_results_class:Average Discounted Reward = -11.442423820495605
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33198.51953125
INFO:tools.evaluation_results_class:Current Best Return = -255.107421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.85466034755135
INFO:tools.evaluation_results_class:Counted Episodes = 1266
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -246.3300018310547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 73.66999053955078
INFO:tools.evaluation_results_class:Average Discounted Reward = -5.618987083435059
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30719.517578125
INFO:tools.evaluation_results_class:Current Best Return = -246.3300018310547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.47275518035303
INFO:tools.evaluation_results_class:Counted Episodes = 1303
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -261.1135559082031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 58.886451721191406
INFO:tools.evaluation_results_class:Average Discounted Reward = -14.373201370239258
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30248.927734375
INFO:tools.evaluation_results_class:Current Best Return = -261.1135559082031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.23101018010964
INFO:tools.evaluation_results_class:Counted Episodes = 1277
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -260.271240234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.7287483215332
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.94229793548584
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31892.0
INFO:tools.evaluation_results_class:Current Best Return = -260.271240234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.89799072642967
INFO:tools.evaluation_results_class:Counted Episodes = 1294
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -262.3542785644531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.64571762084961
INFO:tools.evaluation_results_class:Average Discounted Reward = -15.878901481628418
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32482.759765625
INFO:tools.evaluation_results_class:Current Best Return = -262.3542785644531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.22309505106048
INFO:tools.evaluation_results_class:Counted Episodes = 1273
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -255.22865295410156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 64.7713394165039
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.211060523986816
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31501.62109375
INFO:tools.evaluation_results_class:Current Best Return = -255.22865295410156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.20731707317073
INFO:tools.evaluation_results_class:Counted Episodes = 1312
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -265.6642761230469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.33571243286133
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.977153778076172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30294.650390625
INFO:tools.evaluation_results_class:Current Best Return = -265.6642761230469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.73492063492064
INFO:tools.evaluation_results_class:Counted Episodes = 1260
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -259.1688537597656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 60.831138610839844
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.700034141540527
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29122.341796875
INFO:tools.evaluation_results_class:Current Best Return = -259.1688537597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.21223857474826
INFO:tools.evaluation_results_class:Counted Episodes = 1291
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -257.5218505859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 62.4781608581543
INFO:tools.evaluation_results_class:Average Discounted Reward = -11.434924125671387
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30184.48046875
INFO:tools.evaluation_results_class:Current Best Return = -257.5218505859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.2911877394636
INFO:tools.evaluation_results_class:Counted Episodes = 1305
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -267.5825500488281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 52.41745376586914
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.958576202392578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33336.1875
INFO:tools.evaluation_results_class:Current Best Return = -267.5825500488281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.25628930817611
INFO:tools.evaluation_results_class:Counted Episodes = 1272
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -256.37353515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.62644958496094
INFO:tools.evaluation_results_class:Average Discounted Reward = -9.741950035095215
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31197.75390625
INFO:tools.evaluation_results_class:Current Best Return = -256.37353515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.52126836813612
INFO:tools.evaluation_results_class:Counted Episodes = 1293
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -250.62852478027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.37147521972656
INFO:tools.evaluation_results_class:Average Discounted Reward = -9.013452529907227
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27034.98828125
INFO:tools.evaluation_results_class:Current Best Return = -250.62852478027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.60188087774294
INFO:tools.evaluation_results_class:Counted Episodes = 1276
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -261.56988525390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 58.43010711669922
INFO:tools.evaluation_results_class:Average Discounted Reward = -15.098221778869629
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30900.71875
INFO:tools.evaluation_results_class:Current Best Return = -261.56988525390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.54070660522274
INFO:tools.evaluation_results_class:Counted Episodes = 1302
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -264.56793212890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.43207931518555
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.23811912536621
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32306.08984375
INFO:tools.evaluation_results_class:Current Best Return = -264.56793212890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.84650805832693
INFO:tools.evaluation_results_class:Counted Episodes = 1303
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -260.308349609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.6916618347168
INFO:tools.evaluation_results_class:Average Discounted Reward = -14.08832836151123
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31987.310546875
INFO:tools.evaluation_results_class:Current Best Return = -260.308349609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.4628921193573
INFO:tools.evaluation_results_class:Counted Episodes = 1307
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -253.0152130126953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 66.98479461669922
INFO:tools.evaluation_results_class:Average Discounted Reward = -8.934651374816895
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26786.580078125
INFO:tools.evaluation_results_class:Current Best Return = -253.0152130126953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.20456273764259
INFO:tools.evaluation_results_class:Counted Episodes = 1315
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -163.3395538330078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 156.6604461669922
INFO:tools.evaluation_results_class:Average Discounted Reward = 63.27176284790039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17344.279296875
INFO:tools.evaluation_results_class:Current Best Return = -163.3395538330078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.04127829560586
INFO:tools.evaluation_results_class:Counted Episodes = 1502
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -139.10032653808594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 180.89967346191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 78.32263946533203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9154.158203125
INFO:tools.evaluation_results_class:Current Best Return = -139.10032653808594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.88372093023256
INFO:tools.evaluation_results_class:Counted Episodes = 1505
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -140.71099853515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 179.28900146484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 79.38119506835938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11721.4052734375
INFO:tools.evaluation_results_class:Current Best Return = -140.71099853515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.9486504279131
INFO:tools.evaluation_results_class:Counted Episodes = 1519
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -110.99140167236328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 209.0085906982422
INFO:tools.evaluation_results_class:Average Discounted Reward = 103.61456298828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9986.8330078125
INFO:tools.evaluation_results_class:Current Best Return = -110.99140167236328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.71560846560847
INFO:tools.evaluation_results_class:Counted Episodes = 1512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -134.99533081054688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 185.00466918945312
INFO:tools.evaluation_results_class:Average Discounted Reward = 82.53614044189453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8287.9296875
INFO:tools.evaluation_results_class:Current Best Return = -134.99533081054688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.32755170113408
INFO:tools.evaluation_results_class:Counted Episodes = 1499
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -203.09671020507812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 116.90328979492188
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.1234016418457
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23813.505859375
INFO:tools.evaluation_results_class:Current Best Return = -203.09671020507812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.14868421052631
INFO:tools.evaluation_results_class:Counted Episodes = 1520
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -229.47470092773438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 90.52529907226562
INFO:tools.evaluation_results_class:Average Discounted Reward = 16.33271598815918
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27484.298828125
INFO:tools.evaluation_results_class:Current Best Return = -229.47470092773438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.18109187749667
INFO:tools.evaluation_results_class:Counted Episodes = 1502
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -226.6304168701172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 93.36958312988281
INFO:tools.evaluation_results_class:Average Discounted Reward = 18.14392852783203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27946.048828125
INFO:tools.evaluation_results_class:Current Best Return = -226.6304168701172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.1714476317545
INFO:tools.evaluation_results_class:Counted Episodes = 1499
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -222.28477478027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 97.71522521972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 22.50153350830078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28282.849609375
INFO:tools.evaluation_results_class:Current Best Return = -222.28477478027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.10761154855643
INFO:tools.evaluation_results_class:Counted Episodes = 1524
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -224.0431365966797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 95.95687103271484
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.6800537109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26409.591796875
INFO:tools.evaluation_results_class:Current Best Return = -224.0431365966797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.09887193098872
INFO:tools.evaluation_results_class:Counted Episodes = 1507
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -227.5477752685547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 92.45222473144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 18.27655601501465
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29006.185546875
INFO:tools.evaluation_results_class:Current Best Return = -227.5477752685547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.57984293193718
INFO:tools.evaluation_results_class:Counted Episodes = 1528
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -227.02259826660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 92.97740936279297
INFO:tools.evaluation_results_class:Average Discounted Reward = 18.31614112854004
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27244.841796875
INFO:tools.evaluation_results_class:Current Best Return = -227.02259826660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.97541528239202
INFO:tools.evaluation_results_class:Counted Episodes = 1505
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -228.89698791503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 91.10301208496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 16.861099243164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27456.837890625
INFO:tools.evaluation_results_class:Current Best Return = -228.89698791503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.76588628762542
INFO:tools.evaluation_results_class:Counted Episodes = 1495
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -223.142578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 96.857421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.31346321105957
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26619.7265625
INFO:tools.evaluation_results_class:Current Best Return = -223.142578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.08278580814718
INFO:tools.evaluation_results_class:Counted Episodes = 1522
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -225.06919860839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 94.93080139160156
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.94080924987793
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26762.203125
INFO:tools.evaluation_results_class:Current Best Return = -225.06919860839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.7478376580173
INFO:tools.evaluation_results_class:Counted Episodes = 1503
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -223.5546112060547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 96.44539642333984
INFO:tools.evaluation_results_class:Average Discounted Reward = 20.612417221069336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27041.142578125
INFO:tools.evaluation_results_class:Current Best Return = -223.5546112060547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.1328947368421
INFO:tools.evaluation_results_class:Counted Episodes = 1520
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -230.4147186279297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 89.58528900146484
INFO:tools.evaluation_results_class:Average Discounted Reward = 15.725808143615723
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26935.125
INFO:tools.evaluation_results_class:Current Best Return = -230.4147186279297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.94661458333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -227.0204315185547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 92.97956848144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 18.055505752563477
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23137.4765625
INFO:tools.evaluation_results_class:Current Best Return = -227.0204315185547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.27554383651945
INFO:tools.evaluation_results_class:Counted Episodes = 1517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -238.37367248535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 81.62632751464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 9.828886985778809
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29865.904296875
INFO:tools.evaluation_results_class:Current Best Return = -238.37367248535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.22074468085107
INFO:tools.evaluation_results_class:Counted Episodes = 1504
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -229.77676391601562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 90.22323608398438
INFO:tools.evaluation_results_class:Average Discounted Reward = 16.24730682373047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25138.892578125
INFO:tools.evaluation_results_class:Current Best Return = -229.77676391601562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.81462140992167
INFO:tools.evaluation_results_class:Counted Episodes = 1532
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -235.59417724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 84.40581512451172
INFO:tools.evaluation_results_class:Average Discounted Reward = 11.6820068359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26203.76953125
INFO:tools.evaluation_results_class:Current Best Return = -235.59417724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.78916060806345
INFO:tools.evaluation_results_class:Counted Episodes = 1513
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -235.1720733642578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 84.82792663574219
INFO:tools.evaluation_results_class:Average Discounted Reward = 11.624229431152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27404.400390625
INFO:tools.evaluation_results_class:Current Best Return = -235.1720733642578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.48709463931172
INFO:tools.evaluation_results_class:Counted Episodes = 1511
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -230.20144653320312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 89.79855346679688
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.945183753967285
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23926.349609375
INFO:tools.evaluation_results_class:Current Best Return = -230.20144653320312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.48584595128374
INFO:tools.evaluation_results_class:Counted Episodes = 1519
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -231.9666290283203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 88.03337860107422
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.173592567443848
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23236.69921875
INFO:tools.evaluation_results_class:Current Best Return = -231.9666290283203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.70493991989319
INFO:tools.evaluation_results_class:Counted Episodes = 1498
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -231.62344360351562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 88.3765640258789
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.772789001464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25451.169921875
INFO:tools.evaluation_results_class:Current Best Return = -231.62344360351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.32784726793943
INFO:tools.evaluation_results_class:Counted Episodes = 1519
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -234.44451904296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 85.55548095703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.621801376342773
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26413.4921875
INFO:tools.evaluation_results_class:Current Best Return = -234.44451904296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.1602101116218
INFO:tools.evaluation_results_class:Counted Episodes = 1523
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -234.3673095703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 85.6326904296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.47170352935791
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24659.6328125
INFO:tools.evaluation_results_class:Current Best Return = -234.3673095703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.82064857710125
INFO:tools.evaluation_results_class:Counted Episodes = 1511
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -231.73916625976562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 88.2608413696289
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.912312507629395
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27069.560546875
INFO:tools.evaluation_results_class:Current Best Return = -231.73916625976562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.0197109067017
INFO:tools.evaluation_results_class:Counted Episodes = 1522
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -232.83432006835938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 87.1656723022461
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.50645637512207
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27831.984375
INFO:tools.evaluation_results_class:Current Best Return = -232.83432006835938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.22796554009278
INFO:tools.evaluation_results_class:Counted Episodes = 1509
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -243.29953002929688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 76.7004623413086
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.00178861618042
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31034.826171875
INFO:tools.evaluation_results_class:Current Best Return = -243.29953002929688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.38899933730947
INFO:tools.evaluation_results_class:Counted Episodes = 1509
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -239.625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 80.375
INFO:tools.evaluation_results_class:Average Discounted Reward = 8.24063777923584
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25924.986328125
INFO:tools.evaluation_results_class:Current Best Return = -239.625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.14893617021276
INFO:tools.evaluation_results_class:Counted Episodes = 1504
INFO:robust_rl.robust_rl_trainer:Iteration 27 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -221.0890350341797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 98.91096496582031
INFO:tools.evaluation_results_class:Average Discounted Reward = 22.2830867767334
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24295.615234375
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.28150572831424
INFO:tools.evaluation_results_class:Counted Episodes = 3055
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.035363674163818
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 31.25%
INFO:agents.father_agent:Step: 5, Training loss: 8.416730880737305
INFO:agents.father_agent:Step: 10, Training loss: 7.496511936187744
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -218.8751220703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 101.1248779296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.6849365234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24862.369140625
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.92363159619796
INFO:tools.evaluation_results_class:Counted Episodes = 3051
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -251.9208526611328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 68.07915496826172
INFO:tools.evaluation_results_class:Average Discounted Reward = -9.099525451660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32581.857421875
INFO:tools.evaluation_results_class:Current Best Return = -251.9208526611328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.1055369751022
INFO:tools.evaluation_results_class:Counted Episodes = 2691
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -222.07936096191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 97.92064666748047
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.7894229888916
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25930.22265625
INFO:tools.evaluation_results_class:Current Best Return = -222.07936096191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.44008852355358
INFO:tools.evaluation_results_class:Counted Episodes = 3163
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 238.56192593039472
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 28 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -224.23562622070312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 95.7643814086914
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.659404754638672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25847.458984375
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.77581699346405
INFO:tools.evaluation_results_class:Counted Episodes = 3060
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.977137565612793
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 7.799287796020508
INFO:agents.father_agent:Step: 10, Training loss: 7.020455837249756
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -220.95281982421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 99.04718017578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 22.301105499267578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24371.44140625
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.22005938634115
INFO:tools.evaluation_results_class:Counted Episodes = 3031
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -256.92681884765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.07318878173828
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.89197063446045
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33352.97265625
INFO:tools.evaluation_results_class:Current Best Return = -256.92681884765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.04006163328197
INFO:tools.evaluation_results_class:Counted Episodes = 2596
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -229.92445373535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 90.07555389404297
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.6052885055542
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25351.740234375
INFO:tools.evaluation_results_class:Current Best Return = -229.92445373535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.86446173800259
INFO:tools.evaluation_results_class:Counted Episodes = 3084
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 239.95080897986767
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 29 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -218.7972412109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 101.2027587890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 24.809612274169922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25012.626953125
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.20736115675321
INFO:tools.evaluation_results_class:Counted Episodes = 3043
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.221512794494629
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 6.577890872955322
INFO:agents.father_agent:Step: 10, Training loss: 7.337311267852783
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -225.7753448486328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 94.22466278076172
INFO:tools.evaluation_results_class:Average Discounted Reward = 18.222061157226562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26872.822265625
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.045197740113
INFO:tools.evaluation_results_class:Counted Episodes = 3009
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -260.1583251953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.841678619384766
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.548362731933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31223.734375
INFO:tools.evaluation_results_class:Current Best Return = -260.1583251953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.70801232665639
INFO:tools.evaluation_results_class:Counted Episodes = 2596
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -231.6635284423828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 88.33646392822266
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.516200065612793
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27096.0546875
INFO:tools.evaluation_results_class:Current Best Return = -231.6635284423828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.01653160453809
INFO:tools.evaluation_results_class:Counted Episodes = 3085
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 241.384680131251
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 30 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -227.74139404296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 92.25860595703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 16.368141174316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26129.447265625
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.92119205298013
INFO:tools.evaluation_results_class:Counted Episodes = 3020
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.586498737335205
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 6.207580089569092
INFO:agents.father_agent:Step: 10, Training loss: 8.183478355407715
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -226.9512939453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 93.04869842529297
INFO:tools.evaluation_results_class:Average Discounted Reward = 16.577354431152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26963.240234375
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.49633088725817
INFO:tools.evaluation_results_class:Counted Episodes = 2998
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -258.4696044921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 61.5303840637207
INFO:tools.evaluation_results_class:Average Discounted Reward = -15.48587703704834
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33577.25390625
INFO:tools.evaluation_results_class:Current Best Return = -258.4696044921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.6423076923077
INFO:tools.evaluation_results_class:Counted Episodes = 2600
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -217.89019775390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 102.10980224609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 24.152265548706055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24115.7578125
INFO:tools.evaluation_results_class:Current Best Return = -217.89019775390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.21608681774657
INFO:tools.evaluation_results_class:Counted Episodes = 3133
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 237.61326182321935
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 31 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -226.66688537597656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 93.33311462402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 16.982393264770508
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28019.580078125
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.0106702234078
INFO:tools.evaluation_results_class:Counted Episodes = 2999
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.458882808685303
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 6.638298034667969
INFO:agents.father_agent:Step: 10, Training loss: 6.85681676864624
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -222.56072998046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 97.43927764892578
INFO:tools.evaluation_results_class:Average Discounted Reward = 20.625638961791992
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28011.294921875
INFO:tools.evaluation_results_class:Current Best Return = -216.29908752441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.46570759451322
INFO:tools.evaluation_results_class:Counted Episodes = 2989
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -263.77691650390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.22307586669922
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.45734214782715
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37174.75
INFO:tools.evaluation_results_class:Current Best Return = -263.77691650390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.64
INFO:tools.evaluation_results_class:Counted Episodes = 2600
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -237.68251037597656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 82.31748962402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 8.594259262084961
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30455.146484375
INFO:tools.evaluation_results_class:Current Best Return = -237.68251037597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.64422442244225
INFO:tools.evaluation_results_class:Counted Episodes = 3030
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 246.80531451715754
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -183.7316436767578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 136.2683563232422
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.91020202636719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20130.251953125
INFO:tools.evaluation_results_class:Current Best Return = -183.7316436767578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.51549755301795
INFO:tools.evaluation_results_class:Counted Episodes = 1226
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -171.46096801757812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 148.53903198242188
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.001766204833984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14742.630859375
INFO:tools.evaluation_results_class:Current Best Return = -171.46096801757812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.36260162601626
INFO:tools.evaluation_results_class:Counted Episodes = 1230
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.59774780273438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 154.40225219726562
INFO:tools.evaluation_results_class:Average Discounted Reward = 46.30326843261719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16496.0546875
INFO:tools.evaluation_results_class:Current Best Return = -165.59774780273438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.79163314561545
INFO:tools.evaluation_results_class:Counted Episodes = 1243
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -144.7752685546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 175.2247314453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.69048309326172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17561.375
INFO:tools.evaluation_results_class:Current Best Return = -144.7752685546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.14066289409863
INFO:tools.evaluation_results_class:Counted Episodes = 1237
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -174.54884338378906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 145.45115661621094
INFO:tools.evaluation_results_class:Average Discounted Reward = 39.97829818725586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12608.572265625
INFO:tools.evaluation_results_class:Current Best Return = -174.54884338378906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.92136616362193
INFO:tools.evaluation_results_class:Counted Episodes = 1259
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -250.69273376464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.30726623535156
INFO:tools.evaluation_results_class:Average Discounted Reward = -8.423050880432129
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34983.66015625
INFO:tools.evaluation_results_class:Current Best Return = -250.69273376464844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.28491620111731
INFO:tools.evaluation_results_class:Counted Episodes = 1253
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
2025-08-20 02:49:05.626427: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-20 02:49:05.628342: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 02:49:05.659531: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-20 02:49:05.659581: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-20 02:49:05.660851: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-20 02:49:05.666577: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 02:49:05.666777: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-20 02:49:06.202222: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/obstacles-8-5/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/obstacles-8-5/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"penalty"}min=? [F ((x = 8) & (y = 8))] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 468 states and 975 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"rewardmodel_penalty"}min=? [F ((x = 8) & (y = 8))] 
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -856.657958984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -719.6859741210938
INFO:tools.evaluation_results_class:Average Discounted Reward = -350.2651672363281
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.4280373831775701
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 197758.859375
INFO:tools.evaluation_results_class:Current Best Return = -856.657958984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.4280373831775701
INFO:tools.evaluation_results_class:Average Episode Length = 499.97196261682245
INFO:tools.evaluation_results_class:Counted Episodes = 535
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 343.9459533691406
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 5, Training loss: 11.805618286132812
INFO:agents.father_agent:Step: 10, Training loss: 5.871819496154785
INFO:agents.father_agent:Step: 15, Training loss: 8.824752807617188
INFO:agents.father_agent:Step: 20, Training loss: 6.336704730987549
INFO:agents.father_agent:Step: 25, Training loss: 7.6102752685546875
INFO:agents.father_agent:Step: 30, Training loss: 7.41141939163208
INFO:agents.father_agent:Step: 35, Training loss: 7.925811290740967
INFO:agents.father_agent:Step: 40, Training loss: 8.81738567352295
INFO:agents.father_agent:Step: 45, Training loss: 6.323082447052002
INFO:agents.father_agent:Step: 50, Training loss: 6.115846633911133
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 32.81%
INFO:agents.father_agent:Step: 55, Training loss: 7.2029194831848145
INFO:agents.father_agent:Step: 60, Training loss: 6.4529290199279785
INFO:agents.father_agent:Step: 65, Training loss: 5.541840076446533
INFO:agents.father_agent:Step: 70, Training loss: 5.760505199432373
INFO:agents.father_agent:Step: 75, Training loss: 6.036221981048584
INFO:agents.father_agent:Step: 80, Training loss: 5.296695232391357
INFO:agents.father_agent:Step: 85, Training loss: 6.665062427520752
INFO:agents.father_agent:Step: 90, Training loss: 5.447378635406494
INFO:agents.father_agent:Step: 95, Training loss: 7.204746723175049
INFO:agents.father_agent:Step: 100, Training loss: 5.542919635772705
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -252.20558166503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.29790496826172
INFO:tools.evaluation_results_class:Average Discounted Reward = -46.7105827331543
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9984484096198604
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 39396.38671875
INFO:tools.evaluation_results_class:Current Best Return = -252.20558166503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9984484096198604
INFO:tools.evaluation_results_class:Average Episode Length = 191.46702870442203
INFO:tools.evaluation_results_class:Counted Episodes = 1289
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 37.50%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -258.6365051269531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 60.365055084228516
INFO:tools.evaluation_results_class:Average Discounted Reward = -49.881141662597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9968798751950078
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41832.3203125
INFO:tools.evaluation_results_class:Current Best Return = -252.20558166503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9984484096198604
INFO:tools.evaluation_results_class:Average Episode Length = 190.10140405616224
INFO:tools.evaluation_results_class:Counted Episodes = 1282
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -512.6520385742188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -244.81643676757812
INFO:tools.evaluation_results_class:Average Discounted Reward = -210.21583557128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.836986301369863
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 118150.09375
INFO:tools.evaluation_results_class:Current Best Return = -512.6520385742188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.836986301369863
INFO:tools.evaluation_results_class:Average Episode Length = 347.54794520547944
INFO:tools.evaluation_results_class:Counted Episodes = 730
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -262.6965637207031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.00789260864258
INFO:tools.evaluation_results_class:Average Discounted Reward = -53.282039642333984
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9928263988522238
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45145.73046875
INFO:tools.evaluation_results_class:Current Best Return = -262.6965637207031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9928263988522238
INFO:tools.evaluation_results_class:Average Episode Length = 198.39885222381636
INFO:tools.evaluation_results_class:Counted Episodes = 1394
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 453.8904124230475
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -529.0199584960938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -258.2507019042969
INFO:tools.evaluation_results_class:Average Discounted Reward = -195.5894775390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8461538461538461
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 198056.28125
INFO:tools.evaluation_results_class:Current Best Return = -529.0199584960938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8461538461538461
INFO:tools.evaluation_results_class:Average Episode Length = 323.94586894586894
INFO:tools.evaluation_results_class:Counted Episodes = 351
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -556.8089599609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -282.6596984863281
INFO:tools.evaluation_results_class:Average Discounted Reward = -191.89149475097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8567164179104477
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 148049.4375
INFO:tools.evaluation_results_class:Current Best Return = -556.8089599609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8567164179104477
INFO:tools.evaluation_results_class:Average Episode Length = 338.67164179104475
INFO:tools.evaluation_results_class:Counted Episodes = 335
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -562.0576171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -290.9337158203125
INFO:tools.evaluation_results_class:Average Discounted Reward = -225.62631225585938
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8472622478386167
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 140563.046875
INFO:tools.evaluation_results_class:Current Best Return = -562.0576171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8472622478386167
INFO:tools.evaluation_results_class:Average Episode Length = 340.671469740634
INFO:tools.evaluation_results_class:Counted Episodes = 347
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -429.60540771484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -155.8704833984375
INFO:tools.evaluation_results_class:Average Discounted Reward = -153.64605712890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8554216867469879
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 98611.4375
INFO:tools.evaluation_results_class:Current Best Return = -429.60540771484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8554216867469879
INFO:tools.evaluation_results_class:Average Episode Length = 336.3192771084337
INFO:tools.evaluation_results_class:Counted Episodes = 332
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -470.6961669921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -194.1179962158203
INFO:tools.evaluation_results_class:Average Discounted Reward = -198.5169219970703
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8643067846607669
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 103827.625
INFO:tools.evaluation_results_class:Current Best Return = -470.6961669921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8643067846607669
INFO:tools.evaluation_results_class:Average Episode Length = 335.23598820059
INFO:tools.evaluation_results_class:Counted Episodes = 339
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -647.7374877929688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -374.9350891113281
INFO:tools.evaluation_results_class:Average Discounted Reward = -274.7637939453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8525073746312685
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 157818.671875
INFO:tools.evaluation_results_class:Current Best Return = -647.7374877929688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8525073746312685
INFO:tools.evaluation_results_class:Average Episode Length = 339.0766961651917
INFO:tools.evaluation_results_class:Counted Episodes = 339
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -285.0544128417969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.474998474121094
INFO:tools.evaluation_results_class:Average Discounted Reward = -55.227195739746094
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9985294117647059
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 72330.03125
INFO:tools.evaluation_results_class:Current Best Return = -285.0544128417969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9985294117647059
INFO:tools.evaluation_results_class:Average Episode Length = 182.89117647058825
INFO:tools.evaluation_results_class:Counted Episodes = 680
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -361.698486328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -42.18333435058594
INFO:tools.evaluation_results_class:Average Discounted Reward = -86.30863189697266
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9984848484848485
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 79814.4453125
INFO:tools.evaluation_results_class:Current Best Return = -361.698486328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9984848484848485
INFO:tools.evaluation_results_class:Average Episode Length = 184.8030303030303
INFO:tools.evaluation_results_class:Counted Episodes = 660
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -285.43511962890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.564884185791016
INFO:tools.evaluation_results_class:Average Discounted Reward = -57.15713882446289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 48411.0703125
INFO:tools.evaluation_results_class:Current Best Return = -285.43511962890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 188.4412213740458
INFO:tools.evaluation_results_class:Counted Episodes = 655
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -218.03713989257812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 101.48737335205078
INFO:tools.evaluation_results_class:Average Discounted Reward = -21.109058380126953
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9985141158989599
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23421.486328125
INFO:tools.evaluation_results_class:Current Best Return = -218.03713989257812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9985141158989599
INFO:tools.evaluation_results_class:Average Episode Length = 182.1887072808321
INFO:tools.evaluation_results_class:Counted Episodes = 673
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -206.4133758544922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 112.1276626586914
INFO:tools.evaluation_results_class:Average Discounted Reward = -21.246612548828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9954407294832827
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27532.673828125
INFO:tools.evaluation_results_class:Current Best Return = -206.4133758544922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9954407294832827
INFO:tools.evaluation_results_class:Average Episode Length = 184.45896656534956
INFO:tools.evaluation_results_class:Counted Episodes = 658
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -397.4262390136719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -78.85693359375
INFO:tools.evaluation_results_class:Average Discounted Reward = -127.8792953491211
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9955290611028316
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 85809.4765625
INFO:tools.evaluation_results_class:Current Best Return = -397.4262390136719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9955290611028316
INFO:tools.evaluation_results_class:Average Episode Length = 190.20417287630403
INFO:tools.evaluation_results_class:Counted Episodes = 671
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 2 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -535.7404174804688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -315.84027099609375
INFO:tools.evaluation_results_class:Average Discounted Reward = -223.26913452148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6871880199667221
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 129588.0078125
INFO:tools.evaluation_results_class:Current Best Return = -252.20558166503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9984484096198604
INFO:tools.evaluation_results_class:Average Episode Length = 404.2645590682196
INFO:tools.evaluation_results_class:Counted Episodes = 601
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.4912285804748535
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 29.69%
INFO:agents.father_agent:Step: 5, Training loss: 7.226690769195557
INFO:agents.father_agent:Step: 10, Training loss: 6.840014934539795
INFO:agents.father_agent:Step: 15, Training loss: 7.01278829574585
INFO:agents.father_agent:Step: 20, Training loss: 6.100678443908691
INFO:agents.father_agent:Step: 25, Training loss: 7.209225177764893
INFO:agents.father_agent:Step: 30, Training loss: 6.530560493469238
INFO:agents.father_agent:Step: 35, Training loss: 9.443056106567383
INFO:agents.father_agent:Step: 40, Training loss: 6.719365119934082
INFO:agents.father_agent:Step: 45, Training loss: 6.3557000160217285
INFO:agents.father_agent:Step: 50, Training loss: 6.124896049499512
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Step: 55, Training loss: 8.310379028320312
INFO:agents.father_agent:Step: 60, Training loss: 8.105969429016113
INFO:agents.father_agent:Step: 65, Training loss: 8.11803150177002
INFO:agents.father_agent:Step: 70, Training loss: 7.1554856300354
INFO:agents.father_agent:Step: 75, Training loss: 7.7095046043396
INFO:agents.father_agent:Step: 80, Training loss: 8.791589736938477
INFO:agents.father_agent:Step: 85, Training loss: 7.265613079071045
INFO:agents.father_agent:Step: 90, Training loss: 8.074203491210938
INFO:agents.father_agent:Step: 95, Training loss: 9.028694152832031
INFO:agents.father_agent:Step: 100, Training loss: 7.854400634765625
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -238.36427307128906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 81.63573455810547
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.49253273010254
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 48807.91796875
INFO:tools.evaluation_results_class:Current Best Return = -238.36427307128906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 152.1508120649652
INFO:tools.evaluation_results_class:Counted Episodes = 1724
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 28.12%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -230.8375701904297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 89.16242980957031
INFO:tools.evaluation_results_class:Average Discounted Reward = -15.721428871154785
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 43865.53125
INFO:tools.evaluation_results_class:Current Best Return = -230.8375701904297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 153.0242774566474
INFO:tools.evaluation_results_class:Counted Episodes = 1730
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -397.13433837890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -88.59701538085938
INFO:tools.evaluation_results_class:Average Discounted Reward = -146.34315490722656
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9641791044776119
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 89958.3984375
INFO:tools.evaluation_results_class:Current Best Return = -397.13433837890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9641791044776119
INFO:tools.evaluation_results_class:Average Episode Length = 261.1333333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1005
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -227.17311096191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 92.82689666748047
INFO:tools.evaluation_results_class:Average Discounted Reward = -11.273168563842773
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41240.44140625
INFO:tools.evaluation_results_class:Current Best Return = -227.17311096191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 147.0875837197321
INFO:tools.evaluation_results_class:Counted Episodes = 1941
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=6, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 401.3706075951025
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=6, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 3 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -248.41854858398438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.58145141601562
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.82598114013672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 49149.8828125
INFO:tools.evaluation_results_class:Current Best Return = -230.8375701904297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 149.47511312217193
INFO:tools.evaluation_results_class:Counted Episodes = 1768
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.799928665161133
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 8.64783000946045
INFO:agents.father_agent:Step: 10, Training loss: 12.38985824584961
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -236.44711303710938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 83.55288696289062
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.179332733154297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 43158.79296875
INFO:tools.evaluation_results_class:Current Best Return = -230.8375701904297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 137.38630556974962
INFO:tools.evaluation_results_class:Counted Episodes = 1957
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -401.5169982910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -86.66905212402344
INFO:tools.evaluation_results_class:Average Discounted Reward = -147.4083251953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9838998211091234
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 90945.390625
INFO:tools.evaluation_results_class:Current Best Return = -401.5169982910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9838998211091234
INFO:tools.evaluation_results_class:Average Episode Length = 239.4722719141324
INFO:tools.evaluation_results_class:Counted Episodes = 1118
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -228.36097717285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 91.63902282714844
INFO:tools.evaluation_results_class:Average Discounted Reward = -5.663845062255859
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45718.3359375
INFO:tools.evaluation_results_class:Current Best Return = -228.36097717285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 135.63901979264844
INFO:tools.evaluation_results_class:Counted Episodes = 2122
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 363.846898732688
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 4 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -251.05233764648438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 68.94766235351562
INFO:tools.evaluation_results_class:Average Discounted Reward = -21.36309051513672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 47705.4140625
INFO:tools.evaluation_results_class:Current Best Return = -230.8375701904297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 136.72459349593495
INFO:tools.evaluation_results_class:Counted Episodes = 1968
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.618130683898926
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 26.56%
INFO:agents.father_agent:Step: 5, Training loss: 11.051568984985352
INFO:agents.father_agent:Step: 10, Training loss: 9.08582878112793
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -224.54225158691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 95.4577407836914
INFO:tools.evaluation_results_class:Average Discounted Reward = -0.12827631831169128
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34675.46484375
INFO:tools.evaluation_results_class:Current Best Return = -224.54225158691406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 125.29968021927822
INFO:tools.evaluation_results_class:Counted Episodes = 2189
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -385.9604797363281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -66.97233581542969
INFO:tools.evaluation_results_class:Average Discounted Reward = -133.93629455566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9968379446640316
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 87707.0703125
INFO:tools.evaluation_results_class:Current Best Return = -385.9604797363281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9968379446640316
INFO:tools.evaluation_results_class:Average Episode Length = 214.11778656126484
INFO:tools.evaluation_results_class:Counted Episodes = 1265
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -229.2379150390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 90.76209259033203
INFO:tools.evaluation_results_class:Average Discounted Reward = -2.7008986473083496
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41699.1640625
INFO:tools.evaluation_results_class:Current Best Return = -229.2379150390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 126.39433551198258
INFO:tools.evaluation_results_class:Counted Episodes = 2295
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 327.10562482339094
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 5 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -236.93287658691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 83.0671157836914
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.3867573738098145
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37854.06640625
INFO:tools.evaluation_results_class:Current Best Return = -224.54225158691406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.2045045045045
INFO:tools.evaluation_results_class:Counted Episodes = 2220
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.832168579101562
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 12.682992935180664
INFO:agents.father_agent:Step: 10, Training loss: 11.151334762573242
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -219.45401000976562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 100.54598999023438
INFO:tools.evaluation_results_class:Average Discounted Reward = 8.757338523864746
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33467.09765625
INFO:tools.evaluation_results_class:Current Best Return = -219.45401000976562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.52675585284281
INFO:tools.evaluation_results_class:Counted Episodes = 2392
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -388.89892578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -69.12669372558594
INFO:tools.evaluation_results_class:Average Discounted Reward = -131.1060791015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.999288256227758
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 87949.421875
INFO:tools.evaluation_results_class:Current Best Return = -388.89892578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.999288256227758
INFO:tools.evaluation_results_class:Average Episode Length = 196.82348754448398
INFO:tools.evaluation_results_class:Counted Episodes = 1405
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -216.34848022460938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 103.65151977539062
INFO:tools.evaluation_results_class:Average Discounted Reward = 10.414789199829102
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31881.263671875
INFO:tools.evaluation_results_class:Current Best Return = -216.34848022460938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.51047016989332
INFO:tools.evaluation_results_class:Counted Episodes = 2531
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 292.10465421396657
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 6 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -230.51417541503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 89.48582458496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.633306086063385
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34127.6328125
INFO:tools.evaluation_results_class:Current Best Return = -219.45401000976562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.07005838198499
INFO:tools.evaluation_results_class:Counted Episodes = 2398
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.637646675109863
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 11.685508728027344
INFO:agents.father_agent:Step: 10, Training loss: 10.648677825927734
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -219.84475708007812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 100.1552505493164
INFO:tools.evaluation_results_class:Average Discounted Reward = 10.603425979614258
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33896.59375
INFO:tools.evaluation_results_class:Current Best Return = -219.45401000976562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.64491284961491
INFO:tools.evaluation_results_class:Counted Episodes = 2467
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -360.7456970214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -40.958168029785156
INFO:tools.evaluation_results_class:Average Discounted Reward = -112.21175384521484
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.99933598937583
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 77714.1953125
INFO:tools.evaluation_results_class:Current Best Return = -360.7456970214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.99933598937583
INFO:tools.evaluation_results_class:Average Episode Length = 185.59893758300132
INFO:tools.evaluation_results_class:Counted Episodes = 1506
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -216.23602294921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 103.76397705078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.17788314819336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31150.19140625
INFO:tools.evaluation_results_class:Current Best Return = -216.23602294921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.67990744311608
INFO:tools.evaluation_results_class:Counted Episodes = 2593
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 285.0758976049846
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=2, o2y=3, o3x=3, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -254.92572021484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 65.07428741455078
INFO:tools.evaluation_results_class:Average Discounted Reward = -44.78572463989258
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44949.5703125
INFO:tools.evaluation_results_class:Current Best Return = -254.92572021484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 178.4742857142857
INFO:tools.evaluation_results_class:Counted Episodes = 700
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -323.8270263671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -4.292150974273682
INFO:tools.evaluation_results_class:Average Discounted Reward = -81.47663116455078
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.998546511627907
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 46505.6875
INFO:tools.evaluation_results_class:Current Best Return = -323.8270263671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.998546511627907
INFO:tools.evaluation_results_class:Average Episode Length = 182.0668604651163
INFO:tools.evaluation_results_class:Counted Episodes = 688
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -341.1515197753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -21.15151596069336
INFO:tools.evaluation_results_class:Average Discounted Reward = -95.09273529052734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 70647.7421875
INFO:tools.evaluation_results_class:Current Best Return = -341.1515197753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 180.93362193362194
INFO:tools.evaluation_results_class:Counted Episodes = 693
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -258.5159912109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 61.484012603759766
INFO:tools.evaluation_results_class:Average Discounted Reward = -49.3657341003418
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36193.73828125
INFO:tools.evaluation_results_class:Current Best Return = -258.5159912109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 181.7703488372093
INFO:tools.evaluation_results_class:Counted Episodes = 688
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -244.71327209472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 75.2867202758789
INFO:tools.evaluation_results_class:Average Discounted Reward = -44.888309478759766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45015.03515625
INFO:tools.evaluation_results_class:Current Best Return = -244.71327209472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 178.1723163841808
INFO:tools.evaluation_results_class:Counted Episodes = 708
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -415.458740234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -95.45875549316406
INFO:tools.evaluation_results_class:Average Discounted Reward = -144.24134826660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 82381.5859375
INFO:tools.evaluation_results_class:Current Best Return = -415.458740234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 182.8958031837916
INFO:tools.evaluation_results_class:Counted Episodes = 691
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -357.0499267578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -37.049930572509766
INFO:tools.evaluation_results_class:Average Discounted Reward = -116.9517593383789
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 60602.15625
INFO:tools.evaluation_results_class:Current Best Return = -357.0499267578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 175.50208044382802
INFO:tools.evaluation_results_class:Counted Episodes = 721
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -415.1072082519531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -95.55853271484375
INFO:tools.evaluation_results_class:Average Discounted Reward = -144.4049530029297
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.998589562764457
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 79774.2109375
INFO:tools.evaluation_results_class:Current Best Return = -415.1072082519531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.998589562764457
INFO:tools.evaluation_results_class:Average Episode Length = 177.52468265162202
INFO:tools.evaluation_results_class:Counted Episodes = 709
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -407.0279235839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -87.0279312133789
INFO:tools.evaluation_results_class:Average Discounted Reward = -139.39639282226562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 76606.734375
INFO:tools.evaluation_results_class:Current Best Return = -407.0279235839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 175.91620111731842
INFO:tools.evaluation_results_class:Counted Episodes = 716
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -421.47509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -101.9302978515625
INFO:tools.evaluation_results_class:Average Discounted Reward = -145.81863403320312
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9985775248933144
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 83630.5234375
INFO:tools.evaluation_results_class:Current Best Return = -421.47509765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9985775248933144
INFO:tools.evaluation_results_class:Average Episode Length = 179.81650071123755
INFO:tools.evaluation_results_class:Counted Episodes = 703
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -414.82806396484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -94.82806396484375
INFO:tools.evaluation_results_class:Average Discounted Reward = -140.300048828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 79559.1875
INFO:tools.evaluation_results_class:Current Best Return = -414.82806396484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 175.22558459422282
INFO:tools.evaluation_results_class:Counted Episodes = 727
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.97430419921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 177.02569580078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.5205078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16849.91796875
INFO:tools.evaluation_results_class:Current Best Return = -142.97430419921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.95341365461847
INFO:tools.evaluation_results_class:Counted Episodes = 1245
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -202.18138122558594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 117.8186264038086
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.799177169799805
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23144.3828125
INFO:tools.evaluation_results_class:Current Best Return = -202.18138122558594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.33441295546558
INFO:tools.evaluation_results_class:Counted Episodes = 1235
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.75828552246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 129.24171447753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.42286682128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29871.4765625
INFO:tools.evaluation_results_class:Current Best Return = -190.75828552246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.01859337105901
INFO:tools.evaluation_results_class:Counted Episodes = 1237
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -147.1714324951172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 172.8285675048828
INFO:tools.evaluation_results_class:Average Discounted Reward = 61.27976989746094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11169.2216796875
INFO:tools.evaluation_results_class:Current Best Return = -147.1714324951172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 113.24816326530612
INFO:tools.evaluation_results_class:Counted Episodes = 1225
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -116.90291595458984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 203.0970916748047
INFO:tools.evaluation_results_class:Average Discounted Reward = 85.8841781616211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11814.2265625
INFO:tools.evaluation_results_class:Current Best Return = -116.90291595458984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.74433656957929
INFO:tools.evaluation_results_class:Counted Episodes = 1236
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -271.6482849121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.35170364379883
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.383441925048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35133.89453125
INFO:tools.evaluation_results_class:Current Best Return = -271.6482849121094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.04051863857374
INFO:tools.evaluation_results_class:Counted Episodes = 1234
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -240.33226013183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 79.66773986816406
INFO:tools.evaluation_results_class:Average Discounted Reward = -9.611520767211914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28674.03125
INFO:tools.evaluation_results_class:Current Best Return = -240.33226013183594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.58747993579455
INFO:tools.evaluation_results_class:Counted Episodes = 1246
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -280.9486389160156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.051368713378906
INFO:tools.evaluation_results_class:Average Discounted Reward = -34.13267517089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 39492.30078125
INFO:tools.evaluation_results_class:Current Best Return = -280.9486389160156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 113.61971830985915
INFO:tools.evaluation_results_class:Counted Episodes = 1207
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -262.1529235839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.84708786010742
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.355953216552734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38457.9921875
INFO:tools.evaluation_results_class:Current Best Return = -262.1529235839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.88511326860842
INFO:tools.evaluation_results_class:Counted Episodes = 1236
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -282.13543701171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.864559173583984
INFO:tools.evaluation_results_class:Average Discounted Reward = -33.889015197753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36175.78125
INFO:tools.evaluation_results_class:Current Best Return = -282.13543701171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.42254663422547
INFO:tools.evaluation_results_class:Counted Episodes = 1233
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -278.36138916015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.63862228393555
INFO:tools.evaluation_results_class:Average Discounted Reward = -30.58389663696289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36401.16796875
INFO:tools.evaluation_results_class:Current Best Return = -278.36138916015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.59775641025641
INFO:tools.evaluation_results_class:Counted Episodes = 1248
INFO:rl_src.agents.recurrent_ppo_agent:Shrinking and perturbing actor and critic networks
INFO:rl_src.agents.recurrent_ppo_agent:Actor and critic networks shrunk and perturbed
INFO:robust_rl.robust_rl_trainer:Iteration 7 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -544.3135986328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -258.45245361328125
INFO:tools.evaluation_results_class:Average Discounted Reward = -201.52090454101562
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8933161953727506
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 201346.921875
INFO:tools.evaluation_results_class:Current Best Return = -219.45401000976562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 300.32647814910024
INFO:tools.evaluation_results_class:Counted Episodes = 778
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.311290740966797
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 7.7040629386901855
INFO:agents.father_agent:Step: 10, Training loss: 8.882111549377441
INFO:agents.father_agent:Step: 15, Training loss: 5.348282814025879
INFO:agents.father_agent:Step: 20, Training loss: 6.963857173919678
INFO:agents.father_agent:Step: 25, Training loss: 7.122574329376221
INFO:agents.father_agent:Step: 30, Training loss: 8.279970169067383
INFO:agents.father_agent:Step: 35, Training loss: 7.514272689819336
INFO:agents.father_agent:Step: 40, Training loss: 9.836220741271973
INFO:agents.father_agent:Step: 45, Training loss: 8.501581192016602
INFO:agents.father_agent:Step: 50, Training loss: 10.997097969055176
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 55, Training loss: 9.169425010681152
INFO:agents.father_agent:Step: 60, Training loss: 9.683364868164062
INFO:agents.father_agent:Step: 65, Training loss: 8.862550735473633
INFO:agents.father_agent:Step: 70, Training loss: 9.187941551208496
INFO:agents.father_agent:Step: 75, Training loss: 8.639352798461914
INFO:agents.father_agent:Step: 80, Training loss: 11.718159675598145
INFO:agents.father_agent:Step: 85, Training loss: 9.720428466796875
INFO:agents.father_agent:Step: 90, Training loss: 11.61417293548584
INFO:agents.father_agent:Step: 95, Training loss: 11.437962532043457
INFO:agents.father_agent:Step: 100, Training loss: 10.433048248291016
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.39773559570312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 131.60226440429688
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.352176666259766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25271.064453125
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.64603773584906
INFO:tools.evaluation_results_class:Counted Episodes = 2650
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 25.00%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -195.1017303466797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 124.89826965332031
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.558528900146484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31298.73046875
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.26149208741522
INFO:tools.evaluation_results_class:Counted Episodes = 2654
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -274.5268249511719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.473167419433594
INFO:tools.evaluation_results_class:Average Discounted Reward = -41.649417877197266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 49121.8359375
INFO:tools.evaluation_results_class:Current Best Return = -274.5268249511719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 141.29640571147218
INFO:tools.evaluation_results_class:Counted Episodes = 2031
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -192.53773498535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 127.46226501464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.454368591308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26122.703125
INFO:tools.evaluation_results_class:Current Best Return = -192.53773498535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.4061246810062
INFO:tools.evaluation_results_class:Counted Episodes = 2743
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 311.9071170921842
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 8 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -206.8296661376953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 113.17034149169922
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.799423217773438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28836.2890625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.71572655958163
INFO:tools.evaluation_results_class:Counted Episodes = 2677
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.589936256408691
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 8.906373977661133
INFO:agents.father_agent:Step: 10, Training loss: 8.977550506591797
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -192.6320037841797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 127.36799621582031
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.58076095581055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26483.697265625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.51556206517759
INFO:tools.evaluation_results_class:Counted Episodes = 2731
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -274.276611328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.72337341308594
INFO:tools.evaluation_results_class:Average Discounted Reward = -38.5089111328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 49594.15234375
INFO:tools.evaluation_results_class:Current Best Return = -274.276611328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 132.93453204241587
INFO:tools.evaluation_results_class:Counted Episodes = 2169
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -202.56451416015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 117.43548583984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.64780044555664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32032.64453125
INFO:tools.evaluation_results_class:Current Best Return = -202.56451416015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.2520325203252
INFO:tools.evaluation_results_class:Counted Episodes = 2829
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 312.84580483672096
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 9 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -207.5847625732422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 112.41522979736328
INFO:tools.evaluation_results_class:Average Discounted Reward = 27.895404815673828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32161.048828125
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.37495422922007
INFO:tools.evaluation_results_class:Counted Episodes = 2731
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.266039848327637
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 8.80604362487793
INFO:agents.father_agent:Step: 10, Training loss: 9.897930145263672
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -197.02195739746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 122.97804260253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.381561279296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25272.689453125
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.50470547229
INFO:tools.evaluation_results_class:Counted Episodes = 2869
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -255.544921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 64.45508575439453
INFO:tools.evaluation_results_class:Average Discounted Reward = -20.077320098876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42193.6640625
INFO:tools.evaluation_results_class:Current Best Return = -255.544921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 122.93167439898777
INFO:tools.evaluation_results_class:Counted Episodes = 2371
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -198.46034240722656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 121.53965759277344
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.1612434387207
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28460.576171875
INFO:tools.evaluation_results_class:Current Best Return = -198.46034240722656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.88423894701316
INFO:tools.evaluation_results_class:Counted Episodes = 2963
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 298.2380923439247
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 10 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -204.56045532226562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 115.43954467773438
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.73469352722168
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29056.22265625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.11327310632383
INFO:tools.evaluation_results_class:Counted Episodes = 2878
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.627095222473145
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 9.5656099319458
INFO:agents.father_agent:Step: 10, Training loss: 6.3777642250061035
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -194.24899291992188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 125.75100708007812
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.69706726074219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26302.228515625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.39165545087484
INFO:tools.evaluation_results_class:Counted Episodes = 2972
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -242.6822509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 77.3177490234375
INFO:tools.evaluation_results_class:Average Discounted Reward = -9.080076217651367
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38888.43359375
INFO:tools.evaluation_results_class:Current Best Return = -242.6822509765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.96830427892235
INFO:tools.evaluation_results_class:Counted Episodes = 2524
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -190.918701171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 129.081298828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.957542419433594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26153.93359375
INFO:tools.evaluation_results_class:Current Best Return = -190.918701171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.33138211382114
INFO:tools.evaluation_results_class:Counted Episodes = 3075
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 286.3302227241558
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 11 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -198.87887573242188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 121.1211166381836
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.279598236083984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27099.185546875
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.40013495276654
INFO:tools.evaluation_results_class:Counted Episodes = 2964
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.151766777038574
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 23.44%
INFO:agents.father_agent:Step: 5, Training loss: 8.596031188964844
INFO:agents.father_agent:Step: 10, Training loss: 7.706402778625488
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -193.3714141845703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 126.62857818603516
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.113643646240234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29829.21484375
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.53967731313797
INFO:tools.evaluation_results_class:Counted Episodes = 3037
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -236.4732208251953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 83.52677917480469
INFO:tools.evaluation_results_class:Average Discounted Reward = -3.674131393432617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37822.03515625
INFO:tools.evaluation_results_class:Current Best Return = -236.4732208251953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.28768171384851
INFO:tools.evaluation_results_class:Counted Episodes = 2614
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.96266174316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 128.03733825683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.33304214477539
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28377.650390625
INFO:tools.evaluation_results_class:Current Best Return = -191.96266174316406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.53973827002872
INFO:tools.evaluation_results_class:Counted Episodes = 3133
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 275.5010682702577
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -121.03739166259766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 198.96261596679688
INFO:tools.evaluation_results_class:Average Discounted Reward = 86.11227416992188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10205.091796875
INFO:tools.evaluation_results_class:Current Best Return = -121.03739166259766
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.5194908512331
INFO:tools.evaluation_results_class:Counted Episodes = 1257
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -218.50994873046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 101.49005889892578
INFO:tools.evaluation_results_class:Average Discounted Reward = 17.18979263305664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31651.453125
INFO:tools.evaluation_results_class:Current Best Return = -218.50994873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.1519490851233
INFO:tools.evaluation_results_class:Counted Episodes = 1257
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -210.53140258789062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.46859741210938
INFO:tools.evaluation_results_class:Average Discounted Reward = 17.519025802612305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21878.67578125
INFO:tools.evaluation_results_class:Current Best Return = -210.53140258789062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.60547504025764
INFO:tools.evaluation_results_class:Counted Episodes = 1242
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -195.05165100097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 124.94834899902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.20497703552246
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15261.998046875
INFO:tools.evaluation_results_class:Current Best Return = -195.05165100097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.83454398708636
INFO:tools.evaluation_results_class:Counted Episodes = 1239
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -147.53097534179688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 172.46902465820312
INFO:tools.evaluation_results_class:Average Discounted Reward = 61.78933334350586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21097.931640625
INFO:tools.evaluation_results_class:Current Best Return = -147.53097534179688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.03378921962992
INFO:tools.evaluation_results_class:Counted Episodes = 1243
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -206.411865234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 113.58814239501953
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.19209098815918
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36980.609375
INFO:tools.evaluation_results_class:Current Best Return = -206.411865234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.6698717948718
INFO:tools.evaluation_results_class:Counted Episodes = 1248
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -184.04112243652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 135.95887756347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 40.953712463378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32125.55078125
INFO:tools.evaluation_results_class:Current Best Return = -184.04112243652344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.94354838709677
INFO:tools.evaluation_results_class:Counted Episodes = 1240
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -203.59112548828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 116.40887451171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.15512466430664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36196.0234375
INFO:tools.evaluation_results_class:Current Best Return = -203.59112548828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.00475435816165
INFO:tools.evaluation_results_class:Counted Episodes = 1262
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -212.15248107910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 107.8475112915039
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.756433486938477
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35836.37109375
INFO:tools.evaluation_results_class:Current Best Return = -212.15248107910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.52969502407704
INFO:tools.evaluation_results_class:Counted Episodes = 1246
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -222.36566162109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 97.63433837890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.811368942260742
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38958.74609375
INFO:tools.evaluation_results_class:Current Best Return = -222.36566162109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.8664546899841
INFO:tools.evaluation_results_class:Counted Episodes = 1258
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -214.94009399414062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 105.05990600585938
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.91075325012207
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40704.19921875
INFO:tools.evaluation_results_class:Current Best Return = -214.94009399414062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.314696485623
INFO:tools.evaluation_results_class:Counted Episodes = 1252
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -310.3734130859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 9.626582145690918
INFO:tools.evaluation_results_class:Average Discounted Reward = -65.40535736083984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38579.96875
INFO:tools.evaluation_results_class:Current Best Return = -310.3734130859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.72784810126582
INFO:tools.evaluation_results_class:Counted Episodes = 1264
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -314.2919921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 5.708006381988525
INFO:tools.evaluation_results_class:Average Discounted Reward = -67.40721893310547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40196.2578125
INFO:tools.evaluation_results_class:Current Best Return = -314.2919921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.27943485086342
INFO:tools.evaluation_results_class:Counted Episodes = 1274
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -303.3699645996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 16.63003921508789
INFO:tools.evaluation_results_class:Average Discounted Reward = -58.8975715637207
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44492.76953125
INFO:tools.evaluation_results_class:Current Best Return = -303.3699645996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.35335968379447
INFO:tools.evaluation_results_class:Counted Episodes = 1265
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -308.48333740234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 11.516666412353516
INFO:tools.evaluation_results_class:Average Discounted Reward = -63.852603912353516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40121.6015625
INFO:tools.evaluation_results_class:Current Best Return = -308.48333740234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.45873015873016
INFO:tools.evaluation_results_class:Counted Episodes = 1260
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -319.335205078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 0.6648089289665222
INFO:tools.evaluation_results_class:Average Discounted Reward = -71.52198791503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41397.84375
INFO:tools.evaluation_results_class:Current Best Return = -319.335205078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.32961783439491
INFO:tools.evaluation_results_class:Counted Episodes = 1256
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -96.5515365600586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 223.44845581054688
INFO:tools.evaluation_results_class:Average Discounted Reward = 117.96135711669922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6974.42578125
INFO:tools.evaluation_results_class:Current Best Return = -96.5515365600586
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.61981258366801
INFO:tools.evaluation_results_class:Counted Episodes = 1494
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.35427856445312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 141.64572143554688
INFO:tools.evaluation_results_class:Average Discounted Reward = 54.44890594482422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19441.21875
INFO:tools.evaluation_results_class:Current Best Return = -178.35427856445312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.3663101604278
INFO:tools.evaluation_results_class:Counted Episodes = 1496
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.14218139648438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 157.85781860351562
INFO:tools.evaluation_results_class:Average Discounted Reward = 69.54660034179688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14192.517578125
INFO:tools.evaluation_results_class:Current Best Return = -162.14218139648438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.03957075788061
INFO:tools.evaluation_results_class:Counted Episodes = 1491
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -158.5013427734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 161.4986572265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.18739700317383
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9262.3251953125
INFO:tools.evaluation_results_class:Current Best Return = -158.5013427734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.76737967914438
INFO:tools.evaluation_results_class:Counted Episodes = 1496
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -94.36540985107422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 225.63458251953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 119.43920135498047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10813.06640625
INFO:tools.evaluation_results_class:Current Best Return = -94.36540985107422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.47510094212652
INFO:tools.evaluation_results_class:Counted Episodes = 1486
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.6630859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 154.3369140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 73.37849426269531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29692.306640625
INFO:tools.evaluation_results_class:Current Best Return = -165.6630859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.44429530201343
INFO:tools.evaluation_results_class:Counted Episodes = 1490
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.80361938476562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 181.19638061523438
INFO:tools.evaluation_results_class:Average Discounted Reward = 89.57292938232422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24987.619140625
INFO:tools.evaluation_results_class:Current Best Return = -138.80361938476562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.13806970509384
INFO:tools.evaluation_results_class:Counted Episodes = 1492
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.30650329589844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 150.69349670410156
INFO:tools.evaluation_results_class:Average Discounted Reward = 70.57658386230469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31966.224609375
INFO:tools.evaluation_results_class:Current Best Return = -169.30650329589844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.7719651240778
INFO:tools.evaluation_results_class:Counted Episodes = 1491
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -156.7040252685547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 163.2959747314453
INFO:tools.evaluation_results_class:Average Discounted Reward = 79.30115509033203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26935.404296875
INFO:tools.evaluation_results_class:Current Best Return = -156.7040252685547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.2469798657718
INFO:tools.evaluation_results_class:Counted Episodes = 1490
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -167.06834411621094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 152.93165588378906
INFO:tools.evaluation_results_class:Average Discounted Reward = 71.25745391845703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28669.669921875
INFO:tools.evaluation_results_class:Current Best Return = -167.06834411621094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.25414731254148
INFO:tools.evaluation_results_class:Counted Episodes = 1507
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.7590789794922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 155.2409210205078
INFO:tools.evaluation_results_class:Average Discounted Reward = 73.59466552734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25443.337890625
INFO:tools.evaluation_results_class:Current Best Return = -164.7590789794922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.88052805280527
INFO:tools.evaluation_results_class:Counted Episodes = 1515
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -281.1829833984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.8170166015625
INFO:tools.evaluation_results_class:Average Discounted Reward = -34.94487762451172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30919.1171875
INFO:tools.evaluation_results_class:Current Best Return = -281.1829833984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.02430790006753
INFO:tools.evaluation_results_class:Counted Episodes = 1481
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -269.34912109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.6508674621582
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.87944793701172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27506.349609375
INFO:tools.evaluation_results_class:Current Best Return = -269.34912109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.50200267022697
INFO:tools.evaluation_results_class:Counted Episodes = 1498
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -269.8025817871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.19742965698242
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.420486450195312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31527.55078125
INFO:tools.evaluation_results_class:Current Best Return = -269.8025817871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.5895875591616
INFO:tools.evaluation_results_class:Counted Episodes = 1479
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -273.6424255371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.35757064819336
INFO:tools.evaluation_results_class:Average Discounted Reward = -29.133073806762695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30153.10546875
INFO:tools.evaluation_results_class:Current Best Return = -273.6424255371094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.45563709139427
INFO:tools.evaluation_results_class:Counted Episodes = 1499
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -272.4739074707031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 47.52610397338867
INFO:tools.evaluation_results_class:Average Discounted Reward = -28.509517669677734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29309.66796875
INFO:tools.evaluation_results_class:Current Best Return = -272.4739074707031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.73226238286479
INFO:tools.evaluation_results_class:Counted Episodes = 1494
INFO:robust_rl.robust_rl_trainer:Iteration 12 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -202.3750457763672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 117.62496185302734
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.9640998840332
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30297.056640625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.42904181758314
INFO:tools.evaluation_results_class:Counted Episodes = 3037
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.409790992736816
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 9.588918685913086
INFO:agents.father_agent:Step: 10, Training loss: 8.400577545166016
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -191.47555541992188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 128.52444458007812
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.30628967285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27990.794921875
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.73449294387923
INFO:tools.evaluation_results_class:Counted Episodes = 3047
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -233.46295166015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 86.53704833984375
INFO:tools.evaluation_results_class:Average Discounted Reward = -0.02637595869600773
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34462.42578125
INFO:tools.evaluation_results_class:Current Best Return = -233.46295166015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.90344311377245
INFO:tools.evaluation_results_class:Counted Episodes = 2672
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -199.31309509277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 120.68689727783203
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.96432876586914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28972.541015625
INFO:tools.evaluation_results_class:Current Best Return = -199.31309509277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.50287539936102
INFO:tools.evaluation_results_class:Counted Episodes = 3130
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 278.38581030506776
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 13 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -202.3521728515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 117.6478271484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.54832077026367
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27543.00390625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.62397909180007
INFO:tools.evaluation_results_class:Counted Episodes = 3061
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.678080558776855
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 6.87103271484375
INFO:agents.father_agent:Step: 10, Training loss: 7.473665714263916
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -198.4787139892578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 121.52128601074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.306907653808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25317.072265625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.75820604484888
INFO:tools.evaluation_results_class:Counted Episodes = 3077
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -236.1572723388672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 83.84272003173828
INFO:tools.evaluation_results_class:Average Discounted Reward = -1.2990667819976807
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35590.484375
INFO:tools.evaluation_results_class:Current Best Return = -236.1572723388672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.38771031455742
INFO:tools.evaluation_results_class:Counted Episodes = 2734
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -202.83287048339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 117.1671371459961
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.771976470947266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29272.5546875
INFO:tools.evaluation_results_class:Current Best Return = -202.83287048339844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.63185951708999
INFO:tools.evaluation_results_class:Counted Episodes = 3189
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 272.86685957828024
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 14 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -203.75234985351562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 116.2476577758789
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.07322692871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28172.943359375
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.61526026511477
INFO:tools.evaluation_results_class:Counted Episodes = 3093
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.834908485412598
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 7.382807731628418
INFO:agents.father_agent:Step: 10, Training loss: 6.422474384307861
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -204.1744842529297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 115.82551574707031
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.92142868041992
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27461.251953125
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.99035989717224
INFO:tools.evaluation_results_class:Counted Episodes = 3112
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -245.64891052246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 74.35108947753906
INFO:tools.evaluation_results_class:Average Discounted Reward = -8.502890586853027
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36776.078125
INFO:tools.evaluation_results_class:Current Best Return = -245.64891052246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.5323714391417
INFO:tools.evaluation_results_class:Counted Episodes = 2703
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -209.50949096679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 110.49051666259766
INFO:tools.evaluation_results_class:Average Discounted Reward = 28.875011444091797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26823.890625
INFO:tools.evaluation_results_class:Current Best Return = -209.50949096679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.10171073094868
INFO:tools.evaluation_results_class:Counted Episodes = 3215
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 272.971080235685
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 15 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -208.92027282714844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 111.0797348022461
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.49437713623047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27048.490234375
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.96958053154019
INFO:tools.evaluation_results_class:Counted Episodes = 3123
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.07187557220459
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 21.88%
INFO:agents.father_agent:Step: 5, Training loss: 7.338593482971191
INFO:agents.father_agent:Step: 10, Training loss: 7.099924087524414
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -211.1415252685547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 108.85846710205078
INFO:tools.evaluation_results_class:Average Discounted Reward = 27.463726043701172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27329.28515625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.65642010886968
INFO:tools.evaluation_results_class:Counted Episodes = 3123
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -252.4626922607422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.53731536865234
INFO:tools.evaluation_results_class:Average Discounted Reward = -15.14897632598877
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38557.3203125
INFO:tools.evaluation_results_class:Current Best Return = -252.4626922607422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.53134328358209
INFO:tools.evaluation_results_class:Counted Episodes = 2680
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -210.320068359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 109.67993927001953
INFO:tools.evaluation_results_class:Average Discounted Reward = 27.432281494140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26700.517578125
INFO:tools.evaluation_results_class:Current Best Return = -210.320068359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.57680250783699
INFO:tools.evaluation_results_class:Counted Episodes = 3190
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 273.6049871039194
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 16 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -218.3152313232422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 101.68476867675781
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.323835372924805
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27795.88671875
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.00192802056556
INFO:tools.evaluation_results_class:Counted Episodes = 3112
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.491086483001709
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 7.312934398651123
INFO:agents.father_agent:Step: 10, Training loss: 7.635580539703369
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -213.57179260253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 106.4281997680664
INFO:tools.evaluation_results_class:Average Discounted Reward = 24.194721221923828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26495.30078125
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.37128072445019
INFO:tools.evaluation_results_class:Counted Episodes = 3092
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -247.21646118164062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.7835464477539
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.832332611083984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34328.3515625
INFO:tools.evaluation_results_class:Current Best Return = -247.21646118164062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.18977020014826
INFO:tools.evaluation_results_class:Counted Episodes = 2698
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -221.9234619140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 98.0765380859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 17.535011291503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28503.556640625
INFO:tools.evaluation_results_class:Current Best Return = -221.9234619140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.77164366373903
INFO:tools.evaluation_results_class:Counted Episodes = 3188
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 273.84738535837437
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -113.83605194091797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 206.1639404296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 93.80852508544922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9746.6669921875
INFO:tools.evaluation_results_class:Current Best Return = -113.83605194091797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.05283605283606
INFO:tools.evaluation_results_class:Counted Episodes = 1287
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -201.7511749267578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 118.24882507324219
INFO:tools.evaluation_results_class:Average Discounted Reward = 25.8094425201416
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19963.53125
INFO:tools.evaluation_results_class:Current Best Return = -201.7511749267578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.06259780907668
INFO:tools.evaluation_results_class:Counted Episodes = 1278
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -213.17457580566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 106.82542419433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.121118545532227
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24254.826171875
INFO:tools.evaluation_results_class:Current Best Return = -213.17457580566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.84839203675345
INFO:tools.evaluation_results_class:Counted Episodes = 1306
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -191.73353576660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 128.26646423339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 24.123327255249023
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15039.23828125
INFO:tools.evaluation_results_class:Current Best Return = -191.73353576660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.33230054221534
INFO:tools.evaluation_results_class:Counted Episodes = 1291
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -124.0420150756836
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 195.95797729492188
INFO:tools.evaluation_results_class:Average Discounted Reward = 85.1507797241211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16498.52734375
INFO:tools.evaluation_results_class:Current Best Return = -124.0420150756836
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.97097020626433
INFO:tools.evaluation_results_class:Counted Episodes = 1309
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -174.4892578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 145.5107421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 58.3548583984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25171.888671875
INFO:tools.evaluation_results_class:Current Best Return = -174.4892578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.23006134969326
INFO:tools.evaluation_results_class:Counted Episodes = 1304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -151.85374450683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 168.14625549316406
INFO:tools.evaluation_results_class:Average Discounted Reward = 69.0704116821289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21930.36328125
INFO:tools.evaluation_results_class:Current Best Return = -151.85374450683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.45788667687596
INFO:tools.evaluation_results_class:Counted Episodes = 1306
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -175.4325408935547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 144.5674591064453
INFO:tools.evaluation_results_class:Average Discounted Reward = 58.654056549072266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30581.447265625
INFO:tools.evaluation_results_class:Current Best Return = -175.4325408935547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.38242097147263
INFO:tools.evaluation_results_class:Counted Episodes = 1297
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.71009826660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 141.28990173339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 55.67359924316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28847.259765625
INFO:tools.evaluation_results_class:Current Best Return = -178.71009826660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.6183500385505
INFO:tools.evaluation_results_class:Counted Episodes = 1297
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -186.10403442382812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 133.89596557617188
INFO:tools.evaluation_results_class:Average Discounted Reward = 48.58690643310547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30282.900390625
INFO:tools.evaluation_results_class:Current Best Return = -186.10403442382812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.54347826086956
INFO:tools.evaluation_results_class:Counted Episodes = 1288
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -192.2083282470703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 127.79166412353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.277164459228516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29824.908203125
INFO:tools.evaluation_results_class:Current Best Return = -192.2083282470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.89779874213836
INFO:tools.evaluation_results_class:Counted Episodes = 1272
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -300.3312683105469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 19.668725967407227
INFO:tools.evaluation_results_class:Average Discounted Reward = -56.71678161621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38192.90234375
INFO:tools.evaluation_results_class:Current Best Return = -300.3312683105469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.52355212355212
INFO:tools.evaluation_results_class:Counted Episodes = 1295
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -304.05047607421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 15.94953441619873
INFO:tools.evaluation_results_class:Average Discounted Reward = -59.43263626098633
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36475.84375
INFO:tools.evaluation_results_class:Current Best Return = -304.05047607421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.07919254658385
INFO:tools.evaluation_results_class:Counted Episodes = 1288
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -307.3130798339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.686908721923828
INFO:tools.evaluation_results_class:Average Discounted Reward = -62.59962844848633
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35457.78515625
INFO:tools.evaluation_results_class:Current Best Return = -307.3130798339844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.1813880126183
INFO:tools.evaluation_results_class:Counted Episodes = 1268
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -297.1431884765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 22.856813430786133
INFO:tools.evaluation_results_class:Average Discounted Reward = -53.93638610839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38163.33984375
INFO:tools.evaluation_results_class:Current Best Return = -297.1431884765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.91608929946112
INFO:tools.evaluation_results_class:Counted Episodes = 1299
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -292.2677307128906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 27.73226547241211
INFO:tools.evaluation_results_class:Average Discounted Reward = -50.418067932128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36102.89453125
INFO:tools.evaluation_results_class:Current Best Return = -292.2677307128906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.30587337909992
INFO:tools.evaluation_results_class:Counted Episodes = 1311
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -305.31072998046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.68928337097168
INFO:tools.evaluation_results_class:Average Discounted Reward = -61.23412322998047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38513.89453125
INFO:tools.evaluation_results_class:Current Best Return = -305.31072998046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.08558211256747
INFO:tools.evaluation_results_class:Counted Episodes = 1297
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -301.56256103515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 18.43745231628418
INFO:tools.evaluation_results_class:Average Discounted Reward = -57.154212951660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34774.48828125
INFO:tools.evaluation_results_class:Current Best Return = -301.56256103515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.32540291634689
INFO:tools.evaluation_results_class:Counted Episodes = 1303
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -302.7838134765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 17.216175079345703
INFO:tools.evaluation_results_class:Average Discounted Reward = -56.19279861450195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41091.07421875
INFO:tools.evaluation_results_class:Current Best Return = -302.7838134765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.08242612752721
INFO:tools.evaluation_results_class:Counted Episodes = 1286
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -302.0312805175781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 17.968727111816406
INFO:tools.evaluation_results_class:Average Discounted Reward = -57.842220306396484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35422.62890625
INFO:tools.evaluation_results_class:Current Best Return = -302.0312805175781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.6788710907704
INFO:tools.evaluation_results_class:Counted Episodes = 1311
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -296.64385986328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 23.356143951416016
INFO:tools.evaluation_results_class:Average Discounted Reward = -50.660606384277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38734.81640625
INFO:tools.evaluation_results_class:Current Best Return = -296.64385986328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.63452566096423
INFO:tools.evaluation_results_class:Counted Episodes = 1286
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -90.659912109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.340087890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 125.02446746826172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6294.12646484375
INFO:tools.evaluation_results_class:Current Best Return = -90.659912109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.61805101373447
INFO:tools.evaluation_results_class:Counted Episodes = 1529
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.50518798828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 150.49481201171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.8227653503418
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11029.421875
INFO:tools.evaluation_results_class:Current Best Return = -169.50518798828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.04663212435233
INFO:tools.evaluation_results_class:Counted Episodes = 1544
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -167.7630157470703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 152.2369842529297
INFO:tools.evaluation_results_class:Average Discounted Reward = 67.80079650878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16068.3603515625
INFO:tools.evaluation_results_class:Current Best Return = -167.7630157470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.42708333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -163.896240234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 156.103759765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 57.556724548339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8457.390625
INFO:tools.evaluation_results_class:Current Best Return = -163.896240234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.44098573281453
INFO:tools.evaluation_results_class:Counted Episodes = 1542
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -85.82353210449219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.1764678955078
INFO:tools.evaluation_results_class:Average Discounted Reward = 129.5225372314453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9751.61328125
INFO:tools.evaluation_results_class:Current Best Return = -85.82353210449219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.67614738202974
INFO:tools.evaluation_results_class:Counted Episodes = 1547
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -146.47523498535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 173.52476501464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 90.48928833007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22784.896484375
INFO:tools.evaluation_results_class:Current Best Return = -146.47523498535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.58930899608866
INFO:tools.evaluation_results_class:Counted Episodes = 1534
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -115.67415618896484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 204.32583618164062
INFO:tools.evaluation_results_class:Average Discounted Reward = 109.36607360839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19102.958984375
INFO:tools.evaluation_results_class:Current Best Return = -115.67415618896484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.84137475214806
INFO:tools.evaluation_results_class:Counted Episodes = 1513
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.3335418701172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 181.6664581298828
INFO:tools.evaluation_results_class:Average Discounted Reward = 98.10629272460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22692.501953125
INFO:tools.evaluation_results_class:Current Best Return = -138.3335418701172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.80413703943115
INFO:tools.evaluation_results_class:Counted Episodes = 1547
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -146.1314239501953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 173.8685760498047
INFO:tools.evaluation_results_class:Average Discounted Reward = 91.05046081542969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23691.35546875
INFO:tools.evaluation_results_class:Current Best Return = -146.1314239501953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.44176968119714
INFO:tools.evaluation_results_class:Counted Episodes = 1537
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -146.1905059814453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 173.8094940185547
INFO:tools.evaluation_results_class:Average Discounted Reward = 91.91104888916016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23669.05078125
INFO:tools.evaluation_results_class:Current Best Return = -146.1905059814453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.52275682704811
INFO:tools.evaluation_results_class:Counted Episodes = 1538
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.39947509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 177.60052490234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 93.5177993774414
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22733.33984375
INFO:tools.evaluation_results_class:Current Best Return = -142.39947509765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.72404730617609
INFO:tools.evaluation_results_class:Counted Episodes = 1522
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -269.80194091796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.19805145263672
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.051877975463867
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26543.763671875
INFO:tools.evaluation_results_class:Current Best Return = -269.80194091796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.21688311688311
INFO:tools.evaluation_results_class:Counted Episodes = 1540
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -274.8966369628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.10335922241211
INFO:tools.evaluation_results_class:Average Discounted Reward = -28.863197326660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28486.349609375
INFO:tools.evaluation_results_class:Current Best Return = -274.8966369628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.66425279789335
INFO:tools.evaluation_results_class:Counted Episodes = 1519
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -271.4246826171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.5753059387207
INFO:tools.evaluation_results_class:Average Discounted Reward = -27.501792907714844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22907.12890625
INFO:tools.evaluation_results_class:Current Best Return = -271.4246826171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.77310924369748
INFO:tools.evaluation_results_class:Counted Episodes = 1547
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -270.563232421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 49.436767578125
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.649106979370117
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28645.548828125
INFO:tools.evaluation_results_class:Current Best Return = -270.563232421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.39765319426337
INFO:tools.evaluation_results_class:Counted Episodes = 1534
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -267.59857177734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 52.40141296386719
INFO:tools.evaluation_results_class:Average Discounted Reward = -23.141740798950195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25558.880859375
INFO:tools.evaluation_results_class:Current Best Return = -267.59857177734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.37122671804752
INFO:tools.evaluation_results_class:Counted Episodes = 1557
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -268.7572937011719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 51.242698669433594
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.203161239624023
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25465.525390625
INFO:tools.evaluation_results_class:Current Best Return = -268.7572937011719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.99805321219986
INFO:tools.evaluation_results_class:Counted Episodes = 1541
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -273.0738525390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.926143646240234
INFO:tools.evaluation_results_class:Average Discounted Reward = -27.673715591430664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27567.482421875
INFO:tools.evaluation_results_class:Current Best Return = -273.0738525390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.88366013071895
INFO:tools.evaluation_results_class:Counted Episodes = 1530
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -279.61175537109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.38823699951172
INFO:tools.evaluation_results_class:Average Discounted Reward = -33.31270217895508
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26426.068359375
INFO:tools.evaluation_results_class:Current Best Return = -279.61175537109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.50326797385621
INFO:tools.evaluation_results_class:Counted Episodes = 1530
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -270.9092712402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 49.09073257446289
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.21091651916504
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27707.09375
INFO:tools.evaluation_results_class:Current Best Return = -270.9092712402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.71943371943372
INFO:tools.evaluation_results_class:Counted Episodes = 1554
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -273.0612487792969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.93876266479492
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.138965606689453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28880.748046875
INFO:tools.evaluation_results_class:Current Best Return = -273.0612487792969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.35374592833877
INFO:tools.evaluation_results_class:Counted Episodes = 1535
INFO:robust_rl.robust_rl_trainer:Iteration 17 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -215.30003356933594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 104.69996643066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.232181549072266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27288.2109375
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.82356708293308
INFO:tools.evaluation_results_class:Counted Episodes = 3123
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.262693881988525
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 6.781640529632568
INFO:agents.father_agent:Step: 10, Training loss: 6.472992420196533
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -219.16439819335938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 100.8355941772461
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.740808486938477
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29142.693359375
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.57299741602067
INFO:tools.evaluation_results_class:Counted Episodes = 3096
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -242.2377166748047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 77.76228332519531
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.239946365356445
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33845.5859375
INFO:tools.evaluation_results_class:Current Best Return = -242.2377166748047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.01528940662541
INFO:tools.evaluation_results_class:Counted Episodes = 2747
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -218.4666290283203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 101.53337097167969
INFO:tools.evaluation_results_class:Average Discounted Reward = 20.27239990234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27475.435546875
INFO:tools.evaluation_results_class:Current Best Return = -218.4666290283203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.1220117975784
INFO:tools.evaluation_results_class:Counted Episodes = 3221
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 271.01600882895156
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 18 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -221.99168395996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 98.0083236694336
INFO:tools.evaluation_results_class:Average Discounted Reward = 18.06503677368164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31016.732421875
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.83098591549296
INFO:tools.evaluation_results_class:Counted Episodes = 3124
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.147149085998535
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 5.315516948699951
INFO:agents.father_agent:Step: 10, Training loss: 6.969302177429199
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -217.819580078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 102.180419921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.035308837890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27848.4453125
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.58093410108765
INFO:tools.evaluation_results_class:Counted Episodes = 3126
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -253.13034057617188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 66.86965942382812
INFO:tools.evaluation_results_class:Average Discounted Reward = -15.836868286132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35498.5078125
INFO:tools.evaluation_results_class:Current Best Return = -253.13034057617188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.23858615611194
INFO:tools.evaluation_results_class:Counted Episodes = 2716
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -220.18653869628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 99.81346130371094
INFO:tools.evaluation_results_class:Average Discounted Reward = 18.77472686767578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28496.4453125
INFO:tools.evaluation_results_class:Current Best Return = -220.18653869628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.37715179968701
INFO:tools.evaluation_results_class:Counted Episodes = 3195
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 272.2115376175773
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 19 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -216.72808837890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 103.27191162109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 20.0706729888916
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25381.427734375
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.94177863083813
INFO:tools.evaluation_results_class:Counted Episodes = 3126
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.656033515930176
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 6.125313758850098
INFO:agents.father_agent:Step: 10, Training loss: 6.597283363342285
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -225.89285278320312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 94.10713958740234
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.975698471069336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27288.994140625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.45663265306122
INFO:tools.evaluation_results_class:Counted Episodes = 3136
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -259.77337646484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 60.22661209106445
INFO:tools.evaluation_results_class:Average Discounted Reward = -20.31219482421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36111.18359375
INFO:tools.evaluation_results_class:Current Best Return = -259.77337646484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.62944162436548
INFO:tools.evaluation_results_class:Counted Episodes = 2758
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -227.21754455566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 92.78246307373047
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.439579963684082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29122.80078125
INFO:tools.evaluation_results_class:Current Best Return = -227.21754455566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.84846606755501
INFO:tools.evaluation_results_class:Counted Episodes = 3227
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 271.9205683818548
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 20 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -226.14857482910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 93.85142517089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.208460807800293
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26833.375
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.78514249119436
INFO:tools.evaluation_results_class:Counted Episodes = 3123
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.68526554107666
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 6.716350555419922
INFO:agents.father_agent:Step: 10, Training loss: 5.827922344207764
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -224.97137451171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 95.02862548828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 15.25382137298584
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25748.400390625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.23027989821882
INFO:tools.evaluation_results_class:Counted Episodes = 3144
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -251.3786163330078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 68.62139129638672
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.078451156616211
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30816.9921875
INFO:tools.evaluation_results_class:Current Best Return = -251.3786163330078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.6777183600713
INFO:tools.evaluation_results_class:Counted Episodes = 2805
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -233.37612915039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 86.62387084960938
INFO:tools.evaluation_results_class:Average Discounted Reward = 8.095239639282227
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27504.56640625
INFO:tools.evaluation_results_class:Current Best Return = -233.37612915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.15425366157682
INFO:tools.evaluation_results_class:Counted Episodes = 3209
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 276.3506928048374
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 21 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -233.1549530029297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 86.84504699707031
INFO:tools.evaluation_results_class:Average Discounted Reward = 9.019207000732422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28249.56640625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.40191693290735
INFO:tools.evaluation_results_class:Counted Episodes = 3130
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.684545516967773
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 6.109445095062256
INFO:agents.father_agent:Step: 10, Training loss: 5.235464572906494
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -229.6484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 90.35155487060547
INFO:tools.evaluation_results_class:Average Discounted Reward = 11.21147632598877
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26987.931640625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.29116338207247
INFO:tools.evaluation_results_class:Counted Episodes = 3146
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -260.50909423828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.49091720581055
INFO:tools.evaluation_results_class:Average Discounted Reward = -21.45743751525879
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34934.58984375
INFO:tools.evaluation_results_class:Current Best Return = -260.50909423828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.79869186046511
INFO:tools.evaluation_results_class:Counted Episodes = 2752
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -230.0215606689453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 89.97843170166016
INFO:tools.evaluation_results_class:Average Discounted Reward = 10.928144454956055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27054.427734375
INFO:tools.evaluation_results_class:Current Best Return = -230.0215606689453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.31669747381393
INFO:tools.evaluation_results_class:Counted Episodes = 3246
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 271.049426812946
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -104.47472381591797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 215.5252685546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 106.44020080566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8908.7470703125
INFO:tools.evaluation_results_class:Current Best Return = -104.47472381591797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 102.53992673992674
INFO:tools.evaluation_results_class:Counted Episodes = 1365
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -188.41358947753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 131.58641052246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 37.47227096557617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15806.1953125
INFO:tools.evaluation_results_class:Current Best Return = -188.41358947753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.87444608567208
INFO:tools.evaluation_results_class:Counted Episodes = 1354
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -197.72767639160156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 122.27232360839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 36.159271240234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22802.330078125
INFO:tools.evaluation_results_class:Current Best Return = -197.72767639160156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.27675276752767
INFO:tools.evaluation_results_class:Counted Episodes = 1355
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -181.83651733398438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 138.16348266601562
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.14912033081055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12106.265625
INFO:tools.evaluation_results_class:Current Best Return = -181.83651733398438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.133431085044
INFO:tools.evaluation_results_class:Counted Episodes = 1364
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -111.000732421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 208.999267578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 97.78679656982422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12095.986328125
INFO:tools.evaluation_results_class:Current Best Return = -111.000732421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.16911764705883
INFO:tools.evaluation_results_class:Counted Episodes = 1360
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.21829223632812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 149.78170776367188
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.22330474853516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27042.83984375
INFO:tools.evaluation_results_class:Current Best Return = -170.21829223632812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.28908554572271
INFO:tools.evaluation_results_class:Counted Episodes = 1356
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -125.86222076416016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 194.1377716064453
INFO:tools.evaluation_results_class:Average Discounted Reward = 91.34565734863281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16861.6796875
INFO:tools.evaluation_results_class:Current Best Return = -125.86222076416016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.76148148148148
INFO:tools.evaluation_results_class:Counted Episodes = 1350
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.44960021972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 149.55039978027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.22928619384766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29988.71875
INFO:tools.evaluation_results_class:Current Best Return = -170.44960021972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.16041206769684
INFO:tools.evaluation_results_class:Counted Episodes = 1359
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.1798858642578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 150.8201141357422
INFO:tools.evaluation_results_class:Average Discounted Reward = 66.7952651977539
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30810.138671875
INFO:tools.evaluation_results_class:Current Best Return = -169.1798858642578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.2834067547724
INFO:tools.evaluation_results_class:Counted Episodes = 1362
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -168.39015197753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 151.60984802246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 68.21051788330078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29540.79296875
INFO:tools.evaluation_results_class:Current Best Return = -168.39015197753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 102.72814107274063
INFO:tools.evaluation_results_class:Counted Episodes = 1361
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.61386108398438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 149.38613891601562
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.10770416259766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28165.998046875
INFO:tools.evaluation_results_class:Current Best Return = -170.61386108398438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.68238761974945
INFO:tools.evaluation_results_class:Counted Episodes = 1357
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -286.0228576660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.97715377807617
INFO:tools.evaluation_results_class:Average Discounted Reward = -44.031578063964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30644.34765625
INFO:tools.evaluation_results_class:Current Best Return = -286.0228576660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.28445099484156
INFO:tools.evaluation_results_class:Counted Episodes = 1357
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -278.070068359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.929927825927734
INFO:tools.evaluation_results_class:Average Discounted Reward = -37.723453521728516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30298.18359375
INFO:tools.evaluation_results_class:Current Best Return = -278.070068359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 102.7956204379562
INFO:tools.evaluation_results_class:Counted Episodes = 1370
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -292.8301086425781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 27.16989517211914
INFO:tools.evaluation_results_class:Average Discounted Reward = -49.34238815307617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30895.7578125
INFO:tools.evaluation_results_class:Current Best Return = -292.8301086425781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.24888226527571
INFO:tools.evaluation_results_class:Counted Episodes = 1342
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -282.5716552734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.42835998535156
INFO:tools.evaluation_results_class:Average Discounted Reward = -42.21730041503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27457.154296875
INFO:tools.evaluation_results_class:Current Best Return = -282.5716552734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.0482553823311
INFO:tools.evaluation_results_class:Counted Episodes = 1347
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -288.8118896484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 31.188112258911133
INFO:tools.evaluation_results_class:Average Discounted Reward = -46.89353561401367
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30054.158203125
INFO:tools.evaluation_results_class:Current Best Return = -288.8118896484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.65387509405568
INFO:tools.evaluation_results_class:Counted Episodes = 1329
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -279.7344970703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.265499114990234
INFO:tools.evaluation_results_class:Average Discounted Reward = -37.76476287841797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30559.96484375
INFO:tools.evaluation_results_class:Current Best Return = -279.7344970703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 102.97957695113057
INFO:tools.evaluation_results_class:Counted Episodes = 1371
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -286.8939514160156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.10603713989258
INFO:tools.evaluation_results_class:Average Discounted Reward = -43.30156326293945
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33972.171875
INFO:tools.evaluation_results_class:Current Best Return = -286.8939514160156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.0677466863034
INFO:tools.evaluation_results_class:Counted Episodes = 1358
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -283.1630554199219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.83694839477539
INFO:tools.evaluation_results_class:Average Discounted Reward = -40.70389175415039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31278.08203125
INFO:tools.evaluation_results_class:Current Best Return = -283.1630554199219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.25804038893044
INFO:tools.evaluation_results_class:Counted Episodes = 1337
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -287.904296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 32.09569549560547
INFO:tools.evaluation_results_class:Average Discounted Reward = -43.91769790649414
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32299.759765625
INFO:tools.evaluation_results_class:Current Best Return = -287.904296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.28338278931751
INFO:tools.evaluation_results_class:Counted Episodes = 1348
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -297.3042297363281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 22.695781707763672
INFO:tools.evaluation_results_class:Average Discounted Reward = -50.32765579223633
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34851.3828125
INFO:tools.evaluation_results_class:Current Best Return = -297.3042297363281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.87638786084382
INFO:tools.evaluation_results_class:Counted Episodes = 1351
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -296.09796142578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 23.90204620361328
INFO:tools.evaluation_results_class:Average Discounted Reward = -48.233455657958984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33997.60546875
INFO:tools.evaluation_results_class:Current Best Return = -296.09796142578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 102.97953216374269
INFO:tools.evaluation_results_class:Counted Episodes = 1368
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -294.8377685546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 25.162240982055664
INFO:tools.evaluation_results_class:Average Discounted Reward = -49.2100944519043
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30191.623046875
INFO:tools.evaluation_results_class:Current Best Return = -294.8377685546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.66076696165192
INFO:tools.evaluation_results_class:Counted Episodes = 1356
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -289.4152526855469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 30.58475112915039
INFO:tools.evaluation_results_class:Average Discounted Reward = -43.72621536254883
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34598.078125
INFO:tools.evaluation_results_class:Current Best Return = -289.4152526855469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.61880088823094
INFO:tools.evaluation_results_class:Counted Episodes = 1351
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -294.0621643066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 25.937824249267578
INFO:tools.evaluation_results_class:Average Discounted Reward = -47.807464599609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32304.568359375
INFO:tools.evaluation_results_class:Current Best Return = -294.0621643066406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.96521095484826
INFO:tools.evaluation_results_class:Counted Episodes = 1351
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -295.95306396484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 24.046945571899414
INFO:tools.evaluation_results_class:Average Discounted Reward = -49.210845947265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33595.796875
INFO:tools.evaluation_results_class:Current Best Return = -295.95306396484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.2086438152012
INFO:tools.evaluation_results_class:Counted Episodes = 1342
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -82.57736206054688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.42263793945312
INFO:tools.evaluation_results_class:Average Discounted Reward = 133.36790466308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5585.3017578125
INFO:tools.evaluation_results_class:Current Best Return = -82.57736206054688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.55626598465473
INFO:tools.evaluation_results_class:Counted Episodes = 1564
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -171.65623474121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 148.34376525878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 60.50463104248047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12017.7841796875
INFO:tools.evaluation_results_class:Current Best Return = -171.65623474121094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.46070287539936
INFO:tools.evaluation_results_class:Counted Episodes = 1565
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -166.27587890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 153.72412109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 70.93628692626953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16886.34375
INFO:tools.evaluation_results_class:Current Best Return = -166.27587890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.59974747474747
INFO:tools.evaluation_results_class:Counted Episodes = 1584
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.62811279296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 159.37188720703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 61.115970611572266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7908.373046875
INFO:tools.evaluation_results_class:Current Best Return = -160.62811279296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.61789137380191
INFO:tools.evaluation_results_class:Counted Episodes = 1565
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.9287109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.0712890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 134.71499633789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8608.8896484375
INFO:tools.evaluation_results_class:Current Best Return = -79.9287109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.7751134154245
INFO:tools.evaluation_results_class:Counted Episodes = 1543
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -136.9813690185547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 183.0186309814453
INFO:tools.evaluation_results_class:Average Discounted Reward = 99.71456146240234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23432.935546875
INFO:tools.evaluation_results_class:Current Best Return = -136.9813690185547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.01670951156812
INFO:tools.evaluation_results_class:Counted Episodes = 1556
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -110.33502197265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 209.66497802734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 115.58624267578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15019.583984375
INFO:tools.evaluation_results_class:Current Best Return = -110.33502197265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.22376738305942
INFO:tools.evaluation_results_class:Counted Episodes = 1582
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.11669921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 176.88330078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 95.64674377441406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27479.93359375
INFO:tools.evaluation_results_class:Current Best Return = -143.11669921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.05931656995487
INFO:tools.evaluation_results_class:Counted Episodes = 1551
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.8013916015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 176.1986083984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 95.93656921386719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25625.921875
INFO:tools.evaluation_results_class:Current Best Return = -143.8013916015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.28399746995572
INFO:tools.evaluation_results_class:Counted Episodes = 1581
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -148.02069091796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 171.97930908203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 91.37837982177734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26549.125
INFO:tools.evaluation_results_class:Current Best Return = -148.02069091796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.36199095022624
INFO:tools.evaluation_results_class:Counted Episodes = 1547
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.17636108398438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 178.82363891601562
INFO:tools.evaluation_results_class:Average Discounted Reward = 95.67029571533203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23621.912109375
INFO:tools.evaluation_results_class:Current Best Return = -141.17636108398438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.45686900958466
INFO:tools.evaluation_results_class:Counted Episodes = 1565
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -262.0044860839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.99550247192383
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.96320152282715
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24871.958984375
INFO:tools.evaluation_results_class:Current Best Return = -262.0044860839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.15552699228792
INFO:tools.evaluation_results_class:Counted Episodes = 1556
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -262.2315673828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.76844024658203
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.120718002319336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23633.1953125
INFO:tools.evaluation_results_class:Current Best Return = -262.2315673828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.7395766516998
INFO:tools.evaluation_results_class:Counted Episodes = 1559
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -263.15887451171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.84114074707031
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.338409423828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23289.173828125
INFO:tools.evaluation_results_class:Current Best Return = -263.15887451171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.12025316455696
INFO:tools.evaluation_results_class:Counted Episodes = 1580
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -257.8208923339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 62.179115295410156
INFO:tools.evaluation_results_class:Average Discounted Reward = -15.041045188903809
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22838.896484375
INFO:tools.evaluation_results_class:Current Best Return = -257.8208923339844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.99113924050633
INFO:tools.evaluation_results_class:Counted Episodes = 1580
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -264.0644226074219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.935585021972656
INFO:tools.evaluation_results_class:Average Discounted Reward = -20.26723861694336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24166.71875
INFO:tools.evaluation_results_class:Current Best Return = -264.0644226074219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.70280612244898
INFO:tools.evaluation_results_class:Counted Episodes = 1568
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -270.47271728515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 49.52729797363281
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.40367317199707
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26485.19921875
INFO:tools.evaluation_results_class:Current Best Return = -270.47271728515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.27488760436738
INFO:tools.evaluation_results_class:Counted Episodes = 1557
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -256.684814453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.315181732177734
INFO:tools.evaluation_results_class:Average Discounted Reward = -15.489507675170898
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23412.484375
INFO:tools.evaluation_results_class:Current Best Return = -256.684814453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.87059577194107
INFO:tools.evaluation_results_class:Counted Episodes = 1561
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -260.568603515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.431396484375
INFO:tools.evaluation_results_class:Average Discounted Reward = -17.97325897216797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23823.484375
INFO:tools.evaluation_results_class:Current Best Return = -260.568603515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.60306317804722
INFO:tools.evaluation_results_class:Counted Episodes = 1567
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -265.0331115722656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.966880798339844
INFO:tools.evaluation_results_class:Average Discounted Reward = -20.294458389282227
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25469.775390625
INFO:tools.evaluation_results_class:Current Best Return = -265.0331115722656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.3859872611465
INFO:tools.evaluation_results_class:Counted Episodes = 1570
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -270.3616638183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 49.63833999633789
INFO:tools.evaluation_results_class:Average Discounted Reward = -23.52608871459961
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25640.6875
INFO:tools.evaluation_results_class:Current Best Return = -270.3616638183594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.12779552715655
INFO:tools.evaluation_results_class:Counted Episodes = 1565
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -271.1318054199219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.868202209472656
INFO:tools.evaluation_results_class:Average Discounted Reward = -23.57219696044922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25686.443359375
INFO:tools.evaluation_results_class:Current Best Return = -271.1318054199219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.72488803582854
INFO:tools.evaluation_results_class:Counted Episodes = 1563
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -270.24615478515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 49.75385665893555
INFO:tools.evaluation_results_class:Average Discounted Reward = -22.934221267700195
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26304.806640625
INFO:tools.evaluation_results_class:Current Best Return = -270.24615478515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.24807197943444
INFO:tools.evaluation_results_class:Counted Episodes = 1556
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -268.5365295410156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 51.463478088378906
INFO:tools.evaluation_results_class:Average Discounted Reward = -22.519723892211914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25215.88671875
INFO:tools.evaluation_results_class:Current Best Return = -268.5365295410156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.89722042663219
INFO:tools.evaluation_results_class:Counted Episodes = 1547
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -269.93316650390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.066837310791016
INFO:tools.evaluation_results_class:Average Discounted Reward = -22.572696685791016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25215.4140625
INFO:tools.evaluation_results_class:Current Best Return = -269.93316650390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.56588160407384
INFO:tools.evaluation_results_class:Counted Episodes = 1571
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -276.3987121582031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.60130310058594
INFO:tools.evaluation_results_class:Average Discounted Reward = -28.216224670410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28272.25
INFO:tools.evaluation_results_class:Current Best Return = -276.3987121582031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.0527687296417
INFO:tools.evaluation_results_class:Counted Episodes = 1535
INFO:robust_rl.robust_rl_trainer:Iteration 22 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -222.83746337890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 97.16253662109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 16.4659366607666
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24618.712890625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.44587386256667
INFO:tools.evaluation_results_class:Counted Episodes = 3187
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.004436492919922
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 5.849732875823975
INFO:agents.father_agent:Step: 10, Training loss: 5.697507381439209
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -234.28973388671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 85.71026611328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.809758186340332
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32004.57421875
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.58842340901823
INFO:tools.evaluation_results_class:Counted Episodes = 3127
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -259.0377502441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 60.962249755859375
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.748512268066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36950.70703125
INFO:tools.evaluation_results_class:Current Best Return = -259.0377502441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.13911219853198
INFO:tools.evaluation_results_class:Counted Episodes = 2861
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -230.0708465576172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 89.92916107177734
INFO:tools.evaluation_results_class:Average Discounted Reward = 10.706620216369629
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28722.046875
INFO:tools.evaluation_results_class:Current Best Return = -230.0708465576172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.33923664122138
INFO:tools.evaluation_results_class:Counted Episodes = 3275
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 266.7382667979848
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 23 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -232.40806579589844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 87.5919418334961
INFO:tools.evaluation_results_class:Average Discounted Reward = 9.121830940246582
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28382.548828125
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.67254408060454
INFO:tools.evaluation_results_class:Counted Episodes = 3176
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.365240097045898
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 4.49222469329834
INFO:agents.father_agent:Step: 10, Training loss: 4.984988212585449
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -229.41358947753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 90.58641815185547
INFO:tools.evaluation_results_class:Average Discounted Reward = 10.786563873291016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29593.9140625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.34375
INFO:tools.evaluation_results_class:Counted Episodes = 3136
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -249.1922607421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 70.80774688720703
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.320307731628418
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33773.10546875
INFO:tools.evaluation_results_class:Current Best Return = -249.1922607421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 101.89073305670816
INFO:tools.evaluation_results_class:Counted Episodes = 2892
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -228.55416870117188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 91.4458236694336
INFO:tools.evaluation_results_class:Average Discounted Reward = 11.0582275390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28428.59765625
INFO:tools.evaluation_results_class:Current Best Return = -228.55416870117188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.73455448618442
INFO:tools.evaluation_results_class:Counted Episodes = 3221
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 264.14522750847453
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 24 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -227.85728454589844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 92.1427230834961
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.460945129394531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28061.4453125
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.30713612071676
INFO:tools.evaluation_results_class:Counted Episodes = 3181
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.550980567932129
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 4.77401065826416
INFO:agents.father_agent:Step: 10, Training loss: 5.05458927154541
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -229.77578735351562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 90.2242202758789
INFO:tools.evaluation_results_class:Average Discounted Reward = 11.893549919128418
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27835.69140625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.69309383710127
INFO:tools.evaluation_results_class:Counted Episodes = 3229
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -247.8511199951172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.14888763427734
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.831952095031738
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31936.072265625
INFO:tools.evaluation_results_class:Current Best Return = -247.8511199951172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.71100607697501
INFO:tools.evaluation_results_class:Counted Episodes = 2962
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -231.1748809814453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 88.82511901855469
INFO:tools.evaluation_results_class:Average Discounted Reward = 11.179306983947754
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29079.255859375
INFO:tools.evaluation_results_class:Current Best Return = -231.1748809814453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.81812652068126
INFO:tools.evaluation_results_class:Counted Episodes = 3288
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 258.0948833804644
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 25 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -226.29345703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 93.70654296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.753419876098633
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25735.69921875
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.42367601246106
INFO:tools.evaluation_results_class:Counted Episodes = 3210
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.051921844482422
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 20.31%
INFO:agents.father_agent:Step: 5, Training loss: 5.954015254974365
INFO:agents.father_agent:Step: 10, Training loss: 4.801324367523193
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -228.46873474121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 91.5312728881836
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.952372550964355
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25698.0625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.12136222910216
INFO:tools.evaluation_results_class:Counted Episodes = 3230
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -248.62225341796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.37774658203125
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.300068378448486
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31840.576171875
INFO:tools.evaluation_results_class:Current Best Return = -248.62225341796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 98.57361758827449
INFO:tools.evaluation_results_class:Counted Episodes = 3002
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -230.85816955566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 89.1418228149414
INFO:tools.evaluation_results_class:Average Discounted Reward = 11.605318069458008
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28504.349609375
INFO:tools.evaluation_results_class:Current Best Return = -230.85816955566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.19572417946402
INFO:tools.evaluation_results_class:Counted Episodes = 3321
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 258.091025683051
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 26 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -229.9645233154297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 90.03547668457031
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.771232604980469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26027.7421875
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.65810397553517
INFO:tools.evaluation_results_class:Counted Episodes = 3270
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.489285945892334
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 5.035699844360352
INFO:agents.father_agent:Step: 10, Training loss: 4.775872230529785
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -227.34104919433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 92.65894317626953
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.73250675201416
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26366.181640625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.0086499845536
INFO:tools.evaluation_results_class:Counted Episodes = 3237
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -251.42091369628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 68.57908630371094
INFO:tools.evaluation_results_class:Average Discounted Reward = -9.438023567199707
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31776.28515625
INFO:tools.evaluation_results_class:Current Best Return = -251.42091369628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.94771674387823
INFO:tools.evaluation_results_class:Counted Episodes = 3022
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -222.8068389892578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 97.19316101074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 18.651580810546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26681.72265625
INFO:tools.evaluation_results_class:Current Best Return = -222.8068389892578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.70775582424064
INFO:tools.evaluation_results_class:Counted Episodes = 3391
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 253.847157252783
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=3, o3y=1, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -90.69055938720703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.3094482421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 119.8642349243164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6325.634765625
INFO:tools.evaluation_results_class:Current Best Return = -90.69055938720703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.34045485871813
INFO:tools.evaluation_results_class:Counted Episodes = 1451
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -179.92532348632812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 140.07467651367188
INFO:tools.evaluation_results_class:Average Discounted Reward = 49.81534194946289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11968.8837890625
INFO:tools.evaluation_results_class:Current Best Return = -179.92532348632812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.37406653088934
INFO:tools.evaluation_results_class:Counted Episodes = 1473
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -189.47698974609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 130.52301025390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 46.78889465332031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21882.359375
INFO:tools.evaluation_results_class:Current Best Return = -189.47698974609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.69418132611638
INFO:tools.evaluation_results_class:Counted Episodes = 1478
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -163.57318115234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 156.42681884765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 55.80283737182617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 10080.724609375
INFO:tools.evaluation_results_class:Current Best Return = -163.57318115234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.78385772913816
INFO:tools.evaluation_results_class:Counted Episodes = 1462
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -88.12806701660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 231.87193298339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 120.49003601074219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7400.77392578125
INFO:tools.evaluation_results_class:Current Best Return = -88.12806701660156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.48228882833787
INFO:tools.evaluation_results_class:Counted Episodes = 1468
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -172.65411376953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 147.34588623046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 64.9336929321289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27759.314453125
INFO:tools.evaluation_results_class:Current Best Return = -172.65411376953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.05068493150685
INFO:tools.evaluation_results_class:Counted Episodes = 1460
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -138.84779357910156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 181.15220642089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 85.35719299316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17858.236328125
INFO:tools.evaluation_results_class:Current Best Return = -138.84779357910156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.91873278236915
INFO:tools.evaluation_results_class:Counted Episodes = 1452
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.96937561035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 149.03062438964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 65.8429946899414
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30123.568359375
INFO:tools.evaluation_results_class:Current Best Return = -170.96937561035156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.88239387613083
INFO:tools.evaluation_results_class:Counted Episodes = 1437
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -170.07473754882812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 149.92526245117188
INFO:tools.evaluation_results_class:Average Discounted Reward = 67.20384979248047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29137.537109375
INFO:tools.evaluation_results_class:Current Best Return = -170.07473754882812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.62422145328719
INFO:tools.evaluation_results_class:Counted Episodes = 1445
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -164.92074584960938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 155.07925415039062
INFO:tools.evaluation_results_class:Average Discounted Reward = 70.99771881103516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25890.8125
INFO:tools.evaluation_results_class:Current Best Return = -164.92074584960938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.96002756719504
INFO:tools.evaluation_results_class:Counted Episodes = 1451
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -162.39175415039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 157.60824584960938
INFO:tools.evaluation_results_class:Average Discounted Reward = 72.26878356933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26237.052734375
INFO:tools.evaluation_results_class:Current Best Return = -162.39175415039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.00962199312715
INFO:tools.evaluation_results_class:Counted Episodes = 1455
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -272.623779296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 47.376224517822266
INFO:tools.evaluation_results_class:Average Discounted Reward = -29.069604873657227
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31337.87890625
INFO:tools.evaluation_results_class:Current Best Return = -272.623779296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.83636363636364
INFO:tools.evaluation_results_class:Counted Episodes = 1430
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -273.1666564941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.83333206176758
INFO:tools.evaluation_results_class:Average Discounted Reward = -30.953201293945312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30629.162109375
INFO:tools.evaluation_results_class:Current Best Return = -273.1666564941406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.72337482710927
INFO:tools.evaluation_results_class:Counted Episodes = 1446
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -271.47314453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.526859283447266
INFO:tools.evaluation_results_class:Average Discounted Reward = -28.478992462158203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30931.791015625
INFO:tools.evaluation_results_class:Current Best Return = -271.47314453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.8030303030303
INFO:tools.evaluation_results_class:Counted Episodes = 1452
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -269.4520263671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.5479850769043
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.07500648498535
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29528.634765625
INFO:tools.evaluation_results_class:Current Best Return = -269.4520263671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.76495132127955
INFO:tools.evaluation_results_class:Counted Episodes = 1438
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -268.6484069824219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 51.35159683227539
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.00417709350586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32301.48046875
INFO:tools.evaluation_results_class:Current Best Return = -268.6484069824219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.55894590846047
INFO:tools.evaluation_results_class:Counted Episodes = 1442
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -261.2008361816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 58.79917526245117
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.817203521728516
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31478.375
INFO:tools.evaluation_results_class:Current Best Return = -261.2008361816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.10453920220083
INFO:tools.evaluation_results_class:Counted Episodes = 1454
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -265.85015869140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.14982986450195
INFO:tools.evaluation_results_class:Average Discounted Reward = -23.36489486694336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28521.69140625
INFO:tools.evaluation_results_class:Current Best Return = -265.85015869140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.72920962199312
INFO:tools.evaluation_results_class:Counted Episodes = 1455
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -269.9814147949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.01859664916992
INFO:tools.evaluation_results_class:Average Discounted Reward = -27.40468406677246
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27545.736328125
INFO:tools.evaluation_results_class:Current Best Return = -269.9814147949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.9435261707989
INFO:tools.evaluation_results_class:Counted Episodes = 1452
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -281.5044860839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.49552536010742
INFO:tools.evaluation_results_class:Average Discounted Reward = -34.972137451171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35188.65625
INFO:tools.evaluation_results_class:Current Best Return = -281.5044860839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.33998623537508
INFO:tools.evaluation_results_class:Counted Episodes = 1453
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -268.3990478515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 51.600955963134766
INFO:tools.evaluation_results_class:Average Discounted Reward = -23.962045669555664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29321.642578125
INFO:tools.evaluation_results_class:Current Best Return = -268.3990478515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.62482946793997
INFO:tools.evaluation_results_class:Counted Episodes = 1466
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -269.3075256347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.692466735839844
INFO:tools.evaluation_results_class:Average Discounted Reward = -24.313562393188477
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27824.3828125
INFO:tools.evaluation_results_class:Current Best Return = -269.3075256347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.6972602739726
INFO:tools.evaluation_results_class:Counted Episodes = 1460
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -265.33587646484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.66413879394531
INFO:tools.evaluation_results_class:Average Discounted Reward = -20.805124282836914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29484.98828125
INFO:tools.evaluation_results_class:Current Best Return = -265.33587646484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.59724137931035
INFO:tools.evaluation_results_class:Counted Episodes = 1450
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -272.8894958496094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 47.110496520996094
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.362520217895508
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31448.388671875
INFO:tools.evaluation_results_class:Current Best Return = -272.8894958496094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.353591160221
INFO:tools.evaluation_results_class:Counted Episodes = 1448
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -264.2413024902344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.758689880371094
INFO:tools.evaluation_results_class:Average Discounted Reward = -19.911409378051758
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29242.255859375
INFO:tools.evaluation_results_class:Current Best Return = -264.2413024902344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.24676209952284
INFO:tools.evaluation_results_class:Counted Episodes = 1467
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -269.99383544921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.006168365478516
INFO:tools.evaluation_results_class:Average Discounted Reward = -24.48138427734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30323.193359375
INFO:tools.evaluation_results_class:Current Best Return = -269.99383544921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.89856065798492
INFO:tools.evaluation_results_class:Counted Episodes = 1459
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -263.2286682128906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.771331787109375
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.730751037597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30950.859375
INFO:tools.evaluation_results_class:Current Best Return = -263.2286682128906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.92764505119455
INFO:tools.evaluation_results_class:Counted Episodes = 1465
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -271.7113037109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.28870391845703
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.19478988647461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31220.24609375
INFO:tools.evaluation_results_class:Current Best Return = -271.7113037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.8926080892608
INFO:tools.evaluation_results_class:Counted Episodes = 1434
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -268.45269775390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 51.54728698730469
INFO:tools.evaluation_results_class:Average Discounted Reward = -23.706769943237305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29409.36328125
INFO:tools.evaluation_results_class:Current Best Return = -268.45269775390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 98.02781641168289
INFO:tools.evaluation_results_class:Counted Episodes = 1438
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -267.6666564941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 52.33333206176758
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.190200805664062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29338.8984375
INFO:tools.evaluation_results_class:Current Best Return = -267.6666564941406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.72677595628416
INFO:tools.evaluation_results_class:Counted Episodes = 1464
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -267.7287902832031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 52.271209716796875
INFO:tools.evaluation_results_class:Average Discounted Reward = -24.68061637878418
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32793.734375
INFO:tools.evaluation_results_class:Current Best Return = -267.7287902832031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 98.2739916550765
INFO:tools.evaluation_results_class:Counted Episodes = 1438
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.14161682128906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.85838317871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 145.06173706054688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4567.6748046875
INFO:tools.evaluation_results_class:Current Best Return = -70.14161682128906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.04761904761905
INFO:tools.evaluation_results_class:Counted Episodes = 1617
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -165.39816284179688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 154.60183715820312
INFO:tools.evaluation_results_class:Average Discounted Reward = 67.55253601074219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8599.8916015625
INFO:tools.evaluation_results_class:Current Best Return = -165.39816284179688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.6
INFO:tools.evaluation_results_class:Counted Episodes = 1630
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -160.51580810546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 159.48419189453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 76.08232116699219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17131.72265625
INFO:tools.evaluation_results_class:Current Best Return = -160.51580810546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.88034717916925
INFO:tools.evaluation_results_class:Counted Episodes = 1613
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -147.13124084472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 172.86875915527344
INFO:tools.evaluation_results_class:Average Discounted Reward = 75.67192077636719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6916.69970703125
INFO:tools.evaluation_results_class:Current Best Return = -147.13124084472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.61121380160198
INFO:tools.evaluation_results_class:Counted Episodes = 1623
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.82048034667969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.17950439453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 159.780029296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4282.20654296875
INFO:tools.evaluation_results_class:Current Best Return = -52.82048034667969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.19925971622455
INFO:tools.evaluation_results_class:Counted Episodes = 1621
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -141.4623565673828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 178.5376434326172
INFO:tools.evaluation_results_class:Average Discounted Reward = 96.14254760742188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23974.142578125
INFO:tools.evaluation_results_class:Current Best Return = -141.4623565673828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.2943372744244
INFO:tools.evaluation_results_class:Counted Episodes = 1607
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -114.60668182373047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 205.39332580566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 112.4859619140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12347.955078125
INFO:tools.evaluation_results_class:Current Best Return = -114.60668182373047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.04761904761905
INFO:tools.evaluation_results_class:Counted Episodes = 1617
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.39492797851562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 176.60507202148438
INFO:tools.evaluation_results_class:Average Discounted Reward = 94.31885528564453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24767.041015625
INFO:tools.evaluation_results_class:Current Best Return = -143.39492797851562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.59703337453647
INFO:tools.evaluation_results_class:Counted Episodes = 1618
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -139.16326904296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 180.83673095703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 97.71012878417969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22799.400390625
INFO:tools.evaluation_results_class:Current Best Return = -139.16326904296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.07730364873223
INFO:tools.evaluation_results_class:Counted Episodes = 1617
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -137.58721923828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 182.41278076171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 98.15642547607422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21706.609375
INFO:tools.evaluation_results_class:Current Best Return = -137.58721923828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.52079453755431
INFO:tools.evaluation_results_class:Counted Episodes = 1611
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -144.73028564453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 175.26971435546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 92.79145812988281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23829.654296875
INFO:tools.evaluation_results_class:Current Best Return = -144.73028564453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.0037546933667
INFO:tools.evaluation_results_class:Counted Episodes = 1598
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -252.61500549316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.38499450683594
INFO:tools.evaluation_results_class:Average Discounted Reward = -8.149421691894531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28534.529296875
INFO:tools.evaluation_results_class:Current Best Return = -252.61500549316406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.46063236205828
INFO:tools.evaluation_results_class:Counted Episodes = 1613
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -254.7997589111328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 65.20024871826172
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.88093090057373
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25089.470703125
INFO:tools.evaluation_results_class:Current Best Return = -254.7997589111328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.16440049443757
INFO:tools.evaluation_results_class:Counted Episodes = 1618
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -248.9573516845703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.04264831542969
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.883640766143799
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22427.525390625
INFO:tools.evaluation_results_class:Current Best Return = -248.9573516845703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.96786155747837
INFO:tools.evaluation_results_class:Counted Episodes = 1618
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -245.2523193359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 74.74768829345703
INFO:tools.evaluation_results_class:Average Discounted Reward = -3.147014617919922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22797.734375
INFO:tools.evaluation_results_class:Current Best Return = -245.2523193359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.57742134484886
INFO:tools.evaluation_results_class:Counted Episodes = 1621
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -257.546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 62.4531364440918
INFO:tools.evaluation_results_class:Average Discounted Reward = -13.065749168395996
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27709.150390625
INFO:tools.evaluation_results_class:Current Best Return = -257.546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.72563625077592
INFO:tools.evaluation_results_class:Counted Episodes = 1611
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -255.08717346191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 64.91282653808594
INFO:tools.evaluation_results_class:Average Discounted Reward = -11.445916175842285
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24233.39453125
INFO:tools.evaluation_results_class:Current Best Return = -255.08717346191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.50186799501869
INFO:tools.evaluation_results_class:Counted Episodes = 1606
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -256.5333251953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.46666717529297
INFO:tools.evaluation_results_class:Average Discounted Reward = -11.9391450881958
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28749.734375
INFO:tools.evaluation_results_class:Current Best Return = -256.5333251953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.80685358255452
INFO:tools.evaluation_results_class:Counted Episodes = 1605
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -250.02920532226562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.97078704833984
INFO:tools.evaluation_results_class:Average Discounted Reward = -7.236603736877441
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24747.103515625
INFO:tools.evaluation_results_class:Current Best Return = -250.02920532226562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.42821628340585
INFO:tools.evaluation_results_class:Counted Episodes = 1609
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -249.103271484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 70.89672088623047
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.677176475524902
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22362.169921875
INFO:tools.evaluation_results_class:Current Best Return = -249.103271484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.97711811997526
INFO:tools.evaluation_results_class:Counted Episodes = 1617
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -252.07972717285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.92027282714844
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.941958904266357
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25128.94140625
INFO:tools.evaluation_results_class:Current Best Return = -252.07972717285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.16069221260815
INFO:tools.evaluation_results_class:Counted Episodes = 1618
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -252.29673767089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.7032699584961
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.945834159851074
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24573.0703125
INFO:tools.evaluation_results_class:Current Best Return = -252.29673767089844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.01789019123997
INFO:tools.evaluation_results_class:Counted Episodes = 1621
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -252.66543579101562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.33456420898438
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.824805736541748
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27210.41015625
INFO:tools.evaluation_results_class:Current Best Return = -252.66543579101562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.24197530864197
INFO:tools.evaluation_results_class:Counted Episodes = 1620
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -244.8919219970703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 75.10807800292969
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.19876737892627716
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22663.029296875
INFO:tools.evaluation_results_class:Current Best Return = -244.8919219970703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.14693381906497
INFO:tools.evaluation_results_class:Counted Episodes = 1647
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -250.57266235351562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.42733764648438
INFO:tools.evaluation_results_class:Average Discounted Reward = -5.3380632400512695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25244.833984375
INFO:tools.evaluation_results_class:Current Best Return = -250.57266235351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.2591218305504
INFO:tools.evaluation_results_class:Counted Episodes = 1617
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -244.54696655273438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 75.4530258178711
INFO:tools.evaluation_results_class:Average Discounted Reward = -0.012110121548175812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25893.0390625
INFO:tools.evaluation_results_class:Current Best Return = -244.54696655273438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.20519159456119
INFO:tools.evaluation_results_class:Counted Episodes = 1618
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -257.58544921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 62.41454315185547
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.8837251663208
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26774.80078125
INFO:tools.evaluation_results_class:Current Best Return = -257.58544921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.64449968924798
INFO:tools.evaluation_results_class:Counted Episodes = 1609
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -251.17654418945312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 68.82345581054688
INFO:tools.evaluation_results_class:Average Discounted Reward = -5.6486005783081055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24163.79296875
INFO:tools.evaluation_results_class:Current Best Return = -251.17654418945312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.85277947464874
INFO:tools.evaluation_results_class:Counted Episodes = 1637
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -252.3758544921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.6241455078125
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.590829849243164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27700.53515625
INFO:tools.evaluation_results_class:Current Best Return = -252.3758544921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.78699690402476
INFO:tools.evaluation_results_class:Counted Episodes = 1615
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -254.50125122070312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 65.4987564086914
INFO:tools.evaluation_results_class:Average Discounted Reward = -11.580540657043457
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25561.91796875
INFO:tools.evaluation_results_class:Current Best Return = -254.50125122070312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.80548628428927
INFO:tools.evaluation_results_class:Counted Episodes = 1604
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -256.42608642578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 63.57391357421875
INFO:tools.evaluation_results_class:Average Discounted Reward = -12.228346824645996
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27635.37890625
INFO:tools.evaluation_results_class:Current Best Return = -256.42608642578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.18012422360249
INFO:tools.evaluation_results_class:Counted Episodes = 1610
INFO:robust_rl.robust_rl_trainer:Iteration 27 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -225.8260498046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 94.1739501953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 15.401535987854004
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26356.20703125
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.65332512315271
INFO:tools.evaluation_results_class:Counted Episodes = 3248
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.340553283691406
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 4.376034736633301
INFO:agents.father_agent:Step: 10, Training loss: 3.8455278873443604
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -220.82943725585938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 99.17057037353516
INFO:tools.evaluation_results_class:Average Discounted Reward = 20.25263023376465
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24824.673828125
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.53146853146853
INFO:tools.evaluation_results_class:Counted Episodes = 3289
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -242.15786743164062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 77.8421401977539
INFO:tools.evaluation_results_class:Average Discounted Reward = -2.4624054431915283
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29777.423828125
INFO:tools.evaluation_results_class:Current Best Return = -242.15786743164062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.02625533311453
INFO:tools.evaluation_results_class:Counted Episodes = 3047
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -225.2868194580078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 94.71318054199219
INFO:tools.evaluation_results_class:Average Discounted Reward = 16.56528663635254
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26306.232421875
INFO:tools.evaluation_results_class:Current Best Return = -225.2868194580078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.2125890736342
INFO:tools.evaluation_results_class:Counted Episodes = 3368
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 247.8403223416778
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 28 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -227.27798461914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 92.72201538085938
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.957589149475098
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26253.279296875
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.68562691131498
INFO:tools.evaluation_results_class:Counted Episodes = 3270
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.6586623191833496
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 3.9978761672973633
INFO:agents.father_agent:Step: 10, Training loss: 4.252756595611572
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -220.57510375976562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 99.42489624023438
INFO:tools.evaluation_results_class:Average Discounted Reward = 19.88353157043457
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27246.712890625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.31207847946045
INFO:tools.evaluation_results_class:Counted Episodes = 3262
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -241.14471435546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 78.85528564453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.06935912370681763
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31606.873046875
INFO:tools.evaluation_results_class:Current Best Return = -241.14471435546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.2240650406504
INFO:tools.evaluation_results_class:Counted Episodes = 3075
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -225.28634643554688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 94.7136459350586
INFO:tools.evaluation_results_class:Average Discounted Reward = 17.482059478759766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28668.55859375
INFO:tools.evaluation_results_class:Current Best Return = -225.28634643554688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.35816448152562
INFO:tools.evaluation_results_class:Counted Episodes = 3356
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 247.5723677754153
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 29 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -221.17979431152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 98.82020568847656
INFO:tools.evaluation_results_class:Average Discounted Reward = 20.261327743530273
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28372.583984375
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.85653235653236
INFO:tools.evaluation_results_class:Counted Episodes = 3276
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.384200096130371
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 3.7639102935791016
INFO:agents.father_agent:Step: 10, Training loss: 3.514407157897949
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -218.09564208984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 101.90435028076172
INFO:tools.evaluation_results_class:Average Discounted Reward = 22.27958869934082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26242.158203125
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.51808706315144
INFO:tools.evaluation_results_class:Counted Episodes = 3262
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -235.88180541992188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 84.11819458007812
INFO:tools.evaluation_results_class:Average Discounted Reward = 4.957912921905518
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31166.21484375
INFO:tools.evaluation_results_class:Current Best Return = -235.88180541992188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.23252818035427
INFO:tools.evaluation_results_class:Counted Episodes = 3105
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -219.94203186035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 100.05796813964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.57794761657715
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26632.298828125
INFO:tools.evaluation_results_class:Current Best Return = -219.94203186035156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.17895362663496
INFO:tools.evaluation_results_class:Counted Episodes = 3364
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 244.45183731479014
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 30 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -215.36151123046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 104.63848114013672
INFO:tools.evaluation_results_class:Average Discounted Reward = 24.961231231689453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25206.251953125
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.37757575757576
INFO:tools.evaluation_results_class:Counted Episodes = 3300
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.874908447265625
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 3.911752223968506
INFO:agents.father_agent:Step: 10, Training loss: 4.036354064941406
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -219.27317810058594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 100.7268295288086
INFO:tools.evaluation_results_class:Average Discounted Reward = 22.388748168945312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26378.37109375
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.09024166411747
INFO:tools.evaluation_results_class:Counted Episodes = 3269
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -241.24009704589844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 78.7599105834961
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.28024742007255554
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31284.84375
INFO:tools.evaluation_results_class:Current Best Return = -241.24009704589844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.1585445094217
INFO:tools.evaluation_results_class:Counted Episodes = 3078
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -219.8721160888672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 100.12788391113281
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.811748504638672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25841.6015625
INFO:tools.evaluation_results_class:Current Best Return = -219.8721160888672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.6915334517466
INFO:tools.evaluation_results_class:Counted Episodes = 3378
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 242.35523776284728
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 31 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -219.94947814941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 100.0505142211914
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.97635269165039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25124.474609375
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.5173463177115
INFO:tools.evaluation_results_class:Counted Episodes = 3286
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.726033687591553
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 18.75%
INFO:agents.father_agent:Step: 5, Training loss: 3.9015259742736816
INFO:agents.father_agent:Step: 10, Training loss: 3.2035040855407715
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -221.75555419921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 98.24444580078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.678876876831055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26433.765625
INFO:tools.evaluation_results_class:Current Best Return = -188.39773559570312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.75190258751903
INFO:tools.evaluation_results_class:Counted Episodes = 3285
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -242.7008819580078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 77.29911804199219
INFO:tools.evaluation_results_class:Average Discounted Reward = -0.16399545967578888
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29999.83203125
INFO:tools.evaluation_results_class:Current Best Return = -242.7008819580078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.75743707093821
INFO:tools.evaluation_results_class:Counted Episodes = 3059
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -217.64849853515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 102.35149383544922
INFO:tools.evaluation_results_class:Average Discounted Reward = 24.5757999420166
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24775.1875
INFO:tools.evaluation_results_class:Current Best Return = -217.64849853515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.20047379330767
INFO:tools.evaluation_results_class:Counted Episodes = 3377
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 240.48680540313833
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=3, o3x=4, o3y=2, o4x=5, o4y=8, o5x=8, o5y=5
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -79.70793151855469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.2920684814453
INFO:tools.evaluation_results_class:Average Discounted Reward = 128.392822265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5682.76220703125
INFO:tools.evaluation_results_class:Current Best Return = -79.70793151855469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.73871409028727
INFO:tools.evaluation_results_class:Counted Episodes = 1462
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -173.78013610839844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 146.21986389160156
INFO:tools.evaluation_results_class:Average Discounted Reward = 55.68980026245117
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12875.380859375
INFO:tools.evaluation_results_class:Current Best Return = -173.78013610839844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.40958904109588
INFO:tools.evaluation_results_class:Counted Episodes = 1460
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -196.26702880859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 123.73297119140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 38.62651062011719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20263.8671875
INFO:tools.evaluation_results_class:Current Best Return = -196.26702880859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.67847411444141
INFO:tools.evaluation_results_class:Counted Episodes = 1468
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -151.02865600585938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 168.97134399414062
INFO:tools.evaluation_results_class:Average Discounted Reward = 67.71138763427734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9107.619140625
INFO:tools.evaluation_results_class:Current Best Return = -151.02865600585938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.35879945429741
INFO:tools.evaluation_results_class:Counted Episodes = 1466
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.55298614501953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.44700622558594
INFO:tools.evaluation_results_class:Average Discounted Reward = 129.7926788330078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6325.3955078125
INFO:tools.evaluation_results_class:Current Best Return = -76.55298614501953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.26358695652173
INFO:tools.evaluation_results_class:Counted Episodes = 1472
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -182.96754455566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 137.03245544433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 55.35120391845703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29211.69921875
INFO:tools.evaluation_results_class:Current Best Return = -182.96754455566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.47194050033806
INFO:tools.evaluation_results_class:Counted Episodes = 1479
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -150.06820678710938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 169.93179321289062
INFO:tools.evaluation_results_class:Average Discounted Reward = 74.39179992675781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17297.390625
INFO:tools.evaluation_results_class:Current Best Return = -150.06820678710938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.32605729877217
INFO:tools.evaluation_results_class:Counted Episodes = 1466
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -183.62008666992188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 136.37991333007812
INFO:tools.evaluation_results_class:Average Discounted Reward = 54.988426208496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31517.978515625
INFO:tools.evaluation_results_class:Current Best Return = -183.62008666992188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.00407055630936
INFO:tools.evaluation_results_class:Counted Episodes = 1474
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -184.43951416015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 135.56048583984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 53.152862548828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30010.6796875
INFO:tools.evaluation_results_class:Current Best Return = -184.43951416015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.67737525632262
INFO:tools.evaluation_results_class:Counted Episodes = 1463
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.9620361328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 141.0379638671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 58.81936264038086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30364.873046875
INFO:tools.evaluation_results_class:Current Best Return = -178.9620361328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.58169491525423
INFO:tools.evaluation_results_class:Counted Episodes = 1475
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -177.1827392578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 142.8172607421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 59.08364486694336
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28829.357421875
INFO:tools.evaluation_results_class:Current Best Return = -177.1827392578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.41537424140256
INFO:tools.evaluation_results_class:Counted Episodes = 1483
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -258.1886291503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 61.81138610839844
INFO:tools.evaluation_results_class:Average Discounted Reward = -14.837401390075684
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32647.0625
INFO:tools.evaluation_results_class:Current Best Return = -258.1886291503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 97.04252400548697
INFO:tools.evaluation_results_class:Counted Episodes = 1458
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -254.51315307617188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 65.48685455322266
INFO:tools.evaluation_results_class:Average Discounted Reward = -11.977768898010254
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29106.345703125
INFO:tools.evaluation_results_class:Current Best Return = -254.51315307617188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.74308833445718
INFO:tools.evaluation_results_class:Counted Episodes = 1483
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -252.7537078857422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 67.24629211425781
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.154094696044922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29472.013671875
INFO:tools.evaluation_results_class:Current Best Return = -252.7537078857422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.5600539811066
INFO:tools.evaluation_results_class:Counted Episodes = 1482
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -250.82052612304688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.1794662475586
INFO:tools.evaluation_results_class:Average Discounted Reward = -8.402369499206543
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33003.0546875
INFO:tools.evaluation_results_class:Current Best Return = -250.82052612304688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 96.21414004078858
INFO:tools.evaluation_results_class:Counted Episodes = 1471
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
2025-08-20 04:49:05.940312: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-20 04:49:05.942208: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 04:49:05.972320: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-20 04:49:05.972363: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-20 04:49:05.973648: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-20 04:49:05.979297: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 04:49:05.979474: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-20 04:49:06.513111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/rover/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/rover/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"value"}max=? [F "empty"] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 86 states and 19241 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"value"}max=? [F "label_empty"] 
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 416.1156005859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 417.1156311035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 329.7729797363281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 67239.59375
INFO:tools.evaluation_results_class:Current Best Return = 416.1156005859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.72353741496599
INFO:tools.evaluation_results_class:Counted Episodes = 3675
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 528.17236328125
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 34.38%
INFO:agents.father_agent:Step: 5, Training loss: 17.73919677734375
INFO:agents.father_agent:Step: 10, Training loss: 20.70901107788086
INFO:agents.father_agent:Step: 15, Training loss: 22.021059036254883
INFO:agents.father_agent:Step: 20, Training loss: 20.306442260742188
INFO:agents.father_agent:Step: 25, Training loss: 23.305007934570312
INFO:agents.father_agent:Step: 30, Training loss: 25.834840774536133
INFO:agents.father_agent:Step: 35, Training loss: 24.608617782592773
INFO:agents.father_agent:Step: 40, Training loss: 22.04049301147461
INFO:agents.father_agent:Step: 45, Training loss: 28.518415451049805
INFO:agents.father_agent:Step: 50, Training loss: 26.08437156677246
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 39.06%
INFO:agents.father_agent:Step: 55, Training loss: 27.83200454711914
INFO:agents.father_agent:Step: 60, Training loss: 27.272430419921875
INFO:agents.father_agent:Step: 65, Training loss: 28.750465393066406
INFO:agents.father_agent:Step: 70, Training loss: 30.375967025756836
INFO:agents.father_agent:Step: 75, Training loss: 25.372711181640625
INFO:agents.father_agent:Step: 80, Training loss: 34.6988639831543
INFO:agents.father_agent:Step: 85, Training loss: 22.354347229003906
INFO:agents.father_agent:Step: 90, Training loss: 27.588546752929688
INFO:agents.father_agent:Step: 95, Training loss: 24.675914764404297
INFO:agents.father_agent:Step: 100, Training loss: 24.599807739257812
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 513.5184936523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 514.5184936523438
INFO:tools.evaluation_results_class:Average Discounted Reward = 425.0608215332031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 121998.6171875
INFO:tools.evaluation_results_class:Current Best Return = 513.5184936523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.553687749527214
INFO:tools.evaluation_results_class:Counted Episodes = 4759
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 45.31%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 523.3862915039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 524.38623046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 431.37481689453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 127236.1328125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.41020191285866
INFO:tools.evaluation_results_class:Counted Episodes = 4705
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 518.7575073242188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 519.7575073242188
INFO:tools.evaluation_results_class:Average Discounted Reward = 427.6248779296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 127567.796875
INFO:tools.evaluation_results_class:Current Best Return = 518.7575073242188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.146716541978385
INFO:tools.evaluation_results_class:Counted Episodes = 4812
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 526.4663696289062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 527.4663696289062
INFO:tools.evaluation_results_class:Average Discounted Reward = 434.70648193359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 128648.9375
INFO:tools.evaluation_results_class:Current Best Return = 526.4663696289062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.96800997091815
INFO:tools.evaluation_results_class:Counted Episodes = 4814
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 229.6659593092708
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 757.6372680664062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 758.6372680664062
INFO:tools.evaluation_results_class:Average Discounted Reward = 610.5889892578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 129719.2734375
INFO:tools.evaluation_results_class:Current Best Return = 757.6372680664062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 74.33987603305785
INFO:tools.evaluation_results_class:Counted Episodes = 1936
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 236.97653198242188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.97653198242188
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.46823120117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24608.994140625
INFO:tools.evaluation_results_class:Current Best Return = 236.97653198242188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.614754098360656
INFO:tools.evaluation_results_class:Counted Episodes = 2684
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 658.1468505859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 659.1468505859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 541.4324951171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 103426.8515625
INFO:tools.evaluation_results_class:Current Best Return = 658.1468505859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.01553930530164
INFO:tools.evaluation_results_class:Counted Episodes = 2188
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 798.8892211914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 799.8892211914062
INFO:tools.evaluation_results_class:Average Discounted Reward = 643.2943725585938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 143699.296875
INFO:tools.evaluation_results_class:Current Best Return = 798.8892211914062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 74.82888540031396
INFO:tools.evaluation_results_class:Counted Episodes = 1911
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 599.6760864257812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 600.6760864257812
INFO:tools.evaluation_results_class:Average Discounted Reward = 497.3756103515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 90015.34375
INFO:tools.evaluation_results_class:Current Best Return = 599.6760864257812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.33652173913043
INFO:tools.evaluation_results_class:Counted Episodes = 2300
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 226.33731079101562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 227.33731079101562
INFO:tools.evaluation_results_class:Average Discounted Reward = 194.4711456298828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24193.96484375
INFO:tools.evaluation_results_class:Current Best Return = 226.33731079101562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.68760968760969
INFO:tools.evaluation_results_class:Counted Episodes = 2849
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 754.8213500976562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 755.8213500976562
INFO:tools.evaluation_results_class:Average Discounted Reward = 608.05224609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 136991.453125
INFO:tools.evaluation_results_class:Current Best Return = 754.8213500976562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.80392156862744
INFO:tools.evaluation_results_class:Counted Episodes = 1938
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 235.48802185058594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.48802185058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.59939575195312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26413.630859375
INFO:tools.evaluation_results_class:Current Best Return = 235.48802185058594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.996371552975326
INFO:tools.evaluation_results_class:Counted Episodes = 2756
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 667.4628295898438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 668.4628295898438
INFO:tools.evaluation_results_class:Average Discounted Reward = 550.6162109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 109779.015625
INFO:tools.evaluation_results_class:Current Best Return = 667.4628295898438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.51883794825238
INFO:tools.evaluation_results_class:Counted Episodes = 2203
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 800.6074829101562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 801.6074829101562
INFO:tools.evaluation_results_class:Average Discounted Reward = 645.260009765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 140486.609375
INFO:tools.evaluation_results_class:Current Best Return = 800.6074829101562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 74.63958333333333
INFO:tools.evaluation_results_class:Counted Episodes = 1920
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 611.9423828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 612.9423828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 507.2507019042969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 93379.65625
INFO:tools.evaluation_results_class:Current Best Return = 611.9423828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.57941047074351
INFO:tools.evaluation_results_class:Counted Episodes = 2273
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 225.28477478027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 226.28477478027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.82528686523438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24918.998046875
INFO:tools.evaluation_results_class:Current Best Return = 225.28477478027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.822920262340354
INFO:tools.evaluation_results_class:Counted Episodes = 2897
INFO:robust_rl.robust_rl_trainer:Iteration 2 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 454.9624328613281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 455.9624328613281
INFO:tools.evaluation_results_class:Average Discounted Reward = 378.5377197265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 116566.0859375
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.08932196508323
INFO:tools.evaluation_results_class:Counted Episodes = 4926
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 27.9095458984375
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 45.31%
INFO:agents.father_agent:Step: 5, Training loss: 18.78399085998535
INFO:agents.father_agent:Step: 10, Training loss: 21.54698944091797
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 451.97650146484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 452.97650146484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 376.2332763671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 116941.0078125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.63872255489022
INFO:tools.evaluation_results_class:Counted Episodes = 5010
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 457.74005126953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 458.74005126953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 379.42901611328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 120285.125
INFO:tools.evaluation_results_class:Current Best Return = 457.74005126953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.84603079384123
INFO:tools.evaluation_results_class:Counted Episodes = 5001
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 472.48895263671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 473.48895263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 390.9094543457031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 123921.0
INFO:tools.evaluation_results_class:Current Best Return = 472.48895263671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.66504460665045
INFO:tools.evaluation_results_class:Counted Episodes = 4932
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 229.6551164344053
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 3 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 415.937744140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 416.937744140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 346.9404296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 108458.9609375
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.93795692550879
INFO:tools.evaluation_results_class:Counted Episodes = 5061
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 18.188894271850586
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 45.31%
INFO:agents.father_agent:Step: 5, Training loss: 18.35248374938965
INFO:agents.father_agent:Step: 10, Training loss: 26.208799362182617
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 426.12481689453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 427.12481689453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 355.068115234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 111074.3828125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.178939034045925
INFO:tools.evaluation_results_class:Counted Episodes = 5052
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 412.2806396484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 413.2806396484375
INFO:tools.evaluation_results_class:Average Discounted Reward = 342.80047607421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 107821.40625
INFO:tools.evaluation_results_class:Current Best Return = 412.2806396484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.486138227254976
INFO:tools.evaluation_results_class:Counted Episodes = 5122
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 414.6877746582031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 415.6877746582031
INFO:tools.evaluation_results_class:Average Discounted Reward = 346.9373779296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 106287.4921875
INFO:tools.evaluation_results_class:Current Best Return = 414.6877746582031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.270151254068544
INFO:tools.evaluation_results_class:Counted Episodes = 5223
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 230.1630839383871
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 4 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 386.2677001953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 387.2677001953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 323.8645324707031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 94441.9921875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.4147836078131
INFO:tools.evaluation_results_class:Counted Episodes = 5222
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 18.80328369140625
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 45.31%
INFO:agents.father_agent:Step: 5, Training loss: 20.625228881835938
INFO:agents.father_agent:Step: 10, Training loss: 14.846004486083984
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 390.0475158691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 391.0475158691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 325.16619873046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 99942.53125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.73784688053533
INFO:tools.evaluation_results_class:Counted Episodes = 5081
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 393.9927062988281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 394.9927062988281
INFO:tools.evaluation_results_class:Average Discounted Reward = 328.2769470214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 97629.0234375
INFO:tools.evaluation_results_class:Current Best Return = 393.9927062988281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.20827666601904
INFO:tools.evaluation_results_class:Counted Episodes = 5147
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 390.71014404296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 391.71014404296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 325.32928466796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 101610.875
INFO:tools.evaluation_results_class:Current Best Return = 390.71014404296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.0318014349428
INFO:tools.evaluation_results_class:Counted Episodes = 5157
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 231.16996629950563
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 5 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 353.40673828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 354.40673828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 298.2852783203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 79474.0078125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.753877779854236
INFO:tools.evaluation_results_class:Counted Episodes = 5351
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 16.901906967163086
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 43.75%
INFO:agents.father_agent:Step: 5, Training loss: 15.881467819213867
INFO:agents.father_agent:Step: 10, Training loss: 17.369647979736328
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 372.7270202636719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 373.7270202636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 312.08538818359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 87767.953125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.583611056655556
INFO:tools.evaluation_results_class:Counted Episodes = 5101
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 368.7030029296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 369.7030029296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 306.9583740234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 87202.734375
INFO:tools.evaluation_results_class:Current Best Return = 368.7030029296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.90894757587702
INFO:tools.evaluation_results_class:Counted Episodes = 5074
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 366.9820556640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 367.9820556640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 306.59075927734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 88830.46875
INFO:tools.evaluation_results_class:Current Best Return = 366.9820556640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.546692233940554
INFO:tools.evaluation_results_class:Counted Episodes = 5215
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 232.43102181651167
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 6 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 356.5894470214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 357.5894470214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 296.6776428222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 91866.6015625
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.77310591035303
INFO:tools.evaluation_results_class:Counted Episodes = 5042
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 16.588109970092773
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 45.31%
INFO:agents.father_agent:Step: 5, Training loss: 16.760147094726562
INFO:agents.father_agent:Step: 10, Training loss: 17.4394588470459
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 363.80938720703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 364.80938720703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 303.3446044921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 83023.6875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.11744491611078
INFO:tools.evaluation_results_class:Counted Episodes = 4947
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 345.23187255859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 346.23187255859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.7047119140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 77835.46875
INFO:tools.evaluation_results_class:Current Best Return = 345.23187255859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.560594214229866
INFO:tools.evaluation_results_class:Counted Episodes = 5116
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 349.28076171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 350.28076171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 292.08367919921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 80711.140625
INFO:tools.evaluation_results_class:Current Best Return = 349.28076171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.33562691131498
INFO:tools.evaluation_results_class:Counted Episodes = 5232
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 234.47899669895605
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 722.0162353515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 723.0162353515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 572.2913208007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 122732.8828125
INFO:tools.evaluation_results_class:Current Best Return = 722.0162353515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.98871968415115
INFO:tools.evaluation_results_class:Counted Episodes = 1773
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 255.4082489013672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.4082336425781
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.77987670898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26134.34765625
INFO:tools.evaluation_results_class:Current Best Return = 255.4082489013672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.82020618556701
INFO:tools.evaluation_results_class:Counted Episodes = 2425
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 656.6105346679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 657.6105346679688
INFO:tools.evaluation_results_class:Average Discounted Reward = 533.2760620117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 101140.3046875
INFO:tools.evaluation_results_class:Current Best Return = 656.6105346679688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 71.27896568871208
INFO:tools.evaluation_results_class:Counted Episodes = 2011
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 818.0195922851562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 819.0195922851562
INFO:tools.evaluation_results_class:Average Discounted Reward = 645.9212036132812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 154726.796875
INFO:tools.evaluation_results_class:Current Best Return = 818.0195922851562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.64746543778801
INFO:tools.evaluation_results_class:Counted Episodes = 1736
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 575.6123657226562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 576.6123657226562
INFO:tools.evaluation_results_class:Average Discounted Reward = 473.66448974609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 81560.4609375
INFO:tools.evaluation_results_class:Current Best Return = 575.6123657226562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.15045871559633
INFO:tools.evaluation_results_class:Counted Episodes = 2180
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 230.0368194580078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 231.0368194580078
INFO:tools.evaluation_results_class:Average Discounted Reward = 196.23574829101562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22320.046875
INFO:tools.evaluation_results_class:Current Best Return = 230.0368194580078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.00883652430044
INFO:tools.evaluation_results_class:Counted Episodes = 2716
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 227.2533416748047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 228.2533416748047
INFO:tools.evaluation_results_class:Average Discounted Reward = 193.89590454101562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21774.26953125
INFO:tools.evaluation_results_class:Current Best Return = 227.2533416748047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.211424332344215
INFO:tools.evaluation_results_class:Counted Episodes = 2696
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 228.4899139404297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 229.4899139404297
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.4981689453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21393.6171875
INFO:tools.evaluation_results_class:Current Best Return = 228.4899139404297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.726238532110095
INFO:tools.evaluation_results_class:Counted Episodes = 2725
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 229.32713317871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 230.32713317871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.76742553710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21415.66015625
INFO:tools.evaluation_results_class:Current Best Return = 229.32713317871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.379925650557624
INFO:tools.evaluation_results_class:Counted Episodes = 2690
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 229.18630981445312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 230.18630981445312
INFO:tools.evaluation_results_class:Average Discounted Reward = 195.498779296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22606.408203125
INFO:tools.evaluation_results_class:Current Best Return = 229.18630981445312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.99116347569956
INFO:tools.evaluation_results_class:Counted Episodes = 2716
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 235.43991088867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.43991088867188
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.651611328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23234.482421875
INFO:tools.evaluation_results_class:Current Best Return = 235.43991088867188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.94571321602396
INFO:tools.evaluation_results_class:Counted Episodes = 2671
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 725.3756103515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 726.3756103515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 578.713623046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 130100.6015625
INFO:tools.evaluation_results_class:Current Best Return = 725.3756103515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.42485078676071
INFO:tools.evaluation_results_class:Counted Episodes = 1843
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 250.41845703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.41845703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.43280029296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26642.78515625
INFO:tools.evaluation_results_class:Current Best Return = 250.41845703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.965975752835355
INFO:tools.evaluation_results_class:Counted Episodes = 2557
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 663.689453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 664.689453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 540.326904296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 107165.7421875
INFO:tools.evaluation_results_class:Current Best Return = 663.689453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 69.80666988883519
INFO:tools.evaluation_results_class:Counted Episodes = 2069
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 789.6650390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 790.6650390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 629.4232177734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 153404.953125
INFO:tools.evaluation_results_class:Current Best Return = 789.6650390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.4094616639478
INFO:tools.evaluation_results_class:Counted Episodes = 1839
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 574.75830078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 575.75830078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 473.68060302734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 87039.515625
INFO:tools.evaluation_results_class:Current Best Return = 574.75830078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.81915846016115
INFO:tools.evaluation_results_class:Counted Episodes = 2234
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 233.65911865234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.65911865234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.9305419921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23553.060546875
INFO:tools.evaluation_results_class:Current Best Return = 233.65911865234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.46364632809646
INFO:tools.evaluation_results_class:Counted Episodes = 2737
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 233.77317810058594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 234.77317810058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.08688354492188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23632.4375
INFO:tools.evaluation_results_class:Current Best Return = 233.77317810058594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.42202835332606
INFO:tools.evaluation_results_class:Counted Episodes = 2751
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 232.9967498779297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.9967498779297
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.56509399414062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23383.037109375
INFO:tools.evaluation_results_class:Current Best Return = 232.9967498779297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.94390155627941
INFO:tools.evaluation_results_class:Counted Episodes = 2763
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 234.78758239746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.78758239746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.81446838378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24366.86328125
INFO:tools.evaluation_results_class:Current Best Return = 234.78758239746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.19317356572259
INFO:tools.evaluation_results_class:Counted Episodes = 2754
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 232.19459533691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.19459533691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 198.63314819335938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24628.58984375
INFO:tools.evaluation_results_class:Current Best Return = 232.19459533691406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.84828828828829
INFO:tools.evaluation_results_class:Counted Episodes = 2775
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 230.3961181640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 231.3961181640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 197.56932067871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23909.927734375
INFO:tools.evaluation_results_class:Current Best Return = 230.3961181640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.754051134317606
INFO:tools.evaluation_results_class:Counted Episodes = 2777
INFO:robust_rl.robust_rl_trainer:Iteration 7 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 335.72186279296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 336.72186279296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 282.7702941894531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 70409.828125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.23493399655634
INFO:tools.evaluation_results_class:Counted Episodes = 5227
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.338789939880371
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 43.75%
INFO:agents.father_agent:Step: 5, Training loss: 15.249438285827637
INFO:agents.father_agent:Step: 10, Training loss: 16.926998138427734
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 337.24127197265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 338.24127197265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 282.5932312011719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 73088.515625
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.979829938698835
INFO:tools.evaluation_results_class:Counted Episodes = 5057
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 322.7252502441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 323.7252502441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 269.36090087890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 66507.2265625
INFO:tools.evaluation_results_class:Current Best Return = 322.7252502441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.709808473592574
INFO:tools.evaluation_results_class:Counted Episodes = 5169
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 329.5224914550781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 330.5224914550781
INFO:tools.evaluation_results_class:Average Discounted Reward = 275.4093017578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 68844.765625
INFO:tools.evaluation_results_class:Current Best Return = 329.5224914550781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.932909196067094
INFO:tools.evaluation_results_class:Counted Episodes = 5187
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 236.50951917517406
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 8 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 331.56549072265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 332.56549072265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 278.01904296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 68640.1328125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.30482115085537
INFO:tools.evaluation_results_class:Counted Episodes = 5144
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.697568893432617
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 45.31%
INFO:agents.father_agent:Step: 5, Training loss: 17.613073348999023
INFO:agents.father_agent:Step: 10, Training loss: 15.902852058410645
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 320.2414245605469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 321.2414245605469
INFO:tools.evaluation_results_class:Average Discounted Reward = 267.3841857910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 66869.5703125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.16274935808809
INFO:tools.evaluation_results_class:Counted Episodes = 5063
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 320.9747619628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 321.9747619628906
INFO:tools.evaluation_results_class:Average Discounted Reward = 266.4376525878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 63885.73828125
INFO:tools.evaluation_results_class:Current Best Return = 320.9747619628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.857572718154465
INFO:tools.evaluation_results_class:Counted Episodes = 4985
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 322.60845947265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 323.60845947265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 269.1126708984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 63550.48828125
INFO:tools.evaluation_results_class:Current Best Return = 322.60845947265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.2064139941691
INFO:tools.evaluation_results_class:Counted Episodes = 5145
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 238.58557200863794
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 9 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 335.0843200683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 336.0843200683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 277.5021667480469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 71082.7734375
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.329426538300545
INFO:tools.evaluation_results_class:Counted Episodes = 4778
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.910066604614258
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 45.31%
INFO:agents.father_agent:Step: 5, Training loss: 17.897859573364258
INFO:agents.father_agent:Step: 10, Training loss: 17.28090476989746
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 311.0130920410156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 312.0130920410156
INFO:tools.evaluation_results_class:Average Discounted Reward = 259.66021728515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 61091.4140625
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.10234966148945
INFO:tools.evaluation_results_class:Counted Episodes = 5022
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 299.2384948730469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.2384948730469
INFO:tools.evaluation_results_class:Average Discounted Reward = 250.18014526367188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 51663.08984375
INFO:tools.evaluation_results_class:Current Best Return = 299.2384948730469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.72919522224398
INFO:tools.evaluation_results_class:Counted Episodes = 5107
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 308.9041442871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 309.9041442871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 257.380859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 58923.4140625
INFO:tools.evaluation_results_class:Current Best Return = 308.9041442871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.76998625024553
INFO:tools.evaluation_results_class:Counted Episodes = 5091
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 240.80191180706103
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 10 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 294.0375061035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 295.0375061035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 247.79930114746094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 51188.6171875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.75251353441609
INFO:tools.evaluation_results_class:Counted Episodes = 5172
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.166133880615234
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 42.19%
INFO:agents.father_agent:Step: 5, Training loss: 18.986764907836914
INFO:agents.father_agent:Step: 10, Training loss: 18.8532657623291
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 303.38824462890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 304.38824462890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 254.53158569335938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 48950.34765625
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.316019900497515
INFO:tools.evaluation_results_class:Counted Episodes = 5025
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 298.4072570800781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 299.4072570800781
INFO:tools.evaluation_results_class:Average Discounted Reward = 247.60543823242188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 52154.1953125
INFO:tools.evaluation_results_class:Current Best Return = 298.4072570800781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.66248228386313
INFO:tools.evaluation_results_class:Counted Episodes = 4939
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 311.8456726074219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 312.8456726074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 259.4612121582031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 56215.3671875
INFO:tools.evaluation_results_class:Current Best Return = 311.8456726074219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.073497104054326
INFO:tools.evaluation_results_class:Counted Episodes = 5007
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 242.88257875648023
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 11 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 305.78533935546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 306.78533935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 255.2815704345703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 52291.10546875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.10634888844382
INFO:tools.evaluation_results_class:Counted Episodes = 4993
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 18.292238235473633
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 42.19%
INFO:agents.father_agent:Step: 5, Training loss: 16.7220401763916
INFO:agents.father_agent:Step: 10, Training loss: 17.252483367919922
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 310.8629455566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 311.8629455566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 258.2581481933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 52253.44921875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.27268933861882
INFO:tools.evaluation_results_class:Counted Episodes = 4793
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 292.72479248046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.72479248046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.64599609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 46375.20703125
INFO:tools.evaluation_results_class:Current Best Return = 292.72479248046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.84385786802031
INFO:tools.evaluation_results_class:Counted Episodes = 4925
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 307.45068359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 308.45068359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 255.49044799804688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 49935.97265625
INFO:tools.evaluation_results_class:Current Best Return = 307.45068359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.674893789196844
INFO:tools.evaluation_results_class:Counted Episodes = 4943
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 244.54532289262258
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 638.2373046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 639.2373657226562
INFO:tools.evaluation_results_class:Average Discounted Reward = 496.8042297363281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 96701.3515625
INFO:tools.evaluation_results_class:Current Best Return = 638.2373046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.17603458925262
INFO:tools.evaluation_results_class:Counted Episodes = 1619
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.223388671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.223388671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.22389221191406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23921.9921875
INFO:tools.evaluation_results_class:Current Best Return = 281.223388671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.5660198769522
INFO:tools.evaluation_results_class:Counted Episodes = 2113
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 632.305908203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 633.305908203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 501.38397216796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 90123.390625
INFO:tools.evaluation_results_class:Current Best Return = 632.305908203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 80.2072072072072
INFO:tools.evaluation_results_class:Counted Episodes = 1776
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 777.6112670898438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 778.6112670898438
INFO:tools.evaluation_results_class:Average Discounted Reward = 603.5361938476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 132476.46875
INFO:tools.evaluation_results_class:Current Best Return = 777.6112670898438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.39097744360902
INFO:tools.evaluation_results_class:Counted Episodes = 1596
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 486.6095275878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 487.6095275878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 398.7381591796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 56455.6953125
INFO:tools.evaluation_results_class:Current Best Return = 486.6095275878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.49428571428571
INFO:tools.evaluation_results_class:Counted Episodes = 2100
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 238.1198272705078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.1198272705078
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.72500610351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18812.744140625
INFO:tools.evaluation_results_class:Current Best Return = 238.1198272705078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.393772171856526
INFO:tools.evaluation_results_class:Counted Episodes = 2537
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.52481079101562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.52481079101562
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.36924743652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20179.419921875
INFO:tools.evaluation_results_class:Current Best Return = 241.52481079101562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.64380798709157
INFO:tools.evaluation_results_class:Counted Episodes = 2479
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 237.40151977539062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.40151977539062
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.5115966796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18646.6328125
INFO:tools.evaluation_results_class:Current Best Return = 237.40151977539062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.090836012861736
INFO:tools.evaluation_results_class:Counted Episodes = 2488
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 237.1852569580078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.1852569580078
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.3616943359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19492.046875
INFO:tools.evaluation_results_class:Current Best Return = 237.1852569580078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.58964143426295
INFO:tools.evaluation_results_class:Counted Episodes = 2510
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 239.21975708007812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.21975708007812
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.00949096679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18754.501953125
INFO:tools.evaluation_results_class:Current Best Return = 239.21975708007812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.41854838709678
INFO:tools.evaluation_results_class:Counted Episodes = 2480
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 238.32138061523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.32138061523438
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.4104461669922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19257.666015625
INFO:tools.evaluation_results_class:Current Best Return = 238.32138061523438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.78414974113899
INFO:tools.evaluation_results_class:Counted Episodes = 2511
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 240.81016540527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.81016540527344
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.84567260742188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19911.931640625
INFO:tools.evaluation_results_class:Current Best Return = 240.81016540527344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.044479745830024
INFO:tools.evaluation_results_class:Counted Episodes = 2518
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 237.3553924560547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.3553924560547
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.6970977783203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18754.7421875
INFO:tools.evaluation_results_class:Current Best Return = 237.3553924560547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.75118858954041
INFO:tools.evaluation_results_class:Counted Episodes = 2524
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 237.56869506835938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.56869506835938
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.5068359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19620.5234375
INFO:tools.evaluation_results_class:Current Best Return = 237.56869506835938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.98247710075667
INFO:tools.evaluation_results_class:Counted Episodes = 2511
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 235.98406982421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.98406982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.0519256591797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19199.052734375
INFO:tools.evaluation_results_class:Current Best Return = 235.98406982421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.039043824701196
INFO:tools.evaluation_results_class:Counted Episodes = 2510
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 238.95652770996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.95652770996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.95657348632812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19671.73828125
INFO:tools.evaluation_results_class:Current Best Return = 238.95652770996094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.390513833992095
INFO:tools.evaluation_results_class:Counted Episodes = 2530
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 657.3018188476562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 658.3018188476562
INFO:tools.evaluation_results_class:Average Discounted Reward = 513.6482543945312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 103838.34375
INFO:tools.evaluation_results_class:Current Best Return = 657.3018188476562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.77317073170731
INFO:tools.evaluation_results_class:Counted Episodes = 1640
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.58612060546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.58612060546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.27232360839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22685.54296875
INFO:tools.evaluation_results_class:Current Best Return = 274.58612060546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.41655540720961
INFO:tools.evaluation_results_class:Counted Episodes = 2247
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 626.9122924804688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 627.9122924804688
INFO:tools.evaluation_results_class:Average Discounted Reward = 501.2705078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 97565.4296875
INFO:tools.evaluation_results_class:Current Best Return = 626.9122924804688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.23135135135135
INFO:tools.evaluation_results_class:Counted Episodes = 1850
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 799.5142822265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 800.5142822265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 620.8788452148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 145343.53125
INFO:tools.evaluation_results_class:Current Best Return = 799.5142822265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.74085554866708
INFO:tools.evaluation_results_class:Counted Episodes = 1613
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 505.6097106933594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 506.6097106933594
INFO:tools.evaluation_results_class:Average Discounted Reward = 415.2583312988281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 62621.296875
INFO:tools.evaluation_results_class:Current Best Return = 505.6097106933594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.8879472693032
INFO:tools.evaluation_results_class:Counted Episodes = 2124
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 236.03741455078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.03741455078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.2654571533203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19249.8671875
INFO:tools.evaluation_results_class:Current Best Return = 236.03741455078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.326263015811804
INFO:tools.evaluation_results_class:Counted Episodes = 2593
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 242.8960723876953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.8960723876953
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.6609649658203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21285.599609375
INFO:tools.evaluation_results_class:Current Best Return = 242.8960723876953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.727131179447255
INFO:tools.evaluation_results_class:Counted Episodes = 2569
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.0007781982422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.0007781982422
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.40512084960938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20205.71875
INFO:tools.evaluation_results_class:Current Best Return = 241.0007781982422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.90362856028092
INFO:tools.evaluation_results_class:Counted Episodes = 2563
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 243.00465393066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.00465393066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 206.1222381591797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21783.544921875
INFO:tools.evaluation_results_class:Current Best Return = 243.00465393066406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.41611778380473
INFO:tools.evaluation_results_class:Counted Episodes = 2581
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 245.58604431152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.58604431152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.57122802734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21278.087890625
INFO:tools.evaluation_results_class:Current Best Return = 245.58604431152344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.09604076832615
INFO:tools.evaluation_results_class:Counted Episodes = 2551
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 245.01748657226562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.01748657226562
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.70623779296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22009.353515625
INFO:tools.evaluation_results_class:Current Best Return = 245.01748657226562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.665112665112666
INFO:tools.evaluation_results_class:Counted Episodes = 2574
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 242.1013946533203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.1013946533203
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.54266357421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19808.646484375
INFO:tools.evaluation_results_class:Current Best Return = 242.1013946533203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.60371517027864
INFO:tools.evaluation_results_class:Counted Episodes = 2584
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 243.6018524169922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.6018524169922
INFO:tools.evaluation_results_class:Average Discounted Reward = 206.6451873779297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20635.4375
INFO:tools.evaluation_results_class:Current Best Return = 243.6018524169922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.678543764523624
INFO:tools.evaluation_results_class:Counted Episodes = 2582
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 244.7579345703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.7579345703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.87550354003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21477.583984375
INFO:tools.evaluation_results_class:Current Best Return = 244.7579345703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.47172734314485
INFO:tools.evaluation_results_class:Counted Episodes = 2582
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 243.3584747314453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.3584747314453
INFO:tools.evaluation_results_class:Average Discounted Reward = 206.60574340820312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19782.369140625
INFO:tools.evaluation_results_class:Current Best Return = 243.3584747314453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.61098221191028
INFO:tools.evaluation_results_class:Counted Episodes = 2586
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 244.984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.46343994140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20189.572265625
INFO:tools.evaluation_results_class:Current Best Return = 244.984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.99258102303788
INFO:tools.evaluation_results_class:Counted Episodes = 2561
INFO:robust_rl.robust_rl_trainer:Iteration 12 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 301.412841796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 302.412841796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 250.390380859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 46191.984375
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.46776580004227
INFO:tools.evaluation_results_class:Counted Episodes = 4731
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 17.490400314331055
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 45.31%
INFO:agents.father_agent:Step: 5, Training loss: 15.67962646484375
INFO:agents.father_agent:Step: 10, Training loss: 18.251035690307617
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 295.63616943359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 296.63616943359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.2344970703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 43954.10546875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.29704190632703
INFO:tools.evaluation_results_class:Counted Episodes = 4868
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 289.2030029296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 290.2030029296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.684814453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 43018.69140625
INFO:tools.evaluation_results_class:Current Best Return = 289.2030029296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.65371170059634
INFO:tools.evaluation_results_class:Counted Episodes = 4863
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 300.40899658203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 301.40899658203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 250.01678466796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44814.03515625
INFO:tools.evaluation_results_class:Current Best Return = 300.40899658203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.25518374050503
INFO:tools.evaluation_results_class:Counted Episodes = 4871
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 245.74446822633396
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 13 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 297.6261901855469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 298.6261901855469
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.10107421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 43639.73828125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.80033174372797
INFO:tools.evaluation_results_class:Counted Episodes = 4823
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 16.682415008544922
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 48.44%
INFO:agents.father_agent:Step: 5, Training loss: 16.998620986938477
INFO:agents.father_agent:Step: 10, Training loss: 16.220104217529297
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 279.1700134277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.1700134277344
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.66595458984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42417.6875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.94318405997238
INFO:tools.evaluation_results_class:Counted Episodes = 5069
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 289.1548767089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 290.1548767089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.0478973388672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44896.234375
INFO:tools.evaluation_results_class:Current Best Return = 289.1548767089844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.43160967472894
INFO:tools.evaluation_results_class:Counted Episodes = 4796
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 288.9228515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.9228515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.0423583984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40219.6171875
INFO:tools.evaluation_results_class:Current Best Return = 288.9228515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.687563606757585
INFO:tools.evaluation_results_class:Counted Episodes = 4913
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 246.2687246655032
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 14 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.5386657714844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.5386657714844
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.3603515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37698.359375
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.39991846718304
INFO:tools.evaluation_results_class:Counted Episodes = 4906
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.485452651977539
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 48.44%
INFO:agents.father_agent:Step: 5, Training loss: 14.315926551818848
INFO:agents.father_agent:Step: 10, Training loss: 13.622920036315918
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 282.6334228515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.6334228515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.18988037109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38572.80859375
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.307405875051714
INFO:tools.evaluation_results_class:Counted Episodes = 4834
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.41357421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.41357421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.6227264404297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40115.99609375
INFO:tools.evaluation_results_class:Current Best Return = 284.41357421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.52640367355458
INFO:tools.evaluation_results_class:Counted Episodes = 4791
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 291.7929382324219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.7929382324219
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.9120330810547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41967.9921875
INFO:tools.evaluation_results_class:Current Best Return = 291.7929382324219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.88230401330838
INFO:tools.evaluation_results_class:Counted Episodes = 4809
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 246.93397292218677
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 15 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.482177734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.482177734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.968017578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35751.61328125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.80016164881794
INFO:tools.evaluation_results_class:Counted Episodes = 4949
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.62603759765625
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 48.44%
INFO:agents.father_agent:Step: 5, Training loss: 11.99096393585205
INFO:agents.father_agent:Step: 10, Training loss: 12.757593154907227
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 277.9107971191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.9107971191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.1903533935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34611.08203125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.31823695915892
INFO:tools.evaluation_results_class:Counted Episodes = 4946
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.2167053222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.2167053222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.41409301757812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36547.08203125
INFO:tools.evaluation_results_class:Current Best Return = 281.2167053222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.1513894649523
INFO:tools.evaluation_results_class:Counted Episodes = 4822
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 291.04254150390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.04254150390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.5296173095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 39958.68359375
INFO:tools.evaluation_results_class:Current Best Return = 291.04254150390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.285714285714285
INFO:tools.evaluation_results_class:Counted Episodes = 4802
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 247.44473167000305
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 16 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.5956726074219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.5956726074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.56849670410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35017.890625
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.59037127702978
INFO:tools.evaluation_results_class:Counted Episodes = 4902
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.035965919494629
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 48.44%
INFO:agents.father_agent:Step: 5, Training loss: 10.508042335510254
INFO:agents.father_agent:Step: 10, Training loss: 10.682905197143555
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 279.7408752441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.7408752441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.51341247558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35271.82421875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.83675961734175
INFO:tools.evaluation_results_class:Counted Episodes = 4913
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.0910949707031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.0910949707031
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.4528045654297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33032.62890625
INFO:tools.evaluation_results_class:Current Best Return = 269.0910949707031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.711273317112735
INFO:tools.evaluation_results_class:Counted Episodes = 4932
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.2010498046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.2010498046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.70050048828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36882.02734375
INFO:tools.evaluation_results_class:Current Best Return = 282.2010498046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.788675429727
INFO:tools.evaluation_results_class:Counted Episodes = 4945
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 248.83246629424406
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 597.2886352539062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 598.2886352539062
INFO:tools.evaluation_results_class:Average Discounted Reward = 462.482177734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 80934.5078125
INFO:tools.evaluation_results_class:Current Best Return = 597.2886352539062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.60313315926894
INFO:tools.evaluation_results_class:Counted Episodes = 1532
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 290.08184814453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 291.08184814453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.91734313964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23672.27734375
INFO:tools.evaluation_results_class:Current Best Return = 290.08184814453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 71.14682539682539
INFO:tools.evaluation_results_class:Counted Episodes = 2016
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 600.8870239257812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 601.8870239257812
INFO:tools.evaluation_results_class:Average Discounted Reward = 474.7593078613281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 81498.796875
INFO:tools.evaluation_results_class:Current Best Return = 600.8870239257812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 82.55678509027373
INFO:tools.evaluation_results_class:Counted Episodes = 1717
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 768.3387451171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 769.3387451171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 591.8690795898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 117824.0546875
INFO:tools.evaluation_results_class:Current Best Return = 768.3387451171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.8339973439575
INFO:tools.evaluation_results_class:Counted Episodes = 1506
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 453.8826904296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 454.8826904296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 371.2681884765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 48854.07421875
INFO:tools.evaluation_results_class:Current Best Return = 453.8826904296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 69.30780416868637
INFO:tools.evaluation_results_class:Counted Episodes = 2063
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 238.48851013183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.48851013183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.24005126953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17293.603515625
INFO:tools.evaluation_results_class:Current Best Return = 238.48851013183594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.70536074163644
INFO:tools.evaluation_results_class:Counted Episodes = 2481
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 239.48046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.48046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.9465789794922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17709.94140625
INFO:tools.evaluation_results_class:Current Best Return = 239.48046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.683044703987115
INFO:tools.evaluation_results_class:Counted Episodes = 2483
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 242.81900024414062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.81900024414062
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.66831970214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17710.18359375
INFO:tools.evaluation_results_class:Current Best Return = 242.81900024414062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.41296371789645
INFO:tools.evaluation_results_class:Counted Episodes = 2453
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 242.62728881835938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.62728881835938
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.568603515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18226.0703125
INFO:tools.evaluation_results_class:Current Best Return = 242.62728881835938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.121792260692466
INFO:tools.evaluation_results_class:Counted Episodes = 2455
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 238.4847869873047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.4847869873047
INFO:tools.evaluation_results_class:Average Discounted Reward = 200.97015380859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17103.63671875
INFO:tools.evaluation_results_class:Current Best Return = 238.4847869873047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.05638945233266
INFO:tools.evaluation_results_class:Counted Episodes = 2465
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 239.46253967285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.46253967285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.0576629638672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16042.138671875
INFO:tools.evaluation_results_class:Current Best Return = 239.46253967285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.12947882736157
INFO:tools.evaluation_results_class:Counted Episodes = 2456
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 244.26446533203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.26446533203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.23748779296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18388.859375
INFO:tools.evaluation_results_class:Current Best Return = 244.26446533203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.88264462809917
INFO:tools.evaluation_results_class:Counted Episodes = 2420
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.94845581054688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.94845581054688
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.55538940429688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18005.57421875
INFO:tools.evaluation_results_class:Current Best Return = 241.94845581054688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.76742268041237
INFO:tools.evaluation_results_class:Counted Episodes = 2425
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 242.33734130859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.33734130859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.5144500732422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18172.447265625
INFO:tools.evaluation_results_class:Current Best Return = 242.33734130859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.003315375051805
INFO:tools.evaluation_results_class:Counted Episodes = 2413
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 245.269775390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.269775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 206.4452667236328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19076.75
INFO:tools.evaluation_results_class:Current Best Return = 245.269775390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.48682042833608
INFO:tools.evaluation_results_class:Counted Episodes = 2428
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.82374572753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.82374572753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.66383361816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17843.919921875
INFO:tools.evaluation_results_class:Current Best Return = 241.82374572753906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.14810281517748
INFO:tools.evaluation_results_class:Counted Episodes = 2451
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 239.81707763671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.81707763671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.383056640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18248.015625
INFO:tools.evaluation_results_class:Current Best Return = 239.81707763671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.06178861788618
INFO:tools.evaluation_results_class:Counted Episodes = 2460
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 240.2127227783203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.2127227783203
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.4750213623047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17993.6640625
INFO:tools.evaluation_results_class:Current Best Return = 240.2127227783203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.00405186385738
INFO:tools.evaluation_results_class:Counted Episodes = 2468
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 242.32179260253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.32179260253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.90806579589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18334.814453125
INFO:tools.evaluation_results_class:Current Best Return = 242.32179260253906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.27820773930753
INFO:tools.evaluation_results_class:Counted Episodes = 2455
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.18038940429688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.18038940429688
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.33255004882812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18018.126953125
INFO:tools.evaluation_results_class:Current Best Return = 241.18038940429688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.706214689265536
INFO:tools.evaluation_results_class:Counted Episodes = 2478
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 240.97030639648438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.97030639648438
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.91744995117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17716.845703125
INFO:tools.evaluation_results_class:Current Best Return = 240.97030639648438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.30919446704638
INFO:tools.evaluation_results_class:Counted Episodes = 2458
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 631.1399536132812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 632.1399536132812
INFO:tools.evaluation_results_class:Average Discounted Reward = 487.9214172363281
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 88683.578125
INFO:tools.evaluation_results_class:Current Best Return = 631.1399536132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.84525357607282
INFO:tools.evaluation_results_class:Counted Episodes = 1538
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 289.3816223144531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 290.3816223144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.0683135986328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23773.955078125
INFO:tools.evaluation_results_class:Current Best Return = 289.3816223144531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 69.2223291626564
INFO:tools.evaluation_results_class:Counted Episodes = 2078
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 628.1704711914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 629.1704711914062
INFO:tools.evaluation_results_class:Average Discounted Reward = 494.7556457519531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 87898.046875
INFO:tools.evaluation_results_class:Current Best Return = 628.1704711914062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.08952603861907
INFO:tools.evaluation_results_class:Counted Episodes = 1709
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 797.6795654296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 798.6795654296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 612.7359008789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 139968.578125
INFO:tools.evaluation_results_class:Current Best Return = 797.6795654296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.94070619586942
INFO:tools.evaluation_results_class:Counted Episodes = 1501
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 474.0920104980469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 475.0920104980469
INFO:tools.evaluation_results_class:Average Discounted Reward = 387.8714904785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 51741.625
INFO:tools.evaluation_results_class:Current Best Return = 474.0920104980469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 69.41329479768787
INFO:tools.evaluation_results_class:Counted Episodes = 2076
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 245.28033447265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.28033447265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.2767333984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18094.39453125
INFO:tools.evaluation_results_class:Current Best Return = 245.28033447265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.220264317180614
INFO:tools.evaluation_results_class:Counted Episodes = 2497
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 244.3575439453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 245.3575439453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 206.2875518798828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18714.73046875
INFO:tools.evaluation_results_class:Current Best Return = 244.3575439453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.17956903431764
INFO:tools.evaluation_results_class:Counted Episodes = 2506
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.0375518798828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.0375518798828
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.78240966796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18532.609375
INFO:tools.evaluation_results_class:Current Best Return = 241.0375518798828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.62450592885376
INFO:tools.evaluation_results_class:Counted Episodes = 2530
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.1625213623047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.1625213623047
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.16835021972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18080.853515625
INFO:tools.evaluation_results_class:Current Best Return = 241.1625213623047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.55001977066034
INFO:tools.evaluation_results_class:Counted Episodes = 2529
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.46327209472656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.46327209472656
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.47019958496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18623.71875
INFO:tools.evaluation_results_class:Current Best Return = 241.46327209472656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.380726698262244
INFO:tools.evaluation_results_class:Counted Episodes = 2532
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 242.40431213378906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.40431213378906
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.68447875976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18007.677734375
INFO:tools.evaluation_results_class:Current Best Return = 242.40431213378906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.01275917065391
INFO:tools.evaluation_results_class:Counted Episodes = 2508
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 247.77029418945312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.77029418945312
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.34925842285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19928.177734375
INFO:tools.evaluation_results_class:Current Best Return = 247.77029418945312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.71089108910891
INFO:tools.evaluation_results_class:Counted Episodes = 2525
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 247.7657012939453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.7657012939453
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.1291961669922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19753.28125
INFO:tools.evaluation_results_class:Current Best Return = 247.7657012939453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.28743961352657
INFO:tools.evaluation_results_class:Counted Episodes = 2484
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 247.58090209960938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.58090209960938
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.43186950683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19300.708984375
INFO:tools.evaluation_results_class:Current Best Return = 247.58090209960938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.202157411106676
INFO:tools.evaluation_results_class:Counted Episodes = 2503
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 249.42308044433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.42308044433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.76315307617188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19542.634765625
INFO:tools.evaluation_results_class:Current Best Return = 249.42308044433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.28846153846154
INFO:tools.evaluation_results_class:Counted Episodes = 2496
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 245.6219940185547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.6219940185547
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.29969787597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19070.279296875
INFO:tools.evaluation_results_class:Current Best Return = 245.6219940185547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.30497592295345
INFO:tools.evaluation_results_class:Counted Episodes = 2492
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 247.17234802246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.17234802246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.7496795654297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18929.62890625
INFO:tools.evaluation_results_class:Current Best Return = 247.17234802246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.296593186372746
INFO:tools.evaluation_results_class:Counted Episodes = 2495
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 249.73397827148438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.73397827148438
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.39205932617188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18878.888671875
INFO:tools.evaluation_results_class:Current Best Return = 249.73397827148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.844014510278114
INFO:tools.evaluation_results_class:Counted Episodes = 2481
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 248.29722595214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.29722595214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.77923583984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19743.880859375
INFO:tools.evaluation_results_class:Current Best Return = 248.29722595214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.292819895707986
INFO:tools.evaluation_results_class:Counted Episodes = 2493
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 246.9124755859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.9124755859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.7196044921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18402.30859375
INFO:tools.evaluation_results_class:Current Best Return = 246.9124755859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.1638689048761
INFO:tools.evaluation_results_class:Counted Episodes = 2502
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 246.86573791503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.86573791503906
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.80882263183594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18746.62890625
INFO:tools.evaluation_results_class:Current Best Return = 246.86573791503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.4312625250501
INFO:tools.evaluation_results_class:Counted Episodes = 2495
INFO:robust_rl.robust_rl_trainer:Iteration 17 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.6791687011719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.6791687011719
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.8626251220703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32770.63671875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.36237144585602
INFO:tools.evaluation_results_class:Counted Episodes = 4959
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.622438430786133
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 48.44%
INFO:agents.father_agent:Step: 5, Training loss: 10.93881893157959
INFO:agents.father_agent:Step: 10, Training loss: 10.275849342346191
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 285.2953796386719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 286.2953796386719
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.19076538085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32732.2421875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.772670288341
INFO:tools.evaluation_results_class:Counted Episodes = 4786
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 264.17767333984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.17767333984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.34959411621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31457.93359375
INFO:tools.evaluation_results_class:Current Best Return = 264.17767333984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.1980753809142
INFO:tools.evaluation_results_class:Counted Episodes = 4988
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.3826599121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.3826599121094
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.31961059570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32042.333984375
INFO:tools.evaluation_results_class:Current Best Return = 276.3826599121094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.96165191740413
INFO:tools.evaluation_results_class:Counted Episodes = 5085
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 250.5378932474212
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 18 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.6244201660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.6244201660156
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.42872619628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31160.46875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.58619289340101
INFO:tools.evaluation_results_class:Counted Episodes = 4925
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.805381774902344
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 48.44%
INFO:agents.father_agent:Step: 5, Training loss: 11.722346305847168
INFO:agents.father_agent:Step: 10, Training loss: 11.96584415435791
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 268.5126953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.5126953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.4219207763672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31731.068359375
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.87751876728566
INFO:tools.evaluation_results_class:Counted Episodes = 5062
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 259.8907775878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.8907775878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.52806091308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27658.73828125
INFO:tools.evaluation_results_class:Current Best Return = 259.8907775878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.82046178343949
INFO:tools.evaluation_results_class:Counted Episodes = 5024
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 261.5763244628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.5763244628906
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.54696655273438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27679.25390625
INFO:tools.evaluation_results_class:Current Best Return = 261.5763244628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.69380733944954
INFO:tools.evaluation_results_class:Counted Episodes = 5232
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 253.10957221947845
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 19 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 285.144775390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 286.144775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.73681640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36897.7265625
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.57895829010168
INFO:tools.evaluation_results_class:Counted Episodes = 4819
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.753438949584961
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 48.44%
INFO:agents.father_agent:Step: 5, Training loss: 9.665942192077637
INFO:agents.father_agent:Step: 10, Training loss: 11.820277214050293
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 282.69696044921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.69696044921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.2646484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30526.904296875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.15232198142415
INFO:tools.evaluation_results_class:Counted Episodes = 4845
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.1059265136719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.1059265136719
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.65005493164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28513.943359375
INFO:tools.evaluation_results_class:Current Best Return = 267.1059265136719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.461965134706816
INFO:tools.evaluation_results_class:Counted Episodes = 5048
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.8118591308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.8118591308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.36285400390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31256.90234375
INFO:tools.evaluation_results_class:Current Best Return = 283.8118591308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.027408303103584
INFO:tools.evaluation_results_class:Counted Episodes = 4962
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 255.29764107994777
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 20 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.6601867675781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.6601867675781
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.1608123779297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31263.306640625
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.67711915840582
INFO:tools.evaluation_results_class:Counted Episodes = 4943
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.962672233581543
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 46.88%
INFO:agents.father_agent:Step: 5, Training loss: 9.063552856445312
INFO:agents.father_agent:Step: 10, Training loss: 11.555500984191895
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 280.03387451171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.03387451171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.711669921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33001.703125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.68673489865543
INFO:tools.evaluation_results_class:Counted Episodes = 4983
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.79534912109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.79534912109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.01698303222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28401.029296875
INFO:tools.evaluation_results_class:Current Best Return = 270.79534912109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.14589486119315
INFO:tools.evaluation_results_class:Counted Episodes = 5079
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 291.4226989746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.4226989746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 244.11172485351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29962.666015625
INFO:tools.evaluation_results_class:Current Best Return = 291.4226989746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.12585444310414
INFO:tools.evaluation_results_class:Counted Episodes = 4974
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 258.4617892505649
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 21 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.27252197265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.27252197265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.99496459960938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30347.296875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.939207229410556
INFO:tools.evaluation_results_class:Counted Episodes = 4869
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.500407218933105
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 46.88%
INFO:agents.father_agent:Step: 5, Training loss: 10.40746021270752
INFO:agents.father_agent:Step: 10, Training loss: 10.988682746887207
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 288.8057861328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.8057861328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.88511657714844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30870.935546875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.09826707441386
INFO:tools.evaluation_results_class:Counted Episodes = 4905
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.8226318359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.8226318359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.2259063720703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26579.783203125
INFO:tools.evaluation_results_class:Current Best Return = 268.8226318359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.77151219512195
INFO:tools.evaluation_results_class:Counted Episodes = 5125
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.650390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.650390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.86257934570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26493.888671875
INFO:tools.evaluation_results_class:Current Best Return = 282.650390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.930316379655586
INFO:tools.evaluation_results_class:Counted Episodes = 4994
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 261.124724355628
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 591.079833984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 592.079833984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 457.8111877441406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 85437.6015625
INFO:tools.evaluation_results_class:Current Best Return = 591.079833984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.70063694267516
INFO:tools.evaluation_results_class:Counted Episodes = 1570
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 305.24237060546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 306.24237060546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.79432678222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23429.12109375
INFO:tools.evaluation_results_class:Current Best Return = 305.24237060546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 70.0475036354823
INFO:tools.evaluation_results_class:Counted Episodes = 2063
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 596.2221069335938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 597.2221069335938
INFO:tools.evaluation_results_class:Average Discounted Reward = 470.5609436035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 80323.1796875
INFO:tools.evaluation_results_class:Current Best Return = 596.2221069335938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 82.50728862973762
INFO:tools.evaluation_results_class:Counted Episodes = 1715
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 747.56787109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 748.56787109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 574.965087890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 123847.2109375
INFO:tools.evaluation_results_class:Current Best Return = 747.56787109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 92.48216644649933
INFO:tools.evaluation_results_class:Counted Episodes = 1514
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 440.8847351074219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 441.8847351074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 361.51519775390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44306.296875
INFO:tools.evaluation_results_class:Current Best Return = 440.8847351074219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.63605930176949
INFO:tools.evaluation_results_class:Counted Episodes = 2091
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 249.56478881835938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.56478881835938
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.35159301757812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17093.322265625
INFO:tools.evaluation_results_class:Current Best Return = 249.56478881835938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.590389917290274
INFO:tools.evaluation_results_class:Counted Episodes = 2539
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 247.28367614746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.28367614746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.48068237304688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16917.068359375
INFO:tools.evaluation_results_class:Current Best Return = 247.28367614746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.45001975503754
INFO:tools.evaluation_results_class:Counted Episodes = 2531
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 249.964111328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.964111328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.5529327392578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16835.46484375
INFO:tools.evaluation_results_class:Current Best Return = 249.964111328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.76714513556619
INFO:tools.evaluation_results_class:Counted Episodes = 2508
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 250.09524536132812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.09524536132812
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.39007568359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17423.80078125
INFO:tools.evaluation_results_class:Current Best Return = 250.09524536132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.69126984126984
INFO:tools.evaluation_results_class:Counted Episodes = 2520
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 250.89378356933594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.89378356933594
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.92620849609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16867.69921875
INFO:tools.evaluation_results_class:Current Best Return = 250.89378356933594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.29338677354709
INFO:tools.evaluation_results_class:Counted Episodes = 2495
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 250.8514404296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.8514404296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.44122314453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17499.36328125
INFO:tools.evaluation_results_class:Current Best Return = 250.8514404296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.46582378506519
INFO:tools.evaluation_results_class:Counted Episodes = 2531
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 251.01673889160156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.01673889160156
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.9120330810547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16870.6171875
INFO:tools.evaluation_results_class:Current Best Return = 251.01673889160156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.24162679425837
INFO:tools.evaluation_results_class:Counted Episodes = 2508
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 252.44921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.44921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.2764892578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17220.287109375
INFO:tools.evaluation_results_class:Current Best Return = 252.44921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.9721226602947
INFO:tools.evaluation_results_class:Counted Episodes = 2511
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 252.74192810058594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.74192810058594
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.62913513183594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17505.654296875
INFO:tools.evaluation_results_class:Current Best Return = 252.74192810058594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.94583831142971
INFO:tools.evaluation_results_class:Counted Episodes = 2511
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 252.83740234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.83740234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.48744201660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16991.69921875
INFO:tools.evaluation_results_class:Current Best Return = 252.83740234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.33480176211454
INFO:tools.evaluation_results_class:Counted Episodes = 2497
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 250.53778076171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.53778076171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.701416015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17439.044921875
INFO:tools.evaluation_results_class:Current Best Return = 250.53778076171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.08796481407437
INFO:tools.evaluation_results_class:Counted Episodes = 2501
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 252.13914489746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.13914489746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.88400268554688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17374.931640625
INFO:tools.evaluation_results_class:Current Best Return = 252.13914489746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.07596961215514
INFO:tools.evaluation_results_class:Counted Episodes = 2501
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 257.5466003417969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.5466003417969
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.9844512939453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18598.40625
INFO:tools.evaluation_results_class:Current Best Return = 257.5466003417969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.88735818476499
INFO:tools.evaluation_results_class:Counted Episodes = 2468
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 257.5677795410156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.5677795410156
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.1264190673828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17942.689453125
INFO:tools.evaluation_results_class:Current Best Return = 257.5677795410156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.79562929987859
INFO:tools.evaluation_results_class:Counted Episodes = 2471
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 254.03652954101562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.03652954101562
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.82904052734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17430.384765625
INFO:tools.evaluation_results_class:Current Best Return = 254.03652954101562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.36290646326776
INFO:tools.evaluation_results_class:Counted Episodes = 2491
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 254.86122131347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.86122131347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.84092712402344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17991.92578125
INFO:tools.evaluation_results_class:Current Best Return = 254.86122131347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.25422365245374
INFO:tools.evaluation_results_class:Counted Episodes = 2486
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 253.01806640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.01806640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.7409210205078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17792.306640625
INFO:tools.evaluation_results_class:Current Best Return = 253.01806640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.5429718875502
INFO:tools.evaluation_results_class:Counted Episodes = 2490
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 253.6222686767578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.6222686767578
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.31808471679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17270.49609375
INFO:tools.evaluation_results_class:Current Best Return = 253.6222686767578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.07395626242545
INFO:tools.evaluation_results_class:Counted Episodes = 2515
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 251.30520629882812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.30520629882812
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.15292358398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17745.76953125
INFO:tools.evaluation_results_class:Current Best Return = 251.30520629882812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.18463987266216
INFO:tools.evaluation_results_class:Counted Episodes = 2513
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 255.78883361816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.7888488769531
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.65611267089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17484.615234375
INFO:tools.evaluation_results_class:Current Best Return = 255.78883361816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.61340826977118
INFO:tools.evaluation_results_class:Counted Episodes = 2491
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 251.46812438964844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.46812438964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.5511932373047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17612.017578125
INFO:tools.evaluation_results_class:Current Best Return = 251.46812438964844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.930677290836655
INFO:tools.evaluation_results_class:Counted Episodes = 2510
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 613.3291625976562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 614.3291625976562
INFO:tools.evaluation_results_class:Average Discounted Reward = 475.27056884765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 83390.375
INFO:tools.evaluation_results_class:Current Best Return = 613.3291625976562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.84547181760608
INFO:tools.evaluation_results_class:Counted Episodes = 1579
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 307.3832092285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 308.3832092285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 252.15310668945312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25897.7890625
INFO:tools.evaluation_results_class:Current Best Return = 307.3832092285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.1066540821142
INFO:tools.evaluation_results_class:Counted Episodes = 2119
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 603.699462890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 604.699462890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 477.7926330566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 81115.984375
INFO:tools.evaluation_results_class:Current Best Return = 603.699462890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.32576617480136
INFO:tools.evaluation_results_class:Counted Episodes = 1762
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 758.1398315429688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 759.1398315429688
INFO:tools.evaluation_results_class:Average Discounted Reward = 584.7266235351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 119087.234375
INFO:tools.evaluation_results_class:Current Best Return = 758.1398315429688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.85442622950819
INFO:tools.evaluation_results_class:Counted Episodes = 1525
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 448.82562255859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 449.82562255859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 367.96026611328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44915.94921875
INFO:tools.evaluation_results_class:Current Best Return = 448.82562255859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.6136255359695
INFO:tools.evaluation_results_class:Counted Episodes = 2099
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 257.95062255859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.95062255859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 218.4449920654297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18473.03515625
INFO:tools.evaluation_results_class:Current Best Return = 257.95062255859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.11050156739812
INFO:tools.evaluation_results_class:Counted Episodes = 2552
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 256.1955261230469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.1955261230469
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.2593231201172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17274.80859375
INFO:tools.evaluation_results_class:Current Best Return = 256.1955261230469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.01688260698862
INFO:tools.evaluation_results_class:Counted Episodes = 2547
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 257.3690185546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.3690185546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 218.46022033691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17592.322265625
INFO:tools.evaluation_results_class:Current Best Return = 257.3690185546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.57896779200621
INFO:tools.evaluation_results_class:Counted Episodes = 2577
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 256.75537109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.75537109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.6284942626953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16721.330078125
INFO:tools.evaluation_results_class:Current Best Return = 256.75537109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.14990215264188
INFO:tools.evaluation_results_class:Counted Episodes = 2555
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 260.9811096191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.9811096191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.1629180908203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17932.728515625
INFO:tools.evaluation_results_class:Current Best Return = 260.9811096191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.333071175776645
INFO:tools.evaluation_results_class:Counted Episodes = 2543
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 256.7615661621094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.7615661621094
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.59971618652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17333.640625
INFO:tools.evaluation_results_class:Current Best Return = 256.7615661621094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.338078291814945
INFO:tools.evaluation_results_class:Counted Episodes = 2529
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 257.24090576171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.24090576171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.8256072998047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17434.283203125
INFO:tools.evaluation_results_class:Current Best Return = 257.24090576171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.01838091513493
INFO:tools.evaluation_results_class:Counted Episodes = 2557
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 255.09925842285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.0992736816406
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.06826782226562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18486.337890625
INFO:tools.evaluation_results_class:Current Best Return = 255.09925842285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.85325029194239
INFO:tools.evaluation_results_class:Counted Episodes = 2569
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 255.26564025878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.2656555175781
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.98915100097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17842.40234375
INFO:tools.evaluation_results_class:Current Best Return = 255.26564025878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.222353404171585
INFO:tools.evaluation_results_class:Counted Episodes = 2541
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 255.95407104492188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.9540710449219
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.6940460205078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18522.056640625
INFO:tools.evaluation_results_class:Current Best Return = 255.95407104492188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.13623871221044
INFO:tools.evaluation_results_class:Counted Episodes = 2547
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 256.6387023925781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.6387023925781
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.42718505859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18736.13671875
INFO:tools.evaluation_results_class:Current Best Return = 256.6387023925781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.629730784237225
INFO:tools.evaluation_results_class:Counted Episodes = 2563
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 256.7078857421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.7078857421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.2469024658203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19579.416015625
INFO:tools.evaluation_results_class:Current Best Return = 256.7078857421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.06007067137809
INFO:tools.evaluation_results_class:Counted Episodes = 2547
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 259.22283935546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.22283935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 219.4178924560547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18916.248046875
INFO:tools.evaluation_results_class:Current Best Return = 259.22283935546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.4964314036479
INFO:tools.evaluation_results_class:Counted Episodes = 2522
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 261.315185546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.315185546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.9512176513672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19045.515625
INFO:tools.evaluation_results_class:Current Best Return = 261.315185546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.26381811054488
INFO:tools.evaluation_results_class:Counted Episodes = 2551
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.3985595703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.3985595703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.71658325195312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18663.935546875
INFO:tools.evaluation_results_class:Current Best Return = 262.3985595703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.9466984884646
INFO:tools.evaluation_results_class:Counted Episodes = 2514
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 258.4026794433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.4026794433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 218.7552490234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19174.3515625
INFO:tools.evaluation_results_class:Current Best Return = 258.4026794433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.06828885400314
INFO:tools.evaluation_results_class:Counted Episodes = 2548
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 257.7256774902344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.7256774902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 218.4283905029297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18423.314453125
INFO:tools.evaluation_results_class:Current Best Return = 257.7256774902344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.819069949198905
INFO:tools.evaluation_results_class:Counted Episodes = 2559
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 254.76663208007812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.76663208007812
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.15725708007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18106.361328125
INFO:tools.evaluation_results_class:Current Best Return = 254.76663208007812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.72345390898483
INFO:tools.evaluation_results_class:Counted Episodes = 2571
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 260.07855224609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.07855224609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.35665893554688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18973.716796875
INFO:tools.evaluation_results_class:Current Best Return = 260.07855224609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.09583660644147
INFO:tools.evaluation_results_class:Counted Episodes = 2546
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 259.2370910644531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.2370910644531
INFO:tools.evaluation_results_class:Average Discounted Reward = 219.48028564453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18643.314453125
INFO:tools.evaluation_results_class:Current Best Return = 259.2370910644531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.14319248826291
INFO:tools.evaluation_results_class:Counted Episodes = 2556
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 258.7101135253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.7101135253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 219.2606658935547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18398.83984375
INFO:tools.evaluation_results_class:Current Best Return = 258.7101135253906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.23119338322174
INFO:tools.evaluation_results_class:Counted Episodes = 2539
INFO:robust_rl.robust_rl_trainer:Iteration 22 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 288.0048522949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.0048522949219
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.86605834960938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28204.935546875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.78140495867768
INFO:tools.evaluation_results_class:Counted Episodes = 4840
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.686262130737305
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 46.88%
INFO:agents.father_agent:Step: 5, Training loss: 12.59090805053711
INFO:agents.father_agent:Step: 10, Training loss: 10.629197120666504
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 263.7973327636719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.7973327636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.06777954101562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24473.673828125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.63925523661753
INFO:tools.evaluation_results_class:Counted Episodes = 5156
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.5431823730469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.5431823730469
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.5448760986328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25569.619140625
INFO:tools.evaluation_results_class:Current Best Return = 269.5431823730469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.387966399687436
INFO:tools.evaluation_results_class:Counted Episodes = 5119
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.7527160644531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.7527160644531
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.1716766357422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26822.994140625
INFO:tools.evaluation_results_class:Current Best Return = 278.7527160644531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.31597222222222
INFO:tools.evaluation_results_class:Counted Episodes = 5184
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 263.503070736267
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 23 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.7842102050781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.7842102050781
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.1397247314453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24683.234375
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.00743639921722
INFO:tools.evaluation_results_class:Counted Episodes = 5110
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.591458320617676
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 45.31%
INFO:agents.father_agent:Step: 5, Training loss: 10.716188430786133
INFO:agents.father_agent:Step: 10, Training loss: 10.636643409729004
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 274.9257507324219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.9257507324219
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.5463104248047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24358.8515625
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.71815718157182
INFO:tools.evaluation_results_class:Counted Episodes = 5166
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.459716796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.459716796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.835205078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24710.51171875
INFO:tools.evaluation_results_class:Current Best Return = 269.459716796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.83842967054646
INFO:tools.evaluation_results_class:Counted Episodes = 5069
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.9274597167969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.9274597167969
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.7301788330078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26552.537109375
INFO:tools.evaluation_results_class:Current Best Return = 282.9274597167969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.84038575083645
INFO:tools.evaluation_results_class:Counted Episodes = 5081
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 265.7192358320967
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 24 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.3932189941406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.3932189941406
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.7552947998047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26342.494140625
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.983213429256594
INFO:tools.evaluation_results_class:Counted Episodes = 5004
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.862672805786133
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 45.31%
INFO:agents.father_agent:Step: 5, Training loss: 10.670241355895996
INFO:agents.father_agent:Step: 10, Training loss: 12.585613250732422
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 282.4476318359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.4476318359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.5965118408203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28709.62109375
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.48557692307692
INFO:tools.evaluation_results_class:Counted Episodes = 4992
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.740234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.740234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.14361572265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24820.62890625
INFO:tools.evaluation_results_class:Current Best Return = 276.740234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.87217149662565
INFO:tools.evaluation_results_class:Counted Episodes = 5038
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 289.2461242675781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 290.2461242675781
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.27328491210938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27272.970703125
INFO:tools.evaluation_results_class:Current Best Return = 289.2461242675781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.01196410767697
INFO:tools.evaluation_results_class:Counted Episodes = 5015
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 267.78417087627
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 25 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 292.12109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.12109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.30642700195312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24806.666015625
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.53447927199191
INFO:tools.evaluation_results_class:Counted Episodes = 4945
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.92747688293457
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 42.19%
INFO:agents.father_agent:Step: 5, Training loss: 12.13270378112793
INFO:agents.father_agent:Step: 10, Training loss: 13.41209602355957
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 273.52435302734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.52435302734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.69337463378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23194.248046875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.18278954398016
INFO:tools.evaluation_results_class:Counted Episodes = 5241
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 259.8930358886719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.8930358886719
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.18319702148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20616.69921875
INFO:tools.evaluation_results_class:Current Best Return = 259.8930358886719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.99625187406297
INFO:tools.evaluation_results_class:Counted Episodes = 5336
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.05303955078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.05303955078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.99609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24931.953125
INFO:tools.evaluation_results_class:Current Best Return = 279.05303955078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.74980724749422
INFO:tools.evaluation_results_class:Counted Episodes = 5188
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 269.6813588705938
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 26 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.231689453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.231689453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.9581298828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25912.8203125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.273114944756735
INFO:tools.evaluation_results_class:Counted Episodes = 5159
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.138790130615234
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 42.19%
INFO:agents.father_agent:Step: 5, Training loss: 11.418481826782227
INFO:agents.father_agent:Step: 10, Training loss: 11.11199951171875
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 280.0362854003906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.0362854003906
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.67984008789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24524.8046875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.20555446900059
INFO:tools.evaluation_results_class:Counted Episodes = 5113
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.2774353027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.2774353027344
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.68527221679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20507.837890625
INFO:tools.evaluation_results_class:Current Best Return = 269.2774353027344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.00662001134859
INFO:tools.evaluation_results_class:Counted Episodes = 5287
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 294.1155700683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 295.1155700683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.74935913085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25196.873046875
INFO:tools.evaluation_results_class:Current Best Return = 294.1155700683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.03570699857172
INFO:tools.evaluation_results_class:Counted Episodes = 4901
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 271.2650425827001
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 573.67138671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 574.67138671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 443.787841796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 73183.5625
INFO:tools.evaluation_results_class:Current Best Return = 573.67138671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.79782469609725
INFO:tools.evaluation_results_class:Counted Episodes = 1563
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 307.9013977050781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 308.9013977050781
INFO:tools.evaluation_results_class:Average Discounted Reward = 252.7085723876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22902.779296875
INFO:tools.evaluation_results_class:Current Best Return = 307.9013977050781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.88262910798122
INFO:tools.evaluation_results_class:Counted Episodes = 2130
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 591.2863159179688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 592.2863159179688
INFO:tools.evaluation_results_class:Average Discounted Reward = 465.6159973144531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 79602.125
INFO:tools.evaluation_results_class:Current Best Return = 591.2863159179688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 82.90685413005272
INFO:tools.evaluation_results_class:Counted Episodes = 1707
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 708.3187866210938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 709.3187866210938
INFO:tools.evaluation_results_class:Average Discounted Reward = 545.8275146484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 113272.9140625
INFO:tools.evaluation_results_class:Current Best Return = 708.3187866210938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.39676375404531
INFO:tools.evaluation_results_class:Counted Episodes = 1545
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 421.2018737792969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 422.2018737792969
INFO:tools.evaluation_results_class:Average Discounted Reward = 345.58056640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40741.62890625
INFO:tools.evaluation_results_class:Current Best Return = 421.2018737792969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.21502347417841
INFO:tools.evaluation_results_class:Counted Episodes = 2130
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 260.8590087890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.8590087890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.3468780517578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17335.548828125
INFO:tools.evaluation_results_class:Current Best Return = 260.8590087890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.39968404423381
INFO:tools.evaluation_results_class:Counted Episodes = 2532
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 260.63092041015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.63092041015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.45492553710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16865.119140625
INFO:tools.evaluation_results_class:Current Best Return = 260.63092041015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.06782334384858
INFO:tools.evaluation_results_class:Counted Episodes = 2536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.2571105957031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.2571105957031
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.6138916015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17661.462890625
INFO:tools.evaluation_results_class:Current Best Return = 262.2571105957031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.46682464454976
INFO:tools.evaluation_results_class:Counted Episodes = 2532
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.6598205566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.6598205566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.0606689453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17845.4296875
INFO:tools.evaluation_results_class:Current Best Return = 262.6598205566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.37647987371744
INFO:tools.evaluation_results_class:Counted Episodes = 2534
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 264.3592224121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.3592224121094
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.58840942382812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17525.662109375
INFO:tools.evaluation_results_class:Current Best Return = 264.3592224121094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.811774128954745
INFO:tools.evaluation_results_class:Counted Episodes = 2497
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.2042541503906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.2042541503906
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.92587280273438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17502.712890625
INFO:tools.evaluation_results_class:Current Best Return = 262.2042541503906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.19085173501577
INFO:tools.evaluation_results_class:Counted Episodes = 2536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 260.6860046386719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.6860046386719
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.85987854003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16592.1796875
INFO:tools.evaluation_results_class:Current Best Return = 260.6860046386719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.1313210505684
INFO:tools.evaluation_results_class:Counted Episodes = 2551
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.7779846191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.7779846191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.54830932617188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16501.755859375
INFO:tools.evaluation_results_class:Current Best Return = 262.7779846191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.458201892744476
INFO:tools.evaluation_results_class:Counted Episodes = 2536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 260.65948486328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.65948486328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.73953247070312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16955.6015625
INFO:tools.evaluation_results_class:Current Best Return = 260.65948486328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.022309197651666
INFO:tools.evaluation_results_class:Counted Episodes = 2555
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.37945556640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.37945556640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.2515411376953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15950.001953125
INFO:tools.evaluation_results_class:Current Best Return = 262.37945556640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.42845942767542
INFO:tools.evaluation_results_class:Counted Episodes = 2551
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 261.53466796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.53466796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.2744598388672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16210.228515625
INFO:tools.evaluation_results_class:Current Best Return = 261.53466796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.404356435643564
INFO:tools.evaluation_results_class:Counted Episodes = 2525
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 259.95892333984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.95892333984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 219.9817657470703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16972.50390625
INFO:tools.evaluation_results_class:Current Best Return = 259.95892333984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.03129890453834
INFO:tools.evaluation_results_class:Counted Episodes = 2556
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 261.8890075683594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.8890075683594
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.48228454589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16733.6796875
INFO:tools.evaluation_results_class:Current Best Return = 261.8890075683594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.316017316017316
INFO:tools.evaluation_results_class:Counted Episodes = 2541
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.0838317871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.0838317871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.8284149169922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17213.27734375
INFO:tools.evaluation_results_class:Current Best Return = 262.0838317871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.3530106257379
INFO:tools.evaluation_results_class:Counted Episodes = 2541
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 261.68170166015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.68170166015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.75039672851562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17055.265625
INFO:tools.evaluation_results_class:Current Best Return = 261.68170166015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.14599686028257
INFO:tools.evaluation_results_class:Counted Episodes = 2548
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 266.3466796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.3466796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.0932159423828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17941.314453125
INFO:tools.evaluation_results_class:Current Best Return = 266.3466796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.869892899643
INFO:tools.evaluation_results_class:Counted Episodes = 2521
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 263.3492736816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.3492736816406
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.5021209716797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16785.375
INFO:tools.evaluation_results_class:Current Best Return = 263.3492736816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.85087719298246
INFO:tools.evaluation_results_class:Counted Episodes = 2508
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.2321472167969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.2321472167969
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.15806579589844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17632.177734375
INFO:tools.evaluation_results_class:Current Best Return = 265.2321472167969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.57380952380952
INFO:tools.evaluation_results_class:Counted Episodes = 2520
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 263.62799072265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.62799072265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.13729858398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16797.318359375
INFO:tools.evaluation_results_class:Current Best Return = 263.62799072265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.7136
INFO:tools.evaluation_results_class:Counted Episodes = 2500
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 264.1991271972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.1991271972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.47669982910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16711.052734375
INFO:tools.evaluation_results_class:Current Best Return = 264.1991271972656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.62321144674086
INFO:tools.evaluation_results_class:Counted Episodes = 2516
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 266.0954895019531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.0954895019531
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.42922973632812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15874.623046875
INFO:tools.evaluation_results_class:Current Best Return = 266.0954895019531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.59270998415214
INFO:tools.evaluation_results_class:Counted Episodes = 2524
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.6631164550781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.6631164550781
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.11703491210938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16270.1015625
INFO:tools.evaluation_results_class:Current Best Return = 262.6631164550781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.78884934756821
INFO:tools.evaluation_results_class:Counted Episodes = 2529
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 264.5670166015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.5670166015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.77293395996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17646.283203125
INFO:tools.evaluation_results_class:Current Best Return = 264.5670166015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.68446026097271
INFO:tools.evaluation_results_class:Counted Episodes = 2529
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.2411804199219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.2411804199219
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.2446746826172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15835.30078125
INFO:tools.evaluation_results_class:Current Best Return = 262.2411804199219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.294901960784316
INFO:tools.evaluation_results_class:Counted Episodes = 2550
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.02777099609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.02777099609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.95777893066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16721.80859375
INFO:tools.evaluation_results_class:Current Best Return = 265.02777099609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.93417922283901
INFO:tools.evaluation_results_class:Counted Episodes = 2522
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.25787353515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.25787353515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.5286865234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17230.494140625
INFO:tools.evaluation_results_class:Current Best Return = 265.25787353515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.44094488188976
INFO:tools.evaluation_results_class:Counted Episodes = 2540
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 591.66845703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 592.66845703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 458.47998046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 76993.4921875
INFO:tools.evaluation_results_class:Current Best Return = 591.66845703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.36734693877551
INFO:tools.evaluation_results_class:Counted Episodes = 1568
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 314.4073486328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 315.4073486328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 258.5404052734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24339.017578125
INFO:tools.evaluation_results_class:Current Best Return = 314.4073486328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.42427093132643
INFO:tools.evaluation_results_class:Counted Episodes = 2126
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 586.2442016601562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 587.2442016601562
INFO:tools.evaluation_results_class:Average Discounted Reward = 464.9217529296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 71691.3125
INFO:tools.evaluation_results_class:Current Best Return = 586.2442016601562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.29047072330654
INFO:tools.evaluation_results_class:Counted Episodes = 1742
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 711.0897827148438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 712.0897827148438
INFO:tools.evaluation_results_class:Average Discounted Reward = 550.029296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 107803.5
INFO:tools.evaluation_results_class:Current Best Return = 711.0897827148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.16570327552986
INFO:tools.evaluation_results_class:Counted Episodes = 1557
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 425.54949951171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 426.54949951171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 349.8793640136719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40206.1875
INFO:tools.evaluation_results_class:Current Best Return = 425.54949951171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.85504500236854
INFO:tools.evaluation_results_class:Counted Episodes = 2111
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.67620849609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.67620849609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.1383819580078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17752.345703125
INFO:tools.evaluation_results_class:Current Best Return = 267.67620849609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.61192873741286
INFO:tools.evaluation_results_class:Counted Episodes = 2582
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.1545715332031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.1545715332031
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.48533630371094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17779.55078125
INFO:tools.evaluation_results_class:Current Best Return = 267.1545715332031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.667447306791566
INFO:tools.evaluation_results_class:Counted Episodes = 2562
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 264.2959289550781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.2959289550781
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.12669372558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17640.103515625
INFO:tools.evaluation_results_class:Current Best Return = 264.2959289550781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.345067698259186
INFO:tools.evaluation_results_class:Counted Episodes = 2585
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.2332458496094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.2332458496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.54039001464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17387.34765625
INFO:tools.evaluation_results_class:Current Best Return = 268.2332458496094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.41069352963967
INFO:tools.evaluation_results_class:Counted Episodes = 2581
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 266.61572265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.61572265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.9611053466797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18196.392578125
INFO:tools.evaluation_results_class:Current Best Return = 266.61572265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.545665634674926
INFO:tools.evaluation_results_class:Counted Episodes = 2584
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 266.2761535644531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.2761535644531
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.98509216308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17201.595703125
INFO:tools.evaluation_results_class:Current Best Return = 266.2761535644531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.446940356312936
INFO:tools.evaluation_results_class:Counted Episodes = 2582
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.6534423828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.6534423828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.28860473632812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16397.70703125
INFO:tools.evaluation_results_class:Current Best Return = 265.6534423828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.767794632438736
INFO:tools.evaluation_results_class:Counted Episodes = 2571
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 264.8043518066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.8043518066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.63206481933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16969.962890625
INFO:tools.evaluation_results_class:Current Best Return = 264.8043518066406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.37814800464936
INFO:tools.evaluation_results_class:Counted Episodes = 2581
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.83123779296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.83123779296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.6982879638672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17262.58203125
INFO:tools.evaluation_results_class:Current Best Return = 268.83123779296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.77760375880971
INFO:tools.evaluation_results_class:Counted Episodes = 2554
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.04669189453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.04669189453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.4752960205078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17109.314453125
INFO:tools.evaluation_results_class:Current Best Return = 267.04669189453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.69182879377432
INFO:tools.evaluation_results_class:Counted Episodes = 2570
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.3962707519531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.3962707519531
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.21414184570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16826.359375
INFO:tools.evaluation_results_class:Current Best Return = 265.3962707519531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.484848484848484
INFO:tools.evaluation_results_class:Counted Episodes = 2574
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.0882873535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.0882873535156
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.9198455810547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18018.28515625
INFO:tools.evaluation_results_class:Current Best Return = 265.0882873535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.19961612284069
INFO:tools.evaluation_results_class:Counted Episodes = 2605
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.35479736328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.35479736328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.9509735107422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17972.087890625
INFO:tools.evaluation_results_class:Current Best Return = 265.35479736328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.244504435017355
INFO:tools.evaluation_results_class:Counted Episodes = 2593
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 264.6009216308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.6009216308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.33267211914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17983.83984375
INFO:tools.evaluation_results_class:Current Best Return = 264.6009216308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.56567222006974
INFO:tools.evaluation_results_class:Counted Episodes = 2581
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 264.4944152832031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.4944152832031
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.15663146972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18264.333984375
INFO:tools.evaluation_results_class:Current Best Return = 264.4944152832031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.22385179467387
INFO:tools.evaluation_results_class:Counted Episodes = 2591
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.8343505859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.8343505859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.8752899169922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16862.48046875
INFO:tools.evaluation_results_class:Current Best Return = 262.8343505859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.209069946195235
INFO:tools.evaluation_results_class:Counted Episodes = 2602
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.0379333496094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.0379333496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.26905822753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17899.32421875
INFO:tools.evaluation_results_class:Current Best Return = 268.0379333496094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.472136222910216
INFO:tools.evaluation_results_class:Counted Episodes = 2584
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.313720703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.313720703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.02224731445312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18900.599609375
INFO:tools.evaluation_results_class:Current Best Return = 270.313720703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.61115414407436
INFO:tools.evaluation_results_class:Counted Episodes = 2582
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.56036376953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.56036376953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.51316833496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17768.654296875
INFO:tools.evaluation_results_class:Current Best Return = 269.56036376953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.89878858929269
INFO:tools.evaluation_results_class:Counted Episodes = 2559
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.06512451171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.06512451171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.68673706054688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17453.798828125
INFO:tools.evaluation_results_class:Current Best Return = 271.06512451171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.27814829344841
INFO:tools.evaluation_results_class:Counted Episodes = 2549
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.622802734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.622802734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.0525360107422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18495.27734375
INFO:tools.evaluation_results_class:Current Best Return = 265.622802734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.58983313930928
INFO:tools.evaluation_results_class:Counted Episodes = 2577
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.41455078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.41455078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.38502502441406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17831.232421875
INFO:tools.evaluation_results_class:Current Best Return = 270.41455078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.80264694433632
INFO:tools.evaluation_results_class:Counted Episodes = 2569
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.4641418457031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.4641418457031
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.7987060546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17349.017578125
INFO:tools.evaluation_results_class:Current Best Return = 267.4641418457031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.61108956960062
INFO:tools.evaluation_results_class:Counted Episodes = 2579
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.8648681640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.8648681640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.88035583496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17356.376953125
INFO:tools.evaluation_results_class:Current Best Return = 265.8648681640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.43474903474903
INFO:tools.evaluation_results_class:Counted Episodes = 2590
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.4124755859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.4124755859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.4484405517578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18184.65625
INFO:tools.evaluation_results_class:Current Best Return = 270.4124755859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.650658404337726
INFO:tools.evaluation_results_class:Counted Episodes = 2582
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 266.74114990234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.74114990234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.1566162109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17962.1640625
INFO:tools.evaluation_results_class:Current Best Return = 266.74114990234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.483050847457626
INFO:tools.evaluation_results_class:Counted Episodes = 2596
INFO:robust_rl.robust_rl_trainer:Iteration 27 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.3940734863281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.3940734863281
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.4838104248047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21716.814453125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.87204156243987
INFO:tools.evaluation_results_class:Counted Episodes = 5197
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.58242416381836
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 42.19%
INFO:agents.father_agent:Step: 5, Training loss: 13.347282409667969
INFO:agents.father_agent:Step: 10, Training loss: 12.83353042602539
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 284.0556945800781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.0556945800781
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.74111938476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20574.78515625
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.12197521148928
INFO:tools.evaluation_results_class:Counted Episodes = 5083
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.0603942871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.0603942871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.5651092529297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21004.22265625
INFO:tools.evaluation_results_class:Current Best Return = 269.0603942871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.29209555253447
INFO:tools.evaluation_results_class:Counted Episodes = 5149
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 286.3316345214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.3316345214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.3434600830078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21314.849609375
INFO:tools.evaluation_results_class:Current Best Return = 286.3316345214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.98004346967002
INFO:tools.evaluation_results_class:Counted Episodes = 5061
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 273.0123691657832
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 28 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.3411865234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.3411865234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.8231964111328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21327.77734375
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.8550501156515
INFO:tools.evaluation_results_class:Counted Episodes = 5188
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.44151782989502
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 43.75%
INFO:agents.father_agent:Step: 5, Training loss: 13.341460227966309
INFO:agents.father_agent:Step: 10, Training loss: 10.859536170959473
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 283.2236022949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.2236022949219
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.17236328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26402.80078125
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.44889583740473
INFO:tools.evaluation_results_class:Counted Episodes = 5117
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.83538818359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.83538818359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.84600830078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22059.61328125
INFO:tools.evaluation_results_class:Current Best Return = 271.83538818359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.49394812680115
INFO:tools.evaluation_results_class:Counted Episodes = 5205
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 292.12109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.12109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.05136108398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21748.37109375
INFO:tools.evaluation_results_class:Current Best Return = 292.12109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.048631620397714
INFO:tools.evaluation_results_class:Counted Episodes = 5079
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 274.3965931100644
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 29 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 291.1611633300781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.1611633300781
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.4586639404297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22415.974609375
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.09313338595106
INFO:tools.evaluation_results_class:Counted Episodes = 5068
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.433721542358398
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 45.31%
INFO:agents.father_agent:Step: 5, Training loss: 15.505889892578125
INFO:agents.father_agent:Step: 10, Training loss: 17.859407424926758
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 293.678955078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 294.678955078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 247.03330993652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23915.2421875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.9014768359296
INFO:tools.evaluation_results_class:Counted Episodes = 4943
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.61505126953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.61505126953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.900390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22041.21484375
INFO:tools.evaluation_results_class:Current Best Return = 279.61505126953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.80815533980582
INFO:tools.evaluation_results_class:Counted Episodes = 5150
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 292.6498718261719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.6498718261719
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.66078186035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23320.845703125
INFO:tools.evaluation_results_class:Current Best Return = 292.6498718261719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.34644894747196
INFO:tools.evaluation_results_class:Counted Episodes = 5083
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 276.108710517093
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 30 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.9125061035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.9125061035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.90606689453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21775.10546875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.188577999233424
INFO:tools.evaluation_results_class:Counted Episodes = 5218
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.410344123840332
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 43.75%
INFO:agents.father_agent:Step: 5, Training loss: 12.777050971984863
INFO:agents.father_agent:Step: 10, Training loss: 13.044462203979492
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 281.6854248046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.6854248046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.0343017578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21285.23046875
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.14332247557003
INFO:tools.evaluation_results_class:Counted Episodes = 5219
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.4031677246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.4031677246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.67950439453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21605.078125
INFO:tools.evaluation_results_class:Current Best Return = 278.4031677246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.965397923875436
INFO:tools.evaluation_results_class:Counted Episodes = 5202
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.4812927246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.4812927246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.07640075683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21612.75390625
INFO:tools.evaluation_results_class:Current Best Return = 278.4812927246094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.42220543806646
INFO:tools.evaluation_results_class:Counted Episodes = 5296
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 276.3532040847019
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 31 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.6760559082031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.6760559082031
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.35752868652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19614.6015625
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.384911470361814
INFO:tools.evaluation_results_class:Counted Episodes = 5196
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.184102058410645
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 43.75%
INFO:agents.father_agent:Step: 5, Training loss: 15.62177848815918
INFO:agents.father_agent:Step: 10, Training loss: 10.976225852966309
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 278.32977294921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.32977294921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.8446044921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21411.759765625
INFO:tools.evaluation_results_class:Current Best Return = 523.3862915039062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.3542196984531
INFO:tools.evaluation_results_class:Counted Episodes = 5107
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.2886962890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.2886962890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.78053283691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21333.69140625
INFO:tools.evaluation_results_class:Current Best Return = 277.2886962890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.422900763358776
INFO:tools.evaluation_results_class:Counted Episodes = 5240
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.4044494628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.4044494628906
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.9202117919922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20350.892578125
INFO:tools.evaluation_results_class:Current Best Return = 278.4044494628906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.87015687015687
INFO:tools.evaluation_results_class:Counted Episodes = 5291
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 276.01665633255794
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 582.3594970703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 583.3594970703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 449.7785339355469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 75207.765625
INFO:tools.evaluation_results_class:Current Best Return = 582.3594970703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.56420233463035
INFO:tools.evaluation_results_class:Counted Episodes = 1542
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 315.62908935546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 316.62908935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 260.2713623046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21399.767578125
INFO:tools.evaluation_results_class:Current Best Return = 315.62908935546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.29263746505126
INFO:tools.evaluation_results_class:Counted Episodes = 2146
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 586.5361328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 587.5361328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 463.97637939453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 73897.96875
INFO:tools.evaluation_results_class:Current Best Return = 586.5361328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 82.2184776292853
INFO:tools.evaluation_results_class:Counted Episodes = 1721
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 707.3619384765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 708.3619384765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 545.1510009765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 102652.6875
INFO:tools.evaluation_results_class:Current Best Return = 707.3619384765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.7127868852459
INFO:tools.evaluation_results_class:Counted Episodes = 1525
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 413.36163330078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 414.36163330078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 339.25433349609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38802.06640625
INFO:tools.evaluation_results_class:Current Best Return = 413.36163330078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.94436586515795
INFO:tools.evaluation_results_class:Counted Episodes = 2121
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.4602966308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.4602966308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.86712646484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16854.078125
INFO:tools.evaluation_results_class:Current Best Return = 267.4602966308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.60829135993801
INFO:tools.evaluation_results_class:Counted Episodes = 2581
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.6676940917969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.6676940917969
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.21966552734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17394.099609375
INFO:tools.evaluation_results_class:Current Best Return = 265.6676940917969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.56832298136646
INFO:tools.evaluation_results_class:Counted Episodes = 2576
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 264.390380859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.390380859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.93905639648438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16961.0
INFO:tools.evaluation_results_class:Current Best Return = 264.390380859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.6799531066823
INFO:tools.evaluation_results_class:Counted Episodes = 2559
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.5799865722656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.5799865722656
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.99925231933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17052.525390625
INFO:tools.evaluation_results_class:Current Best Return = 265.5799865722656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.5200467107824
INFO:tools.evaluation_results_class:Counted Episodes = 2569
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 264.7021179199219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.7021179199219
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.63645935058594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16193.6064453125
INFO:tools.evaluation_results_class:Current Best Return = 264.7021179199219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.26924564796905
INFO:tools.evaluation_results_class:Counted Episodes = 2585
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.7076721191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.7076721191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.42813110351562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16323.7080078125
INFO:tools.evaluation_results_class:Current Best Return = 265.7076721191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.20208252988816
INFO:tools.evaluation_results_class:Counted Episodes = 2593
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.8420715332031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.8420715332031
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.4902801513672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17905.9765625
INFO:tools.evaluation_results_class:Current Best Return = 267.8420715332031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.074276778733385
INFO:tools.evaluation_results_class:Counted Episodes = 2558
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.2642517089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.2642517089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.83526611328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16489.720703125
INFO:tools.evaluation_results_class:Current Best Return = 267.2642517089844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.59138533178114
INFO:tools.evaluation_results_class:Counted Episodes = 2577
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.4052429199219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.4052429199219
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.25921630859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17487.5234375
INFO:tools.evaluation_results_class:Current Best Return = 268.4052429199219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.943114947038055
INFO:tools.evaluation_results_class:Counted Episodes = 2549
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.2659912109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.2659912109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.91085815429688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17251.47265625
INFO:tools.evaluation_results_class:Current Best Return = 269.2659912109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.09865824782952
INFO:tools.evaluation_results_class:Counted Episodes = 2534
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.224609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.224609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.30294799804688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17566.97265625
INFO:tools.evaluation_results_class:Current Best Return = 268.224609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.84453125
INFO:tools.evaluation_results_class:Counted Episodes = 2560
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 263.7490234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.7490234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.1598663330078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15944.3388671875
INFO:tools.evaluation_results_class:Current Best Return = 263.7490234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.292212798766386
INFO:tools.evaluation_results_class:Counted Episodes = 2594
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.84014892578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.84014892578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.30245971679688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16343.2841796875
INFO:tools.evaluation_results_class:Current Best Return = 265.84014892578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.8098016336056
INFO:tools.evaluation_results_class:Counted Episodes = 2571
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 266.2325439453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.2325439453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.02584838867188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16478.830078125
INFO:tools.evaluation_results_class:Current Best Return = 266.2325439453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.72248062015504
INFO:tools.evaluation_results_class:Counted Episodes = 2580
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.4508972167969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.4508972167969
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.04681396484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16612.61328125
INFO:tools.evaluation_results_class:Current Best Return = 268.4508972167969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.62275915822291
INFO:tools.evaluation_results_class:Counted Episodes = 2566
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.00274658203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.00274658203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.3197021484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16998.75390625
INFO:tools.evaluation_results_class:Current Best Return = 268.00274658203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.894962905115186
INFO:tools.evaluation_results_class:Counted Episodes = 2561
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.7541198730469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.7541198730469
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.959716796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16625.693359375
INFO:tools.evaluation_results_class:Current Best Return = 267.7541198730469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.871774824081314
INFO:tools.evaluation_results_class:Counted Episodes = 2558
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.97186279296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.97186279296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.4270782470703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16356.7890625
INFO:tools.evaluation_results_class:Current Best Return = 267.97186279296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.86674482219617
INFO:tools.evaluation_results_class:Counted Episodes = 2559
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.3072509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.3072509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.07211303710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17629.478515625
INFO:tools.evaluation_results_class:Current Best Return = 269.3072509765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.89941291585127
INFO:tools.evaluation_results_class:Counted Episodes = 2555
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 266.0015563964844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.0015563964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.3207550048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17154.685546875
INFO:tools.evaluation_results_class:Current Best Return = 266.0015563964844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.80046765393609
INFO:tools.evaluation_results_class:Counted Episodes = 2566
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.2529296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.2529296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.42176818847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17646.634765625
INFO:tools.evaluation_results_class:Current Best Return = 271.2529296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.26274509803922
INFO:tools.evaluation_results_class:Counted Episodes = 2550
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
2025-08-20 06:49:06.225701: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-20 06:49:06.227610: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 06:49:06.258640: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-20 06:49:06.258691: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-20 06:49:06.259960: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-20 06:49:06.265740: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-20 06:49:06.265925: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-20 06:49:06.832201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/subset_evaluation/rover/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/subset_evaluation/rover/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"value"}max=? [F "empty"] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 86 states and 19241 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"value"}max=? [F "label_empty"] 
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Starting extraction loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 1 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 336.0445861816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 337.0445861816406
INFO:tools.evaluation_results_class:Average Discounted Reward = 267.2794494628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44515.0078125
INFO:tools.evaluation_results_class:Current Best Return = 336.0445861816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.74756756756757
INFO:tools.evaluation_results_class:Counted Episodes = 3700
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 777.5645751953125
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 39.06%
INFO:agents.father_agent:Step: 5, Training loss: 18.190425872802734
INFO:agents.father_agent:Step: 10, Training loss: 18.413272857666016
INFO:agents.father_agent:Step: 15, Training loss: 19.893529891967773
INFO:agents.father_agent:Step: 20, Training loss: 19.347864151000977
INFO:agents.father_agent:Step: 25, Training loss: 20.643840789794922
INFO:agents.father_agent:Step: 30, Training loss: 18.943708419799805
INFO:agents.father_agent:Step: 35, Training loss: 19.69251823425293
INFO:agents.father_agent:Step: 40, Training loss: 22.217864990234375
INFO:agents.father_agent:Step: 45, Training loss: 20.82318878173828
INFO:agents.father_agent:Step: 50, Training loss: 27.057405471801758
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 45.31%
INFO:agents.father_agent:Step: 55, Training loss: 21.19540023803711
INFO:agents.father_agent:Step: 60, Training loss: 22.993450164794922
INFO:agents.father_agent:Step: 65, Training loss: 19.64035415649414
INFO:agents.father_agent:Step: 70, Training loss: 25.098886489868164
INFO:agents.father_agent:Step: 75, Training loss: 26.35082244873047
INFO:agents.father_agent:Step: 80, Training loss: 25.975000381469727
INFO:agents.father_agent:Step: 85, Training loss: 24.019969940185547
INFO:agents.father_agent:Step: 90, Training loss: 25.573701858520508
INFO:agents.father_agent:Step: 95, Training loss: 22.166231155395508
INFO:agents.father_agent:Step: 100, Training loss: 23.550260543823242
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 426.69317626953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 427.69317626953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 351.6133117675781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 71251.78125
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.33958931331419
INFO:tools.evaluation_results_class:Counted Episodes = 4529
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 50.00%
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 417.8212585449219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 418.8212585449219
INFO:tools.evaluation_results_class:Average Discounted Reward = 344.3069763183594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 69070.0390625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.33370068327089
INFO:tools.evaluation_results_class:Counted Episodes = 4537
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 437.2684631347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 438.2684631347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 356.8572998046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 78201.0390625
INFO:tools.evaluation_results_class:Current Best Return = 437.2684631347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.7247000226398
INFO:tools.evaluation_results_class:Counted Episodes = 4417
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 430.19158935546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 431.19158935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 353.8691101074219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 75843.34375
INFO:tools.evaluation_results_class:Current Best Return = 430.19158935546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.74403470715835
INFO:tools.evaluation_results_class:Counted Episodes = 4610
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 245.62480858997893
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 691.461669921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 692.461669921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 550.9669189453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 113872.8359375
INFO:tools.evaluation_results_class:Current Best Return = 691.461669921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.91812227074236
INFO:tools.evaluation_results_class:Counted Episodes = 1832
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.7882385253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.7882385253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.09239196777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26475.162109375
INFO:tools.evaluation_results_class:Current Best Return = 284.7882385253906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.9387848332572
INFO:tools.evaluation_results_class:Counted Episodes = 2189
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 288.6626892089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.6626892089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.05557250976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27764.43359375
INFO:tools.evaluation_results_class:Current Best Return = 288.6626892089844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.564331210191085
INFO:tools.evaluation_results_class:Counted Episodes = 2355
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 466.0623779296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 467.0623779296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 387.010498046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 53303.34375
INFO:tools.evaluation_results_class:Current Best Return = 466.0623779296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.609032258064516
INFO:tools.evaluation_results_class:Counted Episodes = 2325
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 614.88720703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 615.88720703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 493.72015380859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 86173.3203125
INFO:tools.evaluation_results_class:Current Best Return = 614.88720703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 75.6042105263158
INFO:tools.evaluation_results_class:Counted Episodes = 1900
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 237.22622680664062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.22622680664062
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.90785217285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19107.671875
INFO:tools.evaluation_results_class:Current Best Return = 237.22622680664062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.356574460022735
INFO:tools.evaluation_results_class:Counted Episodes = 2639
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 701.8824462890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 702.8824462890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 561.7131958007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 110570.96875
INFO:tools.evaluation_results_class:Current Best Return = 701.8824462890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.39397201291712
INFO:tools.evaluation_results_class:Counted Episodes = 1858
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 293.6661071777344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 294.6661071777344
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.76084899902344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27811.484375
INFO:tools.evaluation_results_class:Current Best Return = 293.6661071777344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.50520126639529
INFO:tools.evaluation_results_class:Counted Episodes = 2211
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 292.60400390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.60400390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 244.84188842773438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29672.736328125
INFO:tools.evaluation_results_class:Current Best Return = 292.60400390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.765494137353436
INFO:tools.evaluation_results_class:Counted Episodes = 2388
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 469.2938232421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 470.2938232421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 392.2616882324219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 53292.96484375
INFO:tools.evaluation_results_class:Current Best Return = 469.2938232421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.082807902480035
INFO:tools.evaluation_results_class:Counted Episodes = 2379
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 623.4415893554688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 624.4415893554688
INFO:tools.evaluation_results_class:Average Discounted Reward = 502.485595703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 87878.0546875
INFO:tools.evaluation_results_class:Current Best Return = 623.4415893554688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 75.14479874542603
INFO:tools.evaluation_results_class:Counted Episodes = 1913
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 242.11618041992188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.11618041992188
INFO:tools.evaluation_results_class:Average Discounted Reward = 206.25775146484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19761.7421875
INFO:tools.evaluation_results_class:Current Best Return = 242.11618041992188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.211618257261414
INFO:tools.evaluation_results_class:Counted Episodes = 2651
INFO:robust_rl.robust_rl_trainer:Iteration 2 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 395.0408020019531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 396.0408020019531
INFO:tools.evaluation_results_class:Average Discounted Reward = 325.7196044921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 69053.0
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.4327320147794
INFO:tools.evaluation_results_class:Counted Episodes = 4601
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 25.632535934448242
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 50.00%
INFO:agents.father_agent:Step: 5, Training loss: 23.4902286529541
INFO:agents.father_agent:Step: 10, Training loss: 20.820812225341797
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 411.7283935546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 412.7283935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 341.23992919921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 75040.8984375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.62114726027397
INFO:tools.evaluation_results_class:Counted Episodes = 4672
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 406.31689453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 407.31689453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 335.3663330078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 73041.9140625
INFO:tools.evaluation_results_class:Current Best Return = 406.31689453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.670643374520665
INFO:tools.evaluation_results_class:Counted Episodes = 4694
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 400.2098388671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 401.2098388671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 331.7735900878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 74214.1875
INFO:tools.evaluation_results_class:Current Best Return = 400.2098388671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.20840266222962
INFO:tools.evaluation_results_class:Counted Episodes = 4808
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 244.1406080735626
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 3 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 376.490234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 377.490234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 313.1766052246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 71498.6328125
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.29786359901397
INFO:tools.evaluation_results_class:Counted Episodes = 4868
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 19.87548828125
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 48.44%
INFO:agents.father_agent:Step: 5, Training loss: 20.19700813293457
INFO:agents.father_agent:Step: 10, Training loss: 25.431669235229492
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 370.107666015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 371.107666015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 309.7197265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 68168.1328125
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.67485898468976
INFO:tools.evaluation_results_class:Counted Episodes = 4964
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 375.60986328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 376.60986328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 313.5721435546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 66697.1875
INFO:tools.evaluation_results_class:Current Best Return = 375.60986328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.07971159623473
INFO:tools.evaluation_results_class:Counted Episodes = 4993
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 375.6707458496094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 376.6707458496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 313.5426330566406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 72928.1328125
INFO:tools.evaluation_results_class:Current Best Return = 375.6707458496094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.39235492176669
INFO:tools.evaluation_results_class:Counted Episodes = 5049
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 242.046338530692
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 4 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 343.7750549316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 344.7750549316406
INFO:tools.evaluation_results_class:Average Discounted Reward = 288.5804443359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 64181.02734375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.71265498917536
INFO:tools.evaluation_results_class:Counted Episodes = 5081
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 21.15619659423828
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 48.44%
INFO:agents.father_agent:Step: 5, Training loss: 20.268795013427734
INFO:agents.father_agent:Step: 10, Training loss: 24.062515258789062
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 352.22509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 353.22509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 295.3916015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 67745.75
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.46868250539957
INFO:tools.evaluation_results_class:Counted Episodes = 5093
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 355.5863037109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 356.5863037109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 297.8958740234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 66262.53125
INFO:tools.evaluation_results_class:Current Best Return = 355.5863037109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.21644685802948
INFO:tools.evaluation_results_class:Counted Episodes = 5156
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 361.5852355957031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 362.5852355957031
INFO:tools.evaluation_results_class:Average Discounted Reward = 302.5921325683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 67196.5859375
INFO:tools.evaluation_results_class:Current Best Return = 361.5852355957031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.42252972130189
INFO:tools.evaluation_results_class:Counted Episodes = 5131
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 241.09069625350196
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 5 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 342.4468078613281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 343.4468078613281
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.66259765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 65760.828125
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.50688384719798
INFO:tools.evaluation_results_class:Counted Episodes = 5157
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 19.912248611450195
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 48.44%
INFO:agents.father_agent:Step: 5, Training loss: 15.446702003479004
INFO:agents.father_agent:Step: 10, Training loss: 19.372316360473633
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 342.07861328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 343.07861328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 287.947998046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 66911.6484375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.992389649923894
INFO:tools.evaluation_results_class:Counted Episodes = 5256
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 338.79522705078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 339.79522705078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 284.6501159667969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 62893.62890625
INFO:tools.evaluation_results_class:Current Best Return = 338.79522705078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.8809201623816
INFO:tools.evaluation_results_class:Counted Episodes = 5173
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 338.0844421386719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 339.0844421386719
INFO:tools.evaluation_results_class:Average Discounted Reward = 283.2966613769531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 68814.4140625
INFO:tools.evaluation_results_class:Current Best Return = 338.0844421386719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.35544120449781
INFO:tools.evaluation_results_class:Counted Episodes = 5247
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 239.77779289462367
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 6 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 313.2151184082031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 314.2151184082031
INFO:tools.evaluation_results_class:Average Discounted Reward = 265.998291015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 54072.19140625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.92210682492582
INFO:tools.evaluation_results_class:Counted Episodes = 5392
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 16.519372940063477
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 48.44%
INFO:agents.father_agent:Step: 5, Training loss: 20.52426528930664
INFO:agents.father_agent:Step: 10, Training loss: 21.74948501586914
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 323.8235168457031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 324.8235168457031
INFO:tools.evaluation_results_class:Average Discounted Reward = 273.4172058105469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 60525.7734375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.416524216524216
INFO:tools.evaluation_results_class:Counted Episodes = 5265
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 327.41015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 328.41015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 275.3542175292969
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 59350.39453125
INFO:tools.evaluation_results_class:Current Best Return = 327.41015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.31338703490602
INFO:tools.evaluation_results_class:Counted Episodes = 5214
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 311.8985900878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 312.8985900878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 264.0262145996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 54886.2109375
INFO:tools.evaluation_results_class:Current Best Return = 311.8985900878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.7683284457478
INFO:tools.evaluation_results_class:Counted Episodes = 5456
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 239.6166429954362
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 735.1058349609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 736.1058349609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 592.7645874023438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 127279.1484375
INFO:tools.evaluation_results_class:Current Best Return = 735.1058349609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.74718526100307
INFO:tools.evaluation_results_class:Counted Episodes = 1954
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.2472839355469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.2472839355469
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.024169921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28562.7109375
INFO:tools.evaluation_results_class:Current Best Return = 281.2472839355469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.41022280471822
INFO:tools.evaluation_results_class:Counted Episodes = 2289
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.4265441894531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.4265441894531
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.11280822753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30585.76171875
INFO:tools.evaluation_results_class:Current Best Return = 281.4265441894531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.843700159489636
INFO:tools.evaluation_results_class:Counted Episodes = 2508
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 486.18133544921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 487.18133544921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 411.255859375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 62291.1796875
INFO:tools.evaluation_results_class:Current Best Return = 486.18133544921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.03841721091049
INFO:tools.evaluation_results_class:Counted Episodes = 2603
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 635.44482421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 636.44482421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 520.2669677734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 96310.7890625
INFO:tools.evaluation_results_class:Current Best Return = 635.44482421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.17752596789424
INFO:tools.evaluation_results_class:Counted Episodes = 2118
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 235.9625701904297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.9625701904297
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.65574645996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21996.560546875
INFO:tools.evaluation_results_class:Current Best Return = 235.9625701904297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.27664884135472
INFO:tools.evaluation_results_class:Counted Episodes = 2805
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 235.30471801757812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 236.30471801757812
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.6279754638672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22202.2421875
INFO:tools.evaluation_results_class:Current Best Return = 235.30471801757812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.882077172737105
INFO:tools.evaluation_results_class:Counted Episodes = 2773
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 236.9935302734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.9935302734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.34738159179688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21720.91015625
INFO:tools.evaluation_results_class:Current Best Return = 236.9935302734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.729039222741996
INFO:tools.evaluation_results_class:Counted Episodes = 2779
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 236.15635681152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.15635681152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.52882385253906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20964.052734375
INFO:tools.evaluation_results_class:Current Best Return = 236.15635681152344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.106768005790805
INFO:tools.evaluation_results_class:Counted Episodes = 2763
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 234.3333282470703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 235.3333282470703
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.29579162597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21719.82421875
INFO:tools.evaluation_results_class:Current Best Return = 234.3333282470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.46953405017921
INFO:tools.evaluation_results_class:Counted Episodes = 2790
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 239.02020263671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.02020263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.80642700195312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22224.34765625
INFO:tools.evaluation_results_class:Current Best Return = 239.02020263671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.90220137134609
INFO:tools.evaluation_results_class:Counted Episodes = 2771
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 728.4741821289062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 729.4741821289062
INFO:tools.evaluation_results_class:Average Discounted Reward = 589.1494140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 123525.765625
INFO:tools.evaluation_results_class:Current Best Return = 728.4741821289062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.2796739684157
INFO:tools.evaluation_results_class:Counted Episodes = 1963
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.94195556640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.94195556640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.01275634765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29348.5078125
INFO:tools.evaluation_results_class:Current Best Return = 280.94195556640625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.143229166666664
INFO:tools.evaluation_results_class:Counted Episodes = 2304
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.44927978515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.44927978515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.82188415527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29892.802734375
INFO:tools.evaluation_results_class:Current Best Return = 276.44927978515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.5955766192733
INFO:tools.evaluation_results_class:Counted Episodes = 2532
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 486.7109069824219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 487.7109069824219
INFO:tools.evaluation_results_class:Average Discounted Reward = 413.4586181640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 60592.8125
INFO:tools.evaluation_results_class:Current Best Return = 486.7109069824219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.93941660433807
INFO:tools.evaluation_results_class:Counted Episodes = 2674
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 648.5277709960938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 649.5277709960938
INFO:tools.evaluation_results_class:Average Discounted Reward = 531.2289428710938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 108046.8125
INFO:tools.evaluation_results_class:Current Best Return = 648.5277709960938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.82305882352941
INFO:tools.evaluation_results_class:Counted Episodes = 2125
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 232.8389892578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 233.8389892578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 199.7890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22727.5859375
INFO:tools.evaluation_results_class:Current Best Return = 232.8389892578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.93714689265537
INFO:tools.evaluation_results_class:Counted Episodes = 2832
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 239.311279296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.311279296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.02725219726562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23016.9609375
INFO:tools.evaluation_results_class:Current Best Return = 239.311279296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.14172089660159
INFO:tools.evaluation_results_class:Counted Episodes = 2766
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 239.9619598388672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.9619598388672
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.59243774414062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23361.763671875
INFO:tools.evaluation_results_class:Current Best Return = 239.9619598388672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.93623188405797
INFO:tools.evaluation_results_class:Counted Episodes = 2760
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 239.74429321289062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.74429321289062
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.4726104736328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22708.939453125
INFO:tools.evaluation_results_class:Current Best Return = 239.74429321289062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.1215088864708
INFO:tools.evaluation_results_class:Counted Episodes = 2757
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.9451446533203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.9451446533203
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.06222534179688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23847.115234375
INFO:tools.evaluation_results_class:Current Best Return = 241.9451446533203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.36432982201235
INFO:tools.evaluation_results_class:Counted Episodes = 2753
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 239.98915100097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.98915100097656
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.7411651611328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22348.751953125
INFO:tools.evaluation_results_class:Current Best Return = 239.98915100097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.05499276410998
INFO:tools.evaluation_results_class:Counted Episodes = 2764
INFO:robust_rl.robust_rl_trainer:Iteration 7 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 317.6776123046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 318.6776123046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 268.9646301269531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 57462.08203125
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.42700661000944
INFO:tools.evaluation_results_class:Counted Episodes = 5295
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 16.065156936645508
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 50.00%
INFO:agents.father_agent:Step: 5, Training loss: 20.1982479095459
INFO:agents.father_agent:Step: 10, Training loss: 16.449586868286133
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 322.8667297363281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 323.8667297363281
INFO:tools.evaluation_results_class:Average Discounted Reward = 273.25244140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 56602.33984375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.3835409588524
INFO:tools.evaluation_results_class:Counted Episodes = 5298
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 307.0242614746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 308.0242614746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 259.2640075683594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 50779.9375
INFO:tools.evaluation_results_class:Current Best Return = 307.0242614746094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.866716445771885
INFO:tools.evaluation_results_class:Counted Episodes = 5357
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 318.7414245605469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 319.7414245605469
INFO:tools.evaluation_results_class:Average Discounted Reward = 267.70135498046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 57927.5390625
INFO:tools.evaluation_results_class:Current Best Return = 318.7414245605469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.9505025602124
INFO:tools.evaluation_results_class:Counted Episodes = 5273
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 239.78635609766033
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 8 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 313.50299072265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 314.50299072265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 265.52337646484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 52187.2265625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.628889307938906
INFO:tools.evaluation_results_class:Counted Episodes = 5303
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 18.713748931884766
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 50.00%
INFO:agents.father_agent:Step: 5, Training loss: 16.93739891052246
INFO:agents.father_agent:Step: 10, Training loss: 18.30230140686035
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 303.2765197753906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 304.2765197753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 256.2506103515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 55169.7421875
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.44461305007587
INFO:tools.evaluation_results_class:Counted Episodes = 5272
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 300.15740966796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 301.15740966796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 253.42323303222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 52827.54296875
INFO:tools.evaluation_results_class:Current Best Return = 300.15740966796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.36023805095778
INFO:tools.evaluation_results_class:Counted Episodes = 5377
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 315.1640319824219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 316.1640319824219
INFO:tools.evaluation_results_class:Average Discounted Reward = 264.85992431640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 59510.2265625
INFO:tools.evaluation_results_class:Current Best Return = 315.1640319824219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.938360037700285
INFO:tools.evaluation_results_class:Counted Episodes = 5305
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 240.64682095484883
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 9 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 305.6968688964844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 306.6968688964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 257.8771667480469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 53456.51171875
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.87315968289921
INFO:tools.evaluation_results_class:Counted Episodes = 5298
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 18.209318161010742
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 50.00%
INFO:agents.father_agent:Step: 5, Training loss: 19.77899932861328
INFO:agents.father_agent:Step: 10, Training loss: 17.727231979370117
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 287.3048095703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 288.3048095703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.56883239746094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 48407.8515625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.991949073207266
INFO:tools.evaluation_results_class:Counted Episodes = 5341
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 287.52972412109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 288.52972412109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.67724609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44743.65625
INFO:tools.evaluation_results_class:Current Best Return = 287.52972412109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.36281859070465
INFO:tools.evaluation_results_class:Counted Episodes = 5336
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 300.8851013183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 301.8851013183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 254.581787109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 47136.7890625
INFO:tools.evaluation_results_class:Current Best Return = 300.8851013183594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.568156214795344
INFO:tools.evaluation_results_class:Counted Episodes = 5326
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 241.96917538699267
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 10 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 292.5071105957031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.5071105957031
INFO:tools.evaluation_results_class:Average Discounted Reward = 247.54005432128906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 46081.64453125
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.0620896642281
INFO:tools.evaluation_results_class:Counted Episodes = 5331
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 20.115320205688477
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 51.56%
INFO:agents.father_agent:Step: 5, Training loss: 18.2333984375
INFO:agents.father_agent:Step: 10, Training loss: 19.22637367248535
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 290.4873352050781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 291.4873352050781
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.54605102539062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 45670.83984375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.467399545109934
INFO:tools.evaluation_results_class:Counted Episodes = 5276
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 294.9775390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 295.9775390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.95394897460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 46795.03515625
INFO:tools.evaluation_results_class:Current Best Return = 294.9775390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.75714845280063
INFO:tools.evaluation_results_class:Counted Episodes = 5106
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 295.1315002441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 296.1315002441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.26980590820312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44908.890625
INFO:tools.evaluation_results_class:Current Best Return = 295.1315002441406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.9873417721519
INFO:tools.evaluation_results_class:Counted Episodes = 5372
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 243.95780079521285
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 11 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 290.0078430175781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 291.0078430175781
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.05068969726562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 43246.44921875
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.19857890800299
INFO:tools.evaluation_results_class:Counted Episodes = 5348
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 18.7166805267334
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 53.12%
INFO:agents.father_agent:Step: 5, Training loss: 18.182422637939453
INFO:agents.father_agent:Step: 10, Training loss: 21.702123641967773
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 282.32342529296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.32342529296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.38619995117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40101.6953125
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.780701754385966
INFO:tools.evaluation_results_class:Counted Episodes = 5244
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 291.3348083496094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.3348083496094
INFO:tools.evaluation_results_class:Average Discounted Reward = 244.7844696044922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42286.06640625
INFO:tools.evaluation_results_class:Current Best Return = 291.3348083496094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.162655279503106
INFO:tools.evaluation_results_class:Counted Episodes = 5152
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 292.60968017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 293.60968017578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.92437744140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 44768.56640625
INFO:tools.evaluation_results_class:Current Best Return = 292.60968017578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.33766481941525
INFO:tools.evaluation_results_class:Counted Episodes = 5233
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 246.11423712851942
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 726.8587036132812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 727.8587036132812
INFO:tools.evaluation_results_class:Average Discounted Reward = 578.799072265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 127604.6328125
INFO:tools.evaluation_results_class:Current Best Return = 726.8587036132812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 77.95204359673025
INFO:tools.evaluation_results_class:Counted Episodes = 1835
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 289.8262939453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 290.8262939453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.52813720703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27202.134765625
INFO:tools.evaluation_results_class:Current Best Return = 289.8262939453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.74965862539827
INFO:tools.evaluation_results_class:Counted Episodes = 2197
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 291.51153564453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.51153564453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 244.2150115966797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29344.22265625
INFO:tools.evaluation_results_class:Current Best Return = 291.51153564453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.75178496430071
INFO:tools.evaluation_results_class:Counted Episodes = 2381
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 482.4261474609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 483.4261474609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 403.0624084472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 55254.30078125
INFO:tools.evaluation_results_class:Current Best Return = 482.4261474609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.550210970464136
INFO:tools.evaluation_results_class:Counted Episodes = 2370
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 627.4467163085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 628.4467163085938
INFO:tools.evaluation_results_class:Average Discounted Reward = 505.6081848144531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 92990.640625
INFO:tools.evaluation_results_class:Current Best Return = 627.4467163085938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 73.73703133025167
INFO:tools.evaluation_results_class:Counted Episodes = 1947
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.4307403564453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.4307403564453
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.66497802734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21153.57421875
INFO:tools.evaluation_results_class:Current Best Return = 241.4307403564453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.35730507191521
INFO:tools.evaluation_results_class:Counted Episodes = 2642
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.3332061767578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.3332061767578
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.84385681152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21857.697265625
INFO:tools.evaluation_results_class:Current Best Return = 241.3332061767578
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.29825213834139
INFO:tools.evaluation_results_class:Counted Episodes = 2689
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 237.6707000732422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.6707000732422
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.03271484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21141.3515625
INFO:tools.evaluation_results_class:Current Best Return = 237.6707000732422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.68281938325991
INFO:tools.evaluation_results_class:Counted Episodes = 2724
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 236.7694854736328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 237.7694854736328
INFO:tools.evaluation_results_class:Average Discounted Reward = 201.89601135253906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21419.255859375
INFO:tools.evaluation_results_class:Current Best Return = 236.7694854736328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.97044698928703
INFO:tools.evaluation_results_class:Counted Episodes = 2707
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 237.9348602294922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 238.9348602294922
INFO:tools.evaluation_results_class:Average Discounted Reward = 202.9665069580078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20572.9921875
INFO:tools.evaluation_results_class:Current Best Return = 237.9348602294922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.03700962250185
INFO:tools.evaluation_results_class:Counted Episodes = 2702
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 238.62120056152344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.62120056152344
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.63926696777344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20806.345703125
INFO:tools.evaluation_results_class:Current Best Return = 238.62120056152344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.2060785767235
INFO:tools.evaluation_results_class:Counted Episodes = 2698
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 238.63584899902344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.63584899902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.1448516845703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19810.451171875
INFO:tools.evaluation_results_class:Current Best Return = 238.63584899902344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.21283018867925
INFO:tools.evaluation_results_class:Counted Episodes = 2650
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 243.0124053955078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.0124053955078
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.17735290527344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20206.5703125
INFO:tools.evaluation_results_class:Current Best Return = 243.0124053955078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.977059044753666
INFO:tools.evaluation_results_class:Counted Episodes = 2659
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 239.61610412597656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.61610412597656
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.1945037841797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20485.453125
INFO:tools.evaluation_results_class:Current Best Return = 239.61610412597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.3553412462908
INFO:tools.evaluation_results_class:Counted Episodes = 2696
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 240.4124298095703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 241.4124298095703
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.0658721923828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20116.1875
INFO:tools.evaluation_results_class:Current Best Return = 240.4124298095703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.99133709981167
INFO:tools.evaluation_results_class:Counted Episodes = 2655
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 241.3119354248047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 242.3119354248047
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.7949981689453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20797.517578125
INFO:tools.evaluation_results_class:Current Best Return = 241.3119354248047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.75
INFO:tools.evaluation_results_class:Counted Episodes = 2664
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 715.1743774414062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 716.1743774414062
INFO:tools.evaluation_results_class:Average Discounted Reward = 574.3556518554688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 120638.7109375
INFO:tools.evaluation_results_class:Current Best Return = 715.1743774414062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 75.83970276008493
INFO:tools.evaluation_results_class:Counted Episodes = 1884
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 290.7907409667969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 291.7907409667969
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.42808532714844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28154.546875
INFO:tools.evaluation_results_class:Current Best Return = 290.7907409667969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.54041983028138
INFO:tools.evaluation_results_class:Counted Episodes = 2239
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 294.33251953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 295.33251953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.7823486328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29004.85546875
INFO:tools.evaluation_results_class:Current Best Return = 294.33251953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.59160781055255
INFO:tools.evaluation_results_class:Counted Episodes = 2407
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 482.9821472167969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 483.9821472167969
INFO:tools.evaluation_results_class:Average Discounted Reward = 405.8255310058594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 54637.34375
INFO:tools.evaluation_results_class:Current Best Return = 482.9821472167969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.29273244011368
INFO:tools.evaluation_results_class:Counted Episodes = 2463
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 631.0110473632812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 632.0110473632812
INFO:tools.evaluation_results_class:Average Discounted Reward = 511.2957763671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 98548.484375
INFO:tools.evaluation_results_class:Current Best Return = 631.0110473632812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 72.2481089258699
INFO:tools.evaluation_results_class:Counted Episodes = 1983
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 243.88766479492188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.88766479492188
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.5116729736328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21355.755859375
INFO:tools.evaluation_results_class:Current Best Return = 243.88766479492188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.98751835535977
INFO:tools.evaluation_results_class:Counted Episodes = 2724
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 238.5897979736328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.5897979736328
INFO:tools.evaluation_results_class:Average Discounted Reward = 203.28085327148438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21836.646484375
INFO:tools.evaluation_results_class:Current Best Return = 238.5897979736328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.918472273228055
INFO:tools.evaluation_results_class:Counted Episodes = 2723
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 238.87173461914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.87173461914062
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.61622619628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20808.69140625
INFO:tools.evaluation_results_class:Current Best Return = 238.87173461914062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.210029069767444
INFO:tools.evaluation_results_class:Counted Episodes = 2752
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 238.3948211669922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 239.3948211669922
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.2061767578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20040.419921875
INFO:tools.evaluation_results_class:Current Best Return = 238.3948211669922
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.67786705624543
INFO:tools.evaluation_results_class:Counted Episodes = 2738
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 239.32339477539062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 240.32339477539062
INFO:tools.evaluation_results_class:Average Discounted Reward = 204.4941864013672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21750.9609375
INFO:tools.evaluation_results_class:Current Best Return = 239.32339477539062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.585303746817026
INFO:tools.evaluation_results_class:Counted Episodes = 2749
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 242.543212890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 243.543212890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.4445037841797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20514.70703125
INFO:tools.evaluation_results_class:Current Best Return = 242.543212890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.94189040088268
INFO:tools.evaluation_results_class:Counted Episodes = 2719
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 245.8717498779297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.8717498779297
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.0129852294922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21196.23828125
INFO:tools.evaluation_results_class:Current Best Return = 245.8717498779297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.446096654275095
INFO:tools.evaluation_results_class:Counted Episodes = 2690
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 246.7587432861328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.7587432861328
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.15139770507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21182.703125
INFO:tools.evaluation_results_class:Current Best Return = 246.7587432861328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.07252912438933
INFO:tools.evaluation_results_class:Counted Episodes = 2661
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 245.95281982421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.95281982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.95639038085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21262.822265625
INFO:tools.evaluation_results_class:Current Best Return = 245.95281982421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.86035192811681
INFO:tools.evaluation_results_class:Counted Episodes = 2671
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 247.8988494873047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.8988494873047
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.6970977783203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22088.3515625
INFO:tools.evaluation_results_class:Current Best Return = 247.8988494873047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.55782818891781
INFO:tools.evaluation_results_class:Counted Episodes = 2689
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 247.15908813476562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.15908813476562
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.4416961669922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22787.83984375
INFO:tools.evaluation_results_class:Current Best Return = 247.15908813476562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.66915052160954
INFO:tools.evaluation_results_class:Counted Episodes = 2684
INFO:robust_rl.robust_rl_trainer:Iteration 12 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 284.0418395996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.0418395996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.92938232421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37793.8984375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.06589595375723
INFO:tools.evaluation_results_class:Counted Episodes = 5190
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 18.53037452697754
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 50.00%
INFO:agents.father_agent:Step: 5, Training loss: 21.414703369140625
INFO:agents.father_agent:Step: 10, Training loss: 20.330976486206055
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 277.1695861816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.1695861816406
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.20074462890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33908.66796875
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.31560217269151
INFO:tools.evaluation_results_class:Counted Episodes = 5339
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.7153015136719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.7153015136719
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.4105682373047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35036.171875
INFO:tools.evaluation_results_class:Current Best Return = 274.7153015136719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.961181594395
INFO:tools.evaluation_results_class:Counted Episodes = 5281
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 294.7192077636719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 295.7192077636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.1934051513672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36829.703125
INFO:tools.evaluation_results_class:Current Best Return = 294.7192077636719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.04639474192925
INFO:tools.evaluation_results_class:Counted Episodes = 5173
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 248.1407563455112
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 13 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 289.8404541015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 290.8404541015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 243.91726684570312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 39860.53515625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.26332412159495
INFO:tools.evaluation_results_class:Counted Episodes = 5066
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 18.78123664855957
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 50.00%
INFO:agents.father_agent:Step: 5, Training loss: 23.674718856811523
INFO:agents.father_agent:Step: 10, Training loss: 19.645973205566406
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 282.4408874511719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.4408874511719
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.0710906982422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35938.515625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.216046966731895
INFO:tools.evaluation_results_class:Counted Episodes = 5110
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.6138000488281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.6138000488281
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.2207794189453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34842.27734375
INFO:tools.evaluation_results_class:Current Best Return = 278.6138000488281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.66640836884928
INFO:tools.evaluation_results_class:Counted Episodes = 5162
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.8916320800781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.8916320800781
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.1360321044922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35060.1171875
INFO:tools.evaluation_results_class:Current Best Return = 281.8916320800781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.90592334494774
INFO:tools.evaluation_results_class:Counted Episodes = 5166
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 250.17581733493202
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 14 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 296.8028564453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 297.8028564453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.3059539794922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 38702.87890625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.84666265784726
INFO:tools.evaluation_results_class:Counted Episodes = 4989
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 21.72132682800293
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 50.00%
INFO:agents.father_agent:Step: 5, Training loss: 19.90843391418457
INFO:agents.father_agent:Step: 10, Training loss: 19.18867301940918
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 266.7732849121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.7732849121094
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.74197387695312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30842.46484375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.59832794983849
INFO:tools.evaluation_results_class:Counted Episodes = 5263
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.61029052734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.61029052734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.31919860839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30839.869140625
INFO:tools.evaluation_results_class:Current Best Return = 270.61029052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.40356658267106
INFO:tools.evaluation_results_class:Counted Episodes = 5159
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.45526123046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.45526123046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.82632446289062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 36375.01171875
INFO:tools.evaluation_results_class:Current Best Return = 280.45526123046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.49636184857424
INFO:tools.evaluation_results_class:Counted Episodes = 5085
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 252.3594382322606
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 15 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.1520690917969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.1520690917969
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.5718994140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30832.94140625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.0501138952164
INFO:tools.evaluation_results_class:Counted Episodes = 5268
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.815254211425781
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 50.00%
INFO:agents.father_agent:Step: 5, Training loss: 20.411983489990234
INFO:agents.father_agent:Step: 10, Training loss: 16.872907638549805
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 276.58319091796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.58319091796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.842041015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28612.00390625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.77170225503892
INFO:tools.evaluation_results_class:Counted Episodes = 5011
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.83721923828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.83721923828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.2362060546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29928.921875
INFO:tools.evaluation_results_class:Current Best Return = 273.83721923828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.57088884470073
INFO:tools.evaluation_results_class:Counted Episodes = 5029
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 288.3309631347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.3309631347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.49371337890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31437.162109375
INFO:tools.evaluation_results_class:Current Best Return = 288.3309631347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.853005791891356
INFO:tools.evaluation_results_class:Counted Episodes = 5007
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 254.237123428878
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 16 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.1803894042969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.1803894042969
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.1035919189453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30145.20703125
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.9771176234444
INFO:tools.evaluation_results_class:Counted Episodes = 4982
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.022679328918457
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 50.00%
INFO:agents.father_agent:Step: 5, Training loss: 17.452390670776367
INFO:agents.father_agent:Step: 10, Training loss: 13.732696533203125
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 282.4702453613281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.4702453613281
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.65513610839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30820.380859375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.25916769674495
INFO:tools.evaluation_results_class:Counted Episodes = 4854
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.5382385253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.5382385253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.47637939453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30114.251953125
INFO:tools.evaluation_results_class:Current Best Return = 278.5382385253906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.246062589486606
INFO:tools.evaluation_results_class:Counted Episodes = 4889
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 278.9998779296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.9998779296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.1516876220703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27981.333984375
INFO:tools.evaluation_results_class:Current Best Return = 278.9998779296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.77892644135189
INFO:tools.evaluation_results_class:Counted Episodes = 5030
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 256.2357453302468
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 685.256103515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 686.256103515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 541.3275146484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 102359.890625
INFO:tools.evaluation_results_class:Current Best Return = 685.256103515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 82.4231884057971
INFO:tools.evaluation_results_class:Counted Episodes = 1725
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 290.17742919921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 291.17742919921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.13287353515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21938.36328125
INFO:tools.evaluation_results_class:Current Best Return = 290.17742919921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.8318246784183
INFO:tools.evaluation_results_class:Counted Episodes = 2099
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 301.0907897949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 302.0907897949219
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.7385711669922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24600.1015625
INFO:tools.evaluation_results_class:Current Best Return = 301.0907897949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.20144209103199
INFO:tools.evaluation_results_class:Counted Episodes = 2219
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 450.5571594238281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 451.5571594238281
INFO:tools.evaluation_results_class:Average Discounted Reward = 369.62847900390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 43695.6875
INFO:tools.evaluation_results_class:Current Best Return = 450.5571594238281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.87326637972262
INFO:tools.evaluation_results_class:Counted Episodes = 2091
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 587.3424682617188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 588.3424682617188
INFO:tools.evaluation_results_class:Average Discounted Reward = 463.81451416015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 75127.453125
INFO:tools.evaluation_results_class:Current Best Return = 587.3424682617188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.06502636203867
INFO:tools.evaluation_results_class:Counted Episodes = 1707
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 248.3552703857422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.3552703857422
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.78956604003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18125.134765625
INFO:tools.evaluation_results_class:Current Best Return = 248.3552703857422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.15151515151515
INFO:tools.evaluation_results_class:Counted Episodes = 2508
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 250.65869140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 251.65869140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.868408203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18699.109375
INFO:tools.evaluation_results_class:Current Best Return = 250.65869140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.9752736116741
INFO:tools.evaluation_results_class:Counted Episodes = 2467
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 251.07981872558594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.07981872558594
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.6049041748047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18989.384765625
INFO:tools.evaluation_results_class:Current Best Return = 251.07981872558594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.813614262560776
INFO:tools.evaluation_results_class:Counted Episodes = 2468
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 252.98370361328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.98370361328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.0972442626953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18247.0546875
INFO:tools.evaluation_results_class:Current Best Return = 252.98370361328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.23258655804481
INFO:tools.evaluation_results_class:Counted Episodes = 2455
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 247.3035125732422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.3035125732422
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.31044006347656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17828.76171875
INFO:tools.evaluation_results_class:Current Best Return = 247.3035125732422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.51269649334946
INFO:tools.evaluation_results_class:Counted Episodes = 2481
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 251.96485900878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.96485900878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.35157775878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18241.8671875
INFO:tools.evaluation_results_class:Current Best Return = 251.96485900878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.69628432956381
INFO:tools.evaluation_results_class:Counted Episodes = 2476
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 245.36392211914062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.36392211914062
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.3860321044922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17741.244140625
INFO:tools.evaluation_results_class:Current Best Return = 245.36392211914062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.77373417721519
INFO:tools.evaluation_results_class:Counted Episodes = 2528
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 243.48989868164062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.48989868164062
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.72108459472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18387.611328125
INFO:tools.evaluation_results_class:Current Best Return = 243.48989868164062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.574316290130795
INFO:tools.evaluation_results_class:Counted Episodes = 2523
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 243.51516723632812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.51516723632812
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.8308563232422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17884.228515625
INFO:tools.evaluation_results_class:Current Best Return = 243.51516723632812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.4399369830642
INFO:tools.evaluation_results_class:Counted Episodes = 2539
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 243.36492919921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.36492919921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.8661346435547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17731.86328125
INFO:tools.evaluation_results_class:Current Best Return = 243.36492919921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.602685624012636
INFO:tools.evaluation_results_class:Counted Episodes = 2532
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 245.25682067871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.25682067871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.52088928222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18474.904296875
INFO:tools.evaluation_results_class:Current Best Return = 245.25682067871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.46661398656657
INFO:tools.evaluation_results_class:Counted Episodes = 2531
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 247.3131103515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.3131103515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.1802520751953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17446.328125
INFO:tools.evaluation_results_class:Current Best Return = 247.3131103515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.157556270096464
INFO:tools.evaluation_results_class:Counted Episodes = 2488
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 249.05661010742188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.05661010742188
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.34194946289062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18271.330078125
INFO:tools.evaluation_results_class:Current Best Return = 249.05661010742188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.297069450020075
INFO:tools.evaluation_results_class:Counted Episodes = 2491
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 245.38568115234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 246.38568115234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 207.3516845703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17509.8125
INFO:tools.evaluation_results_class:Current Best Return = 245.38568115234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.00079522862823
INFO:tools.evaluation_results_class:Counted Episodes = 2515
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 243.58554077148438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 244.58554077148438
INFO:tools.evaluation_results_class:Average Discounted Reward = 205.7449493408203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17225.734375
INFO:tools.evaluation_results_class:Current Best Return = 243.58554077148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.755827736072696
INFO:tools.evaluation_results_class:Counted Episodes = 2531
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 248.13101196289062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.13101196289062
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.4187469482422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17859.517578125
INFO:tools.evaluation_results_class:Current Best Return = 248.13101196289062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.322916666666664
INFO:tools.evaluation_results_class:Counted Episodes = 2496
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 666.2587280273438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 667.2587280273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 529.1420288085938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 105778.71875
INFO:tools.evaluation_results_class:Current Best Return = 666.2587280273438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.04444444444445
INFO:tools.evaluation_results_class:Counted Episodes = 1800
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 291.4076843261719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.4076843261719
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.0918731689453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24395.16796875
INFO:tools.evaluation_results_class:Current Best Return = 291.4076843261719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.50460405156538
INFO:tools.evaluation_results_class:Counted Episodes = 2172
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 299.2012023925781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.2012023925781
INFO:tools.evaluation_results_class:Average Discounted Reward = 248.74095153808594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27418.755859375
INFO:tools.evaluation_results_class:Current Best Return = 299.2012023925781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.737207285342585
INFO:tools.evaluation_results_class:Counted Episodes = 2306
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 451.9344482421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 452.9344482421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 372.2763671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 50087.1171875
INFO:tools.evaluation_results_class:Current Best Return = 451.9344482421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.21144967682363
INFO:tools.evaluation_results_class:Counted Episodes = 2166
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 578.3270874023438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 579.3270874023438
INFO:tools.evaluation_results_class:Average Discounted Reward = 459.66583251953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 83881.3359375
INFO:tools.evaluation_results_class:Current Best Return = 578.3270874023438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 78.9525934188511
INFO:tools.evaluation_results_class:Counted Episodes = 1793
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 252.12680053710938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.12680053710938
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.1901397705078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19422.0078125
INFO:tools.evaluation_results_class:Current Best Return = 252.12680053710938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.95385808452889
INFO:tools.evaluation_results_class:Counted Episodes = 2579
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 255.73098754882812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.7309875488281
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.4771270751953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18314.234375
INFO:tools.evaluation_results_class:Current Best Return = 255.73098754882812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.812202852614895
INFO:tools.evaluation_results_class:Counted Episodes = 2524
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 254.94085693359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.94085693359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.7339324951172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17979.947265625
INFO:tools.evaluation_results_class:Current Best Return = 254.94085693359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.58832807570978
INFO:tools.evaluation_results_class:Counted Episodes = 2536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 256.4501037597656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.4501037597656
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.26455688476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17465.20703125
INFO:tools.evaluation_results_class:Current Best Return = 256.4501037597656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.26027397260274
INFO:tools.evaluation_results_class:Counted Episodes = 2555
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 253.41641235351562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.41641235351562
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.58084106445312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18205.978515625
INFO:tools.evaluation_results_class:Current Best Return = 253.41641235351562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.33908948194662
INFO:tools.evaluation_results_class:Counted Episodes = 2548
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 255.5778350830078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.57781982421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.51756286621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18083.865234375
INFO:tools.evaluation_results_class:Current Best Return = 255.5778350830078
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.465408805031444
INFO:tools.evaluation_results_class:Counted Episodes = 2544
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 248.8614959716797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.8614959716797
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.5448455810547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18261.26171875
INFO:tools.evaluation_results_class:Current Best Return = 248.8614959716797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.20970266040688
INFO:tools.evaluation_results_class:Counted Episodes = 2556
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 252.30128479003906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.30128479003906
INFO:tools.evaluation_results_class:Average Discounted Reward = 213.94998168945312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18856.599609375
INFO:tools.evaluation_results_class:Current Best Return = 252.30128479003906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.77782086079876
INFO:tools.evaluation_results_class:Counted Episodes = 2579
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 249.20970153808594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.20970153808594
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.38844299316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17419.48046875
INFO:tools.evaluation_results_class:Current Best Return = 249.20970153808594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.026796116504855
INFO:tools.evaluation_results_class:Counted Episodes = 2575
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 247.9217987060547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 248.9217987060547
INFO:tools.evaluation_results_class:Average Discounted Reward = 210.49278259277344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18248.30859375
INFO:tools.evaluation_results_class:Current Best Return = 247.9217987060547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.26579352850539
INFO:tools.evaluation_results_class:Counted Episodes = 2596
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 249.50115966796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 250.50115966796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.5938720703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18551.83984375
INFO:tools.evaluation_results_class:Current Best Return = 249.50115966796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.5754060324826
INFO:tools.evaluation_results_class:Counted Episodes = 2586
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 258.39300537109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.39300537109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 218.3936309814453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19317.333984375
INFO:tools.evaluation_results_class:Current Best Return = 258.39300537109375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.429583005507475
INFO:tools.evaluation_results_class:Counted Episodes = 2542
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 256.5829772949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.5829772949219
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.40335083007812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18886.990234375
INFO:tools.evaluation_results_class:Current Best Return = 256.5829772949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.23656335817968
INFO:tools.evaluation_results_class:Counted Episodes = 2549
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 256.5306091308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.5306091308594
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.4344024658203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19106.056640625
INFO:tools.evaluation_results_class:Current Best Return = 256.5306091308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.277864992150704
INFO:tools.evaluation_results_class:Counted Episodes = 2548
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 256.0495300292969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.0495300292969
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.03321838378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18775.962890625
INFO:tools.evaluation_results_class:Current Best Return = 256.0495300292969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.16116352201258
INFO:tools.evaluation_results_class:Counted Episodes = 2544
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 255.22406005859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.22406005859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.06732177734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19039.8671875
INFO:tools.evaluation_results_class:Current Best Return = 255.22406005859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.340408805031444
INFO:tools.evaluation_results_class:Counted Episodes = 2544
INFO:robust_rl.robust_rl_trainer:Iteration 17 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.4849548339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.4849548339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.30357360839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27532.93359375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.83376989723957
INFO:tools.evaluation_results_class:Counted Episodes = 4963
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.657492637634277
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 50.00%
INFO:agents.father_agent:Step: 5, Training loss: 15.67636775970459
INFO:agents.father_agent:Step: 10, Training loss: 15.318154335021973
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 286.77581787109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.77581787109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.14894104003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28200.615234375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.737711864406776
INFO:tools.evaluation_results_class:Counted Episodes = 4720
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.65216064453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.65216064453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.97994995117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26281.9921875
INFO:tools.evaluation_results_class:Current Best Return = 265.65216064453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.47338662507311
INFO:tools.evaluation_results_class:Counted Episodes = 5129
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 260.389404296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.389404296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 219.697509765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25491.54296875
INFO:tools.evaluation_results_class:Current Best Return = 260.389404296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.85019011406844
INFO:tools.evaluation_results_class:Counted Episodes = 5260
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 257.28729160595736
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 18 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.27447509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.27447509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.50543212890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24830.11328125
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.984199091447756
INFO:tools.evaluation_results_class:Counted Episodes = 5063
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.131150245666504
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 50.00%
INFO:agents.father_agent:Step: 5, Training loss: 12.855506896972656
INFO:agents.father_agent:Step: 10, Training loss: 14.477306365966797
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 283.4400634765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.4400634765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.86785888671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27652.994140625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.87683900798655
INFO:tools.evaluation_results_class:Counted Episodes = 4758
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.12774658203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.12774658203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.9257049560547
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24964.986328125
INFO:tools.evaluation_results_class:Current Best Return = 273.12774658203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.57963606624412
INFO:tools.evaluation_results_class:Counted Episodes = 4891
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.2835388183594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.2835388183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.88246154785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26165.919921875
INFO:tools.evaluation_results_class:Current Best Return = 271.2835388183594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.12090781281382
INFO:tools.evaluation_results_class:Counted Episodes = 4979
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 258.0233291340976
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 19 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.5226745605469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.5226745605469
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.1073760986328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27298.2265625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.21864021492044
INFO:tools.evaluation_results_class:Counted Episodes = 4839
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.226788520812988
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 50.00%
INFO:agents.father_agent:Step: 5, Training loss: 11.778100967407227
INFO:agents.father_agent:Step: 10, Training loss: 13.97136402130127
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 271.8050537109375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.8050537109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.16436767578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24826.87109375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.88629971876256
INFO:tools.evaluation_results_class:Counted Episodes = 4978
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 260.0408935546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.0408935546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.3888397216797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23305.96484375
INFO:tools.evaluation_results_class:Current Best Return = 260.0408935546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.66585711394455
INFO:tools.evaluation_results_class:Counted Episodes = 4941
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.6142272949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.6142272949219
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.74835205078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25476.94921875
INFO:tools.evaluation_results_class:Current Best Return = 274.6142272949219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.76165803108808
INFO:tools.evaluation_results_class:Counted Episodes = 5018
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 258.9006600894576
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 20 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.9100646972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.9100646972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.8800048828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25570.333984375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.48204276520656
INFO:tools.evaluation_results_class:Counted Episodes = 4817
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.243081092834473
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 50.00%
INFO:agents.father_agent:Step: 5, Training loss: 11.341833114624023
INFO:agents.father_agent:Step: 10, Training loss: 11.786175727844238
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 284.89105224609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.89105224609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.51947021484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28076.712890625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.94857380803664
INFO:tools.evaluation_results_class:Counted Episodes = 4803
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 258.6494445800781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.6494445800781
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.8877716064453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20950.947265625
INFO:tools.evaluation_results_class:Current Best Return = 258.6494445800781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.429875024796665
INFO:tools.evaluation_results_class:Counted Episodes = 5041
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.71063232421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.71063232421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.27352905273438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25406.923828125
INFO:tools.evaluation_results_class:Current Best Return = 272.71063232421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.20520402128918
INFO:tools.evaluation_results_class:Counted Episodes = 5073
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 260.5939689711292
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 21 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 295.90679931640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 296.90679931640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 247.211669921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23483.84375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.774871355060036
INFO:tools.evaluation_results_class:Counted Episodes = 4664
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.119630813598633
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 48.44%
INFO:agents.father_agent:Step: 5, Training loss: 10.951600074768066
INFO:agents.father_agent:Step: 10, Training loss: 11.46639347076416
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 274.08355712890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.08355712890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.29669189453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24039.818359375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.62188254223653
INFO:tools.evaluation_results_class:Counted Episodes = 4972
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.6047058105469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.6047058105469
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.8715362548828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22700.84375
INFO:tools.evaluation_results_class:Current Best Return = 276.6047058105469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.19055374592834
INFO:tools.evaluation_results_class:Counted Episodes = 4912
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.65887451171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.65887451171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.6616973876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25753.478515625
INFO:tools.evaluation_results_class:Current Best Return = 281.65887451171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.01961176706023
INFO:tools.evaluation_results_class:Counted Episodes = 4997
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 263.4293649672373
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 647.6957397460938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 648.6957397460938
INFO:tools.evaluation_results_class:Average Discounted Reward = 510.352783203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 90681.609375
INFO:tools.evaluation_results_class:Current Best Return = 647.6957397460938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.10385064177363
INFO:tools.evaluation_results_class:Counted Episodes = 1714
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 293.55645751953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 294.55645751953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.12464904785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21197.54296875
INFO:tools.evaluation_results_class:Current Best Return = 293.55645751953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.3691433980123
INFO:tools.evaluation_results_class:Counted Episodes = 2113
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 302.8443603515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 303.8443603515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 251.23716735839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23240.115234375
INFO:tools.evaluation_results_class:Current Best Return = 302.8443603515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.6350626118068
INFO:tools.evaluation_results_class:Counted Episodes = 2236
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 431.2243957519531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 432.2243957519531
INFO:tools.evaluation_results_class:Average Discounted Reward = 351.7109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 42271.234375
INFO:tools.evaluation_results_class:Current Best Return = 431.2243957519531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 69.95024390243903
INFO:tools.evaluation_results_class:Counted Episodes = 2050
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 556.5042724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 557.5042724609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 438.5618896484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 63876.08984375
INFO:tools.evaluation_results_class:Current Best Return = 556.5042724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.25148986889154
INFO:tools.evaluation_results_class:Counted Episodes = 1678
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 248.4886474609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 249.4886474609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 209.7078399658203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15747.9248046875
INFO:tools.evaluation_results_class:Current Best Return = 248.4886474609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.18478693747511
INFO:tools.evaluation_results_class:Counted Episodes = 2511
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 256.32513427734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.32513427734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.16468811035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16720.21875
INFO:tools.evaluation_results_class:Current Best Return = 256.32513427734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.09560306575232
INFO:tools.evaluation_results_class:Counted Episodes = 2479
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 253.90243530273438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.90243530273438
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.0325164794922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15746.599609375
INFO:tools.evaluation_results_class:Current Best Return = 253.90243530273438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.25609756097561
INFO:tools.evaluation_results_class:Counted Episodes = 2460
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 255.97813415527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.9781188964844
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.33985900878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16919.97265625
INFO:tools.evaluation_results_class:Current Best Return = 255.97813415527344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.21911705143783
INFO:tools.evaluation_results_class:Counted Episodes = 2469
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 257.6330871582031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.6330871582031
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.9629669189453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17573.603515625
INFO:tools.evaluation_results_class:Current Best Return = 257.6330871582031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.24258431531898
INFO:tools.evaluation_results_class:Counted Episodes = 2461
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 259.9836730957031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.9836730957031
INFO:tools.evaluation_results_class:Average Discounted Reward = 218.66464233398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17394.5703125
INFO:tools.evaluation_results_class:Current Best Return = 259.9836730957031
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.69714285714286
INFO:tools.evaluation_results_class:Counted Episodes = 2450
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 256.28729248046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.28729248046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.5576171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17302.1015625
INFO:tools.evaluation_results_class:Current Best Return = 256.28729248046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.422149570903144
INFO:tools.evaluation_results_class:Counted Episodes = 2447
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 257.77093505859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.77093505859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.5081329345703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18050.140625
INFO:tools.evaluation_results_class:Current Best Return = 257.77093505859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.72085385878489
INFO:tools.evaluation_results_class:Counted Episodes = 2436
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 258.2341003417969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.2341003417969
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.0735626220703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17555.896484375
INFO:tools.evaluation_results_class:Current Best Return = 258.2341003417969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.760985626283365
INFO:tools.evaluation_results_class:Counted Episodes = 2435
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 258.22222900390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 259.22222900390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.9379119873047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17581.83984375
INFO:tools.evaluation_results_class:Current Best Return = 258.22222900390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.949794238683126
INFO:tools.evaluation_results_class:Counted Episodes = 2430
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 255.90234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.90234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.43992614746094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18378.357421875
INFO:tools.evaluation_results_class:Current Best Return = 255.90234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.73629995879687
INFO:tools.evaluation_results_class:Counted Episodes = 2427
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 251.1011962890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.1011962890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 211.92913818359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17124.888671875
INFO:tools.evaluation_results_class:Current Best Return = 251.1011962890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.85952380952381
INFO:tools.evaluation_results_class:Counted Episodes = 2520
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 252.0994873046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.0994873046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.9220428466797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17086.9140625
INFO:tools.evaluation_results_class:Current Best Return = 252.0994873046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.110267678785455
INFO:tools.evaluation_results_class:Counted Episodes = 2503
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 246.91485595703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 247.91485595703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 208.7265167236328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16707.15625
INFO:tools.evaluation_results_class:Current Best Return = 246.91485595703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.46059405940594
INFO:tools.evaluation_results_class:Counted Episodes = 2525
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 251.66799926757812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 252.66799926757812
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.31690979003906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17464.740234375
INFO:tools.evaluation_results_class:Current Best Return = 251.66799926757812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.29591018444266
INFO:tools.evaluation_results_class:Counted Episodes = 2494
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 253.56082153320312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.56082153320312
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.0389404296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16613.154296875
INFO:tools.evaluation_results_class:Current Best Return = 253.56082153320312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.576475311120035
INFO:tools.evaluation_results_class:Counted Episodes = 2491
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 252.56509399414062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 253.56509399414062
INFO:tools.evaluation_results_class:Average Discounted Reward = 212.998046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16178.0517578125
INFO:tools.evaluation_results_class:Current Best Return = 252.56509399414062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.05126118795769
INFO:tools.evaluation_results_class:Counted Episodes = 2458
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 254.1179962158203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.1179962158203
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.1574249267578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17011.142578125
INFO:tools.evaluation_results_class:Current Best Return = 254.1179962158203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.723318566250505
INFO:tools.evaluation_results_class:Counted Episodes = 2483
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 254.1322784423828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.1322784423828
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.18307495117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17333.806640625
INFO:tools.evaluation_results_class:Current Best Return = 254.1322784423828
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.846278317152105
INFO:tools.evaluation_results_class:Counted Episodes = 2472
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 255.7390899658203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.7391052246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.4561767578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16895.900390625
INFO:tools.evaluation_results_class:Current Best Return = 255.7390899658203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.849757673667206
INFO:tools.evaluation_results_class:Counted Episodes = 2476
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 254.01698303222656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.01698303222656
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.28802490234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16373.244140625
INFO:tools.evaluation_results_class:Current Best Return = 254.01698303222656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.7168284789644
INFO:tools.evaluation_results_class:Counted Episodes = 2472
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 660.2733764648438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 661.2733764648438
INFO:tools.evaluation_results_class:Average Discounted Reward = 520.6956176757812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 96771.3359375
INFO:tools.evaluation_results_class:Current Best Return = 660.2733764648438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 82.34126523505513
INFO:tools.evaluation_results_class:Counted Episodes = 1723
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 304.69189453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 305.69189453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 249.92079162597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24401.11328125
INFO:tools.evaluation_results_class:Current Best Return = 304.69189453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 67.82773308092759
INFO:tools.evaluation_results_class:Counted Episodes = 2113
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 312.951904296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 313.951904296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 259.7227478027344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24361.701171875
INFO:tools.evaluation_results_class:Current Best Return = 312.951904296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.96846846846847
INFO:tools.evaluation_results_class:Counted Episodes = 2220
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 448.4651794433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 449.4651794433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 365.2345886230469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 43178.25390625
INFO:tools.evaluation_results_class:Current Best Return = 448.4651794433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 71.54228855721394
INFO:tools.evaluation_results_class:Counted Episodes = 2010
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 568.6832885742188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 569.6832885742188
INFO:tools.evaluation_results_class:Average Discounted Reward = 447.4195861816406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 68634.34375
INFO:tools.evaluation_results_class:Current Best Return = 568.6832885742188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.37764705882353
INFO:tools.evaluation_results_class:Counted Episodes = 1700
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.32135009765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.32135009765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.317138671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16609.5703125
INFO:tools.evaluation_results_class:Current Best Return = 262.32135009765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.882519176423095
INFO:tools.evaluation_results_class:Counted Episodes = 2477
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 256.4833984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.4833984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.30587768554688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18631.146484375
INFO:tools.evaluation_results_class:Current Best Return = 256.4833984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.78481012658228
INFO:tools.evaluation_results_class:Counted Episodes = 2528
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 259.0762023925781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.0762023925781
INFO:tools.evaluation_results_class:Average Discounted Reward = 218.7222137451172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18393.4296875
INFO:tools.evaluation_results_class:Current Best Return = 259.0762023925781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.22027134876297
INFO:tools.evaluation_results_class:Counted Episodes = 2506
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 256.0924072265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.0924072265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.5729217529297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17984.7109375
INFO:tools.evaluation_results_class:Current Best Return = 256.0924072265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.251370399373535
INFO:tools.evaluation_results_class:Counted Episodes = 2554
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 254.4588165283203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 255.4588165283203
INFO:tools.evaluation_results_class:Average Discounted Reward = 215.054931640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17653.294921875
INFO:tools.evaluation_results_class:Current Best Return = 254.4588165283203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.25019607843137
INFO:tools.evaluation_results_class:Counted Episodes = 2550
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 257.96209716796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.96209716796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.7833251953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17968.830078125
INFO:tools.evaluation_results_class:Current Best Return = 257.96209716796875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.61216429699842
INFO:tools.evaluation_results_class:Counted Episodes = 2532
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 261.5956726074219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.5956726074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.777587890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17988.87109375
INFO:tools.evaluation_results_class:Current Best Return = 261.5956726074219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.47588424437299
INFO:tools.evaluation_results_class:Counted Episodes = 2488
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 257.84381103515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 258.84381103515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 217.7910919189453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17636.2578125
INFO:tools.evaluation_results_class:Current Best Return = 257.84381103515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.926073131955484
INFO:tools.evaluation_results_class:Counted Episodes = 2516
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 260.7162780761719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.7162780761719
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.5701904296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17425.16796875
INFO:tools.evaluation_results_class:Current Best Return = 260.7162780761719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.65017807677088
INFO:tools.evaluation_results_class:Counted Episodes = 2527
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 261.2206726074219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.2206726074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.87242126464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17769.939453125
INFO:tools.evaluation_results_class:Current Best Return = 261.2206726074219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.1248508946322
INFO:tools.evaluation_results_class:Counted Episodes = 2515
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 261.73626708984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.73626708984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.97166442871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17532.9921875
INFO:tools.evaluation_results_class:Current Best Return = 261.73626708984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.253778838504374
INFO:tools.evaluation_results_class:Counted Episodes = 2514
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 253.9226531982422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.9226531982422
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.87252807617188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16551.876953125
INFO:tools.evaluation_results_class:Current Best Return = 253.9226531982422
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.43488555643252
INFO:tools.evaluation_results_class:Counted Episodes = 2534
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 253.85069274902344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.85069274902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.87306213378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16774.30859375
INFO:tools.evaluation_results_class:Current Best Return = 253.85069274902344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.322593320235754
INFO:tools.evaluation_results_class:Counted Episodes = 2545
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 255.2598419189453
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.2598571777344
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.1123809814453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16823.556640625
INFO:tools.evaluation_results_class:Current Best Return = 255.2598419189453
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.28031496062992
INFO:tools.evaluation_results_class:Counted Episodes = 2540
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 253.66575622558594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 254.66575622558594
INFO:tools.evaluation_results_class:Average Discounted Reward = 214.54217529296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17797.994140625
INFO:tools.evaluation_results_class:Current Best Return = 253.66575622558594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.19827586206897
INFO:tools.evaluation_results_class:Counted Episodes = 2552
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 255.70977783203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 256.70977783203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 216.47743225097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15969.8828125
INFO:tools.evaluation_results_class:Current Best Return = 255.70977783203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.53943217665615
INFO:tools.evaluation_results_class:Counted Episodes = 2536
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.3101501464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.3101501464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.21499633789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17703.662109375
INFO:tools.evaluation_results_class:Current Best Return = 262.3101501464844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.77219766974689
INFO:tools.evaluation_results_class:Counted Episodes = 2489
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 259.9441223144531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 260.9441223144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 219.4527130126953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17173.599609375
INFO:tools.evaluation_results_class:Current Best Return = 259.9441223144531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.20989624900239
INFO:tools.evaluation_results_class:Counted Episodes = 2506
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 260.5644226074219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 261.5644226074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.04598999023438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17504.318359375
INFO:tools.evaluation_results_class:Current Best Return = 260.5644226074219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.12764260071799
INFO:tools.evaluation_results_class:Counted Episodes = 2507
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.3969421386719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.3969421386719
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.07421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18060.60546875
INFO:tools.evaluation_results_class:Current Best Return = 262.3969421386719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.96847210994341
INFO:tools.evaluation_results_class:Counted Episodes = 2474
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 261.4029541015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 262.4029541015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 220.76953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17699.38671875
INFO:tools.evaluation_results_class:Current Best Return = 261.4029541015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.144280589876445
INFO:tools.evaluation_results_class:Counted Episodes = 2509
INFO:robust_rl.robust_rl_trainer:Iteration 22 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.7914733886719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.7914733886719
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.10240173339844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22465.1328125
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.131936360108654
INFO:tools.evaluation_results_class:Counted Episodes = 5154
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.528258323669434
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 46.88%
INFO:agents.father_agent:Step: 5, Training loss: 13.441315650939941
INFO:agents.father_agent:Step: 10, Training loss: 11.729084014892578
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 296.4493408203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 297.4493408203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 246.67861938476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24204.060546875
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 62.897463689572945
INFO:tools.evaluation_results_class:Counted Episodes = 4613
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.1348876953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.1348876953125
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.81809997558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22374.716796875
INFO:tools.evaluation_results_class:Current Best Return = 272.1348876953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.82536546819439
INFO:tools.evaluation_results_class:Counted Episodes = 5062
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.5690612792969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.5690612792969
INFO:tools.evaluation_results_class:Average Discounted Reward = 233.00173950195312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24453.123046875
INFO:tools.evaluation_results_class:Current Best Return = 277.5690612792969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.70259481037924
INFO:tools.evaluation_results_class:Counted Episodes = 5010
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 267.16917051329403
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 23 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.955810546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.955810546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.83316040039062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24482.431640625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.95721711315474
INFO:tools.evaluation_results_class:Counted Episodes = 5002
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 13.015473365783691
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 46.88%
INFO:agents.father_agent:Step: 5, Training loss: 10.690011024475098
INFO:agents.father_agent:Step: 10, Training loss: 11.781841278076172
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 280.25445556640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.25445556640625
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.7167510986328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21692.666015625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.52348178137652
INFO:tools.evaluation_results_class:Counted Episodes = 4940
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.3173828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.3173828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.97311401367188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22470.125
INFO:tools.evaluation_results_class:Current Best Return = 274.3173828125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.28097868981847
INFO:tools.evaluation_results_class:Counted Episodes = 5068
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 280.444580078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 281.444580078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 236.5458526611328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22453.111328125
INFO:tools.evaluation_results_class:Current Best Return = 280.444580078125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.94610314075223
INFO:tools.evaluation_results_class:Counted Episodes = 5158
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 270.8931342118914
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 24 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 276.3887939453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 277.3887939453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.64781188964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23594.796875
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.61717030218131
INFO:tools.evaluation_results_class:Counted Episodes = 4997
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.555538177490234
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 46.88%
INFO:agents.father_agent:Step: 5, Training loss: 13.76988410949707
INFO:agents.father_agent:Step: 10, Training loss: 12.131355285644531
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 284.65740966796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 285.65740966796875
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.34632873535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21945.59375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.013659531090724
INFO:tools.evaluation_results_class:Counted Episodes = 4905
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.217041015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.217041015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.19198608398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23095.6015625
INFO:tools.evaluation_results_class:Current Best Return = 283.217041015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.99198396793587
INFO:tools.evaluation_results_class:Counted Episodes = 4990
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.4789123535156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.4789123535156
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.21884155273438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21450.599609375
INFO:tools.evaluation_results_class:Current Best Return = 281.4789123535156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.50438169425511
INFO:tools.evaluation_results_class:Counted Episodes = 5135
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=4/5, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 273.14874013313266
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=4/5, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 25 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.00244140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.00244140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.77232360839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19097.408203125
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.895241903238706
INFO:tools.evaluation_results_class:Counted Episodes = 5002
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 11.668123245239258
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 48.44%
INFO:agents.father_agent:Step: 5, Training loss: 14.38276481628418
INFO:agents.father_agent:Step: 10, Training loss: 11.51445484161377
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 278.0843505859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 279.0843505859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.03427124023438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21848.4453125
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.70332628340704
INFO:tools.evaluation_results_class:Counted Episodes = 5201
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 279.2396240234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 280.2396240234375
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.34442138671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21015.27734375
INFO:tools.evaluation_results_class:Current Best Return = 279.2396240234375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.67097158752235
INFO:tools.evaluation_results_class:Counted Episodes = 5033
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.4168701171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.4168701171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.27099609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21757.724609375
INFO:tools.evaluation_results_class:Current Best Return = 283.4168701171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.90989263803681
INFO:tools.evaluation_results_class:Counted Episodes = 5216
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=4/5, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 274.0218242957384
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=4/5, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 26 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.80242919921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.80242919921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.55641174316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19625.4375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.77721221613156
INFO:tools.evaluation_results_class:Counted Episodes = 5108
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.849751472473145
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 48.44%
INFO:agents.father_agent:Step: 5, Training loss: 14.291378021240234
INFO:agents.father_agent:Step: 10, Training loss: 15.594803810119629
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 286.9547424316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.9547424316406
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.21495056152344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21166.638671875
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.85237999598313
INFO:tools.evaluation_results_class:Counted Episodes = 4979
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 275.9049987792969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 276.9049987792969
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.99951171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20492.58984375
INFO:tools.evaluation_results_class:Current Best Return = 275.9049987792969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.72709513576871
INFO:tools.evaluation_results_class:Counted Episodes = 5119
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 286.250732421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 287.250732421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.3318328857422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21780.408203125
INFO:tools.evaluation_results_class:Current Best Return = 286.250732421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.97651470298007
INFO:tools.evaluation_results_class:Counted Episodes = 5067
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 275.03977773396036
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 626.8092041015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 627.8092041015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 496.4247741699219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 83570.125
INFO:tools.evaluation_results_class:Current Best Return = 626.8092041015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.31246442800227
INFO:tools.evaluation_results_class:Counted Episodes = 1757
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 310.9064025878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 311.9064025878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 256.43768310546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22371.01171875
INFO:tools.evaluation_results_class:Current Best Return = 310.9064025878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.72685185185185
INFO:tools.evaluation_results_class:Counted Episodes = 2160
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 320.7557067871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 321.7557067871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 266.2584228515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23877.908203125
INFO:tools.evaluation_results_class:Current Best Return = 320.7557067871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 64.24723083739477
INFO:tools.evaluation_results_class:Counted Episodes = 2257
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 428.3473205566406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 429.3473205566406
INFO:tools.evaluation_results_class:Average Discounted Reward = 348.9597473144531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40593.50390625
INFO:tools.evaluation_results_class:Current Best Return = 428.3473205566406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 71.23715415019763
INFO:tools.evaluation_results_class:Counted Episodes = 2024
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 544.5453491210938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 545.5453491210938
INFO:tools.evaluation_results_class:Average Discounted Reward = 427.8546447753906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 64456.74609375
INFO:tools.evaluation_results_class:Current Best Return = 544.5453491210938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.87661574618096
INFO:tools.evaluation_results_class:Counted Episodes = 1702
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.4988098144531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.4988098144531
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.2888641357422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17445.455078125
INFO:tools.evaluation_results_class:Current Best Return = 270.4988098144531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.25160256410256
INFO:tools.evaluation_results_class:Counted Episodes = 2496
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 266.2427062988281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.2427062988281
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.18283081054688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18013.3203125
INFO:tools.evaluation_results_class:Current Best Return = 266.2427062988281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.03077522399688
INFO:tools.evaluation_results_class:Counted Episodes = 2567
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.67724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.67724609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.44381713867188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16276.9658203125
INFO:tools.evaluation_results_class:Current Best Return = 262.67724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.581170089112746
INFO:tools.evaluation_results_class:Counted Episodes = 2581
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 264.63214111328125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.63214111328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.19512939453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16292.61328125
INFO:tools.evaluation_results_class:Current Best Return = 264.63214111328125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.526964560862865
INFO:tools.evaluation_results_class:Counted Episodes = 2596
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.1761779785156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.1761779785156
INFO:tools.evaluation_results_class:Average Discounted Reward = 221.948974609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17170.890625
INFO:tools.evaluation_results_class:Current Best Return = 262.1761779785156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.44255975327679
INFO:tools.evaluation_results_class:Counted Episodes = 2594
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 264.4576721191406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 265.4576721191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.55360412597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17707.939453125
INFO:tools.evaluation_results_class:Current Best Return = 264.4576721191406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.85914943425673
INFO:tools.evaluation_results_class:Counted Episodes = 2563
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.3863525390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.3863525390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.5402374267578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17203.302734375
INFO:tools.evaluation_results_class:Current Best Return = 271.3863525390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.93097976993257
INFO:tools.evaluation_results_class:Counted Episodes = 2521
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.0250244140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.0250244140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.09812927246094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17333.87109375
INFO:tools.evaluation_results_class:Current Best Return = 271.0250244140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.728247914183555
INFO:tools.evaluation_results_class:Counted Episodes = 2517
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.4781799316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.4781799316406
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.93101501464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17041.13671875
INFO:tools.evaluation_results_class:Current Best Return = 273.4781799316406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.40608730476572
INFO:tools.evaluation_results_class:Counted Episodes = 2497
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.87603759765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.87603759765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.1970672607422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16359.7958984375
INFO:tools.evaluation_results_class:Current Best Return = 268.87603759765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.28537265842965
INFO:tools.evaluation_results_class:Counted Episodes = 2509
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.1313171386719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.1313171386719
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.08108520507812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16839.078125
INFO:tools.evaluation_results_class:Current Best Return = 270.1313171386719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.11539992041385
INFO:tools.evaluation_results_class:Counted Episodes = 2513
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.04827880859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.04827880859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.09153747558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17179.62890625
INFO:tools.evaluation_results_class:Current Best Return = 269.04827880859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.99840383080607
INFO:tools.evaluation_results_class:Counted Episodes = 2506
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.3846130371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.3846130371094
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.19952392578125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16709.345703125
INFO:tools.evaluation_results_class:Current Best Return = 270.3846130371094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.28285256410256
INFO:tools.evaluation_results_class:Counted Episodes = 2496
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.182373046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.182373046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.88482666015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17537.787109375
INFO:tools.evaluation_results_class:Current Best Return = 269.182373046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.34068136272545
INFO:tools.evaluation_results_class:Counted Episodes = 2495
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.8897399902344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.8897399902344
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.7136688232422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17583.333984375
INFO:tools.evaluation_results_class:Current Best Return = 270.8897399902344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.96257961783439
INFO:tools.evaluation_results_class:Counted Episodes = 2512
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.94097900390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.94097900390625
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.04774475097656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17744.798828125
INFO:tools.evaluation_results_class:Current Best Return = 268.94097900390625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.89309932189868
INFO:tools.evaluation_results_class:Counted Episodes = 2507
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 263.5512390136719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.5512390136719
INFO:tools.evaluation_results_class:Average Discounted Reward = 223.028076171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16537.125
INFO:tools.evaluation_results_class:Current Best Return = 263.5512390136719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.93443266588143
INFO:tools.evaluation_results_class:Counted Episodes = 2547
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.383544921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.383544921875
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.02635192871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16490.84375
INFO:tools.evaluation_results_class:Current Best Return = 262.383544921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.713509316770185
INFO:tools.evaluation_results_class:Counted Episodes = 2576
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 266.1793518066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.1793518066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.99301147460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17629.09375
INFO:tools.evaluation_results_class:Current Best Return = 266.1793518066406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.25196232339089
INFO:tools.evaluation_results_class:Counted Episodes = 2548
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 262.6580810546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 263.6580810546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 222.2837371826172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16948.96484375
INFO:tools.evaluation_results_class:Current Best Return = 262.6580810546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.66354410616706
INFO:tools.evaluation_results_class:Counted Episodes = 2562
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 265.7875061035156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 266.7875061035156
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.62774658203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17366.90625
INFO:tools.evaluation_results_class:Current Best Return = 265.7875061035156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.166535742340926
INFO:tools.evaluation_results_class:Counted Episodes = 2546
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.7909851074219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.7909851074219
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.4978485107422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16336.03515625
INFO:tools.evaluation_results_class:Current Best Return = 267.7909851074219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.78939034045922
INFO:tools.evaluation_results_class:Counted Episodes = 2526
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.0898132324219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.0898132324219
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.9283905029297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17148.046875
INFO:tools.evaluation_results_class:Current Best Return = 270.0898132324219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.09500998003992
INFO:tools.evaluation_results_class:Counted Episodes = 2505
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.667724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.667724609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.2412567138672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16340.8212890625
INFO:tools.evaluation_results_class:Current Best Return = 267.667724609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.84780023781213
INFO:tools.evaluation_results_class:Counted Episodes = 2523
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 266.7966003417969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.7966003417969
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.5918731689453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16757.064453125
INFO:tools.evaluation_results_class:Current Best Return = 266.7966003417969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.48815165876777
INFO:tools.evaluation_results_class:Counted Episodes = 2532
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.388916015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.388916015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.13597106933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17021.138671875
INFO:tools.evaluation_results_class:Current Best Return = 270.388916015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.10849621061029
INFO:tools.evaluation_results_class:Counted Episodes = 2507
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 613.5233154296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 614.5233154296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 486.980712890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 86364.0703125
INFO:tools.evaluation_results_class:Current Best Return = 613.5233154296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.38830083565459
INFO:tools.evaluation_results_class:Counted Episodes = 1795
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 312.66424560546875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 313.66424560546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 257.9193115234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22682.9765625
INFO:tools.evaluation_results_class:Current Best Return = 312.66424560546875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 66.50114942528735
INFO:tools.evaluation_results_class:Counted Episodes = 2175
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 324.2277526855469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 325.2277526855469
INFO:tools.evaluation_results_class:Average Discounted Reward = 269.49658203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24373.19921875
INFO:tools.evaluation_results_class:Current Best Return = 324.2277526855469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.477152899824254
INFO:tools.evaluation_results_class:Counted Episodes = 2276
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 431.45526123046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 432.45526123046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 351.2476806640625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 41370.47265625
INFO:tools.evaluation_results_class:Current Best Return = 431.45526123046875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 70.67453294001966
INFO:tools.evaluation_results_class:Counted Episodes = 2034
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 533.5145874023438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 534.5145874023438
INFO:tools.evaluation_results_class:Average Discounted Reward = 422.37725830078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 60565.953125
INFO:tools.evaluation_results_class:Current Best Return = 533.5145874023438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.3398392652124
INFO:tools.evaluation_results_class:Counted Episodes = 1742
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.24322509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.24322509765625
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.170166015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17804.9453125
INFO:tools.evaluation_results_class:Current Best Return = 272.24322509765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.50177095631641
INFO:tools.evaluation_results_class:Counted Episodes = 2541
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.1700134277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.1700134277344
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.93528747558594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18047.064453125
INFO:tools.evaluation_results_class:Current Best Return = 273.1700134277344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.41991341991342
INFO:tools.evaluation_results_class:Counted Episodes = 2541
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.3663635253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.3663635253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.11488342285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17532.423828125
INFO:tools.evaluation_results_class:Current Best Return = 273.3663635253906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.69627279936558
INFO:tools.evaluation_results_class:Counted Episodes = 2522
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.1571350097656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.1571350097656
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.8633575439453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18064.6171875
INFO:tools.evaluation_results_class:Current Best Return = 274.1571350097656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.54283458349783
INFO:tools.evaluation_results_class:Counted Episodes = 2533
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.1198425292969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.1198425292969
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.5085906982422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17030.140625
INFO:tools.evaluation_results_class:Current Best Return = 271.1198425292969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.34145383104126
INFO:tools.evaluation_results_class:Counted Episodes = 2545
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.7218017578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.7218017578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.35304260253906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18047.138671875
INFO:tools.evaluation_results_class:Current Best Return = 273.7218017578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.75781559161061
INFO:tools.evaluation_results_class:Counted Episodes = 2527
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.3163757324219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.3163757324219
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.9229278564453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17704.5859375
INFO:tools.evaluation_results_class:Current Best Return = 268.3163757324219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.12710207274149
INFO:tools.evaluation_results_class:Counted Episodes = 2557
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.679443359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.679443359375
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.950439453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17997.8828125
INFO:tools.evaluation_results_class:Current Best Return = 270.679443359375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.981029810298104
INFO:tools.evaluation_results_class:Counted Episodes = 2583
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.15533447265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.15533447265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.5580596923828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17482.265625
INFO:tools.evaluation_results_class:Current Best Return = 272.15533447265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.001941747572815
INFO:tools.evaluation_results_class:Counted Episodes = 2575
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.58270263671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.58270263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.2745819091797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17438.54296875
INFO:tools.evaluation_results_class:Current Best Return = 269.58270263671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.77279752704791
INFO:tools.evaluation_results_class:Counted Episodes = 2588
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 267.3570251464844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 268.3570251464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 226.4284210205078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17053.0234375
INFO:tools.evaluation_results_class:Current Best Return = 267.3570251464844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.56491499227202
INFO:tools.evaluation_results_class:Counted Episodes = 2588
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.37115478515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.37115478515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.75494384765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17196.87890625
INFO:tools.evaluation_results_class:Current Best Return = 271.37115478515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.383766745468876
INFO:tools.evaluation_results_class:Counted Episodes = 2538
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.15283203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.15283203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.00839233398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16855.353515625
INFO:tools.evaluation_results_class:Current Best Return = 274.15283203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.56003159557662
INFO:tools.evaluation_results_class:Counted Episodes = 2532
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.9787292480469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.9787292480469
INFO:tools.evaluation_results_class:Average Discounted Reward = 230.90818786621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16925.169921875
INFO:tools.evaluation_results_class:Current Best Return = 272.9787292480469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.44444444444444
INFO:tools.evaluation_results_class:Counted Episodes = 2538
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.3364562988281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.3364562988281
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.84417724609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17117.556640625
INFO:tools.evaluation_results_class:Current Best Return = 271.3364562988281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.191090269636575
INFO:tools.evaluation_results_class:Counted Episodes = 2559
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.9275817871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.9275817871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.88563537597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17699.69140625
INFO:tools.evaluation_results_class:Current Best Return = 273.9275817871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.43170254403131
INFO:tools.evaluation_results_class:Counted Episodes = 2555
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 263.9965515136719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 264.9965515136719
INFO:tools.evaluation_results_class:Average Discounted Reward = 224.35049438476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16032.90234375
INFO:tools.evaluation_results_class:Current Best Return = 263.9965515136719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.03921568627451
INFO:tools.evaluation_results_class:Counted Episodes = 2601
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 268.559814453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 269.559814453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.5537109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17411.86328125
INFO:tools.evaluation_results_class:Current Best Return = 268.559814453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.57065427797135
INFO:tools.evaluation_results_class:Counted Episodes = 2583
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.1152648925781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.1152648925781
INFO:tools.evaluation_results_class:Average Discounted Reward = 227.92466735839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16393.220703125
INFO:tools.evaluation_results_class:Current Best Return = 269.1152648925781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.88863019014358
INFO:tools.evaluation_results_class:Counted Episodes = 2577
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.3981628417969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.3981628417969
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.36386108398438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17319.08203125
INFO:tools.evaluation_results_class:Current Best Return = 269.3981628417969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.61496913580247
INFO:tools.evaluation_results_class:Counted Episodes = 2592
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 266.22119140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 267.22119140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 225.99313354492188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17233.994140625
INFO:tools.evaluation_results_class:Current Best Return = 266.22119140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.205069124423964
INFO:tools.evaluation_results_class:Counted Episodes = 2604
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.3550720214844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.3550720214844
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.9684295654297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16967.72265625
INFO:tools.evaluation_results_class:Current Best Return = 271.3550720214844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.247446975648074
INFO:tools.evaluation_results_class:Counted Episodes = 2546
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 269.4815368652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 270.4815368652344
INFO:tools.evaluation_results_class:Average Discounted Reward = 228.1644744873047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16513.8125
INFO:tools.evaluation_results_class:Current Best Return = 269.4815368652344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.26944226237235
INFO:tools.evaluation_results_class:Counted Episodes = 2546
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 270.5629577636719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 271.5629577636719
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.90487670898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16186.5703125
INFO:tools.evaluation_results_class:Current Best Return = 270.5629577636719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.59293752425301
INFO:tools.evaluation_results_class:Counted Episodes = 2577
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 271.1640930175781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 272.1640930175781
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.8124542236328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17130.80078125
INFO:tools.evaluation_results_class:Current Best Return = 271.1640930175781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.155503329416376
INFO:tools.evaluation_results_class:Counted Episodes = 2553
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 273.243408203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 274.243408203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.34300231933594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17486.220703125
INFO:tools.evaluation_results_class:Current Best Return = 273.243408203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.44230011815675
INFO:tools.evaluation_results_class:Counted Episodes = 2539
INFO:robust_rl.robust_rl_trainer:Iteration 27 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 285.15277099609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 286.15277099609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 240.294189453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20910.97265625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.675862068965515
INFO:tools.evaluation_results_class:Counted Episodes = 5075
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 15.2574462890625
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 46.88%
INFO:agents.father_agent:Step: 5, Training loss: 16.19172477722168
INFO:agents.father_agent:Step: 10, Training loss: 16.90097999572754
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 274.20855712890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.20855712890625
INFO:tools.evaluation_results_class:Average Discounted Reward = 231.98074340820312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20290.5078125
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.2065673921344
INFO:tools.evaluation_results_class:Counted Episodes = 5238
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 272.0514831542969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 273.0514831542969
INFO:tools.evaluation_results_class:Average Discounted Reward = 229.0428009033203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20428.005859375
INFO:tools.evaluation_results_class:Current Best Return = 272.0514831542969
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.953027950310556
INFO:tools.evaluation_results_class:Counted Episodes = 5152
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.3652038574219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.3652038574219
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.96730041503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20192.068359375
INFO:tools.evaluation_results_class:Current Best Return = 283.3652038574219
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.94612403100775
INFO:tools.evaluation_results_class:Counted Episodes = 5160
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 275.5768180138109
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 28 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.1463317871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.1463317871094
INFO:tools.evaluation_results_class:Average Discounted Reward = 238.37835693359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18040.212890625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.4945251841529
INFO:tools.evaluation_results_class:Counted Episodes = 5023
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.672737121582031
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 46.88%
INFO:agents.father_agent:Step: 5, Training loss: 14.614681243896484
INFO:agents.father_agent:Step: 10, Training loss: 14.737311363220215
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 285.2564392089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 286.2564392089844
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.94895935058594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20752.076171875
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.700662517566755
INFO:tools.evaluation_results_class:Counted Episodes = 4981
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.7575988769531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.7575988769531
INFO:tools.evaluation_results_class:Average Discounted Reward = 234.1404266357422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20008.310546875
INFO:tools.evaluation_results_class:Current Best Return = 277.7575988769531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.18700290979631
INFO:tools.evaluation_results_class:Counted Episodes = 5155
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.1720275878906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.1720275878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.2105255126953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18847.845703125
INFO:tools.evaluation_results_class:Current Best Return = 274.1720275878906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.76088185415489
INFO:tools.evaluation_results_class:Counted Episodes = 5307
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 275.1235039076544
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 29 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 291.8631896972656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.8631896972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 244.7271728515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21295.162109375
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.97473512632437
INFO:tools.evaluation_results_class:Counted Episodes = 4908
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.523053169250488
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 45.31%
INFO:agents.father_agent:Step: 5, Training loss: 17.21352195739746
INFO:agents.father_agent:Step: 10, Training loss: 18.197349548339844
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 299.2020263671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 300.2020263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 250.6746063232422
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23216.998046875
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 60.29035617579671
INFO:tools.evaluation_results_class:Counted Episodes = 4801
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 282.1193542480469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.1193542480469
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.0528106689453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19644.037109375
INFO:tools.evaluation_results_class:Current Best Return = 282.1193542480469
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.32476265822785
INFO:tools.evaluation_results_class:Counted Episodes = 5056
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 281.85516357421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 282.85516357421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 237.63267517089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20214.48046875
INFO:tools.evaluation_results_class:Current Best Return = 281.85516357421875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.98695818948983
INFO:tools.evaluation_results_class:Counted Episodes = 5214
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 275.557326294153
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 30 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 274.62725830078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 275.62725830078125
INFO:tools.evaluation_results_class:Average Discounted Reward = 232.71383666992188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18571.8046875
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.92205508888455
INFO:tools.evaluation_results_class:Counted Episodes = 5119
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 14.098104476928711
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 45.31%
INFO:agents.father_agent:Step: 5, Training loss: 17.8533878326416
INFO:agents.father_agent:Step: 10, Training loss: 17.486392974853516
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 285.6021423339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 286.6021423339844
INFO:tools.evaluation_results_class:Average Discounted Reward = 241.76246643066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18518.201171875
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.99290220820189
INFO:tools.evaluation_results_class:Counted Episodes = 5072
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 256.8479919433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 257.8479919433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 218.86328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15937.2001953125
INFO:tools.evaluation_results_class:Current Best Return = 256.8479919433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.87015115643781
INFO:tools.evaluation_results_class:Counted Episodes = 5491
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 288.6274108886719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 289.6274108886719
INFO:tools.evaluation_results_class:Average Discounted Reward = 242.28213500976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21830.734375
INFO:tools.evaluation_results_class:Current Best Return = 288.6274108886719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.85377821393523
INFO:tools.evaluation_results_class:Counted Episodes = 5095
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 275.6501060398022
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:robust_rl.robust_rl_trainer:Iteration 31 of extraction RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 283.6437683105469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 284.6437683105469
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.76942443847656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20045.1640625
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.756107171000785
INFO:tools.evaluation_results_class:Counted Episodes = 5076
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 10.139270782470703
INFO:agents.father_agent:Percentage of dormant neurons in actor network: 45.31%
INFO:agents.father_agent:Step: 5, Training loss: 16.07135581970215
INFO:agents.father_agent:Step: 10, Training loss: 14.92672348022461
INFO:agents.father_agent:Training finished.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 282.3729248046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 283.3729248046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 239.6200714111328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17516.388671875
INFO:tools.evaluation_results_class:Current Best Return = 426.69317626953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.48873348873349
INFO:tools.evaluation_results_class:Counted Episodes = 5148
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 277.0589294433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 278.0589294433594
INFO:tools.evaluation_results_class:Average Discounted Reward = 235.20774841308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17860.681640625
INFO:tools.evaluation_results_class:Current Best Return = 277.0589294433594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.03930131004367
INFO:tools.evaluation_results_class:Counted Episodes = 5267
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 291.85430908203125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 292.85430908203125
INFO:tools.evaluation_results_class:Average Discounted Reward = 245.935546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23537.55859375
INFO:tools.evaluation_results_class:Current Best Return = 291.85430908203125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.1627408993576
INFO:tools.evaluation_results_class:Counted Episodes = 5137
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 6000
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 276.7800386025708
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: pr1=2/5, pr2=7/10, pr3=1, pr4=1/10, energy_low=9/10, energy_medium=17/20, energy_high=4/5, bat_capacity=5
INFO:environment.vectorized_sim_initializer:Compiling model rover...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:rl_src.environment.environment_wrapper_vec:Observation 10 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Observation 12 not found in the state to observation map.
ERROR:rl_src.environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:rl_src.environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 603.5535888671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 604.5535888671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 480.796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 81089.8984375
INFO:tools.evaluation_results_class:Current Best Return = 603.5535888671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.07330721880247
INFO:tools.evaluation_results_class:Counted Episodes = 1787
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 309.33624267578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 310.33624267578125
INFO:tools.evaluation_results_class:Average Discounted Reward = 255.6244354248047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21908.71875
INFO:tools.evaluation_results_class:Current Best Return = 309.33624267578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.56204379562044
INFO:tools.evaluation_results_class:Counted Episodes = 2192
INFO:rl_src.environment.environment_wrapper_vec:Resetting the environment.

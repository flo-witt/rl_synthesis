2025-08-04 01:36:18.080784: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-04 01:36:18.082634: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-04 01:36:18.112690: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-04 01:36:18.112734: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-04 01:36:18.114167: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-04 01:36:18.119740: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-04 01:36:18.119920: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-04 01:36:18.657679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/models_robust_subset/network/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/models_robust_subset/network/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"packets_sent"}max=? [F "done"] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 5480 states and 71240 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"packets_sent"}max=? [F "label_done"] 
INFO:environment.vectorized_sim_initializer:Compiling model network...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:environment.environment_wrapper_vec:Observation 15 not found in the state to observation map.
ERROR:environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Iteration 1 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.457691192626953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.57691192626953
INFO:tools.evaluation_results_class:Average Discounted Reward = 27.274019241333008
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12.867053031921387
INFO:tools.evaluation_results_class:Current Best Return = 4.457691192626953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 40.77756118774414
INFO:agents.father_agent:Step: 5, Training loss: 9.024862289428711
INFO:agents.father_agent:Step: 10, Training loss: 7.329563617706299
INFO:agents.father_agent:Step: 15, Training loss: 8.727416038513184
INFO:agents.father_agent:Step: 20, Training loss: 5.13070821762085
INFO:agents.father_agent:Step: 25, Training loss: 6.151610851287842
INFO:agents.father_agent:Step: 30, Training loss: 10.59021282196045
INFO:agents.father_agent:Step: 35, Training loss: 7.437355995178223
INFO:agents.father_agent:Step: 40, Training loss: 8.077902793884277
INFO:agents.father_agent:Step: 45, Training loss: 7.703792572021484
INFO:agents.father_agent:Step: 50, Training loss: 7.0886454582214355
INFO:agents.father_agent:Step: 55, Training loss: 10.087032318115234
INFO:agents.father_agent:Step: 60, Training loss: 9.012996673583984
INFO:agents.father_agent:Step: 65, Training loss: 6.875132083892822
INFO:agents.father_agent:Step: 70, Training loss: 10.34157657623291
INFO:agents.father_agent:Step: 75, Training loss: 4.0617451667785645
INFO:agents.father_agent:Step: 80, Training loss: 9.005496978759766
INFO:agents.father_agent:Step: 85, Training loss: 5.699516773223877
INFO:agents.father_agent:Step: 90, Training loss: 8.408390045166016
INFO:agents.father_agent:Step: 95, Training loss: 6.8638834953308105
INFO:agents.father_agent:Step: 100, Training loss: 8.219350814819336
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.7931294441223145
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 69.9312973022461
INFO:tools.evaluation_results_class:Average Discounted Reward = 42.124908447265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.33406639099121
INFO:tools.evaluation_results_class:Current Best Return = 6.7931294441223145
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 7.3579888343811035
INFO:agents.father_agent:Step: 110, Training loss: 5.391605854034424
INFO:agents.father_agent:Step: 115, Training loss: 8.887060165405273
INFO:agents.father_agent:Step: 120, Training loss: 4.173063278198242
INFO:agents.father_agent:Step: 125, Training loss: 14.776205062866211
INFO:agents.father_agent:Step: 130, Training loss: 6.410762310028076
INFO:agents.father_agent:Step: 135, Training loss: 6.417742729187012
INFO:agents.father_agent:Step: 140, Training loss: 9.810702323913574
INFO:agents.father_agent:Step: 145, Training loss: 6.609403133392334
INFO:agents.father_agent:Step: 150, Training loss: 5.2983012199401855
INFO:agents.father_agent:Step: 155, Training loss: 8.55798053741455
INFO:agents.father_agent:Step: 160, Training loss: 4.635095596313477
INFO:agents.father_agent:Step: 165, Training loss: 10.208885192871094
INFO:agents.father_agent:Step: 170, Training loss: 7.254748344421387
INFO:agents.father_agent:Step: 175, Training loss: 5.535185813903809
INFO:agents.father_agent:Step: 180, Training loss: 25.106481552124023
INFO:agents.father_agent:Step: 185, Training loss: 4.838332653045654
INFO:agents.father_agent:Step: 190, Training loss: 5.258443832397461
INFO:agents.father_agent:Step: 195, Training loss: 8.664834022521973
INFO:agents.father_agent:Step: 200, Training loss: 5.0070319175720215
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.905531883239746
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.0553207397461
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.00294494628906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20.87059211730957
INFO:tools.evaluation_results_class:Current Best Return = 6.905531883239746
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 8.897716522216797
INFO:agents.father_agent:Step: 210, Training loss: 4.284863471984863
INFO:agents.father_agent:Step: 215, Training loss: 10.206619262695312
INFO:agents.father_agent:Step: 220, Training loss: 4.3658037185668945
INFO:agents.father_agent:Step: 225, Training loss: 17.602157592773438
INFO:agents.father_agent:Step: 230, Training loss: 6.421186923980713
INFO:agents.father_agent:Step: 235, Training loss: 5.529078483581543
INFO:agents.father_agent:Step: 240, Training loss: 8.186779975891113
INFO:agents.father_agent:Step: 245, Training loss: 6.986887454986572
INFO:agents.father_agent:Step: 250, Training loss: 5.250755310058594
INFO:agents.father_agent:Step: 255, Training loss: 8.351794242858887
INFO:agents.father_agent:Step: 260, Training loss: 4.43132209777832
INFO:agents.father_agent:Step: 265, Training loss: 10.350676536560059
INFO:agents.father_agent:Step: 270, Training loss: 7.297483921051025
INFO:agents.father_agent:Step: 275, Training loss: 5.0135345458984375
INFO:agents.father_agent:Step: 280, Training loss: 29.37197494506836
INFO:agents.father_agent:Step: 285, Training loss: 5.778932094573975
INFO:agents.father_agent:Step: 290, Training loss: 5.547001838684082
INFO:agents.father_agent:Step: 295, Training loss: 8.950382232666016
INFO:agents.father_agent:Step: 300, Training loss: 4.783108711242676
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 6.964637756347656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 71.64637756347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.401939392089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.46780776977539
INFO:tools.evaluation_results_class:Current Best Return = 6.964637756347656
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 8.887983322143555
INFO:agents.father_agent:Step: 310, Training loss: 4.231456756591797
INFO:agents.father_agent:Step: 315, Training loss: 10.118907928466797
INFO:agents.father_agent:Step: 320, Training loss: 3.9783079624176025
INFO:agents.father_agent:Step: 325, Training loss: 18.13740348815918
INFO:agents.father_agent:Step: 330, Training loss: 5.930866241455078
INFO:agents.father_agent:Step: 335, Training loss: 5.613546848297119
INFO:agents.father_agent:Step: 340, Training loss: 8.311352729797363
INFO:agents.father_agent:Step: 345, Training loss: 6.805087566375732
INFO:agents.father_agent:Step: 350, Training loss: 5.218208312988281
INFO:agents.father_agent:Step: 355, Training loss: 8.271872520446777
INFO:agents.father_agent:Step: 360, Training loss: 3.901956558227539
INFO:agents.father_agent:Step: 365, Training loss: 9.449458122253418
INFO:agents.father_agent:Step: 370, Training loss: 6.5833258628845215
INFO:agents.father_agent:Step: 375, Training loss: 5.333332061767578
INFO:agents.father_agent:Step: 380, Training loss: 25.201335906982422
INFO:agents.father_agent:Step: 385, Training loss: 4.679454326629639
INFO:agents.father_agent:Step: 390, Training loss: 5.572408199310303
INFO:agents.father_agent:Step: 395, Training loss: 8.33862590789795
INFO:agents.father_agent:Step: 400, Training loss: 4.940920352935791
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.02879524230957
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.28794860839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.99516677856445
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.111574172973633
INFO:tools.evaluation_results_class:Current Best Return = 7.02879524230957
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 8.378866195678711
INFO:agents.father_agent:Step: 410, Training loss: 4.742955684661865
INFO:agents.father_agent:Step: 415, Training loss: 8.924211502075195
INFO:agents.father_agent:Step: 420, Training loss: 4.390854358673096
INFO:agents.father_agent:Step: 425, Training loss: 17.292301177978516
INFO:agents.father_agent:Step: 430, Training loss: 6.111320495605469
INFO:agents.father_agent:Step: 435, Training loss: 5.160029411315918
INFO:agents.father_agent:Step: 440, Training loss: 8.68087387084961
INFO:agents.father_agent:Step: 445, Training loss: 6.35728120803833
INFO:agents.father_agent:Step: 450, Training loss: 4.644852638244629
INFO:agents.father_agent:Step: 455, Training loss: 7.608680725097656
INFO:agents.father_agent:Step: 460, Training loss: 4.61376953125
INFO:agents.father_agent:Step: 465, Training loss: 9.188505172729492
INFO:agents.father_agent:Step: 470, Training loss: 7.355996608734131
INFO:agents.father_agent:Step: 475, Training loss: 5.790318012237549
INFO:agents.father_agent:Step: 480, Training loss: 24.75125503540039
INFO:agents.father_agent:Step: 485, Training loss: 5.04893159866333
INFO:agents.father_agent:Step: 490, Training loss: 4.932112216949463
INFO:agents.father_agent:Step: 495, Training loss: 8.69629192352295
INFO:agents.father_agent:Step: 500, Training loss: 5.09339714050293
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.040161609649658
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.40161895751953
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.96372985839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.540950775146484
INFO:tools.evaluation_results_class:Current Best Return = 7.040161609649658
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 505, Training loss: 8.235074043273926
INFO:agents.father_agent:Step: 510, Training loss: 4.197559833526611
INFO:agents.father_agent:Step: 515, Training loss: 8.582061767578125
INFO:agents.father_agent:Step: 520, Training loss: 4.023375034332275
INFO:agents.father_agent:Step: 525, Training loss: 16.69824981689453
INFO:agents.father_agent:Step: 530, Training loss: 6.14216947555542
INFO:agents.father_agent:Step: 535, Training loss: 5.024662971496582
INFO:agents.father_agent:Step: 540, Training loss: 9.313725471496582
INFO:agents.father_agent:Step: 545, Training loss: 6.897148609161377
INFO:agents.father_agent:Step: 550, Training loss: 4.751757621765137
INFO:agents.father_agent:Step: 555, Training loss: 8.287343978881836
INFO:agents.father_agent:Step: 560, Training loss: 3.9408326148986816
INFO:agents.father_agent:Step: 565, Training loss: 9.681117057800293
INFO:agents.father_agent:Step: 570, Training loss: 6.361125469207764
INFO:agents.father_agent:Step: 575, Training loss: 5.010738849639893
INFO:agents.father_agent:Step: 580, Training loss: 25.949438095092773
INFO:agents.father_agent:Step: 585, Training loss: 5.164243221282959
INFO:agents.father_agent:Step: 590, Training loss: 4.650311470031738
INFO:agents.father_agent:Step: 595, Training loss: 8.255566596984863
INFO:agents.father_agent:Step: 600, Training loss: 5.603331089019775
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.053043842315674
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.53043365478516
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.240901947021484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.16869354248047
INFO:tools.evaluation_results_class:Current Best Return = 7.053043842315674
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 605, Training loss: 7.962635517120361
INFO:agents.father_agent:Step: 610, Training loss: 4.1271491050720215
INFO:agents.father_agent:Step: 615, Training loss: 9.038569450378418
INFO:agents.father_agent:Step: 620, Training loss: 4.3445725440979
INFO:agents.father_agent:Step: 625, Training loss: 18.38054847717285
INFO:agents.father_agent:Step: 630, Training loss: 6.108132362365723
INFO:agents.father_agent:Step: 635, Training loss: 5.489806652069092
INFO:agents.father_agent:Step: 640, Training loss: 8.642924308776855
INFO:agents.father_agent:Step: 645, Training loss: 6.686585426330566
INFO:agents.father_agent:Step: 650, Training loss: 5.0672383308410645
INFO:agents.father_agent:Step: 655, Training loss: 8.14083480834961
INFO:agents.father_agent:Step: 660, Training loss: 4.086982250213623
INFO:agents.father_agent:Step: 665, Training loss: 8.818286895751953
INFO:agents.father_agent:Step: 670, Training loss: 6.560178279876709
INFO:agents.father_agent:Step: 675, Training loss: 4.803225517272949
INFO:agents.father_agent:Step: 680, Training loss: 25.19452476501465
INFO:agents.father_agent:Step: 685, Training loss: 4.7554473876953125
INFO:agents.father_agent:Step: 690, Training loss: 4.730129241943359
INFO:agents.father_agent:Step: 695, Training loss: 8.063855171203613
INFO:agents.father_agent:Step: 700, Training loss: 5.266360759735107
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.023743152618408
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.23743438720703
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.89374923706055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.698347091674805
INFO:tools.evaluation_results_class:Current Best Return = 7.053043842315674
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 705, Training loss: 7.9726715087890625
INFO:agents.father_agent:Step: 710, Training loss: 4.0734076499938965
INFO:agents.father_agent:Step: 715, Training loss: 9.185783386230469
INFO:agents.father_agent:Step: 720, Training loss: 4.377612113952637
INFO:agents.father_agent:Step: 725, Training loss: 17.90696907043457
INFO:agents.father_agent:Step: 730, Training loss: 5.849182605743408
INFO:agents.father_agent:Step: 735, Training loss: 5.546435832977295
INFO:agents.father_agent:Step: 740, Training loss: 8.768556594848633
INFO:agents.father_agent:Step: 745, Training loss: 6.347276210784912
INFO:agents.father_agent:Step: 750, Training loss: 5.1507720947265625
INFO:agents.father_agent:Step: 755, Training loss: 8.162646293640137
INFO:agents.father_agent:Step: 760, Training loss: 4.383450031280518
INFO:agents.father_agent:Step: 765, Training loss: 9.355917930603027
INFO:agents.father_agent:Step: 770, Training loss: 6.593504428863525
INFO:agents.father_agent:Step: 775, Training loss: 4.268640518188477
INFO:agents.father_agent:Step: 780, Training loss: 25.03973960876465
INFO:agents.father_agent:Step: 785, Training loss: 4.732102870941162
INFO:agents.father_agent:Step: 790, Training loss: 4.489087104797363
INFO:agents.father_agent:Step: 795, Training loss: 8.027262687683105
INFO:agents.father_agent:Step: 800, Training loss: 4.97500467300415
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.049002170562744
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.49002075195312
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.135772705078125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.501008987426758
INFO:tools.evaluation_results_class:Current Best Return = 7.053043842315674
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 805, Training loss: 7.973785877227783
INFO:agents.father_agent:Step: 810, Training loss: 4.923445701599121
INFO:agents.father_agent:Step: 815, Training loss: 8.697172164916992
INFO:agents.father_agent:Step: 820, Training loss: 3.9199230670928955
INFO:agents.father_agent:Step: 825, Training loss: 17.480844497680664
INFO:agents.father_agent:Step: 830, Training loss: 5.762115001678467
INFO:agents.father_agent:Step: 835, Training loss: 4.455330848693848
INFO:agents.father_agent:Step: 840, Training loss: 8.502988815307617
INFO:agents.father_agent:Step: 845, Training loss: 6.644558429718018
INFO:agents.father_agent:Step: 850, Training loss: 4.841068267822266
INFO:agents.father_agent:Step: 855, Training loss: 8.432689666748047
INFO:agents.father_agent:Step: 860, Training loss: 3.7259409427642822
INFO:agents.father_agent:Step: 865, Training loss: 9.713996887207031
INFO:agents.father_agent:Step: 870, Training loss: 6.478046417236328
INFO:agents.father_agent:Step: 875, Training loss: 5.474172115325928
INFO:agents.father_agent:Step: 880, Training loss: 26.027084350585938
INFO:agents.father_agent:Step: 885, Training loss: 4.943960666656494
INFO:agents.father_agent:Step: 890, Training loss: 4.974199295043945
INFO:agents.father_agent:Step: 895, Training loss: 8.479183197021484
INFO:agents.father_agent:Step: 900, Training loss: 4.756387710571289
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.080828666687012
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.80828857421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.23691940307617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.69490623474121
INFO:tools.evaluation_results_class:Current Best Return = 7.080828666687012
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 905, Training loss: 8.015375137329102
INFO:agents.father_agent:Step: 910, Training loss: 3.971184730529785
INFO:agents.father_agent:Step: 915, Training loss: 8.861385345458984
INFO:agents.father_agent:Step: 920, Training loss: 3.9756526947021484
INFO:agents.father_agent:Step: 925, Training loss: 17.251419067382812
INFO:agents.father_agent:Step: 930, Training loss: 6.0713605880737305
INFO:agents.father_agent:Step: 935, Training loss: 5.301004886627197
INFO:agents.father_agent:Step: 940, Training loss: 8.938029289245605
INFO:agents.father_agent:Step: 945, Training loss: 6.318135738372803
INFO:agents.father_agent:Step: 950, Training loss: 4.823968410491943
INFO:agents.father_agent:Step: 955, Training loss: 8.38923454284668
INFO:agents.father_agent:Step: 960, Training loss: 3.7637674808502197
INFO:agents.father_agent:Step: 965, Training loss: 9.116562843322754
INFO:agents.father_agent:Step: 970, Training loss: 6.566771984100342
INFO:agents.father_agent:Step: 975, Training loss: 4.454960346221924
INFO:agents.father_agent:Step: 980, Training loss: 26.69468879699707
INFO:agents.father_agent:Step: 985, Training loss: 4.43866491317749
INFO:agents.father_agent:Step: 990, Training loss: 4.521157264709473
INFO:agents.father_agent:Step: 995, Training loss: 8.096141815185547
INFO:agents.father_agent:Step: 1000, Training loss: 4.950533866882324
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.115685939788818
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 73.1568603515625
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.588138580322266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.28795623779297
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1005, Training loss: 7.955477714538574
INFO:agents.father_agent:Step: 1010, Training loss: 4.217742919921875
INFO:agents.father_agent:Step: 1015, Training loss: 9.43401050567627
INFO:agents.father_agent:Step: 1020, Training loss: 3.914067506790161
INFO:agents.father_agent:Step: 1025, Training loss: 18.063047409057617
INFO:agents.father_agent:Step: 1030, Training loss: 6.078985214233398
INFO:agents.father_agent:Step: 1035, Training loss: 4.961230754852295
INFO:agents.father_agent:Step: 1040, Training loss: 7.5471577644348145
INFO:agents.father_agent:Step: 1045, Training loss: 6.546300888061523
INFO:agents.father_agent:Step: 1050, Training loss: 4.992739200592041
INFO:agents.father_agent:Step: 1055, Training loss: 8.370173454284668
INFO:agents.father_agent:Step: 1060, Training loss: 3.918513774871826
INFO:agents.father_agent:Step: 1065, Training loss: 9.137871742248535
INFO:agents.father_agent:Step: 1070, Training loss: 6.320713996887207
INFO:agents.father_agent:Step: 1075, Training loss: 4.401865482330322
INFO:agents.father_agent:Step: 1080, Training loss: 24.61760711669922
INFO:agents.father_agent:Step: 1085, Training loss: 4.456479549407959
INFO:agents.father_agent:Step: 1090, Training loss: 4.575422286987305
INFO:agents.father_agent:Step: 1095, Training loss: 8.078025817871094
INFO:agents.father_agent:Step: 1100, Training loss: 4.781339168548584
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.039151191711426
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.39151000976562
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.093048095703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.310667037963867
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1105, Training loss: 7.979801177978516
INFO:agents.father_agent:Step: 1110, Training loss: 4.110084533691406
INFO:agents.father_agent:Step: 1115, Training loss: 9.439743041992188
INFO:agents.father_agent:Step: 1120, Training loss: 4.160921096801758
INFO:agents.father_agent:Step: 1125, Training loss: 17.466489791870117
INFO:agents.father_agent:Step: 1130, Training loss: 6.263816833496094
INFO:agents.father_agent:Step: 1135, Training loss: 4.552971363067627
INFO:agents.father_agent:Step: 1140, Training loss: 8.501410484313965
INFO:agents.father_agent:Step: 1145, Training loss: 6.2151899337768555
INFO:agents.father_agent:Step: 1150, Training loss: 4.595488548278809
INFO:agents.father_agent:Step: 1155, Training loss: 8.16843318939209
INFO:agents.father_agent:Step: 1160, Training loss: 3.6069467067718506
INFO:agents.father_agent:Step: 1165, Training loss: 9.138551712036133
INFO:agents.father_agent:Step: 1170, Training loss: 6.829479694366455
INFO:agents.father_agent:Step: 1175, Training loss: 4.342653274536133
INFO:agents.father_agent:Step: 1180, Training loss: 26.42935562133789
INFO:agents.father_agent:Step: 1185, Training loss: 4.541413307189941
INFO:agents.father_agent:Step: 1190, Training loss: 4.171260833740234
INFO:agents.father_agent:Step: 1195, Training loss: 7.751965522766113
INFO:agents.father_agent:Step: 1200, Training loss: 5.153995990753174
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.050265312194824
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.50265502929688
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.17142105102539
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.4491024017334
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1205, Training loss: 7.938199996948242
INFO:agents.father_agent:Step: 1210, Training loss: 3.846207857131958
INFO:agents.father_agent:Step: 1215, Training loss: 8.31517219543457
INFO:agents.father_agent:Step: 1220, Training loss: 3.696096658706665
INFO:agents.father_agent:Step: 1225, Training loss: 16.663591384887695
INFO:agents.father_agent:Step: 1230, Training loss: 5.964085578918457
INFO:agents.father_agent:Step: 1235, Training loss: 4.78794002532959
INFO:agents.father_agent:Step: 1240, Training loss: 8.441182136535645
INFO:agents.father_agent:Step: 1245, Training loss: 6.1548752784729
INFO:agents.father_agent:Step: 1250, Training loss: 4.719407558441162
INFO:agents.father_agent:Step: 1255, Training loss: 8.23715877532959
INFO:agents.father_agent:Step: 1260, Training loss: 3.971121311187744
INFO:agents.father_agent:Step: 1265, Training loss: 9.189708709716797
INFO:agents.father_agent:Step: 1270, Training loss: 6.33173131942749
INFO:agents.father_agent:Step: 1275, Training loss: 4.878102779388428
INFO:agents.father_agent:Step: 1280, Training loss: 25.42669677734375
INFO:agents.father_agent:Step: 1285, Training loss: 4.586902618408203
INFO:agents.father_agent:Step: 1290, Training loss: 4.141887187957764
INFO:agents.father_agent:Step: 1295, Training loss: 7.895353317260742
INFO:agents.father_agent:Step: 1300, Training loss: 5.201075553894043
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.074008464813232
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.7400894165039
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.32765579223633
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.276411056518555
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1305, Training loss: 7.527155876159668
INFO:agents.father_agent:Step: 1310, Training loss: 3.9219837188720703
INFO:agents.father_agent:Step: 1315, Training loss: 9.174166679382324
INFO:agents.father_agent:Step: 1320, Training loss: 4.140868186950684
INFO:agents.father_agent:Step: 1325, Training loss: 17.24014663696289
INFO:agents.father_agent:Step: 1330, Training loss: 5.697369575500488
INFO:agents.father_agent:Step: 1335, Training loss: 4.537062168121338
INFO:agents.father_agent:Step: 1340, Training loss: 8.28515338897705
INFO:agents.father_agent:Step: 1345, Training loss: 7.023104190826416
INFO:agents.father_agent:Step: 1350, Training loss: 4.812169551849365
INFO:agents.father_agent:Step: 1355, Training loss: 8.238702774047852
INFO:agents.father_agent:Step: 1360, Training loss: 4.503171920776367
INFO:agents.father_agent:Step: 1365, Training loss: 9.561517715454102
INFO:agents.father_agent:Step: 1370, Training loss: 6.523221492767334
INFO:agents.father_agent:Step: 1375, Training loss: 4.221332550048828
INFO:agents.father_agent:Step: 1380, Training loss: 25.654001235961914
INFO:agents.father_agent:Step: 1385, Training loss: 4.844359874725342
INFO:agents.father_agent:Step: 1390, Training loss: 4.502007961273193
INFO:agents.father_agent:Step: 1395, Training loss: 8.348024368286133
INFO:agents.father_agent:Step: 1400, Training loss: 4.559902667999268
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.065673351287842
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.65673065185547
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.25789260864258
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.225290298461914
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 7.034099578857422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.34099578857422
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.00801086425781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.56968879699707
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.1242738065168
INFO:tools.evaluation_results_class:Counted Episodes = 3959
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.046827793121338
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.46827697753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 44.13024139404297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.532047271728516
INFO:tools.evaluation_results_class:Current Best Return = 7.046827793121338
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 80.92346424974824
INFO:tools.evaluation_results_class:Counted Episodes = 3972
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 7.0108256340026855
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 72.10826110839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 43.873348236083984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21.270526885986328
INFO:tools.evaluation_results_class:Current Best Return = 7.0108256340026855
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 80.92346424974824
INFO:tools.evaluation_results_class:Counted Episodes = 3972
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 16.2508 achieved after 792.2 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.9328 achieved after 792.21 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.609 achieved after 792.26 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.1685 achieved after 792.3 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 7.5614 achieved after 792.34 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 6.7158 achieved after 792.36 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 5.5249 achieved after 792.38 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 3.7746 achieved after 792.38 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.7746132606522247
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:robust_rl.robust_rl_trainer:Iteration 2 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.403942108154297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.03942108154297
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.062835693359375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.406464576721191
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.90844165119269
INFO:tools.evaluation_results_class:Counted Episodes = 7001
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 12.38756275177002
INFO:agents.father_agent:Step: 5, Training loss: 6.417876720428467
INFO:agents.father_agent:Step: 10, Training loss: 16.203420639038086
INFO:agents.father_agent:Step: 15, Training loss: 4.9626688957214355
INFO:agents.father_agent:Step: 20, Training loss: 25.047170639038086
INFO:agents.father_agent:Step: 25, Training loss: 12.098066329956055
INFO:agents.father_agent:Step: 30, Training loss: 7.195621490478516
INFO:agents.father_agent:Step: 35, Training loss: 14.559027671813965
INFO:agents.father_agent:Step: 40, Training loss: 6.987435817718506
INFO:agents.father_agent:Step: 45, Training loss: 9.52330493927002
INFO:agents.father_agent:Step: 50, Training loss: 12.43138599395752
INFO:agents.father_agent:Step: 55, Training loss: 6.765566349029541
INFO:agents.father_agent:Step: 60, Training loss: 13.678986549377441
INFO:agents.father_agent:Step: 65, Training loss: 9.660581588745117
INFO:agents.father_agent:Step: 70, Training loss: 7.344988822937012
INFO:agents.father_agent:Step: 75, Training loss: 17.06846046447754
INFO:agents.father_agent:Step: 80, Training loss: 6.13948917388916
INFO:agents.father_agent:Step: 85, Training loss: 14.830805778503418
INFO:agents.father_agent:Step: 90, Training loss: 10.999381065368652
INFO:agents.father_agent:Step: 95, Training loss: 6.801144599914551
INFO:agents.father_agent:Step: 100, Training loss: 14.962964057922363
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.437223434448242
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.37223434448242
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.34308624267578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.36889934539795
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.90844165119269
INFO:tools.evaluation_results_class:Counted Episodes = 7001
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 9.382793426513672
INFO:agents.father_agent:Step: 110, Training loss: 8.128978729248047
INFO:agents.father_agent:Step: 115, Training loss: 9.367907524108887
INFO:agents.father_agent:Step: 120, Training loss: 5.896580696105957
INFO:agents.father_agent:Step: 125, Training loss: 12.005600929260254
INFO:agents.father_agent:Step: 130, Training loss: 8.911103248596191
INFO:agents.father_agent:Step: 135, Training loss: 7.873692035675049
INFO:agents.father_agent:Step: 140, Training loss: 10.279001235961914
INFO:agents.father_agent:Step: 145, Training loss: 8.417340278625488
INFO:agents.father_agent:Step: 150, Training loss: 7.905322074890137
INFO:agents.father_agent:Step: 155, Training loss: 10.496614456176758
INFO:agents.father_agent:Step: 160, Training loss: 6.433138370513916
INFO:agents.father_agent:Step: 165, Training loss: 11.035139083862305
INFO:agents.father_agent:Step: 170, Training loss: 7.589251518249512
INFO:agents.father_agent:Step: 175, Training loss: 8.495037078857422
INFO:agents.father_agent:Step: 180, Training loss: 10.643389701843262
INFO:agents.father_agent:Step: 185, Training loss: 6.639495372772217
INFO:agents.father_agent:Step: 190, Training loss: 6.734719276428223
INFO:agents.father_agent:Step: 195, Training loss: 9.715599060058594
INFO:agents.father_agent:Step: 200, Training loss: 9.113243103027344
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.478074550628662
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.78074645996094
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.634971618652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.33093547821045
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.90844165119269
INFO:tools.evaluation_results_class:Counted Episodes = 7001
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 8.382497787475586
INFO:agents.father_agent:Step: 210, Training loss: 7.63386869430542
INFO:agents.father_agent:Step: 215, Training loss: 9.426125526428223
INFO:agents.father_agent:Step: 220, Training loss: 5.7563934326171875
INFO:agents.father_agent:Step: 225, Training loss: 10.45729923248291
INFO:agents.father_agent:Step: 230, Training loss: 7.93340539932251
INFO:agents.father_agent:Step: 235, Training loss: 7.432378768920898
INFO:agents.father_agent:Step: 240, Training loss: 8.887104988098145
INFO:agents.father_agent:Step: 245, Training loss: 7.892246723175049
INFO:agents.father_agent:Step: 250, Training loss: 8.012486457824707
INFO:agents.father_agent:Step: 255, Training loss: 8.936629295349121
INFO:agents.father_agent:Step: 260, Training loss: 6.011392116546631
INFO:agents.father_agent:Step: 265, Training loss: 9.939997673034668
INFO:agents.father_agent:Step: 270, Training loss: 7.9264373779296875
INFO:agents.father_agent:Step: 275, Training loss: 9.494595527648926
INFO:agents.father_agent:Step: 280, Training loss: 11.084484100341797
INFO:agents.father_agent:Step: 285, Training loss: 6.033105373382568
INFO:agents.father_agent:Step: 290, Training loss: 6.922336578369141
INFO:agents.father_agent:Step: 295, Training loss: 8.985939025878906
INFO:agents.father_agent:Step: 300, Training loss: 7.66467809677124
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.423939228057861
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.23939514160156
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.22550964355469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.486466407775879
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.90844165119269
INFO:tools.evaluation_results_class:Counted Episodes = 7001
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 8.672874450683594
INFO:agents.father_agent:Step: 310, Training loss: 7.474850177764893
INFO:agents.father_agent:Step: 315, Training loss: 8.812288284301758
INFO:agents.father_agent:Step: 320, Training loss: 5.8771185874938965
INFO:agents.father_agent:Step: 325, Training loss: 11.911898612976074
INFO:agents.father_agent:Step: 330, Training loss: 8.036971092224121
INFO:agents.father_agent:Step: 335, Training loss: 6.761069297790527
INFO:agents.father_agent:Step: 340, Training loss: 9.101456642150879
INFO:agents.father_agent:Step: 345, Training loss: 7.833388328552246
INFO:agents.father_agent:Step: 350, Training loss: 7.334747791290283
INFO:agents.father_agent:Step: 355, Training loss: 9.042901992797852
INFO:agents.father_agent:Step: 360, Training loss: 6.028038501739502
INFO:agents.father_agent:Step: 365, Training loss: 10.55138874053955
INFO:agents.father_agent:Step: 370, Training loss: 7.6354451179504395
INFO:agents.father_agent:Step: 375, Training loss: 8.686180114746094
INFO:agents.father_agent:Step: 380, Training loss: 10.596264839172363
INFO:agents.father_agent:Step: 385, Training loss: 6.355492115020752
INFO:agents.father_agent:Step: 390, Training loss: 7.398764133453369
INFO:agents.father_agent:Step: 395, Training loss: 8.855416297912598
INFO:agents.father_agent:Step: 400, Training loss: 7.425912380218506
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.444793701171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.44793701171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.38071823120117
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.360077857971191
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.90844165119269
INFO:tools.evaluation_results_class:Counted Episodes = 7001
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 8.219520568847656
INFO:agents.father_agent:Step: 410, Training loss: 7.338738918304443
INFO:agents.father_agent:Step: 415, Training loss: 8.361550331115723
INFO:agents.father_agent:Step: 420, Training loss: 5.261867523193359
INFO:agents.father_agent:Step: 425, Training loss: 10.405523300170898
INFO:agents.father_agent:Step: 430, Training loss: 7.694761276245117
INFO:agents.father_agent:Step: 435, Training loss: 6.7988810539245605
INFO:agents.father_agent:Step: 440, Training loss: 8.218293190002441
INFO:agents.father_agent:Step: 445, Training loss: 7.609635829925537
INFO:agents.father_agent:Step: 450, Training loss: 7.166176795959473
INFO:agents.father_agent:Step: 455, Training loss: 8.463540077209473
INFO:agents.father_agent:Step: 460, Training loss: 5.837388038635254
INFO:agents.father_agent:Step: 465, Training loss: 9.831298828125
INFO:agents.father_agent:Step: 470, Training loss: 6.92857027053833
INFO:agents.father_agent:Step: 475, Training loss: 9.784326553344727
INFO:agents.father_agent:Step: 480, Training loss: 10.191575050354004
INFO:agents.father_agent:Step: 485, Training loss: 5.774395942687988
INFO:agents.father_agent:Step: 490, Training loss: 6.944167137145996
INFO:agents.father_agent:Step: 495, Training loss: 8.261350631713867
INFO:agents.father_agent:Step: 500, Training loss: 7.382389545440674
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.480931282043457
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.8093147277832
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.646156311035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.429895401000977
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.90844165119269
INFO:tools.evaluation_results_class:Counted Episodes = 7001
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.484787940979004
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.847877502441406
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.6951904296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.139212608337402
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.90844165119269
INFO:tools.evaluation_results_class:Counted Episodes = 7001
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.41665506362915
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.16654968261719
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.273048400878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8.125357627868652
INFO:tools.evaluation_results_class:Current Best Return = 4.41665506362915
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.625121443442055
INFO:tools.evaluation_results_class:Counted Episodes = 7205
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.435947418212891
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.359474182128906
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.45440673828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7.917513847351074
INFO:tools.evaluation_results_class:Current Best Return = 4.435947418212891
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 45.625121443442055
INFO:tools.evaluation_results_class:Counted Episodes = 7205
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 16.2603 achieved after 1242.98 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.9374 achieved after 1243.0 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.6144 achieved after 1243.05 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.1744 achieved after 1243.09 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 7.5673 achieved after 1243.13 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 6.7204 achieved after 1243.15 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 5.5263 achieved after 1243.17 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 3.7678 achieved after 1243.17 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.7678377210585503
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:robust_rl.robust_rl_trainer:Iteration 3 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.048919677734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.48919677734375
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.12118911743164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.573966026306152
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.91631487459647
INFO:tools.evaluation_results_class:Counted Episodes = 8054
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.985179424285889
INFO:agents.father_agent:Step: 5, Training loss: 8.026209831237793
INFO:agents.father_agent:Step: 10, Training loss: 8.97091007232666
INFO:agents.father_agent:Step: 15, Training loss: 6.702341556549072
INFO:agents.father_agent:Step: 20, Training loss: 12.238347053527832
INFO:agents.father_agent:Step: 25, Training loss: 7.87360954284668
INFO:agents.father_agent:Step: 30, Training loss: 7.600316524505615
INFO:agents.father_agent:Step: 35, Training loss: 9.27884578704834
INFO:agents.father_agent:Step: 40, Training loss: 6.2266645431518555
INFO:agents.father_agent:Step: 45, Training loss: 9.839570045471191
INFO:agents.father_agent:Step: 50, Training loss: 8.748648643493652
INFO:agents.father_agent:Step: 55, Training loss: 8.02119255065918
INFO:agents.father_agent:Step: 60, Training loss: 9.189713478088379
INFO:agents.father_agent:Step: 65, Training loss: 7.612025260925293
INFO:agents.father_agent:Step: 70, Training loss: 9.218679428100586
INFO:agents.father_agent:Step: 75, Training loss: 9.084508895874023
INFO:agents.father_agent:Step: 80, Training loss: 6.149250030517578
INFO:agents.father_agent:Step: 85, Training loss: 11.894145011901855
INFO:agents.father_agent:Step: 90, Training loss: 8.172625541687012
INFO:agents.father_agent:Step: 95, Training loss: 9.402008056640625
INFO:agents.father_agent:Step: 100, Training loss: 8.051250457763672
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9914329051971436
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.914329528808594
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.6192626953125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.7850022315979
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.91631487459647
INFO:tools.evaluation_results_class:Counted Episodes = 8054
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 6.210322380065918
INFO:agents.father_agent:Step: 110, Training loss: 8.884469032287598
INFO:agents.father_agent:Step: 115, Training loss: 6.565694332122803
INFO:agents.father_agent:Step: 120, Training loss: 6.460468769073486
INFO:agents.father_agent:Step: 125, Training loss: 10.165428161621094
INFO:agents.father_agent:Step: 130, Training loss: 6.331076145172119
INFO:agents.father_agent:Step: 135, Training loss: 8.547256469726562
INFO:agents.father_agent:Step: 140, Training loss: 7.07229471206665
INFO:agents.father_agent:Step: 145, Training loss: 6.819034576416016
INFO:agents.father_agent:Step: 150, Training loss: 7.736717224121094
INFO:agents.father_agent:Step: 155, Training loss: 7.241245746612549
INFO:agents.father_agent:Step: 160, Training loss: 7.387612819671631
INFO:agents.father_agent:Step: 165, Training loss: 8.524864196777344
INFO:agents.father_agent:Step: 170, Training loss: 6.389388561248779
INFO:agents.father_agent:Step: 175, Training loss: 10.0
INFO:agents.father_agent:Step: 180, Training loss: 7.521238327026367
INFO:agents.father_agent:Step: 185, Training loss: 6.496154308319092
INFO:agents.father_agent:Step: 190, Training loss: 8.01513957977295
INFO:agents.father_agent:Step: 195, Training loss: 7.47514533996582
INFO:agents.father_agent:Step: 200, Training loss: 8.689620971679688
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.0743727684021
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.74372863769531
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.27379608154297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.672020435333252
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.91631487459647
INFO:tools.evaluation_results_class:Counted Episodes = 8054
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 7.093098163604736
INFO:agents.father_agent:Step: 210, Training loss: 9.75420093536377
INFO:agents.father_agent:Step: 215, Training loss: 7.354057788848877
INFO:agents.father_agent:Step: 220, Training loss: 5.848879337310791
INFO:agents.father_agent:Step: 225, Training loss: 10.23741626739502
INFO:agents.father_agent:Step: 230, Training loss: 6.5014142990112305
INFO:agents.father_agent:Step: 235, Training loss: 6.372105598449707
INFO:agents.father_agent:Step: 240, Training loss: 7.794630527496338
INFO:agents.father_agent:Step: 245, Training loss: 7.944447040557861
INFO:agents.father_agent:Step: 250, Training loss: 8.757734298706055
INFO:agents.father_agent:Step: 255, Training loss: 7.821774482727051
INFO:agents.father_agent:Step: 260, Training loss: 7.623211860656738
INFO:agents.father_agent:Step: 265, Training loss: 9.328943252563477
INFO:agents.father_agent:Step: 270, Training loss: 5.769943714141846
INFO:agents.father_agent:Step: 275, Training loss: 9.523579597473145
INFO:agents.father_agent:Step: 280, Training loss: 6.921686172485352
INFO:agents.father_agent:Step: 285, Training loss: 5.569825649261475
INFO:agents.father_agent:Step: 290, Training loss: 7.518280029296875
INFO:agents.father_agent:Step: 295, Training loss: 7.700876712799072
INFO:agents.father_agent:Step: 300, Training loss: 9.598238945007324
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.074000358581543
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.74000549316406
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.3250617980957
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.66822624206543
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.91631487459647
INFO:tools.evaluation_results_class:Counted Episodes = 8054
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 6.195636749267578
INFO:agents.father_agent:Step: 310, Training loss: 8.27463150024414
INFO:agents.father_agent:Step: 315, Training loss: 7.5262250900268555
INFO:agents.father_agent:Step: 320, Training loss: 5.3919291496276855
INFO:agents.father_agent:Step: 325, Training loss: 10.675625801086426
INFO:agents.father_agent:Step: 330, Training loss: 6.288567066192627
INFO:agents.father_agent:Step: 335, Training loss: 7.014297008514404
INFO:agents.father_agent:Step: 340, Training loss: 7.5953755378723145
INFO:agents.father_agent:Step: 345, Training loss: 7.079802989959717
INFO:agents.father_agent:Step: 350, Training loss: 10.130539894104004
INFO:agents.father_agent:Step: 355, Training loss: 7.57766580581665
INFO:agents.father_agent:Step: 360, Training loss: 6.6741251945495605
INFO:agents.father_agent:Step: 365, Training loss: 8.09565258026123
INFO:agents.father_agent:Step: 370, Training loss: 5.850675582885742
INFO:agents.father_agent:Step: 375, Training loss: 9.489513397216797
INFO:agents.father_agent:Step: 380, Training loss: 8.15880012512207
INFO:agents.father_agent:Step: 385, Training loss: 5.290923118591309
INFO:agents.father_agent:Step: 390, Training loss: 7.855042934417725
INFO:agents.father_agent:Step: 395, Training loss: 7.58971643447876
INFO:agents.father_agent:Step: 400, Training loss: 6.753902435302734
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.009063720703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.09063720703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.820701599121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.741288185119629
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.91631487459647
INFO:tools.evaluation_results_class:Counted Episodes = 8054
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 6.779077053070068
INFO:agents.father_agent:Step: 410, Training loss: 8.675451278686523
INFO:agents.father_agent:Step: 415, Training loss: 6.697527885437012
INFO:agents.father_agent:Step: 420, Training loss: 5.763476848602295
INFO:agents.father_agent:Step: 425, Training loss: 10.470711708068848
INFO:agents.father_agent:Step: 430, Training loss: 6.306999683380127
INFO:agents.father_agent:Step: 435, Training loss: 7.702497482299805
INFO:agents.father_agent:Step: 440, Training loss: 7.610500335693359
INFO:agents.father_agent:Step: 445, Training loss: 6.435463905334473
INFO:agents.father_agent:Step: 450, Training loss: 8.162418365478516
INFO:agents.father_agent:Step: 455, Training loss: 7.51401948928833
INFO:agents.father_agent:Step: 460, Training loss: 6.773531913757324
INFO:agents.father_agent:Step: 465, Training loss: 8.10786247253418
INFO:agents.father_agent:Step: 470, Training loss: 6.112651824951172
INFO:agents.father_agent:Step: 475, Training loss: 9.991646766662598
INFO:agents.father_agent:Step: 480, Training loss: 7.536590099334717
INFO:agents.father_agent:Step: 485, Training loss: 5.723441123962402
INFO:agents.father_agent:Step: 490, Training loss: 7.877964973449707
INFO:agents.father_agent:Step: 495, Training loss: 8.090744018554688
INFO:agents.father_agent:Step: 500, Training loss: 8.610665321350098
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.030667781829834
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.306678771972656
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.90964889526367
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.739933490753174
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.91631487459647
INFO:tools.evaluation_results_class:Counted Episodes = 8054
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 4.058852672576904
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.58852767944336
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.1875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.681413173675537
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.91631487459647
INFO:tools.evaluation_results_class:Counted Episodes = 8054
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.036571979522705
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.36572265625
INFO:tools.evaluation_results_class:Average Discounted Reward = 33.04642105102539
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.722259521484375
INFO:tools.evaluation_results_class:Current Best Return = 4.036571979522705
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.865902232951115
INFO:tools.evaluation_results_class:Counted Episodes = 8285
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 4.01448392868042
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.144840240478516
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.83651351928711
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4.652294158935547
INFO:tools.evaluation_results_class:Current Best Return = 4.01448392868042
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 39.865902232951115
INFO:tools.evaluation_results_class:Counted Episodes = 8285
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 16.2607 achieved after 1710.17 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.9372 achieved after 1710.18 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.6156 achieved after 1710.23 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.1782 achieved after 1710.28 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 7.5758 achieved after 1710.31 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 6.7372 achieved after 1710.34 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 5.5567 achieved after 1710.36 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 3.8057 achieved after 1710.36 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.8056547296639236
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:robust_rl.robust_rl_trainer:Iteration 4 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9421780109405518
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.42177963256836
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.848609924316406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.4135894775390625
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.01785503133499
INFO:tools.evaluation_results_class:Counted Episodes = 8457
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.826718330383301
INFO:agents.father_agent:Step: 5, Training loss: 8.330248832702637
INFO:agents.father_agent:Step: 10, Training loss: 7.667562484741211
INFO:agents.father_agent:Step: 15, Training loss: 5.734113693237305
INFO:agents.father_agent:Step: 20, Training loss: 9.751160621643066
INFO:agents.father_agent:Step: 25, Training loss: 6.745364665985107
INFO:agents.father_agent:Step: 30, Training loss: 6.994411468505859
INFO:agents.father_agent:Step: 35, Training loss: 7.404919147491455
INFO:agents.father_agent:Step: 40, Training loss: 5.2286834716796875
INFO:agents.father_agent:Step: 45, Training loss: 9.438570022583008
INFO:agents.father_agent:Step: 50, Training loss: 6.5734124183654785
INFO:agents.father_agent:Step: 55, Training loss: 7.225346088409424
INFO:agents.father_agent:Step: 60, Training loss: 7.668219566345215
INFO:agents.father_agent:Step: 65, Training loss: 7.026156425476074
INFO:agents.father_agent:Step: 70, Training loss: 9.412274360656738
INFO:agents.father_agent:Step: 75, Training loss: 7.868618011474609
INFO:agents.father_agent:Step: 80, Training loss: 6.283116340637207
INFO:agents.father_agent:Step: 85, Training loss: 10.367999076843262
INFO:agents.father_agent:Step: 90, Training loss: 8.1660737991333
INFO:agents.father_agent:Step: 95, Training loss: 6.719328880310059
INFO:agents.father_agent:Step: 100, Training loss: 6.6920318603515625
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9321272373199463
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.32127380371094
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.75306701660156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.518155574798584
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.01785503133499
INFO:tools.evaluation_results_class:Counted Episodes = 8457
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 6.22281551361084
INFO:agents.father_agent:Step: 110, Training loss: 7.873537540435791
INFO:agents.father_agent:Step: 115, Training loss: 5.946705341339111
INFO:agents.father_agent:Step: 120, Training loss: 5.0041913986206055
INFO:agents.father_agent:Step: 125, Training loss: 8.537503242492676
INFO:agents.father_agent:Step: 130, Training loss: 5.8376898765563965
INFO:agents.father_agent:Step: 135, Training loss: 7.453362941741943
INFO:agents.father_agent:Step: 140, Training loss: 6.6148786544799805
INFO:agents.father_agent:Step: 145, Training loss: 6.023221015930176
INFO:agents.father_agent:Step: 150, Training loss: 9.769789695739746
INFO:agents.father_agent:Step: 155, Training loss: 6.181936264038086
INFO:agents.father_agent:Step: 160, Training loss: 7.790012359619141
INFO:agents.father_agent:Step: 165, Training loss: 7.726555824279785
INFO:agents.father_agent:Step: 170, Training loss: 5.2803544998168945
INFO:agents.father_agent:Step: 175, Training loss: 9.969145774841309
INFO:agents.father_agent:Step: 180, Training loss: 5.932159900665283
INFO:agents.father_agent:Step: 185, Training loss: 4.8499250411987305
INFO:agents.father_agent:Step: 190, Training loss: 8.831583976745605
INFO:agents.father_agent:Step: 195, Training loss: 6.735345363616943
INFO:agents.father_agent:Step: 200, Training loss: 7.867538928985596
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.935792922973633
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.35792922973633
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.833377838134766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.5007848739624023
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.01785503133499
INFO:tools.evaluation_results_class:Counted Episodes = 8457
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 5.077755928039551
INFO:agents.father_agent:Step: 210, Training loss: 7.607669353485107
INFO:agents.father_agent:Step: 215, Training loss: 5.772989749908447
INFO:agents.father_agent:Step: 220, Training loss: 5.2806243896484375
INFO:agents.father_agent:Step: 225, Training loss: 9.180622100830078
INFO:agents.father_agent:Step: 230, Training loss: 5.7948150634765625
INFO:agents.father_agent:Step: 235, Training loss: 7.540974140167236
INFO:agents.father_agent:Step: 240, Training loss: 6.343990325927734
INFO:agents.father_agent:Step: 245, Training loss: 6.433342933654785
INFO:agents.father_agent:Step: 250, Training loss: 9.63245964050293
INFO:agents.father_agent:Step: 255, Training loss: 6.604213714599609
INFO:agents.father_agent:Step: 260, Training loss: 8.124626159667969
INFO:agents.father_agent:Step: 265, Training loss: 7.223119258880615
INFO:agents.father_agent:Step: 270, Training loss: 5.552745342254639
INFO:agents.father_agent:Step: 275, Training loss: 9.888229370117188
INFO:agents.father_agent:Step: 280, Training loss: 6.342864513397217
INFO:agents.father_agent:Step: 285, Training loss: 5.0029401779174805
INFO:agents.father_agent:Step: 290, Training loss: 8.751822471618652
INFO:agents.father_agent:Step: 295, Training loss: 6.898906707763672
INFO:agents.father_agent:Step: 300, Training loss: 8.154959678649902
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.879153251647949
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.791534423828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.334228515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.5214016437530518
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.01785503133499
INFO:tools.evaluation_results_class:Counted Episodes = 8457
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 6.125875949859619
INFO:agents.father_agent:Step: 310, Training loss: 7.287238597869873
INFO:agents.father_agent:Step: 315, Training loss: 6.001137733459473
INFO:agents.father_agent:Step: 320, Training loss: 4.853942394256592
INFO:agents.father_agent:Step: 325, Training loss: 8.878273010253906
INFO:agents.father_agent:Step: 330, Training loss: 6.0000996589660645
INFO:agents.father_agent:Step: 335, Training loss: 7.525752067565918
INFO:agents.father_agent:Step: 340, Training loss: 7.080361843109131
INFO:agents.father_agent:Step: 345, Training loss: 6.89122200012207
INFO:agents.father_agent:Step: 350, Training loss: 8.717740058898926
INFO:agents.father_agent:Step: 355, Training loss: 7.460745334625244
INFO:agents.father_agent:Step: 360, Training loss: 6.482461452484131
INFO:agents.father_agent:Step: 365, Training loss: 8.110943794250488
INFO:agents.father_agent:Step: 370, Training loss: 5.6157636642456055
INFO:agents.father_agent:Step: 375, Training loss: 12.211925506591797
INFO:agents.father_agent:Step: 380, Training loss: 5.824305057525635
INFO:agents.father_agent:Step: 385, Training loss: 4.507104873657227
INFO:agents.father_agent:Step: 390, Training loss: 9.704183578491211
INFO:agents.father_agent:Step: 395, Training loss: 6.544467449188232
INFO:agents.father_agent:Step: 400, Training loss: 8.368202209472656
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9191203117370605
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.19120407104492
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.64487075805664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.542234420776367
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.01785503133499
INFO:tools.evaluation_results_class:Counted Episodes = 8457
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 5.858969211578369
INFO:agents.father_agent:Step: 410, Training loss: 8.149328231811523
INFO:agents.father_agent:Step: 415, Training loss: 6.334781646728516
INFO:agents.father_agent:Step: 420, Training loss: 4.299221038818359
INFO:agents.father_agent:Step: 425, Training loss: 9.87716007232666
INFO:agents.father_agent:Step: 430, Training loss: 6.174161911010742
INFO:agents.father_agent:Step: 435, Training loss: 7.52056360244751
INFO:agents.father_agent:Step: 440, Training loss: 6.9818925857543945
INFO:agents.father_agent:Step: 445, Training loss: 6.128872394561768
INFO:agents.father_agent:Step: 450, Training loss: 9.505453109741211
INFO:agents.father_agent:Step: 455, Training loss: 6.47317361831665
INFO:agents.father_agent:Step: 460, Training loss: 6.27708101272583
INFO:agents.father_agent:Step: 465, Training loss: 8.060705184936523
INFO:agents.father_agent:Step: 470, Training loss: 5.340506553649902
INFO:agents.father_agent:Step: 475, Training loss: 9.202858924865723
INFO:agents.father_agent:Step: 480, Training loss: 6.53425931930542
INFO:agents.father_agent:Step: 485, Training loss: 3.921677350997925
INFO:agents.father_agent:Step: 490, Training loss: 9.016637802124023
INFO:agents.father_agent:Step: 495, Training loss: 6.602774143218994
INFO:agents.father_agent:Step: 500, Training loss: 8.70166015625
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8842380046844482
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.84238052368164
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.39542770385742
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.4231605529785156
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.01785503133499
INFO:tools.evaluation_results_class:Counted Episodes = 8457
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.9386308193206787
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.38630676269531
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.81618881225586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.542053699493408
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 38.01785503133499
INFO:tools.evaluation_results_class:Counted Episodes = 8457
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.9046690464019775
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.04669189453125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.57957458496094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.3066165447235107
INFO:tools.evaluation_results_class:Current Best Return = 3.9046690464019775
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.961798783985316
INFO:tools.evaluation_results_class:Counted Episodes = 8717
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.862338066101074
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.62337875366211
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.25027847290039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.4237473011016846
INFO:tools.evaluation_results_class:Current Best Return = 3.862338066101074
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.961798783985316
INFO:tools.evaluation_results_class:Counted Episodes = 8717
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 16.2583 achieved after 2196.89 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.9357 achieved after 2196.91 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.6129 achieved after 2196.96 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.1734 achieved after 2197.0 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 7.5668 achieved after 2197.04 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 6.7211 achieved after 2197.07 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 5.5282 achieved after 2197.08 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 3.7544 achieved after 2197.09 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.754425786950337
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:robust_rl.robust_rl_trainer:Iteration 5 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.869964122772217
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.69963836669922
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.49467849731445
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.091647148132324
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.33066295135261
INFO:tools.evaluation_results_class:Counted Episodes = 8613
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.184844970703125
INFO:agents.father_agent:Step: 5, Training loss: 9.366878509521484
INFO:agents.father_agent:Step: 10, Training loss: 5.833259105682373
INFO:agents.father_agent:Step: 15, Training loss: 4.755753517150879
INFO:agents.father_agent:Step: 20, Training loss: 8.745116233825684
INFO:agents.father_agent:Step: 25, Training loss: 5.227350234985352
INFO:agents.father_agent:Step: 30, Training loss: 6.3989715576171875
INFO:agents.father_agent:Step: 35, Training loss: 5.956223011016846
INFO:agents.father_agent:Step: 40, Training loss: 3.975506544113159
INFO:agents.father_agent:Step: 45, Training loss: 10.906702995300293
INFO:agents.father_agent:Step: 50, Training loss: 4.813185214996338
INFO:agents.father_agent:Step: 55, Training loss: 8.650592803955078
INFO:agents.father_agent:Step: 60, Training loss: 6.7321858406066895
INFO:agents.father_agent:Step: 65, Training loss: 5.3427414894104
INFO:agents.father_agent:Step: 70, Training loss: 9.437533378601074
INFO:agents.father_agent:Step: 75, Training loss: 5.478023052215576
INFO:agents.father_agent:Step: 80, Training loss: 6.211095333099365
INFO:agents.father_agent:Step: 85, Training loss: 9.192536354064941
INFO:agents.father_agent:Step: 90, Training loss: 5.547519207000732
INFO:agents.father_agent:Step: 95, Training loss: 7.799657821655273
INFO:agents.father_agent:Step: 100, Training loss: 5.4142608642578125
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8455822467803955
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.4558219909668
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.28487777709961
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.011566638946533
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.33066295135261
INFO:tools.evaluation_results_class:Counted Episodes = 8613
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 2.9011805057525635
INFO:agents.father_agent:Step: 110, Training loss: 7.80238676071167
INFO:agents.father_agent:Step: 115, Training loss: 4.169583320617676
INFO:agents.father_agent:Step: 120, Training loss: 2.7911078929901123
INFO:agents.father_agent:Step: 125, Training loss: 9.409956932067871
INFO:agents.father_agent:Step: 130, Training loss: 3.3733582496643066
INFO:agents.father_agent:Step: 135, Training loss: 9.517197608947754
INFO:agents.father_agent:Step: 140, Training loss: 4.967364311218262
INFO:agents.father_agent:Step: 145, Training loss: 4.004377841949463
INFO:agents.father_agent:Step: 150, Training loss: 10.998913764953613
INFO:agents.father_agent:Step: 155, Training loss: 4.829784870147705
INFO:agents.father_agent:Step: 160, Training loss: 9.666074752807617
INFO:agents.father_agent:Step: 165, Training loss: 5.57592248916626
INFO:agents.father_agent:Step: 170, Training loss: 3.555432081222534
INFO:agents.father_agent:Step: 175, Training loss: 14.292140007019043
INFO:agents.father_agent:Step: 180, Training loss: 4.856604099273682
INFO:agents.father_agent:Step: 185, Training loss: 2.7976388931274414
INFO:agents.father_agent:Step: 190, Training loss: 12.187124252319336
INFO:agents.father_agent:Step: 195, Training loss: 4.073938846588135
INFO:agents.father_agent:Step: 200, Training loss: 14.387188911437988
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8607919216156006
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.60791778564453
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.43046188354492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.0321710109710693
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.33066295135261
INFO:tools.evaluation_results_class:Counted Episodes = 8613
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 2.9904403686523438
INFO:agents.father_agent:Step: 210, Training loss: 9.822875022888184
INFO:agents.father_agent:Step: 215, Training loss: 4.812005043029785
INFO:agents.father_agent:Step: 220, Training loss: 2.3430700302124023
INFO:agents.father_agent:Step: 225, Training loss: 10.102492332458496
INFO:agents.father_agent:Step: 230, Training loss: 3.1692776679992676
INFO:agents.father_agent:Step: 235, Training loss: 10.038187026977539
INFO:agents.father_agent:Step: 240, Training loss: 4.662831783294678
INFO:agents.father_agent:Step: 245, Training loss: 3.9765148162841797
INFO:agents.father_agent:Step: 250, Training loss: 9.329374313354492
INFO:agents.father_agent:Step: 255, Training loss: 4.139962196350098
INFO:agents.father_agent:Step: 260, Training loss: 7.552330493927002
INFO:agents.father_agent:Step: 265, Training loss: 6.462860107421875
INFO:agents.father_agent:Step: 270, Training loss: 2.9297749996185303
INFO:agents.father_agent:Step: 275, Training loss: 12.691600799560547
INFO:agents.father_agent:Step: 280, Training loss: 3.8157546520233154
INFO:agents.father_agent:Step: 285, Training loss: 2.114419460296631
INFO:agents.father_agent:Step: 290, Training loss: 12.268378257751465
INFO:agents.father_agent:Step: 295, Training loss: 3.5489799976348877
INFO:agents.father_agent:Step: 300, Training loss: 15.073895454406738
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.923719882965088
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.23719787597656
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.89854049682617
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.0698812007904053
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.33066295135261
INFO:tools.evaluation_results_class:Counted Episodes = 8613
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 2.527456760406494
INFO:agents.father_agent:Step: 310, Training loss: 8.277128219604492
INFO:agents.father_agent:Step: 315, Training loss: 4.121854782104492
INFO:agents.father_agent:Step: 320, Training loss: 2.4464833736419678
INFO:agents.father_agent:Step: 325, Training loss: 13.046262741088867
INFO:agents.father_agent:Step: 330, Training loss: 2.4523465633392334
INFO:agents.father_agent:Step: 335, Training loss: 12.222023963928223
INFO:agents.father_agent:Step: 340, Training loss: 4.241297721862793
INFO:agents.father_agent:Step: 345, Training loss: 3.4636948108673096
INFO:agents.father_agent:Step: 350, Training loss: 9.648477554321289
INFO:agents.father_agent:Step: 355, Training loss: 3.472379684448242
INFO:agents.father_agent:Step: 360, Training loss: 7.516140937805176
INFO:agents.father_agent:Step: 365, Training loss: 5.217021942138672
INFO:agents.father_agent:Step: 370, Training loss: 2.9890971183776855
INFO:agents.father_agent:Step: 375, Training loss: 12.251391410827637
INFO:agents.father_agent:Step: 380, Training loss: 3.6179561614990234
INFO:agents.father_agent:Step: 385, Training loss: 2.0787513256073
INFO:agents.father_agent:Step: 390, Training loss: 12.12830638885498
INFO:agents.father_agent:Step: 395, Training loss: 3.140648603439331
INFO:agents.father_agent:Step: 400, Training loss: 12.570854187011719
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8587019443511963
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.58702087402344
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.40769577026367
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.181126356124878
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.33066295135261
INFO:tools.evaluation_results_class:Counted Episodes = 8613
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 2.2490339279174805
INFO:agents.father_agent:Step: 410, Training loss: 7.409260272979736
INFO:agents.father_agent:Step: 415, Training loss: 3.806365728378296
INFO:agents.father_agent:Step: 420, Training loss: 2.187223196029663
INFO:agents.father_agent:Step: 425, Training loss: 11.015110969543457
INFO:agents.father_agent:Step: 430, Training loss: 2.1715614795684814
INFO:agents.father_agent:Step: 435, Training loss: 10.905207633972168
INFO:agents.father_agent:Step: 440, Training loss: 4.442923545837402
INFO:agents.father_agent:Step: 445, Training loss: 2.9340732097625732
INFO:agents.father_agent:Step: 450, Training loss: 8.02698802947998
INFO:agents.father_agent:Step: 455, Training loss: 2.725099563598633
INFO:agents.father_agent:Step: 460, Training loss: 7.526796340942383
INFO:agents.father_agent:Step: 465, Training loss: 5.347140312194824
INFO:agents.father_agent:Step: 470, Training loss: 3.0113606452941895
INFO:agents.father_agent:Step: 475, Training loss: 12.77774715423584
INFO:agents.father_agent:Step: 480, Training loss: 3.580697774887085
INFO:agents.father_agent:Step: 485, Training loss: 2.219528913497925
INFO:agents.father_agent:Step: 490, Training loss: 11.760579109191895
INFO:agents.father_agent:Step: 495, Training loss: 2.345916748046875
INFO:agents.father_agent:Step: 500, Training loss: 12.877084732055664
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.912574052810669
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.12574005126953
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.84074020385742
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.000483989715576
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.33066295135261
INFO:tools.evaluation_results_class:Counted Episodes = 8613
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.861720561981201
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.61720657348633
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.41928482055664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.0760838985443115
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.33066295135261
INFO:tools.evaluation_results_class:Counted Episodes = 8613
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8204753398895264
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.20475387573242
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.09250259399414
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.9899580478668213
INFO:tools.evaluation_results_class:Current Best Return = 3.8204753398895264
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.29552877576304
INFO:tools.evaluation_results_class:Counted Episodes = 8879
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7723841667175293
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.72384262084961
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.738265991210938
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.1063170433044434
INFO:tools.evaluation_results_class:Current Best Return = 3.7723841667175293
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.29552877576304
INFO:tools.evaluation_results_class:Counted Episodes = 8879
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 16.2579 achieved after 2706.35 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.9356 achieved after 2706.37 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.6133 achieved after 2706.42 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.1748 achieved after 2706.46 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 7.5705 achieved after 2706.5 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 6.7285 achieved after 2706.53 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 5.5417 achieved after 2706.54 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 3.7768 achieved after 2706.55 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.776782549310915
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:robust_rl.robust_rl_trainer:Iteration 6 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7994229793548584
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.99422836303711
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.007354736328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8527867794036865
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.107097518753605
INFO:tools.evaluation_results_class:Counted Episodes = 8665
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.0719552040100098
INFO:agents.father_agent:Step: 5, Training loss: 10.187420845031738
INFO:agents.father_agent:Step: 10, Training loss: 2.722442150115967
INFO:agents.father_agent:Step: 15, Training loss: 3.170745849609375
INFO:agents.father_agent:Step: 20, Training loss: 5.987428188323975
INFO:agents.father_agent:Step: 25, Training loss: 4.149923801422119
INFO:agents.father_agent:Step: 30, Training loss: 7.623318195343018
INFO:agents.father_agent:Step: 35, Training loss: 4.01612663269043
INFO:agents.father_agent:Step: 40, Training loss: 1.9851418733596802
INFO:agents.father_agent:Step: 45, Training loss: 10.31527328491211
INFO:agents.father_agent:Step: 50, Training loss: 2.4069907665252686
INFO:agents.father_agent:Step: 55, Training loss: 10.531530380249023
INFO:agents.father_agent:Step: 60, Training loss: 4.507774829864502
INFO:agents.father_agent:Step: 65, Training loss: 3.3808224201202393
INFO:agents.father_agent:Step: 70, Training loss: 7.750626564025879
INFO:agents.father_agent:Step: 75, Training loss: 2.2791359424591064
INFO:agents.father_agent:Step: 80, Training loss: 2.8084237575531006
INFO:agents.father_agent:Step: 85, Training loss: 5.525795936584473
INFO:agents.father_agent:Step: 90, Training loss: 2.3808186054229736
INFO:agents.father_agent:Step: 95, Training loss: 7.531071662902832
INFO:agents.father_agent:Step: 100, Training loss: 3.794133186340332
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8244662284851074
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.24466323852539
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.16522979736328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8884029388427734
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.107097518753605
INFO:tools.evaluation_results_class:Counted Episodes = 8665
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 1.7234326601028442
INFO:agents.father_agent:Step: 110, Training loss: 7.43731164932251
INFO:agents.father_agent:Step: 115, Training loss: 3.899744987487793
INFO:agents.father_agent:Step: 120, Training loss: 2.182933807373047
INFO:agents.father_agent:Step: 125, Training loss: 10.208123207092285
INFO:agents.father_agent:Step: 130, Training loss: 2.192234992980957
INFO:agents.father_agent:Step: 135, Training loss: 11.12224292755127
INFO:agents.father_agent:Step: 140, Training loss: 4.118497371673584
INFO:agents.father_agent:Step: 145, Training loss: 3.2408154010772705
INFO:agents.father_agent:Step: 150, Training loss: 6.624578952789307
INFO:agents.father_agent:Step: 155, Training loss: 3.1423227787017822
INFO:agents.father_agent:Step: 160, Training loss: 6.0345458984375
INFO:agents.father_agent:Step: 165, Training loss: 4.488063812255859
INFO:agents.father_agent:Step: 170, Training loss: 3.414140224456787
INFO:agents.father_agent:Step: 175, Training loss: 13.898149490356445
INFO:agents.father_agent:Step: 180, Training loss: 4.749277591705322
INFO:agents.father_agent:Step: 185, Training loss: 2.6006100177764893
INFO:agents.father_agent:Step: 190, Training loss: 10.369091033935547
INFO:agents.father_agent:Step: 195, Training loss: 3.2039341926574707
INFO:agents.father_agent:Step: 200, Training loss: 11.122496604919434
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8024234771728516
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.024234771728516
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.0158576965332
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.875908374786377
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.107097518753605
INFO:tools.evaluation_results_class:Counted Episodes = 8665
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 3.05324387550354
INFO:agents.father_agent:Step: 210, Training loss: 5.667359828948975
INFO:agents.father_agent:Step: 215, Training loss: 3.371492862701416
INFO:agents.father_agent:Step: 220, Training loss: 2.5275940895080566
INFO:agents.father_agent:Step: 225, Training loss: 11.199350357055664
INFO:agents.father_agent:Step: 230, Training loss: 2.365687131881714
INFO:agents.father_agent:Step: 235, Training loss: 10.335440635681152
INFO:agents.father_agent:Step: 240, Training loss: 3.8401737213134766
INFO:agents.father_agent:Step: 245, Training loss: 4.029098033905029
INFO:agents.father_agent:Step: 250, Training loss: 5.855014324188232
INFO:agents.father_agent:Step: 255, Training loss: 2.824605703353882
INFO:agents.father_agent:Step: 260, Training loss: 5.255957126617432
INFO:agents.father_agent:Step: 265, Training loss: 4.979446887969971
INFO:agents.father_agent:Step: 270, Training loss: 4.367362976074219
INFO:agents.father_agent:Step: 275, Training loss: 13.508987426757812
INFO:agents.father_agent:Step: 280, Training loss: 4.268010139465332
INFO:agents.father_agent:Step: 285, Training loss: 2.6173501014709473
INFO:agents.father_agent:Step: 290, Training loss: 12.315828323364258
INFO:agents.father_agent:Step: 295, Training loss: 1.8914101123809814
INFO:agents.father_agent:Step: 300, Training loss: 11.858185768127441
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.892671585083008
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.92671585083008
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.79137420654297
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8985788822174072
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.107097518753605
INFO:tools.evaluation_results_class:Counted Episodes = 8665
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.6118640899658203
INFO:agents.father_agent:Step: 310, Training loss: 6.784079074859619
INFO:agents.father_agent:Step: 315, Training loss: 3.2925257682800293
INFO:agents.father_agent:Step: 320, Training loss: 3.0252492427825928
INFO:agents.father_agent:Step: 325, Training loss: 9.046988487243652
INFO:agents.father_agent:Step: 330, Training loss: 2.787785530090332
INFO:agents.father_agent:Step: 335, Training loss: 11.32198715209961
INFO:agents.father_agent:Step: 340, Training loss: 3.531095266342163
INFO:agents.father_agent:Step: 345, Training loss: 3.324106454849243
INFO:agents.father_agent:Step: 350, Training loss: 5.30120849609375
INFO:agents.father_agent:Step: 355, Training loss: 2.636777877807617
INFO:agents.father_agent:Step: 360, Training loss: 5.0140700340271
INFO:agents.father_agent:Step: 365, Training loss: 4.038641452789307
INFO:agents.father_agent:Step: 370, Training loss: 4.016721725463867
INFO:agents.father_agent:Step: 375, Training loss: 12.353010177612305
INFO:agents.father_agent:Step: 380, Training loss: 4.745486259460449
INFO:agents.father_agent:Step: 385, Training loss: 2.8323051929473877
INFO:agents.father_agent:Step: 390, Training loss: 11.753619194030762
INFO:agents.father_agent:Step: 395, Training loss: 1.864595651626587
INFO:agents.father_agent:Step: 400, Training loss: 12.34022331237793
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8650894165039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.65089416503906
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.56306838989258
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.923403024673462
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.107097518753605
INFO:tools.evaluation_results_class:Counted Episodes = 8665
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.6296924352645874
INFO:agents.father_agent:Step: 410, Training loss: 5.834266185760498
INFO:agents.father_agent:Step: 415, Training loss: 3.2990689277648926
INFO:agents.father_agent:Step: 420, Training loss: 2.7108311653137207
INFO:agents.father_agent:Step: 425, Training loss: 10.561236381530762
INFO:agents.father_agent:Step: 430, Training loss: 2.2804105281829834
INFO:agents.father_agent:Step: 435, Training loss: 9.920533180236816
INFO:agents.father_agent:Step: 440, Training loss: 3.1620330810546875
INFO:agents.father_agent:Step: 445, Training loss: 3.7134437561035156
INFO:agents.father_agent:Step: 450, Training loss: 5.297243595123291
INFO:agents.father_agent:Step: 455, Training loss: 2.8032703399658203
INFO:agents.father_agent:Step: 460, Training loss: 5.255821228027344
INFO:agents.father_agent:Step: 465, Training loss: 4.19777250289917
INFO:agents.father_agent:Step: 470, Training loss: 4.638354778289795
INFO:agents.father_agent:Step: 475, Training loss: 12.550248146057129
INFO:agents.father_agent:Step: 480, Training loss: 4.353201389312744
INFO:agents.father_agent:Step: 485, Training loss: 2.3764891624450684
INFO:agents.father_agent:Step: 490, Training loss: 10.192273139953613
INFO:agents.father_agent:Step: 495, Training loss: 1.5773277282714844
INFO:agents.father_agent:Step: 500, Training loss: 13.884888648986816
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.880669355392456
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.80669403076172
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.649742126464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.0345773696899414
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.107097518753605
INFO:tools.evaluation_results_class:Counted Episodes = 8665
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.861396312713623
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.61396408081055
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.5076789855957
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3.038261651992798
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.107097518753605
INFO:tools.evaluation_results_class:Counted Episodes = 8665
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7983880043029785
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.98387908935547
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.044403076171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.865095615386963
INFO:tools.evaluation_results_class:Current Best Return = 3.7983880043029785
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.07880891078025
INFO:tools.evaluation_results_class:Counted Episodes = 8933
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7768945693969727
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.76894760131836
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.82832908630371
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.83917498588562
INFO:tools.evaluation_results_class:Current Best Return = 3.7768945693969727
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 37.07880891078025
INFO:tools.evaluation_results_class:Counted Episodes = 8933
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 16.2579 achieved after 3234.13 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.9353 achieved after 3234.14 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.6118 achieved after 3234.19 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.1709 achieved after 3234.24 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 7.562 achieved after 3234.27 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 6.7121 achieved after 3234.3 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 5.5117 achieved after 3234.32 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 3.7216 achieved after 3234.32 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.721624875113317
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:robust_rl.robust_rl_trainer:Iteration 7 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.839949369430542
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.39949417114258
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.35537338256836
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8116867542266846
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.99631803014613
INFO:tools.evaluation_results_class:Counted Episodes = 8691
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.351035118103027
INFO:agents.father_agent:Step: 5, Training loss: 7.762302875518799
INFO:agents.father_agent:Step: 10, Training loss: 3.0993940830230713
INFO:agents.father_agent:Step: 15, Training loss: 4.141723155975342
INFO:agents.father_agent:Step: 20, Training loss: 13.483725547790527
INFO:agents.father_agent:Step: 25, Training loss: 7.055055141448975
INFO:agents.father_agent:Step: 30, Training loss: 6.958075523376465
INFO:agents.father_agent:Step: 35, Training loss: 4.366670608520508
INFO:agents.father_agent:Step: 40, Training loss: 3.3665308952331543
INFO:agents.father_agent:Step: 45, Training loss: 14.611316680908203
INFO:agents.father_agent:Step: 50, Training loss: 4.839386463165283
INFO:agents.father_agent:Step: 55, Training loss: 2.4558610916137695
INFO:agents.father_agent:Step: 60, Training loss: 11.86650276184082
INFO:agents.father_agent:Step: 65, Training loss: 1.7124996185302734
INFO:agents.father_agent:Step: 70, Training loss: 13.9921293258667
INFO:agents.father_agent:Step: 75, Training loss: 5.0304341316223145
INFO:agents.father_agent:Step: 80, Training loss: 2.4430525302886963
INFO:agents.father_agent:Step: 85, Training loss: 10.067699432373047
INFO:agents.father_agent:Step: 90, Training loss: 3.1197173595428467
INFO:agents.father_agent:Step: 95, Training loss: 2.4358372688293457
INFO:agents.father_agent:Step: 100, Training loss: 8.582883834838867
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.814635753631592
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.146358489990234
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.14222717285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8611643314361572
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.99631803014613
INFO:tools.evaluation_results_class:Counted Episodes = 8691
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 1.4764988422393799
INFO:agents.father_agent:Step: 110, Training loss: 10.533841133117676
INFO:agents.father_agent:Step: 115, Training loss: 3.332817792892456
INFO:agents.father_agent:Step: 120, Training loss: 3.2956466674804688
INFO:agents.father_agent:Step: 125, Training loss: 6.594254970550537
INFO:agents.father_agent:Step: 130, Training loss: 2.0587899684906006
INFO:agents.father_agent:Step: 135, Training loss: 3.097724199295044
INFO:agents.father_agent:Step: 140, Training loss: 5.531781196594238
INFO:agents.father_agent:Step: 145, Training loss: 1.865691065788269
INFO:agents.father_agent:Step: 150, Training loss: 6.0790696144104
INFO:agents.father_agent:Step: 155, Training loss: 3.321206569671631
INFO:agents.father_agent:Step: 160, Training loss: 6.710293292999268
INFO:agents.father_agent:Step: 165, Training loss: 6.259959697723389
INFO:agents.father_agent:Step: 170, Training loss: 2.4101908206939697
INFO:agents.father_agent:Step: 175, Training loss: 2.441179037094116
INFO:agents.father_agent:Step: 180, Training loss: 5.868897438049316
INFO:agents.father_agent:Step: 185, Training loss: 3.8482282161712646
INFO:agents.father_agent:Step: 190, Training loss: 6.5640716552734375
INFO:agents.father_agent:Step: 195, Training loss: 3.353949546813965
INFO:agents.father_agent:Step: 200, Training loss: 3.5827982425689697
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.841330051422119
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.413299560546875
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.37933349609375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.9264979362487793
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.99631803014613
INFO:tools.evaluation_results_class:Counted Episodes = 8691
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 1.884299874305725
INFO:agents.father_agent:Step: 210, Training loss: 10.92070198059082
INFO:agents.father_agent:Step: 215, Training loss: 3.2644007205963135
INFO:agents.father_agent:Step: 220, Training loss: 3.203831911087036
INFO:agents.father_agent:Step: 225, Training loss: 6.462731838226318
INFO:agents.father_agent:Step: 230, Training loss: 2.1504132747650146
INFO:agents.father_agent:Step: 235, Training loss: 3.5920844078063965
INFO:agents.father_agent:Step: 240, Training loss: 6.532120227813721
INFO:agents.father_agent:Step: 245, Training loss: 1.8902981281280518
INFO:agents.father_agent:Step: 250, Training loss: 8.181938171386719
INFO:agents.father_agent:Step: 255, Training loss: 3.254882574081421
INFO:agents.father_agent:Step: 260, Training loss: 6.065718173980713
INFO:agents.father_agent:Step: 265, Training loss: 7.387580871582031
INFO:agents.father_agent:Step: 270, Training loss: 2.1737990379333496
INFO:agents.father_agent:Step: 275, Training loss: 2.9508514404296875
INFO:agents.father_agent:Step: 280, Training loss: 6.051645755767822
INFO:agents.father_agent:Step: 285, Training loss: 3.1119422912597656
INFO:agents.father_agent:Step: 290, Training loss: 7.01229190826416
INFO:agents.father_agent:Step: 295, Training loss: 2.9443836212158203
INFO:agents.father_agent:Step: 300, Training loss: 3.251077890396118
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7968013286590576
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.968013763427734
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.014373779296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.845374584197998
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.99631803014613
INFO:tools.evaluation_results_class:Counted Episodes = 8691
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.9199610948562622
INFO:agents.father_agent:Step: 310, Training loss: 9.202116966247559
INFO:agents.father_agent:Step: 315, Training loss: 3.3946142196655273
INFO:agents.father_agent:Step: 320, Training loss: 2.981001853942871
INFO:agents.father_agent:Step: 325, Training loss: 6.183464050292969
INFO:agents.father_agent:Step: 330, Training loss: 1.935246229171753
INFO:agents.father_agent:Step: 335, Training loss: 2.8356082439422607
INFO:agents.father_agent:Step: 340, Training loss: 4.720474720001221
INFO:agents.father_agent:Step: 345, Training loss: 2.13576602935791
INFO:agents.father_agent:Step: 350, Training loss: 6.501455783843994
INFO:agents.father_agent:Step: 355, Training loss: 3.2487545013427734
INFO:agents.father_agent:Step: 360, Training loss: 5.498256683349609
INFO:agents.father_agent:Step: 365, Training loss: 5.850615978240967
INFO:agents.father_agent:Step: 370, Training loss: 2.193014621734619
INFO:agents.father_agent:Step: 375, Training loss: 2.5325684547424316
INFO:agents.father_agent:Step: 380, Training loss: 5.278538227081299
INFO:agents.father_agent:Step: 385, Training loss: 3.509160041809082
INFO:agents.father_agent:Step: 390, Training loss: 6.478369235992432
INFO:agents.father_agent:Step: 395, Training loss: 2.803030490875244
INFO:agents.father_agent:Step: 400, Training loss: 3.116391181945801
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8259117603302
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.259117126464844
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.24671936035156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.91538405418396
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.99631803014613
INFO:tools.evaluation_results_class:Counted Episodes = 8691
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.9010354280471802
INFO:agents.father_agent:Step: 410, Training loss: 9.008338928222656
INFO:agents.father_agent:Step: 415, Training loss: 3.1708271503448486
INFO:agents.father_agent:Step: 420, Training loss: 3.2235348224639893
INFO:agents.father_agent:Step: 425, Training loss: 7.316220760345459
INFO:agents.father_agent:Step: 430, Training loss: 1.9656522274017334
INFO:agents.father_agent:Step: 435, Training loss: 4.051978588104248
INFO:agents.father_agent:Step: 440, Training loss: 6.862582206726074
INFO:agents.father_agent:Step: 445, Training loss: 2.045027494430542
INFO:agents.father_agent:Step: 450, Training loss: 8.094766616821289
INFO:agents.father_agent:Step: 455, Training loss: 2.8704731464385986
INFO:agents.father_agent:Step: 460, Training loss: 4.842562675476074
INFO:agents.father_agent:Step: 465, Training loss: 7.381604194641113
INFO:agents.father_agent:Step: 470, Training loss: 2.3139169216156006
INFO:agents.father_agent:Step: 475, Training loss: 2.542762041091919
INFO:agents.father_agent:Step: 480, Training loss: 4.304431438446045
INFO:agents.father_agent:Step: 485, Training loss: 2.9253175258636475
INFO:agents.father_agent:Step: 490, Training loss: 6.877511024475098
INFO:agents.father_agent:Step: 495, Training loss: 2.92153263092041
INFO:agents.father_agent:Step: 500, Training loss: 2.857797861099243
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.857208490371704
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.572086334228516
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.47652816772461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.783085346221924
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.99631803014613
INFO:tools.evaluation_results_class:Counted Episodes = 8691
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.838223457336426
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.382232666015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.35893630981445
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.866245746612549
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.99631803014613
INFO:tools.evaluation_results_class:Counted Episodes = 8691
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.807142972946167
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.07143020629883
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.09839630126953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8654847145080566
INFO:tools.evaluation_results_class:Current Best Return = 3.807142972946167
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.97142857142857
INFO:tools.evaluation_results_class:Counted Episodes = 8960
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8623883724212646
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.62388229370117
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.54190444946289
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.781843900680542
INFO:tools.evaluation_results_class:Current Best Return = 3.8623883724212646
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.97142857142857
INFO:tools.evaluation_results_class:Counted Episodes = 8960
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 16.2639 achieved after 3766.37 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.9388 achieved after 3766.38 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.6181 achieved after 3766.44 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.1823 achieved after 3766.49 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 7.5826 achieved after 3766.53 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 6.7486 achieved after 3766.57 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 5.5758 achieved after 3766.58 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 3.8354 achieved after 3766.59 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.83538283968006
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:robust_rl.robust_rl_trainer:Iteration 8 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8816635608673096
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.81663513183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.696224212646484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8026325702667236
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.339686632156372
INFO:agents.father_agent:Step: 5, Training loss: 6.091845989227295
INFO:agents.father_agent:Step: 10, Training loss: 2.975306987762451
INFO:agents.father_agent:Step: 15, Training loss: 2.874542236328125
INFO:agents.father_agent:Step: 20, Training loss: 12.467548370361328
INFO:agents.father_agent:Step: 25, Training loss: 6.303670406341553
INFO:agents.father_agent:Step: 30, Training loss: 5.2053704261779785
INFO:agents.father_agent:Step: 35, Training loss: 4.006241321563721
INFO:agents.father_agent:Step: 40, Training loss: 3.2556958198547363
INFO:agents.father_agent:Step: 45, Training loss: 11.870099067687988
INFO:agents.father_agent:Step: 50, Training loss: 3.2202563285827637
INFO:agents.father_agent:Step: 55, Training loss: 2.126706123352051
INFO:agents.father_agent:Step: 60, Training loss: 11.05693531036377
INFO:agents.father_agent:Step: 65, Training loss: 2.0939278602600098
INFO:agents.father_agent:Step: 70, Training loss: 10.66547966003418
INFO:agents.father_agent:Step: 75, Training loss: 5.602288722991943
INFO:agents.father_agent:Step: 80, Training loss: 2.3250741958618164
INFO:agents.father_agent:Step: 85, Training loss: 12.14225959777832
INFO:agents.father_agent:Step: 90, Training loss: 2.3026278018951416
INFO:agents.father_agent:Step: 95, Training loss: 2.6466708183288574
INFO:agents.father_agent:Step: 100, Training loss: 8.553017616271973
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.853860378265381
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.538604736328125
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.46595764160156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8573198318481445
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 1.4778220653533936
INFO:agents.father_agent:Step: 110, Training loss: 8.372349739074707
INFO:agents.father_agent:Step: 115, Training loss: 3.434147834777832
INFO:agents.father_agent:Step: 120, Training loss: 2.454637289047241
INFO:agents.father_agent:Step: 125, Training loss: 7.3394012451171875
INFO:agents.father_agent:Step: 130, Training loss: 1.850752592086792
INFO:agents.father_agent:Step: 135, Training loss: 3.023620843887329
INFO:agents.father_agent:Step: 140, Training loss: 7.101358413696289
INFO:agents.father_agent:Step: 145, Training loss: 1.958263635635376
INFO:agents.father_agent:Step: 150, Training loss: 8.353581428527832
INFO:agents.father_agent:Step: 155, Training loss: 3.042348861694336
INFO:agents.father_agent:Step: 160, Training loss: 4.254422664642334
INFO:agents.father_agent:Step: 165, Training loss: 7.711719989776611
INFO:agents.father_agent:Step: 170, Training loss: 2.134114980697632
INFO:agents.father_agent:Step: 175, Training loss: 2.671074867248535
INFO:agents.father_agent:Step: 180, Training loss: 4.393424034118652
INFO:agents.father_agent:Step: 185, Training loss: 3.099900484085083
INFO:agents.father_agent:Step: 190, Training loss: 7.367100715637207
INFO:agents.father_agent:Step: 195, Training loss: 2.8814697265625
INFO:agents.father_agent:Step: 200, Training loss: 2.761329174041748
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.859145164489746
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.591453552246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.498905181884766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8275864124298096
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 1.905538558959961
INFO:agents.father_agent:Step: 210, Training loss: 9.92701530456543
INFO:agents.father_agent:Step: 215, Training loss: 2.9975533485412598
INFO:agents.father_agent:Step: 220, Training loss: 3.1029083728790283
INFO:agents.father_agent:Step: 225, Training loss: 10.172311782836914
INFO:agents.father_agent:Step: 230, Training loss: 2.242030620574951
INFO:agents.father_agent:Step: 235, Training loss: 2.733243703842163
INFO:agents.father_agent:Step: 240, Training loss: 5.353206157684326
INFO:agents.father_agent:Step: 245, Training loss: 2.4664249420166016
INFO:agents.father_agent:Step: 250, Training loss: 5.877459526062012
INFO:agents.father_agent:Step: 255, Training loss: 3.09617280960083
INFO:agents.father_agent:Step: 260, Training loss: 4.611827850341797
INFO:agents.father_agent:Step: 265, Training loss: 7.6818528175354
INFO:agents.father_agent:Step: 270, Training loss: 1.8640865087509155
INFO:agents.father_agent:Step: 275, Training loss: 2.6533732414245605
INFO:agents.father_agent:Step: 280, Training loss: 4.630881309509277
INFO:agents.father_agent:Step: 285, Training loss: 3.553647994995117
INFO:agents.father_agent:Step: 290, Training loss: 7.323150634765625
INFO:agents.father_agent:Step: 295, Training loss: 3.2856993675231934
INFO:agents.father_agent:Step: 300, Training loss: 2.661482334136963
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8096277713775635
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.09627914428711
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.1450080871582
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.750178337097168
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 2.198657989501953
INFO:agents.father_agent:Step: 310, Training loss: 7.465720176696777
INFO:agents.father_agent:Step: 315, Training loss: 3.7119176387786865
INFO:agents.father_agent:Step: 320, Training loss: 3.4236159324645996
INFO:agents.father_agent:Step: 325, Training loss: 6.1355671882629395
INFO:agents.father_agent:Step: 330, Training loss: 2.0257012844085693
INFO:agents.father_agent:Step: 335, Training loss: 3.1152124404907227
INFO:agents.father_agent:Step: 340, Training loss: 6.705038547515869
INFO:agents.father_agent:Step: 345, Training loss: 1.9561158418655396
INFO:agents.father_agent:Step: 350, Training loss: 7.635452747344971
INFO:agents.father_agent:Step: 355, Training loss: 3.16263747215271
INFO:agents.father_agent:Step: 360, Training loss: 4.724668979644775
INFO:agents.father_agent:Step: 365, Training loss: 7.359150409698486
INFO:agents.father_agent:Step: 370, Training loss: 1.872562050819397
INFO:agents.father_agent:Step: 375, Training loss: 3.3910412788391113
INFO:agents.father_agent:Step: 380, Training loss: 6.002757549285889
INFO:agents.father_agent:Step: 385, Training loss: 2.760039806365967
INFO:agents.father_agent:Step: 390, Training loss: 7.030352592468262
INFO:agents.father_agent:Step: 395, Training loss: 2.9017789363861084
INFO:agents.father_agent:Step: 400, Training loss: 3.0438616275787354
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8466222286224365
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.46622085571289
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.406009674072266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.804715156555176
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 2.0689094066619873
INFO:agents.father_agent:Step: 410, Training loss: 9.60531997680664
INFO:agents.father_agent:Step: 415, Training loss: 3.338715076446533
INFO:agents.father_agent:Step: 420, Training loss: 2.7540719509124756
INFO:agents.father_agent:Step: 425, Training loss: 7.6281609535217285
INFO:agents.father_agent:Step: 430, Training loss: 1.7081959247589111
INFO:agents.father_agent:Step: 435, Training loss: 2.937425374984741
INFO:agents.father_agent:Step: 440, Training loss: 6.904670715332031
INFO:agents.father_agent:Step: 445, Training loss: 2.0758419036865234
INFO:agents.father_agent:Step: 450, Training loss: 7.204975128173828
INFO:agents.father_agent:Step: 455, Training loss: 2.5588788986206055
INFO:agents.father_agent:Step: 460, Training loss: 4.821878910064697
INFO:agents.father_agent:Step: 465, Training loss: 6.065908908843994
INFO:agents.father_agent:Step: 470, Training loss: 2.3314502239227295
INFO:agents.father_agent:Step: 475, Training loss: 2.610057830810547
INFO:agents.father_agent:Step: 480, Training loss: 4.337525844573975
INFO:agents.father_agent:Step: 485, Training loss: 3.9324100017547607
INFO:agents.father_agent:Step: 490, Training loss: 5.88637113571167
INFO:agents.father_agent:Step: 495, Training loss: 2.4900970458984375
INFO:agents.father_agent:Step: 500, Training loss: 2.731175661087036
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.845588207244873
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.45588302612305
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.41742706298828
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.752581834793091
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.8232996463775635
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.23299789428711
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.233802795410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.766111373901367
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8306920528411865
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.30691909790039
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.28352355957031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7232322692871094
INFO:tools.evaluation_results_class:Current Best Return = 3.8306920528411865
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.97142857142857
INFO:tools.evaluation_results_class:Counted Episodes = 8960
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.806919574737549
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.06919479370117
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.079185485839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.795085906982422
INFO:tools.evaluation_results_class:Current Best Return = 3.806919574737549
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.97142857142857
INFO:tools.evaluation_results_class:Counted Episodes = 8960
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 16.2649 achieved after 4296.76 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.9394 achieved after 4296.78 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.6187 achieved after 4296.84 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.1829 achieved after 4296.89 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 7.5832 achieved after 4296.93 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 6.7491 achieved after 4296.96 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 5.5763 achieved after 4296.98 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 3.8355 achieved after 4296.99 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.8354580153400044
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:robust_rl.robust_rl_trainer:Iteration 9 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8130743503570557
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.13074493408203
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.14689254760742
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8151278495788574
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.075714111328125
INFO:agents.father_agent:Step: 5, Training loss: 7.18493127822876
INFO:agents.father_agent:Step: 10, Training loss: 2.850421667098999
INFO:agents.father_agent:Step: 15, Training loss: 2.878507137298584
INFO:agents.father_agent:Step: 20, Training loss: 9.255040168762207
INFO:agents.father_agent:Step: 25, Training loss: 4.926477432250977
INFO:agents.father_agent:Step: 30, Training loss: 7.1120219230651855
INFO:agents.father_agent:Step: 35, Training loss: 5.008702754974365
INFO:agents.father_agent:Step: 40, Training loss: 2.4684805870056152
INFO:agents.father_agent:Step: 45, Training loss: 11.59588623046875
INFO:agents.father_agent:Step: 50, Training loss: 2.861881971359253
INFO:agents.father_agent:Step: 55, Training loss: 1.9208173751831055
INFO:agents.father_agent:Step: 60, Training loss: 11.151726722717285
INFO:agents.father_agent:Step: 65, Training loss: 1.6069893836975098
INFO:agents.father_agent:Step: 70, Training loss: 10.957202911376953
INFO:agents.father_agent:Step: 75, Training loss: 5.338050842285156
INFO:agents.father_agent:Step: 80, Training loss: 2.2251527309417725
INFO:agents.father_agent:Step: 85, Training loss: 12.652589797973633
INFO:agents.father_agent:Step: 90, Training loss: 2.495358943939209
INFO:agents.father_agent:Step: 95, Training loss: 2.934440851211548
INFO:agents.father_agent:Step: 100, Training loss: 8.462738990783691
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.806410789489746
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.064109802246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.102664947509766
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.80684757232666
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 1.5137460231781006
INFO:agents.father_agent:Step: 110, Training loss: 9.11426830291748
INFO:agents.father_agent:Step: 115, Training loss: 3.148897647857666
INFO:agents.father_agent:Step: 120, Training loss: 2.93658185005188
INFO:agents.father_agent:Step: 125, Training loss: 9.106851577758789
INFO:agents.father_agent:Step: 130, Training loss: 1.9351190328598022
INFO:agents.father_agent:Step: 135, Training loss: 3.3156838417053223
INFO:agents.father_agent:Step: 140, Training loss: 6.511909008026123
INFO:agents.father_agent:Step: 145, Training loss: 1.935013771057129
INFO:agents.father_agent:Step: 150, Training loss: 7.3025360107421875
INFO:agents.father_agent:Step: 155, Training loss: 2.965163469314575
INFO:agents.father_agent:Step: 160, Training loss: 4.75372314453125
INFO:agents.father_agent:Step: 165, Training loss: 8.155389785766602
INFO:agents.father_agent:Step: 170, Training loss: 2.140310764312744
INFO:agents.father_agent:Step: 175, Training loss: 2.7767083644866943
INFO:agents.father_agent:Step: 180, Training loss: 5.237601280212402
INFO:agents.father_agent:Step: 185, Training loss: 3.501744270324707
INFO:agents.father_agent:Step: 190, Training loss: 6.639151096343994
INFO:agents.father_agent:Step: 195, Training loss: 2.9247326850891113
INFO:agents.father_agent:Step: 200, Training loss: 2.677396774291992
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.829733371734619
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.297332763671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.315128326416016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.854740858078003
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 2.105529308319092
INFO:agents.father_agent:Step: 210, Training loss: 9.213653564453125
INFO:agents.father_agent:Step: 215, Training loss: 2.9111552238464355
INFO:agents.father_agent:Step: 220, Training loss: 2.9085330963134766
INFO:agents.father_agent:Step: 225, Training loss: 5.5488972663879395
INFO:agents.father_agent:Step: 230, Training loss: 1.7617506980895996
INFO:agents.father_agent:Step: 235, Training loss: 3.1761691570281982
INFO:agents.father_agent:Step: 240, Training loss: 6.378402233123779
INFO:agents.father_agent:Step: 245, Training loss: 2.130831480026245
INFO:agents.father_agent:Step: 250, Training loss: 8.297378540039062
INFO:agents.father_agent:Step: 255, Training loss: 2.957550048828125
INFO:agents.father_agent:Step: 260, Training loss: 5.239718437194824
INFO:agents.father_agent:Step: 265, Training loss: 6.810388088226318
INFO:agents.father_agent:Step: 270, Training loss: 1.9899557828903198
INFO:agents.father_agent:Step: 275, Training loss: 2.6376192569732666
INFO:agents.father_agent:Step: 280, Training loss: 6.300259590148926
INFO:agents.father_agent:Step: 285, Training loss: 2.460141181945801
INFO:agents.father_agent:Step: 290, Training loss: 6.729541778564453
INFO:agents.father_agent:Step: 295, Training loss: 2.781092882156372
INFO:agents.father_agent:Step: 300, Training loss: 2.8620171546936035
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8329503536224365
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.32950210571289
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.33428192138672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8294014930725098
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.9633312225341797
INFO:agents.father_agent:Step: 310, Training loss: 7.531116962432861
INFO:agents.father_agent:Step: 315, Training loss: 3.0791587829589844
INFO:agents.father_agent:Step: 320, Training loss: 3.120920419692993
INFO:agents.father_agent:Step: 325, Training loss: 7.868830680847168
INFO:agents.father_agent:Step: 330, Training loss: 1.9314298629760742
INFO:agents.father_agent:Step: 335, Training loss: 3.1821796894073486
INFO:agents.father_agent:Step: 340, Training loss: 7.432191371917725
INFO:agents.father_agent:Step: 345, Training loss: 2.1333348751068115
INFO:agents.father_agent:Step: 350, Training loss: 7.5290985107421875
INFO:agents.father_agent:Step: 355, Training loss: 3.2461044788360596
INFO:agents.father_agent:Step: 360, Training loss: 4.796077728271484
INFO:agents.father_agent:Step: 365, Training loss: 6.443077564239502
INFO:agents.father_agent:Step: 370, Training loss: 1.8372430801391602
INFO:agents.father_agent:Step: 375, Training loss: 2.907008171081543
INFO:agents.father_agent:Step: 380, Training loss: 5.939875602722168
INFO:agents.father_agent:Step: 385, Training loss: 2.5283424854278564
INFO:agents.father_agent:Step: 390, Training loss: 7.295657157897949
INFO:agents.father_agent:Step: 395, Training loss: 2.7584869861602783
INFO:agents.father_agent:Step: 400, Training loss: 2.9145424365997314
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.816176414489746
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.161766052246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.201847076416016
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.793414831161499
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 2.138516426086426
INFO:agents.father_agent:Step: 410, Training loss: 7.815556526184082
INFO:agents.father_agent:Step: 415, Training loss: 3.496044635772705
INFO:agents.father_agent:Step: 420, Training loss: 3.3243658542633057
INFO:agents.father_agent:Step: 425, Training loss: 7.0143961906433105
INFO:agents.father_agent:Step: 430, Training loss: 1.7809702157974243
INFO:agents.father_agent:Step: 435, Training loss: 3.0725810527801514
INFO:agents.father_agent:Step: 440, Training loss: 6.2109375
INFO:agents.father_agent:Step: 445, Training loss: 1.9671781063079834
INFO:agents.father_agent:Step: 450, Training loss: 8.177705764770508
INFO:agents.father_agent:Step: 455, Training loss: 2.873392105102539
INFO:agents.father_agent:Step: 460, Training loss: 4.653843879699707
INFO:agents.father_agent:Step: 465, Training loss: 8.189990043640137
INFO:agents.father_agent:Step: 470, Training loss: 1.8379294872283936
INFO:agents.father_agent:Step: 475, Training loss: 3.020094394683838
INFO:agents.father_agent:Step: 480, Training loss: 5.241842746734619
INFO:agents.father_agent:Step: 485, Training loss: 3.0599541664123535
INFO:agents.father_agent:Step: 490, Training loss: 7.535806179046631
INFO:agents.father_agent:Step: 495, Training loss: 2.3767685890197754
INFO:agents.father_agent:Step: 500, Training loss: 2.502225875854492
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8038833141326904
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.03883361816406
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.10183334350586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.79965877532959
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.8073298931121826
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.073299407958984
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.13193130493164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8007686138153076
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8555803298950195
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.55580520629883
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.531246185302734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.776017904281616
INFO:tools.evaluation_results_class:Current Best Return = 3.8555803298950195
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.97142857142857
INFO:tools.evaluation_results_class:Counted Episodes = 8960
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7811384201049805
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.81138229370117
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.895360946655273
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.9053361415863037
INFO:tools.evaluation_results_class:Current Best Return = 3.7811384201049805
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.97142857142857
INFO:tools.evaluation_results_class:Counted Episodes = 8960
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 16.263 achieved after 4828.93 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.9384 achieved after 4828.95 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.6173 achieved after 4829.01 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.181 achieved after 4829.06 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 7.5805 achieved after 4829.1 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 6.7457 achieved after 4829.14 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 5.5721 achieved after 4829.15 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 3.8318 achieved after 4829.16 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.8317568061776424
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:robust_rl.robust_rl_trainer:Iteration 10 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.802504539489746
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.025047302246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.064453125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.792912006378174
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.5179526805877686
INFO:agents.father_agent:Step: 5, Training loss: 7.5916595458984375
INFO:agents.father_agent:Step: 10, Training loss: 2.5390944480895996
INFO:agents.father_agent:Step: 15, Training loss: 2.4292802810668945
INFO:agents.father_agent:Step: 20, Training loss: 10.41596508026123
INFO:agents.father_agent:Step: 25, Training loss: 4.570313930511475
INFO:agents.father_agent:Step: 30, Training loss: 5.81519889831543
INFO:agents.father_agent:Step: 35, Training loss: 4.653468132019043
INFO:agents.father_agent:Step: 40, Training loss: 3.3900561332702637
INFO:agents.father_agent:Step: 45, Training loss: 10.402019500732422
INFO:agents.father_agent:Step: 50, Training loss: 2.187908411026001
INFO:agents.father_agent:Step: 55, Training loss: 1.9441949129104614
INFO:agents.father_agent:Step: 60, Training loss: 10.445590019226074
INFO:agents.father_agent:Step: 65, Training loss: 1.6088111400604248
INFO:agents.father_agent:Step: 70, Training loss: 8.218683242797852
INFO:agents.father_agent:Step: 75, Training loss: 5.561556339263916
INFO:agents.father_agent:Step: 80, Training loss: 2.424544095993042
INFO:agents.father_agent:Step: 85, Training loss: 12.106766700744629
INFO:agents.father_agent:Step: 90, Training loss: 1.9250452518463135
INFO:agents.father_agent:Step: 95, Training loss: 2.938490152359009
INFO:agents.father_agent:Step: 100, Training loss: 9.707918167114258
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.59375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.525936126708984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.792494773864746
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 1.2622615098953247
INFO:agents.father_agent:Step: 110, Training loss: 9.297030448913574
INFO:agents.father_agent:Step: 115, Training loss: 3.0892882347106934
INFO:agents.father_agent:Step: 120, Training loss: 2.761456251144409
INFO:agents.father_agent:Step: 125, Training loss: 7.85115385055542
INFO:agents.father_agent:Step: 130, Training loss: 1.7479004859924316
INFO:agents.father_agent:Step: 135, Training loss: 3.3677780628204346
INFO:agents.father_agent:Step: 140, Training loss: 7.881444931030273
INFO:agents.father_agent:Step: 145, Training loss: 1.872943639755249
INFO:agents.father_agent:Step: 150, Training loss: 7.332067012786865
INFO:agents.father_agent:Step: 155, Training loss: 2.6547608375549316
INFO:agents.father_agent:Step: 160, Training loss: 4.728217124938965
INFO:agents.father_agent:Step: 165, Training loss: 8.350406646728516
INFO:agents.father_agent:Step: 170, Training loss: 1.8960810899734497
INFO:agents.father_agent:Step: 175, Training loss: 4.061440467834473
INFO:agents.father_agent:Step: 180, Training loss: 6.86021614074707
INFO:agents.father_agent:Step: 185, Training loss: 2.8439559936523438
INFO:agents.father_agent:Step: 190, Training loss: 8.645142555236816
INFO:agents.father_agent:Step: 195, Training loss: 2.592388868331909
INFO:agents.father_agent:Step: 200, Training loss: 3.147996425628662
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8546645641326904
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.54664611816406
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.5009765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.9233856201171875
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 2.185615301132202
INFO:agents.father_agent:Step: 210, Training loss: 9.501500129699707
INFO:agents.father_agent:Step: 215, Training loss: 3.034507989883423
INFO:agents.father_agent:Step: 220, Training loss: 2.9086930751800537
INFO:agents.father_agent:Step: 225, Training loss: 9.454779624938965
INFO:agents.father_agent:Step: 230, Training loss: 1.8189197778701782
INFO:agents.father_agent:Step: 235, Training loss: 4.113461494445801
INFO:agents.father_agent:Step: 240, Training loss: 5.8660502433776855
INFO:agents.father_agent:Step: 245, Training loss: 2.6902835369110107
INFO:agents.father_agent:Step: 250, Training loss: 8.328633308410645
INFO:agents.father_agent:Step: 255, Training loss: 2.771390676498413
INFO:agents.father_agent:Step: 260, Training loss: 4.50369119644165
INFO:agents.father_agent:Step: 265, Training loss: 7.779959201812744
INFO:agents.father_agent:Step: 270, Training loss: 2.0658318996429443
INFO:agents.father_agent:Step: 275, Training loss: 3.3295507431030273
INFO:agents.father_agent:Step: 280, Training loss: 4.873960971832275
INFO:agents.father_agent:Step: 285, Training loss: 3.5089499950408936
INFO:agents.father_agent:Step: 290, Training loss: 7.412741661071777
INFO:agents.father_agent:Step: 295, Training loss: 2.681668758392334
INFO:agents.father_agent:Step: 300, Training loss: 2.710465908050537
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.835822582244873
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.35822677612305
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.30865478515625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.816680908203125
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.7914280891418457
INFO:agents.father_agent:Step: 310, Training loss: 7.519181728363037
INFO:agents.father_agent:Step: 315, Training loss: 3.6231026649475098
INFO:agents.father_agent:Step: 320, Training loss: 3.0531280040740967
INFO:agents.father_agent:Step: 325, Training loss: 9.552677154541016
INFO:agents.father_agent:Step: 330, Training loss: 1.722190260887146
INFO:agents.father_agent:Step: 335, Training loss: 3.058628559112549
INFO:agents.father_agent:Step: 340, Training loss: 6.879865646362305
INFO:agents.father_agent:Step: 345, Training loss: 1.9873430728912354
INFO:agents.father_agent:Step: 350, Training loss: 7.242784023284912
INFO:agents.father_agent:Step: 355, Training loss: 2.531538248062134
INFO:agents.father_agent:Step: 360, Training loss: 4.4447197914123535
INFO:agents.father_agent:Step: 365, Training loss: 7.195532321929932
INFO:agents.father_agent:Step: 370, Training loss: 2.0231361389160156
INFO:agents.father_agent:Step: 375, Training loss: 2.7803890705108643
INFO:agents.father_agent:Step: 380, Training loss: 5.058902263641357
INFO:agents.father_agent:Step: 385, Training loss: 2.735546350479126
INFO:agents.father_agent:Step: 390, Training loss: 7.436685562133789
INFO:agents.father_agent:Step: 395, Training loss: 2.526890993118286
INFO:agents.father_agent:Step: 400, Training loss: 2.657656192779541
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8681066036224365
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.68106460571289
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.624603271484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7934954166412354
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.817090630531311
INFO:agents.father_agent:Step: 410, Training loss: 7.532382488250732
INFO:agents.father_agent:Step: 415, Training loss: 2.8647074699401855
INFO:agents.father_agent:Step: 420, Training loss: 2.856619358062744
INFO:agents.father_agent:Step: 425, Training loss: 8.472614288330078
INFO:agents.father_agent:Step: 430, Training loss: 1.6273393630981445
INFO:agents.father_agent:Step: 435, Training loss: 3.372295379638672
INFO:agents.father_agent:Step: 440, Training loss: 7.5395941734313965
INFO:agents.father_agent:Step: 445, Training loss: 2.210836887359619
INFO:agents.father_agent:Step: 450, Training loss: 8.938122749328613
INFO:agents.father_agent:Step: 455, Training loss: 3.361198663711548
INFO:agents.father_agent:Step: 460, Training loss: 5.140048980712891
INFO:agents.father_agent:Step: 465, Training loss: 7.147316932678223
INFO:agents.father_agent:Step: 470, Training loss: 1.8344212770462036
INFO:agents.father_agent:Step: 475, Training loss: 2.985969066619873
INFO:agents.father_agent:Step: 480, Training loss: 6.212823867797852
INFO:agents.father_agent:Step: 485, Training loss: 2.5919361114501953
INFO:agents.father_agent:Step: 490, Training loss: 8.219646453857422
INFO:agents.father_agent:Step: 495, Training loss: 2.5470242500305176
INFO:agents.father_agent:Step: 500, Training loss: 3.1382830142974854
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7999770641326904
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.99977111816406
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.05741500854492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.846364974975586
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.851792335510254
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.517921447753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.455596923828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7852494716644287
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8101563453674316
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.1015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.1009407043457
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7312583923339844
INFO:tools.evaluation_results_class:Current Best Return = 3.8101563453674316
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.97142857142857
INFO:tools.evaluation_results_class:Counted Episodes = 8960
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.862053632736206
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.62053680419922
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.522762298583984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.729185104370117
INFO:tools.evaluation_results_class:Current Best Return = 3.862053632736206
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.97142857142857
INFO:tools.evaluation_results_class:Counted Episodes = 8960
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 16.2648 achieved after 5361.43 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.9393 achieved after 5361.45 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.6186 achieved after 5361.51 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.1826 achieved after 5361.56 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 7.5827 achieved after 5361.6 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 6.7485 achieved after 5361.64 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 5.5755 achieved after 5361.65 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 3.8348 achieved after 5361.66 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.8348099332015533
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:robust_rl.robust_rl_trainer:Iteration 11 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.833754539489746
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.337547302246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.31354522705078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.829554557800293
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.9119198322296143
INFO:agents.father_agent:Step: 5, Training loss: 9.276389122009277
INFO:agents.father_agent:Step: 10, Training loss: 3.2014570236206055
INFO:agents.father_agent:Step: 15, Training loss: 3.318302631378174
INFO:agents.father_agent:Step: 20, Training loss: 11.778739929199219
INFO:agents.father_agent:Step: 25, Training loss: 3.6140084266662598
INFO:agents.father_agent:Step: 30, Training loss: 6.631209850311279
INFO:agents.father_agent:Step: 35, Training loss: 4.115622043609619
INFO:agents.father_agent:Step: 40, Training loss: 3.6297292709350586
INFO:agents.father_agent:Step: 45, Training loss: 11.369105339050293
INFO:agents.father_agent:Step: 50, Training loss: 2.3905608654022217
INFO:agents.father_agent:Step: 55, Training loss: 2.8052053451538086
INFO:agents.father_agent:Step: 60, Training loss: 11.114618301391602
INFO:agents.father_agent:Step: 65, Training loss: 2.062610387802124
INFO:agents.father_agent:Step: 70, Training loss: 8.414782524108887
INFO:agents.father_agent:Step: 75, Training loss: 4.252863883972168
INFO:agents.father_agent:Step: 80, Training loss: 3.3891336917877197
INFO:agents.father_agent:Step: 85, Training loss: 10.002708435058594
INFO:agents.father_agent:Step: 90, Training loss: 2.2446014881134033
INFO:agents.father_agent:Step: 95, Training loss: 3.335132360458374
INFO:agents.father_agent:Step: 100, Training loss: 8.493612289428711
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.871208667755127
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.71208572387695
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.64164733886719
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.768453598022461
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 2.4941728115081787
INFO:agents.father_agent:Step: 110, Training loss: 6.506911754608154
INFO:agents.father_agent:Step: 115, Training loss: 3.349405288696289
INFO:agents.father_agent:Step: 120, Training loss: 3.337557077407837
INFO:agents.father_agent:Step: 125, Training loss: 7.48399543762207
INFO:agents.father_agent:Step: 130, Training loss: 1.6149095296859741
INFO:agents.father_agent:Step: 135, Training loss: 2.6841394901275635
INFO:agents.father_agent:Step: 140, Training loss: 6.152385711669922
INFO:agents.father_agent:Step: 145, Training loss: 1.8217779397964478
INFO:agents.father_agent:Step: 150, Training loss: 9.402318954467773
INFO:agents.father_agent:Step: 155, Training loss: 2.874377727508545
INFO:agents.father_agent:Step: 160, Training loss: 4.463714122772217
INFO:agents.father_agent:Step: 165, Training loss: 7.001596450805664
INFO:agents.father_agent:Step: 170, Training loss: 1.86375093460083
INFO:agents.father_agent:Step: 175, Training loss: 3.3539493083953857
INFO:agents.father_agent:Step: 180, Training loss: 5.810507297515869
INFO:agents.father_agent:Step: 185, Training loss: 3.0086331367492676
INFO:agents.father_agent:Step: 190, Training loss: 7.208395957946777
INFO:agents.father_agent:Step: 195, Training loss: 2.818432569503784
INFO:agents.father_agent:Step: 200, Training loss: 2.7934579849243164
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.857192039489746
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.571922302246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.51026916503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.845989227294922
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 1.7171934843063354
INFO:agents.father_agent:Step: 210, Training loss: 9.043685913085938
INFO:agents.father_agent:Step: 215, Training loss: 2.9012625217437744
INFO:agents.father_agent:Step: 220, Training loss: 2.6223061084747314
INFO:agents.father_agent:Step: 225, Training loss: 6.9807233810424805
INFO:agents.father_agent:Step: 230, Training loss: 1.7227003574371338
INFO:agents.father_agent:Step: 235, Training loss: 4.048462867736816
INFO:agents.father_agent:Step: 240, Training loss: 6.758408546447754
INFO:agents.father_agent:Step: 245, Training loss: 2.5253703594207764
INFO:agents.father_agent:Step: 250, Training loss: 8.935576438903809
INFO:agents.father_agent:Step: 255, Training loss: 2.6584677696228027
INFO:agents.father_agent:Step: 260, Training loss: 5.462985038757324
INFO:agents.father_agent:Step: 265, Training loss: 7.583872318267822
INFO:agents.father_agent:Step: 270, Training loss: 2.19783878326416
INFO:agents.father_agent:Step: 275, Training loss: 4.004909038543701
INFO:agents.father_agent:Step: 280, Training loss: 4.900020599365234
INFO:agents.father_agent:Step: 285, Training loss: 3.150129795074463
INFO:agents.father_agent:Step: 290, Training loss: 6.957213401794434
INFO:agents.father_agent:Step: 295, Training loss: 2.6635899543762207
INFO:agents.father_agent:Step: 300, Training loss: 3.1891963481903076
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.829733371734619
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.297332763671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.29405975341797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8110828399658203
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.9841746091842651
INFO:agents.father_agent:Step: 310, Training loss: 10.088465690612793
INFO:agents.father_agent:Step: 315, Training loss: 3.2030792236328125
INFO:agents.father_agent:Step: 320, Training loss: 3.1858174800872803
INFO:agents.father_agent:Step: 325, Training loss: 6.857501983642578
INFO:agents.father_agent:Step: 330, Training loss: 2.0114355087280273
INFO:agents.father_agent:Step: 335, Training loss: 5.379680156707764
INFO:agents.father_agent:Step: 340, Training loss: 5.520293235778809
INFO:agents.father_agent:Step: 345, Training loss: 3.5554308891296387
INFO:agents.father_agent:Step: 350, Training loss: 6.653943061828613
INFO:agents.father_agent:Step: 355, Training loss: 2.9794087409973145
INFO:agents.father_agent:Step: 360, Training loss: 4.232909202575684
INFO:agents.father_agent:Step: 365, Training loss: 8.624074935913086
INFO:agents.father_agent:Step: 370, Training loss: 2.1032211780548096
INFO:agents.father_agent:Step: 375, Training loss: 4.133941173553467
INFO:agents.father_agent:Step: 380, Training loss: 4.267813682556152
INFO:agents.father_agent:Step: 385, Training loss: 3.702080726623535
INFO:agents.father_agent:Step: 390, Training loss: 7.101923942565918
INFO:agents.father_agent:Step: 395, Training loss: 2.5513949394226074
INFO:agents.father_agent:Step: 400, Training loss: 3.1057281494140625
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7842371463775635
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.84237289428711
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.910144805908203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7549171447753906
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.8663994073867798
INFO:agents.father_agent:Step: 410, Training loss: 7.943571090698242
INFO:agents.father_agent:Step: 415, Training loss: 2.911432981491089
INFO:agents.father_agent:Step: 420, Training loss: 3.161376476287842
INFO:agents.father_agent:Step: 425, Training loss: 7.672493934631348
INFO:agents.father_agent:Step: 430, Training loss: 1.8906046152114868
INFO:agents.father_agent:Step: 435, Training loss: 2.320671319961548
INFO:agents.father_agent:Step: 440, Training loss: 5.062077045440674
INFO:agents.father_agent:Step: 445, Training loss: 2.8456709384918213
INFO:agents.father_agent:Step: 450, Training loss: 6.602761745452881
INFO:agents.father_agent:Step: 455, Training loss: 2.8040812015533447
INFO:agents.father_agent:Step: 460, Training loss: 5.291407108306885
INFO:agents.father_agent:Step: 465, Training loss: 7.485123634338379
INFO:agents.father_agent:Step: 470, Training loss: 1.8130362033843994
INFO:agents.father_agent:Step: 475, Training loss: 2.92183256149292
INFO:agents.father_agent:Step: 480, Training loss: 5.437462329864502
INFO:agents.father_agent:Step: 485, Training loss: 3.502734661102295
INFO:agents.father_agent:Step: 490, Training loss: 7.980164527893066
INFO:agents.father_agent:Step: 495, Training loss: 2.5399973392486572
INFO:agents.father_agent:Step: 500, Training loss: 2.7525758743286133
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8255975246429443
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.25597381591797
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.24603271484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.829418182373047
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.8196232318878174
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.196231842041016
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.219478607177734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7505524158477783
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7858259677886963
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.85825729370117
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.930335998535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.705357074737549
INFO:tools.evaluation_results_class:Current Best Return = 3.7858259677886963
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.97142857142857
INFO:tools.evaluation_results_class:Counted Episodes = 8960
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8274552822113037
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.27455520629883
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.244422912597656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8164334297180176
INFO:tools.evaluation_results_class:Current Best Return = 3.8274552822113037
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.97142857142857
INFO:tools.evaluation_results_class:Counted Episodes = 8960
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 16.2652 achieved after 5893.69 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.9395 achieved after 5893.71 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.6188 achieved after 5893.77 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.1828 achieved after 5893.82 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 7.5829 achieved after 5893.86 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 6.7487 achieved after 5893.89 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 5.5757 achieved after 5893.91 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 3.8352 achieved after 5893.92 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.835241477564578
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:robust_rl.robust_rl_trainer:Iteration 12 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.849264621734619
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.492645263671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.47004318237305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.805403470993042
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.5685296058654785
INFO:agents.father_agent:Step: 5, Training loss: 9.270694732666016
INFO:agents.father_agent:Step: 10, Training loss: 2.509066581726074
INFO:agents.father_agent:Step: 15, Training loss: 2.678582191467285
INFO:agents.father_agent:Step: 20, Training loss: 8.219481468200684
INFO:agents.father_agent:Step: 25, Training loss: 4.069088935852051
INFO:agents.father_agent:Step: 30, Training loss: 6.255424976348877
INFO:agents.father_agent:Step: 35, Training loss: 5.79219388961792
INFO:agents.father_agent:Step: 40, Training loss: 2.3004119396209717
INFO:agents.father_agent:Step: 45, Training loss: 12.109490394592285
INFO:agents.father_agent:Step: 50, Training loss: 2.634922981262207
INFO:agents.father_agent:Step: 55, Training loss: 2.212395429611206
INFO:agents.father_agent:Step: 60, Training loss: 10.857016563415527
INFO:agents.father_agent:Step: 65, Training loss: 1.7351999282836914
INFO:agents.father_agent:Step: 70, Training loss: 9.913840293884277
INFO:agents.father_agent:Step: 75, Training loss: 4.176126956939697
INFO:agents.father_agent:Step: 80, Training loss: 2.228334665298462
INFO:agents.father_agent:Step: 85, Training loss: 9.700729370117188
INFO:agents.father_agent:Step: 90, Training loss: 2.567323684692383
INFO:agents.father_agent:Step: 95, Training loss: 4.48264217376709
INFO:agents.father_agent:Step: 100, Training loss: 9.373969078063965
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.826401710510254
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.264015197753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.264060974121094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7978737354278564
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 1.7967345714569092
INFO:agents.father_agent:Step: 110, Training loss: 9.988525390625
INFO:agents.father_agent:Step: 115, Training loss: 2.923233985900879
INFO:agents.father_agent:Step: 120, Training loss: 3.342719793319702
INFO:agents.father_agent:Step: 125, Training loss: 8.063470840454102
INFO:agents.father_agent:Step: 130, Training loss: 1.9007138013839722
INFO:agents.father_agent:Step: 135, Training loss: 4.093021869659424
INFO:agents.father_agent:Step: 140, Training loss: 7.065999507904053
INFO:agents.father_agent:Step: 145, Training loss: 2.4631237983703613
INFO:agents.father_agent:Step: 150, Training loss: 8.817376136779785
INFO:agents.father_agent:Step: 155, Training loss: 2.5999982357025146
INFO:agents.father_agent:Step: 160, Training loss: 4.693004608154297
INFO:agents.father_agent:Step: 165, Training loss: 8.30479621887207
INFO:agents.father_agent:Step: 170, Training loss: 2.071131944656372
INFO:agents.father_agent:Step: 175, Training loss: 4.518749237060547
INFO:agents.father_agent:Step: 180, Training loss: 5.091583251953125
INFO:agents.father_agent:Step: 185, Training loss: 3.37147855758667
INFO:agents.father_agent:Step: 190, Training loss: 8.845818519592285
INFO:agents.father_agent:Step: 195, Training loss: 2.779404401779175
INFO:agents.father_agent:Step: 200, Training loss: 3.381937026977539
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.788717746734619
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.887176513671875
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.960494995117188
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7020280361175537
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 2.0966131687164307
INFO:agents.father_agent:Step: 210, Training loss: 6.016366004943848
INFO:agents.father_agent:Step: 215, Training loss: 3.078190326690674
INFO:agents.father_agent:Step: 220, Training loss: 3.5043747425079346
INFO:agents.father_agent:Step: 225, Training loss: 6.34509801864624
INFO:agents.father_agent:Step: 230, Training loss: 1.834415078163147
INFO:agents.father_agent:Step: 235, Training loss: 2.561635732650757
INFO:agents.father_agent:Step: 240, Training loss: 4.779938697814941
INFO:agents.father_agent:Step: 245, Training loss: 2.6626596450805664
INFO:agents.father_agent:Step: 250, Training loss: 5.381217002868652
INFO:agents.father_agent:Step: 255, Training loss: 3.214794635772705
INFO:agents.father_agent:Step: 260, Training loss: 5.229091167449951
INFO:agents.father_agent:Step: 265, Training loss: 8.381858825683594
INFO:agents.father_agent:Step: 270, Training loss: 1.9399311542510986
INFO:agents.father_agent:Step: 275, Training loss: 3.1731202602386475
INFO:agents.father_agent:Step: 280, Training loss: 5.899733543395996
INFO:agents.father_agent:Step: 285, Training loss: 2.7586615085601807
INFO:agents.father_agent:Step: 290, Training loss: 9.366435050964355
INFO:agents.father_agent:Step: 295, Training loss: 2.46785044670105
INFO:agents.father_agent:Step: 300, Training loss: 2.7645022869110107
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.841911792755127
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.41911697387695
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.422367095947266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7700445652008057
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.877767562866211
INFO:agents.father_agent:Step: 310, Training loss: 7.061864376068115
INFO:agents.father_agent:Step: 315, Training loss: 3.0628292560577393
INFO:agents.father_agent:Step: 320, Training loss: 3.1557843685150146
INFO:agents.father_agent:Step: 325, Training loss: 6.383800029754639
INFO:agents.father_agent:Step: 330, Training loss: 1.7845004796981812
INFO:agents.father_agent:Step: 335, Training loss: 3.8177297115325928
INFO:agents.father_agent:Step: 340, Training loss: 6.831672191619873
INFO:agents.father_agent:Step: 345, Training loss: 2.105618715286255
INFO:agents.father_agent:Step: 350, Training loss: 6.172521591186523
INFO:agents.father_agent:Step: 355, Training loss: 2.709872007369995
INFO:agents.father_agent:Step: 360, Training loss: 4.7349419593811035
INFO:agents.father_agent:Step: 365, Training loss: 6.981454372406006
INFO:agents.father_agent:Step: 370, Training loss: 2.0611839294433594
INFO:agents.father_agent:Step: 375, Training loss: 4.291133880615234
INFO:agents.father_agent:Step: 380, Training loss: 5.549395561218262
INFO:agents.father_agent:Step: 385, Training loss: 2.9187545776367188
INFO:agents.father_agent:Step: 390, Training loss: 8.271099090576172
INFO:agents.father_agent:Step: 395, Training loss: 2.760244369506836
INFO:agents.father_agent:Step: 400, Training loss: 3.3658432960510254
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7898666858673096
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.89866638183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.936128616333008
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7052695751190186
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 2.2024314403533936
INFO:agents.father_agent:Step: 410, Training loss: 8.993056297302246
INFO:agents.father_agent:Step: 415, Training loss: 2.9789624214172363
INFO:agents.father_agent:Step: 420, Training loss: 3.7423694133758545
INFO:agents.father_agent:Step: 425, Training loss: 8.817747116088867
INFO:agents.father_agent:Step: 430, Training loss: 1.7997355461120605
INFO:agents.father_agent:Step: 435, Training loss: 4.2452168464660645
INFO:agents.father_agent:Step: 440, Training loss: 4.831036567687988
INFO:agents.father_agent:Step: 445, Training loss: 3.1631338596343994
INFO:agents.father_agent:Step: 450, Training loss: 6.552071571350098
INFO:agents.father_agent:Step: 455, Training loss: 2.4721572399139404
INFO:agents.father_agent:Step: 460, Training loss: 4.688177108764648
INFO:agents.father_agent:Step: 465, Training loss: 8.046647071838379
INFO:agents.father_agent:Step: 470, Training loss: 2.2245638370513916
INFO:agents.father_agent:Step: 475, Training loss: 4.368278980255127
INFO:agents.father_agent:Step: 480, Training loss: 4.497190952301025
INFO:agents.father_agent:Step: 485, Training loss: 3.910534143447876
INFO:agents.father_agent:Step: 490, Training loss: 7.492663383483887
INFO:agents.father_agent:Step: 495, Training loss: 2.45289945602417
INFO:agents.father_agent:Step: 500, Training loss: 3.0645995140075684
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8640854358673096
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.64085388183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.56708908081055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.866063117980957
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.83984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.3984375
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.37538146972656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8105173110961914
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8348214626312256
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.34821319580078
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.30670166015625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.782090902328491
INFO:tools.evaluation_results_class:Current Best Return = 3.8348214626312256
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.97142857142857
INFO:tools.evaluation_results_class:Counted Episodes = 8960
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.6907365322113037
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.90736770629883
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.171003341674805
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.777682065963745
INFO:tools.evaluation_results_class:Current Best Return = 3.6907365322113037
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.97142857142857
INFO:tools.evaluation_results_class:Counted Episodes = 8960
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 16.2616 achieved after 6430.11 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.9374 achieved after 6430.13 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.6146 achieved after 6430.19 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.1746 achieved after 6430.24 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 7.567 achieved after 6430.28 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 6.7189 achieved after 6430.31 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 5.5226 achieved after 6430.33 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 3.7418 achieved after 6430.33 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.7418467327972085
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:robust_rl.robust_rl_trainer:Iteration 13 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.7806756496429443
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.80675506591797
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.933996200561523
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.736708402633667
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.892845630645752
INFO:agents.father_agent:Step: 5, Training loss: 10.30966567993164
INFO:agents.father_agent:Step: 10, Training loss: 2.6062638759613037
INFO:agents.father_agent:Step: 15, Training loss: 3.1413731575012207
INFO:agents.father_agent:Step: 20, Training loss: 12.197035789489746
INFO:agents.father_agent:Step: 25, Training loss: 2.8088159561157227
INFO:agents.father_agent:Step: 30, Training loss: 5.803823471069336
INFO:agents.father_agent:Step: 35, Training loss: 3.928253412246704
INFO:agents.father_agent:Step: 40, Training loss: 3.6226649284362793
INFO:agents.father_agent:Step: 45, Training loss: 9.968116760253906
INFO:agents.father_agent:Step: 50, Training loss: 2.510371208190918
INFO:agents.father_agent:Step: 55, Training loss: 2.5732288360595703
INFO:agents.father_agent:Step: 60, Training loss: 8.938006401062012
INFO:agents.father_agent:Step: 65, Training loss: 2.103659152984619
INFO:agents.father_agent:Step: 70, Training loss: 7.087220191955566
INFO:agents.father_agent:Step: 75, Training loss: 4.809662342071533
INFO:agents.father_agent:Step: 80, Training loss: 3.181227684020996
INFO:agents.father_agent:Step: 85, Training loss: 9.981780052185059
INFO:agents.father_agent:Step: 90, Training loss: 2.195923089981079
INFO:agents.father_agent:Step: 95, Training loss: 3.453861713409424
INFO:agents.father_agent:Step: 100, Training loss: 8.562695503234863
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.793428421020508
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.93428421020508
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.97650146484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7429440021514893
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 1.6376492977142334
INFO:agents.father_agent:Step: 110, Training loss: 8.718240737915039
INFO:agents.father_agent:Step: 115, Training loss: 3.2325451374053955
INFO:agents.father_agent:Step: 120, Training loss: 3.162255048751831
INFO:agents.father_agent:Step: 125, Training loss: 8.64883804321289
INFO:agents.father_agent:Step: 130, Training loss: 1.8017399311065674
INFO:agents.father_agent:Step: 135, Training loss: 2.6475818157196045
INFO:agents.father_agent:Step: 140, Training loss: 5.462102890014648
INFO:agents.father_agent:Step: 145, Training loss: 2.418290376663208
INFO:agents.father_agent:Step: 150, Training loss: 6.767095565795898
INFO:agents.father_agent:Step: 155, Training loss: 3.0072386264801025
INFO:agents.father_agent:Step: 160, Training loss: 4.536225318908691
INFO:agents.father_agent:Step: 165, Training loss: 7.539671897888184
INFO:agents.father_agent:Step: 170, Training loss: 1.768500804901123
INFO:agents.father_agent:Step: 175, Training loss: 3.2313594818115234
INFO:agents.father_agent:Step: 180, Training loss: 6.119612693786621
INFO:agents.father_agent:Step: 185, Training loss: 2.939176559448242
INFO:agents.father_agent:Step: 190, Training loss: 8.563477516174316
INFO:agents.father_agent:Step: 195, Training loss: 2.7800660133361816
INFO:agents.father_agent:Step: 200, Training loss: 2.7363297939300537
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8563878536224365
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.56387710571289
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.52762222290039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.7796971797943115
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 1.9286246299743652
INFO:agents.father_agent:Step: 210, Training loss: 6.337158679962158
INFO:agents.father_agent:Step: 215, Training loss: 3.2228944301605225
INFO:agents.father_agent:Step: 220, Training loss: 2.8691956996917725
INFO:agents.father_agent:Step: 225, Training loss: 8.110013008117676
INFO:agents.father_agent:Step: 230, Training loss: 1.9653722047805786
INFO:agents.father_agent:Step: 235, Training loss: 3.010180711746216
INFO:agents.father_agent:Step: 240, Training loss: 6.367502212524414
INFO:agents.father_agent:Step: 245, Training loss: 2.2128520011901855
INFO:agents.father_agent:Step: 250, Training loss: 6.897539138793945
INFO:agents.father_agent:Step: 255, Training loss: 2.955329179763794
INFO:agents.father_agent:Step: 260, Training loss: 4.742700099945068
INFO:agents.father_agent:Step: 265, Training loss: 7.284110069274902
INFO:agents.father_agent:Step: 270, Training loss: 1.945235252380371
INFO:agents.father_agent:Step: 275, Training loss: 3.6653032302856445
INFO:agents.father_agent:Step: 280, Training loss: 6.422576904296875
INFO:agents.father_agent:Step: 285, Training loss: 2.639817953109741
INFO:agents.father_agent:Step: 290, Training loss: 8.134844779968262
INFO:agents.father_agent:Step: 295, Training loss: 2.5405967235565186
INFO:agents.father_agent:Step: 300, Training loss: 3.0165112018585205
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.855009078979492
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.55009078979492
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.46334457397461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.787801742553711
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.907104253768921
INFO:agents.father_agent:Step: 310, Training loss: 8.81090259552002
INFO:agents.father_agent:Step: 315, Training loss: 2.6967179775238037
INFO:agents.father_agent:Step: 320, Training loss: 3.225560426712036
INFO:agents.father_agent:Step: 325, Training loss: 7.589245319366455
INFO:agents.father_agent:Step: 330, Training loss: 1.8352774381637573
INFO:agents.father_agent:Step: 335, Training loss: 5.643152713775635
INFO:agents.father_agent:Step: 340, Training loss: 5.182854652404785
INFO:agents.father_agent:Step: 345, Training loss: 3.0263447761535645
INFO:agents.father_agent:Step: 350, Training loss: 6.738466262817383
INFO:agents.father_agent:Step: 355, Training loss: 3.004432439804077
INFO:agents.father_agent:Step: 360, Training loss: 4.766778469085693
INFO:agents.father_agent:Step: 365, Training loss: 6.725231170654297
INFO:agents.father_agent:Step: 370, Training loss: 2.134427547454834
INFO:agents.father_agent:Step: 375, Training loss: 3.7181766033172607
INFO:agents.father_agent:Step: 380, Training loss: 4.229954719543457
INFO:agents.father_agent:Step: 385, Training loss: 3.661616563796997
INFO:agents.father_agent:Step: 390, Training loss: 9.211181640625
INFO:agents.father_agent:Step: 395, Training loss: 2.50479793548584
INFO:agents.father_agent:Step: 400, Training loss: 3.8516080379486084
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.837660789489746
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.376609802246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.36517333984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8875937461853027
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 2.221212148666382
INFO:agents.father_agent:Step: 410, Training loss: 7.470772743225098
INFO:agents.father_agent:Step: 415, Training loss: 3.0371341705322266
INFO:agents.father_agent:Step: 420, Training loss: 3.758090019226074
INFO:agents.father_agent:Step: 425, Training loss: 8.265425682067871
INFO:agents.father_agent:Step: 430, Training loss: 1.9116926193237305
INFO:agents.father_agent:Step: 435, Training loss: 4.811936378479004
INFO:agents.father_agent:Step: 440, Training loss: 4.597585201263428
INFO:agents.father_agent:Step: 445, Training loss: 3.385554075241089
INFO:agents.father_agent:Step: 450, Training loss: 7.548033237457275
INFO:agents.father_agent:Step: 455, Training loss: 2.9153292179107666
INFO:agents.father_agent:Step: 460, Training loss: 5.25887393951416
INFO:agents.father_agent:Step: 465, Training loss: 8.360776901245117
INFO:agents.father_agent:Step: 470, Training loss: 2.24931263923645
INFO:agents.father_agent:Step: 475, Training loss: 3.781651258468628
INFO:agents.father_agent:Step: 480, Training loss: 4.068367004394531
INFO:agents.father_agent:Step: 485, Training loss: 4.063801288604736
INFO:agents.father_agent:Step: 490, Training loss: 7.721217155456543
INFO:agents.father_agent:Step: 495, Training loss: 2.510927677154541
INFO:agents.father_agent:Step: 500, Training loss: 4.313070774078369
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.878676414489746
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.786766052246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.709259033203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.902330160140991
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 3.8524816036224365
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.52481460571289
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.45633316040039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.786372661590576
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.843526840209961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.43526840209961
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.36046600341797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.831319570541382
INFO:tools.evaluation_results_class:Current Best Return = 3.843526840209961
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.97142857142857
INFO:tools.evaluation_results_class:Counted Episodes = 8960
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.845200777053833
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.45200729370117
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.39807891845703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.783961296081543
INFO:tools.evaluation_results_class:Current Best Return = 3.845200777053833
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.97142857142857
INFO:tools.evaluation_results_class:Counted Episodes = 8960
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 140
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 16.2645 achieved after 6966.34 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.9391 achieved after 6966.35 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.6186 achieved after 6966.41 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 8.183 achieved after 6966.46 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 7.5835 achieved after 6966.5 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 6.7499 achieved after 6966.54 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 5.5777 achieved after 6966.55 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 3.8374 achieved after 6966.56 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 3.837429907044528
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
INFO:robust_rl.robust_rl_trainer:Iteration 14 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.791245460510254
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.912452697753906
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.986127853393555
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.881168842315674
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.9992618560791016
INFO:agents.father_agent:Step: 5, Training loss: 8.235310554504395
INFO:agents.father_agent:Step: 10, Training loss: 2.668881893157959
INFO:agents.father_agent:Step: 15, Training loss: 4.287040710449219
INFO:agents.father_agent:Step: 20, Training loss: 8.500064849853516
INFO:agents.father_agent:Step: 25, Training loss: 3.886871337890625
INFO:agents.father_agent:Step: 30, Training loss: 5.604959487915039
INFO:agents.father_agent:Step: 35, Training loss: 3.905719757080078
INFO:agents.father_agent:Step: 40, Training loss: 4.609639644622803
INFO:agents.father_agent:Step: 45, Training loss: 11.225821495056152
INFO:agents.father_agent:Step: 50, Training loss: 2.3242440223693848
INFO:agents.father_agent:Step: 55, Training loss: 3.778480291366577
INFO:agents.father_agent:Step: 60, Training loss: 9.850653648376465
INFO:agents.father_agent:Step: 65, Training loss: 2.4704477787017822
INFO:agents.father_agent:Step: 70, Training loss: 8.597042083740234
INFO:agents.father_agent:Step: 75, Training loss: 4.580101013183594
INFO:agents.father_agent:Step: 80, Training loss: 3.9199295043945312
INFO:agents.father_agent:Step: 85, Training loss: 9.310284614562988
INFO:agents.father_agent:Step: 90, Training loss: 2.0948119163513184
INFO:agents.father_agent:Step: 95, Training loss: 3.9115335941314697
INFO:agents.father_agent:Step: 100, Training loss: 10.366399765014648
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.838005542755127
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.38005447387695
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.359920501708984
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.89011812210083
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 2.1798346042633057
INFO:agents.father_agent:Step: 110, Training loss: 7.644306659698486
INFO:agents.father_agent:Step: 115, Training loss: 3.054002523422241
INFO:agents.father_agent:Step: 120, Training loss: 3.7030937671661377
INFO:agents.father_agent:Step: 125, Training loss: 7.105411529541016
INFO:agents.father_agent:Step: 130, Training loss: 1.9015889167785645
INFO:agents.father_agent:Step: 135, Training loss: 3.0564393997192383
INFO:agents.father_agent:Step: 140, Training loss: 5.668569564819336
INFO:agents.father_agent:Step: 145, Training loss: 2.5723323822021484
INFO:agents.father_agent:Step: 150, Training loss: 6.748780727386475
INFO:agents.father_agent:Step: 155, Training loss: 3.2329342365264893
INFO:agents.father_agent:Step: 160, Training loss: 5.094907283782959
INFO:agents.father_agent:Step: 165, Training loss: 7.164921760559082
INFO:agents.father_agent:Step: 170, Training loss: 2.0695276260375977
INFO:agents.father_agent:Step: 175, Training loss: 3.8826096057891846
INFO:agents.father_agent:Step: 180, Training loss: 5.049933910369873
INFO:agents.father_agent:Step: 185, Training loss: 3.924740791320801
INFO:agents.father_agent:Step: 190, Training loss: 6.98721170425415
INFO:agents.father_agent:Step: 195, Training loss: 2.5920588970184326
INFO:agents.father_agent:Step: 200, Training loss: 3.0906248092651367
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8353631496429443
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.35363006591797
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.34152603149414
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8592686653137207
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 2.309492588043213
INFO:agents.father_agent:Step: 210, Training loss: 6.9053850173950195
INFO:agents.father_agent:Step: 215, Training loss: 2.5264289379119873
INFO:agents.father_agent:Step: 220, Training loss: 3.2673189640045166
INFO:agents.father_agent:Step: 225, Training loss: 6.60481595993042
INFO:agents.father_agent:Step: 230, Training loss: 1.6896717548370361
INFO:agents.father_agent:Step: 235, Training loss: 3.4741902351379395
INFO:agents.father_agent:Step: 240, Training loss: 6.178483963012695
INFO:agents.father_agent:Step: 245, Training loss: 2.6187634468078613
INFO:agents.father_agent:Step: 250, Training loss: 7.913905620574951
INFO:agents.father_agent:Step: 255, Training loss: 2.643554449081421
INFO:agents.father_agent:Step: 260, Training loss: 4.8528618812561035
INFO:agents.father_agent:Step: 265, Training loss: 7.169557571411133
INFO:agents.father_agent:Step: 270, Training loss: 2.16561222076416
INFO:agents.father_agent:Step: 275, Training loss: 4.543408393859863
INFO:agents.father_agent:Step: 280, Training loss: 5.787985801696777
INFO:agents.father_agent:Step: 285, Training loss: 3.465158700942993
INFO:agents.father_agent:Step: 290, Training loss: 7.898748874664307
INFO:agents.father_agent:Step: 295, Training loss: 2.723541736602783
INFO:agents.father_agent:Step: 300, Training loss: 3.3030219078063965
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8523666858673096
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.52366638183594
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.475914001464844
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.820920705795288
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 2.168210744857788
INFO:agents.father_agent:Step: 310, Training loss: 6.107001781463623
INFO:agents.father_agent:Step: 315, Training loss: 3.0613582134246826
INFO:agents.father_agent:Step: 320, Training loss: 3.215277671813965
INFO:agents.father_agent:Step: 325, Training loss: 9.051335334777832
INFO:agents.father_agent:Step: 330, Training loss: 1.8097574710845947
INFO:agents.father_agent:Step: 335, Training loss: 3.023037910461426
INFO:agents.father_agent:Step: 340, Training loss: 5.158415794372559
INFO:agents.father_agent:Step: 345, Training loss: 2.6347696781158447
INFO:agents.father_agent:Step: 350, Training loss: 5.618186950683594
INFO:agents.father_agent:Step: 355, Training loss: 2.827810287475586
INFO:agents.father_agent:Step: 360, Training loss: 4.467956066131592
INFO:agents.father_agent:Step: 365, Training loss: 7.585238933563232
INFO:agents.father_agent:Step: 370, Training loss: 2.099247932434082
INFO:agents.father_agent:Step: 375, Training loss: 3.326240301132202
INFO:agents.father_agent:Step: 380, Training loss: 4.461455345153809
INFO:agents.father_agent:Step: 385, Training loss: 4.380317211151123
INFO:agents.father_agent:Step: 390, Training loss: 7.470366477966309
INFO:agents.father_agent:Step: 395, Training loss: 2.5806713104248047
INFO:agents.father_agent:Step: 400, Training loss: 3.1872544288635254
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 3.8034236431121826
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.034236907958984
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.07727813720703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2.8160223960876465
INFO:tools.evaluation_results_class:Current Best Return = 7.115685939788818
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 36.94117647058823
INFO:tools.evaluation_results_class:Counted Episodes = 8704
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.7848963737487793

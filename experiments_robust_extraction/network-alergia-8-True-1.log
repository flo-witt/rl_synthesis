2025-08-04 01:36:20,394 - sketch.py:80 - loading sketch from /home/ihudak/synthesis/models_robust_subset/network/sketch.templ ...
2025-08-04 01:36:20,394 - sketch.py:84 - assuming sketch in PRISM format...
2025-08-04 01:36:20,397 - prism_parser.py:31 - PRISM model type: POMDP
2025-08-04 01:36:20,397 - prism_parser.py:40 - processing hole definitions...
2025-08-04 01:36:20,398 - prism_parser.py:220 - loading properties from /home/ihudak/synthesis/models_robust_subset/network/sketch.props ...
2025-08-04 01:36:20,398 - prism_parser.py:236 - found the following specification: optimality: R{"packets_sent"}max=? [F "done"] 
2025-08-04 01:36:20,399 - jani.py:41 - constructing JANI program...
2025-08-04 01:36:20,403 - jani.py:61 - constructing the quotient...
2025-08-04 01:36:20,596 - jani.py:66 - associating choices of the quotient with hole assignments...
2025-08-04 01:36:20,832 - sketch.py:135 - sketch parsing OK
2025-08-04 01:36:20,847 - sketch.py:144 - WARNING: choice labeling for the quotient is not canonic
2025-08-04 01:36:20,847 - sketch.py:148 - constructed explicit quotient having 5480 states and 71240 choices
2025-08-04 01:36:20,847 - sketch.py:154 - found the following specification optimality: R{"packets_sent"}max=? [F "label_done"] 
2025-08-04 01:36:21,085 - vectorized_sim_initializer.py:61 - Compiling model network...
Hello <stormpy.storage.storage.StateValuation object at 0x7a91879193b0>
2025-08-04 01:36:21,140 - storm_vec_env.py:70 - Computing row map
2025-08-04 01:36:21,146 - storm_vec_env.py:97 - Computing transitions
2025-08-04 01:36:21,157 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 01:36:21,157 - storm_vec_env.py:114 - Computing sinks
2025-08-04 01:36:21,157 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 01:36:21,161 - storm_vec_env.py:143 - Computing labels
2025-08-04 01:36:21,161 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 01:36:21,161 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 01:36:21,161 - storm_vec_env.py:175 - Computing observations
2025-08-04 01:36:21,237 - environment_wrapper_vec.py:135 - Observation 15 not found in the state to observation map.
2025-08-04 01:36:21,237 - environment_wrapper_vec.py:146 - Grid-like renderer not possible to initialize.
2025-08-04 01:36:21,344 - environment_wrapper_vec.py:153 - Vectorized simulator initialized with 256 environments.
2025-08-04 01:36:21,564 - recurrent_ppo_agent.py:98 - Agent initialized
2025-08-04 01:36:21,585 - recurrent_ppo_agent.py:100 - Replay buffer initialized
Using unmasked training with policy wrapper.
2025-08-04 01:36:21,595 - recurrent_ppo_agent.py:112 - Collector driver initialized
2025-08-04 01:36:21,601 - robust_rl_trainer.py:432 - Iteration 1 of pure RL loop
2025-08-04 01:36:21,643 - storm_vec_env.py:70 - Computing row map
2025-08-04 01:36:21,680 - storm_vec_env.py:97 - Computing transitions
2025-08-04 01:36:21,747 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 01:36:21,748 - storm_vec_env.py:114 - Computing sinks
2025-08-04 01:36:21,748 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 01:36:21,767 - storm_vec_env.py:143 - Computing labels
2025-08-04 01:36:21,767 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 01:36:21,768 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 01:36:21,768 - storm_vec_env.py:175 - Computing observations
2025-08-04 01:36:21,853 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 01:36:21,853 - father_agent.py:540 - Before training evaluation.
2025-08-04 01:36:21,975 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:36:21,984 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:36:29,409 - evaluation_results_class.py:131 - Average Return = 4.457691192626953
2025-08-04 01:36:29,409 - evaluation_results_class.py:133 - Average Virtual Goal Value = 46.57691192626953
2025-08-04 01:36:29,409 - evaluation_results_class.py:135 - Average Discounted Reward = 27.274019241333008
2025-08-04 01:36:29,409 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:36:29,409 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:36:29,409 - evaluation_results_class.py:141 - Variance of Return = 12.867053031921387
2025-08-04 01:36:29,409 - evaluation_results_class.py:143 - Current Best Return = 4.457691192626953
2025-08-04 01:36:29,409 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:36:29,409 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:36:29,409 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:36:29,549 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:36:29,560 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:36:29,658 - father_agent.py:436 - Training agent on-policy
2025-08-04 01:36:43,943 - father_agent.py:386 - Step: 0, Training loss: 40.77756118774414
2025-08-04 01:36:52,944 - father_agent.py:386 - Step: 5, Training loss: 9.024862289428711
2025-08-04 01:36:54,512 - father_agent.py:386 - Step: 10, Training loss: 7.329563617706299
2025-08-04 01:36:56,081 - father_agent.py:386 - Step: 15, Training loss: 8.727416038513184
2025-08-04 01:36:57,660 - father_agent.py:386 - Step: 20, Training loss: 5.13070821762085
2025-08-04 01:36:59,235 - father_agent.py:386 - Step: 25, Training loss: 6.151610851287842
2025-08-04 01:37:00,792 - father_agent.py:386 - Step: 30, Training loss: 10.59021282196045
2025-08-04 01:37:02,359 - father_agent.py:386 - Step: 35, Training loss: 7.437355995178223
2025-08-04 01:37:03,929 - father_agent.py:386 - Step: 40, Training loss: 8.077902793884277
2025-08-04 01:37:05,479 - father_agent.py:386 - Step: 45, Training loss: 7.703792572021484
2025-08-04 01:37:07,046 - father_agent.py:386 - Step: 50, Training loss: 7.0886454582214355
2025-08-04 01:37:08,597 - father_agent.py:386 - Step: 55, Training loss: 10.087032318115234
2025-08-04 01:37:10,173 - father_agent.py:386 - Step: 60, Training loss: 9.012996673583984
2025-08-04 01:37:11,711 - father_agent.py:386 - Step: 65, Training loss: 6.875132083892822
2025-08-04 01:37:13,269 - father_agent.py:386 - Step: 70, Training loss: 10.34157657623291
2025-08-04 01:37:14,836 - father_agent.py:386 - Step: 75, Training loss: 4.0617451667785645
2025-08-04 01:37:16,393 - father_agent.py:386 - Step: 80, Training loss: 9.005496978759766
2025-08-04 01:37:17,961 - father_agent.py:386 - Step: 85, Training loss: 5.699516773223877
2025-08-04 01:37:19,524 - father_agent.py:386 - Step: 90, Training loss: 8.408390045166016
2025-08-04 01:37:21,079 - father_agent.py:386 - Step: 95, Training loss: 6.8638834953308105
2025-08-04 01:37:22,638 - father_agent.py:386 - Step: 100, Training loss: 8.219350814819336
2025-08-04 01:37:22,795 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:37:22,798 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:37:29,685 - evaluation_results_class.py:131 - Average Return = 6.7931294441223145
2025-08-04 01:37:29,685 - evaluation_results_class.py:133 - Average Virtual Goal Value = 69.9312973022461
2025-08-04 01:37:29,685 - evaluation_results_class.py:135 - Average Discounted Reward = 42.124908447265625
2025-08-04 01:37:29,685 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:37:29,685 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:37:29,685 - evaluation_results_class.py:141 - Variance of Return = 21.33406639099121
2025-08-04 01:37:29,685 - evaluation_results_class.py:143 - Current Best Return = 6.7931294441223145
2025-08-04 01:37:29,685 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:37:29,685 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:37:29,685 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:37:29,837 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:37:29,845 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:37:32,204 - father_agent.py:386 - Step: 105, Training loss: 7.3579888343811035
2025-08-04 01:37:33,756 - father_agent.py:386 - Step: 110, Training loss: 5.391605854034424
2025-08-04 01:37:35,309 - father_agent.py:386 - Step: 115, Training loss: 8.887060165405273
2025-08-04 01:37:36,863 - father_agent.py:386 - Step: 120, Training loss: 4.173063278198242
2025-08-04 01:37:38,407 - father_agent.py:386 - Step: 125, Training loss: 14.776205062866211
2025-08-04 01:37:39,944 - father_agent.py:386 - Step: 130, Training loss: 6.410762310028076
2025-08-04 01:37:41,458 - father_agent.py:386 - Step: 135, Training loss: 6.417742729187012
2025-08-04 01:37:42,983 - father_agent.py:386 - Step: 140, Training loss: 9.810702323913574
2025-08-04 01:37:44,500 - father_agent.py:386 - Step: 145, Training loss: 6.609403133392334
2025-08-04 01:37:46,038 - father_agent.py:386 - Step: 150, Training loss: 5.2983012199401855
2025-08-04 01:37:47,572 - father_agent.py:386 - Step: 155, Training loss: 8.55798053741455
2025-08-04 01:37:49,111 - father_agent.py:386 - Step: 160, Training loss: 4.635095596313477
2025-08-04 01:37:50,642 - father_agent.py:386 - Step: 165, Training loss: 10.208885192871094
2025-08-04 01:37:52,200 - father_agent.py:386 - Step: 170, Training loss: 7.254748344421387
2025-08-04 01:37:53,752 - father_agent.py:386 - Step: 175, Training loss: 5.535185813903809
2025-08-04 01:37:55,296 - father_agent.py:386 - Step: 180, Training loss: 25.106481552124023
2025-08-04 01:37:56,828 - father_agent.py:386 - Step: 185, Training loss: 4.838332653045654
2025-08-04 01:37:58,360 - father_agent.py:386 - Step: 190, Training loss: 5.258443832397461
2025-08-04 01:37:59,893 - father_agent.py:386 - Step: 195, Training loss: 8.664834022521973
2025-08-04 01:38:01,420 - father_agent.py:386 - Step: 200, Training loss: 5.0070319175720215
2025-08-04 01:38:01,577 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:38:01,579 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:38:08,407 - evaluation_results_class.py:131 - Average Return = 6.905531883239746
2025-08-04 01:38:08,407 - evaluation_results_class.py:133 - Average Virtual Goal Value = 71.0553207397461
2025-08-04 01:38:08,407 - evaluation_results_class.py:135 - Average Discounted Reward = 43.00294494628906
2025-08-04 01:38:08,407 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:38:08,407 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:38:08,407 - evaluation_results_class.py:141 - Variance of Return = 20.87059211730957
2025-08-04 01:38:08,407 - evaluation_results_class.py:143 - Current Best Return = 6.905531883239746
2025-08-04 01:38:08,407 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:38:08,407 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:38:08,407 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:38:08,561 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:38:08,570 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:38:10,929 - father_agent.py:386 - Step: 205, Training loss: 8.897716522216797
2025-08-04 01:38:12,480 - father_agent.py:386 - Step: 210, Training loss: 4.284863471984863
2025-08-04 01:38:14,079 - father_agent.py:386 - Step: 215, Training loss: 10.206619262695312
2025-08-04 01:38:15,675 - father_agent.py:386 - Step: 220, Training loss: 4.3658037185668945
2025-08-04 01:38:17,269 - father_agent.py:386 - Step: 225, Training loss: 17.602157592773438
2025-08-04 01:38:18,866 - father_agent.py:386 - Step: 230, Training loss: 6.421186923980713
2025-08-04 01:38:20,435 - father_agent.py:386 - Step: 235, Training loss: 5.529078483581543
2025-08-04 01:38:22,021 - father_agent.py:386 - Step: 240, Training loss: 8.186779975891113
2025-08-04 01:38:23,575 - father_agent.py:386 - Step: 245, Training loss: 6.986887454986572
2025-08-04 01:38:25,153 - father_agent.py:386 - Step: 250, Training loss: 5.250755310058594
2025-08-04 01:38:26,709 - father_agent.py:386 - Step: 255, Training loss: 8.351794242858887
2025-08-04 01:38:28,280 - father_agent.py:386 - Step: 260, Training loss: 4.43132209777832
2025-08-04 01:38:29,926 - father_agent.py:386 - Step: 265, Training loss: 10.350676536560059
2025-08-04 01:38:31,515 - father_agent.py:386 - Step: 270, Training loss: 7.297483921051025
2025-08-04 01:38:33,103 - father_agent.py:386 - Step: 275, Training loss: 5.0135345458984375
2025-08-04 01:38:34,726 - father_agent.py:386 - Step: 280, Training loss: 29.37197494506836
2025-08-04 01:38:36,331 - father_agent.py:386 - Step: 285, Training loss: 5.778932094573975
2025-08-04 01:38:37,920 - father_agent.py:386 - Step: 290, Training loss: 5.547001838684082
2025-08-04 01:38:39,532 - father_agent.py:386 - Step: 295, Training loss: 8.950382232666016
2025-08-04 01:38:41,113 - father_agent.py:386 - Step: 300, Training loss: 4.783108711242676
2025-08-04 01:38:41,273 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:38:41,275 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:38:48,221 - evaluation_results_class.py:131 - Average Return = 6.964637756347656
2025-08-04 01:38:48,221 - evaluation_results_class.py:133 - Average Virtual Goal Value = 71.64637756347656
2025-08-04 01:38:48,221 - evaluation_results_class.py:135 - Average Discounted Reward = 43.401939392089844
2025-08-04 01:38:48,221 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:38:48,221 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:38:48,221 - evaluation_results_class.py:141 - Variance of Return = 21.46780776977539
2025-08-04 01:38:48,221 - evaluation_results_class.py:143 - Current Best Return = 6.964637756347656
2025-08-04 01:38:48,221 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:38:48,222 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:38:48,222 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:38:48,375 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:38:48,383 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:38:50,727 - father_agent.py:386 - Step: 305, Training loss: 8.887983322143555
2025-08-04 01:38:52,291 - father_agent.py:386 - Step: 310, Training loss: 4.231456756591797
2025-08-04 01:38:53,844 - father_agent.py:386 - Step: 315, Training loss: 10.118907928466797
2025-08-04 01:38:55,395 - father_agent.py:386 - Step: 320, Training loss: 3.9783079624176025
2025-08-04 01:38:56,939 - father_agent.py:386 - Step: 325, Training loss: 18.13740348815918
2025-08-04 01:38:58,486 - father_agent.py:386 - Step: 330, Training loss: 5.930866241455078
2025-08-04 01:39:00,036 - father_agent.py:386 - Step: 335, Training loss: 5.613546848297119
2025-08-04 01:39:01,603 - father_agent.py:386 - Step: 340, Training loss: 8.311352729797363
2025-08-04 01:39:03,138 - father_agent.py:386 - Step: 345, Training loss: 6.805087566375732
2025-08-04 01:39:04,688 - father_agent.py:386 - Step: 350, Training loss: 5.218208312988281
2025-08-04 01:39:06,247 - father_agent.py:386 - Step: 355, Training loss: 8.271872520446777
2025-08-04 01:39:07,810 - father_agent.py:386 - Step: 360, Training loss: 3.901956558227539
2025-08-04 01:39:09,433 - father_agent.py:386 - Step: 365, Training loss: 9.449458122253418
2025-08-04 01:39:11,010 - father_agent.py:386 - Step: 370, Training loss: 6.5833258628845215
2025-08-04 01:39:12,550 - father_agent.py:386 - Step: 375, Training loss: 5.333332061767578
2025-08-04 01:39:14,092 - father_agent.py:386 - Step: 380, Training loss: 25.201335906982422
2025-08-04 01:39:15,704 - father_agent.py:386 - Step: 385, Training loss: 4.679454326629639
2025-08-04 01:39:17,315 - father_agent.py:386 - Step: 390, Training loss: 5.572408199310303
2025-08-04 01:39:18,977 - father_agent.py:386 - Step: 395, Training loss: 8.33862590789795
2025-08-04 01:39:20,601 - father_agent.py:386 - Step: 400, Training loss: 4.940920352935791
2025-08-04 01:39:20,760 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:39:20,762 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:39:27,634 - evaluation_results_class.py:131 - Average Return = 7.02879524230957
2025-08-04 01:39:27,635 - evaluation_results_class.py:133 - Average Virtual Goal Value = 72.28794860839844
2025-08-04 01:39:27,635 - evaluation_results_class.py:135 - Average Discounted Reward = 43.99516677856445
2025-08-04 01:39:27,635 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:39:27,635 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:39:27,635 - evaluation_results_class.py:141 - Variance of Return = 21.111574172973633
2025-08-04 01:39:27,635 - evaluation_results_class.py:143 - Current Best Return = 7.02879524230957
2025-08-04 01:39:27,635 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:39:27,635 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:39:27,635 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:39:27,787 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:39:27,796 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:39:30,197 - father_agent.py:386 - Step: 405, Training loss: 8.378866195678711
2025-08-04 01:39:31,800 - father_agent.py:386 - Step: 410, Training loss: 4.742955684661865
2025-08-04 01:39:33,396 - father_agent.py:386 - Step: 415, Training loss: 8.924211502075195
2025-08-04 01:39:34,995 - father_agent.py:386 - Step: 420, Training loss: 4.390854358673096
2025-08-04 01:39:36,612 - father_agent.py:386 - Step: 425, Training loss: 17.292301177978516
2025-08-04 01:39:38,245 - father_agent.py:386 - Step: 430, Training loss: 6.111320495605469
2025-08-04 01:39:39,818 - father_agent.py:386 - Step: 435, Training loss: 5.160029411315918
2025-08-04 01:39:41,341 - father_agent.py:386 - Step: 440, Training loss: 8.68087387084961
2025-08-04 01:39:42,856 - father_agent.py:386 - Step: 445, Training loss: 6.35728120803833
2025-08-04 01:39:44,461 - father_agent.py:386 - Step: 450, Training loss: 4.644852638244629
2025-08-04 01:39:46,077 - father_agent.py:386 - Step: 455, Training loss: 7.608680725097656
2025-08-04 01:39:47,679 - father_agent.py:386 - Step: 460, Training loss: 4.61376953125
2025-08-04 01:39:49,242 - father_agent.py:386 - Step: 465, Training loss: 9.188505172729492
2025-08-04 01:39:50,780 - father_agent.py:386 - Step: 470, Training loss: 7.355996608734131
2025-08-04 01:39:52,308 - father_agent.py:386 - Step: 475, Training loss: 5.790318012237549
2025-08-04 01:39:53,849 - father_agent.py:386 - Step: 480, Training loss: 24.75125503540039
2025-08-04 01:39:55,391 - father_agent.py:386 - Step: 485, Training loss: 5.04893159866333
2025-08-04 01:39:56,933 - father_agent.py:386 - Step: 490, Training loss: 4.932112216949463
2025-08-04 01:39:58,483 - father_agent.py:386 - Step: 495, Training loss: 8.69629192352295
2025-08-04 01:40:00,013 - father_agent.py:386 - Step: 500, Training loss: 5.09339714050293
2025-08-04 01:40:00,170 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:40:00,173 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:40:07,313 - evaluation_results_class.py:131 - Average Return = 7.040161609649658
2025-08-04 01:40:07,313 - evaluation_results_class.py:133 - Average Virtual Goal Value = 72.40161895751953
2025-08-04 01:40:07,313 - evaluation_results_class.py:135 - Average Discounted Reward = 43.96372985839844
2025-08-04 01:40:07,313 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:40:07,313 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:40:07,313 - evaluation_results_class.py:141 - Variance of Return = 21.540950775146484
2025-08-04 01:40:07,313 - evaluation_results_class.py:143 - Current Best Return = 7.040161609649658
2025-08-04 01:40:07,313 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:40:07,313 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:40:07,313 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:40:07,467 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:40:07,476 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:40:09,839 - father_agent.py:386 - Step: 505, Training loss: 8.235074043273926
2025-08-04 01:40:11,358 - father_agent.py:386 - Step: 510, Training loss: 4.197559833526611
2025-08-04 01:40:12,895 - father_agent.py:386 - Step: 515, Training loss: 8.582061767578125
2025-08-04 01:40:14,418 - father_agent.py:386 - Step: 520, Training loss: 4.023375034332275
2025-08-04 01:40:15,950 - father_agent.py:386 - Step: 525, Training loss: 16.69824981689453
2025-08-04 01:40:17,471 - father_agent.py:386 - Step: 530, Training loss: 6.14216947555542
2025-08-04 01:40:19,006 - father_agent.py:386 - Step: 535, Training loss: 5.024662971496582
2025-08-04 01:40:20,575 - father_agent.py:386 - Step: 540, Training loss: 9.313725471496582
2025-08-04 01:40:22,136 - father_agent.py:386 - Step: 545, Training loss: 6.897148609161377
2025-08-04 01:40:23,668 - father_agent.py:386 - Step: 550, Training loss: 4.751757621765137
2025-08-04 01:40:25,208 - father_agent.py:386 - Step: 555, Training loss: 8.287343978881836
2025-08-04 01:40:26,750 - father_agent.py:386 - Step: 560, Training loss: 3.9408326148986816
2025-08-04 01:40:28,286 - father_agent.py:386 - Step: 565, Training loss: 9.681117057800293
2025-08-04 01:40:29,833 - father_agent.py:386 - Step: 570, Training loss: 6.361125469207764
2025-08-04 01:40:31,357 - father_agent.py:386 - Step: 575, Training loss: 5.010738849639893
2025-08-04 01:40:32,914 - father_agent.py:386 - Step: 580, Training loss: 25.949438095092773
2025-08-04 01:40:34,479 - father_agent.py:386 - Step: 585, Training loss: 5.164243221282959
2025-08-04 01:40:36,027 - father_agent.py:386 - Step: 590, Training loss: 4.650311470031738
2025-08-04 01:40:37,577 - father_agent.py:386 - Step: 595, Training loss: 8.255566596984863
2025-08-04 01:40:39,121 - father_agent.py:386 - Step: 600, Training loss: 5.603331089019775
2025-08-04 01:40:39,275 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:40:39,277 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:40:46,259 - evaluation_results_class.py:131 - Average Return = 7.053043842315674
2025-08-04 01:40:46,259 - evaluation_results_class.py:133 - Average Virtual Goal Value = 72.53043365478516
2025-08-04 01:40:46,259 - evaluation_results_class.py:135 - Average Discounted Reward = 44.240901947021484
2025-08-04 01:40:46,259 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:40:46,259 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:40:46,259 - evaluation_results_class.py:141 - Variance of Return = 21.16869354248047
2025-08-04 01:40:46,259 - evaluation_results_class.py:143 - Current Best Return = 7.053043842315674
2025-08-04 01:40:46,259 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:40:46,259 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:40:46,259 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:40:46,411 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:40:46,420 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:40:48,786 - father_agent.py:386 - Step: 605, Training loss: 7.962635517120361
2025-08-04 01:40:50,350 - father_agent.py:386 - Step: 610, Training loss: 4.1271491050720215
2025-08-04 01:40:51,921 - father_agent.py:386 - Step: 615, Training loss: 9.038569450378418
2025-08-04 01:40:53,477 - father_agent.py:386 - Step: 620, Training loss: 4.3445725440979
2025-08-04 01:40:55,029 - father_agent.py:386 - Step: 625, Training loss: 18.38054847717285
2025-08-04 01:40:56,583 - father_agent.py:386 - Step: 630, Training loss: 6.108132362365723
2025-08-04 01:40:58,124 - father_agent.py:386 - Step: 635, Training loss: 5.489806652069092
2025-08-04 01:40:59,658 - father_agent.py:386 - Step: 640, Training loss: 8.642924308776855
2025-08-04 01:41:01,176 - father_agent.py:386 - Step: 645, Training loss: 6.686585426330566
2025-08-04 01:41:02,722 - father_agent.py:386 - Step: 650, Training loss: 5.0672383308410645
2025-08-04 01:41:04,255 - father_agent.py:386 - Step: 655, Training loss: 8.14083480834961
2025-08-04 01:41:05,809 - father_agent.py:386 - Step: 660, Training loss: 4.086982250213623
2025-08-04 01:41:07,349 - father_agent.py:386 - Step: 665, Training loss: 8.818286895751953
2025-08-04 01:41:08,893 - father_agent.py:386 - Step: 670, Training loss: 6.560178279876709
2025-08-04 01:41:10,427 - father_agent.py:386 - Step: 675, Training loss: 4.803225517272949
2025-08-04 01:41:11,984 - father_agent.py:386 - Step: 680, Training loss: 25.19452476501465
2025-08-04 01:41:13,533 - father_agent.py:386 - Step: 685, Training loss: 4.7554473876953125
2025-08-04 01:41:15,091 - father_agent.py:386 - Step: 690, Training loss: 4.730129241943359
2025-08-04 01:41:16,650 - father_agent.py:386 - Step: 695, Training loss: 8.063855171203613
2025-08-04 01:41:18,194 - father_agent.py:386 - Step: 700, Training loss: 5.266360759735107
2025-08-04 01:41:18,352 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:41:18,355 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:41:25,310 - evaluation_results_class.py:131 - Average Return = 7.023743152618408
2025-08-04 01:41:25,310 - evaluation_results_class.py:133 - Average Virtual Goal Value = 72.23743438720703
2025-08-04 01:41:25,310 - evaluation_results_class.py:135 - Average Discounted Reward = 43.89374923706055
2025-08-04 01:41:25,310 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:41:25,310 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:41:25,310 - evaluation_results_class.py:141 - Variance of Return = 21.698347091674805
2025-08-04 01:41:25,310 - evaluation_results_class.py:143 - Current Best Return = 7.053043842315674
2025-08-04 01:41:25,310 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:41:25,310 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:41:25,310 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:41:25,464 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:41:25,473 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:41:27,819 - father_agent.py:386 - Step: 705, Training loss: 7.9726715087890625
2025-08-04 01:41:29,365 - father_agent.py:386 - Step: 710, Training loss: 4.0734076499938965
2025-08-04 01:41:30,920 - father_agent.py:386 - Step: 715, Training loss: 9.185783386230469
2025-08-04 01:41:32,462 - father_agent.py:386 - Step: 720, Training loss: 4.377612113952637
2025-08-04 01:41:33,983 - father_agent.py:386 - Step: 725, Training loss: 17.90696907043457
2025-08-04 01:41:35,523 - father_agent.py:386 - Step: 730, Training loss: 5.849182605743408
2025-08-04 01:41:37,044 - father_agent.py:386 - Step: 735, Training loss: 5.546435832977295
2025-08-04 01:41:38,586 - father_agent.py:386 - Step: 740, Training loss: 8.768556594848633
2025-08-04 01:41:40,117 - father_agent.py:386 - Step: 745, Training loss: 6.347276210784912
2025-08-04 01:41:41,665 - father_agent.py:386 - Step: 750, Training loss: 5.1507720947265625
2025-08-04 01:41:43,210 - father_agent.py:386 - Step: 755, Training loss: 8.162646293640137
2025-08-04 01:41:44,746 - father_agent.py:386 - Step: 760, Training loss: 4.383450031280518
2025-08-04 01:41:46,280 - father_agent.py:386 - Step: 765, Training loss: 9.355917930603027
2025-08-04 01:41:47,800 - father_agent.py:386 - Step: 770, Training loss: 6.593504428863525
2025-08-04 01:41:49,316 - father_agent.py:386 - Step: 775, Training loss: 4.268640518188477
2025-08-04 01:41:50,857 - father_agent.py:386 - Step: 780, Training loss: 25.03973960876465
2025-08-04 01:41:52,389 - father_agent.py:386 - Step: 785, Training loss: 4.732102870941162
2025-08-04 01:41:53,921 - father_agent.py:386 - Step: 790, Training loss: 4.489087104797363
2025-08-04 01:41:55,456 - father_agent.py:386 - Step: 795, Training loss: 8.027262687683105
2025-08-04 01:41:56,985 - father_agent.py:386 - Step: 800, Training loss: 4.97500467300415
2025-08-04 01:41:57,145 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:41:57,147 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:42:04,033 - evaluation_results_class.py:131 - Average Return = 7.049002170562744
2025-08-04 01:42:04,033 - evaluation_results_class.py:133 - Average Virtual Goal Value = 72.49002075195312
2025-08-04 01:42:04,033 - evaluation_results_class.py:135 - Average Discounted Reward = 44.135772705078125
2025-08-04 01:42:04,033 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:42:04,033 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:42:04,033 - evaluation_results_class.py:141 - Variance of Return = 21.501008987426758
2025-08-04 01:42:04,034 - evaluation_results_class.py:143 - Current Best Return = 7.053043842315674
2025-08-04 01:42:04,034 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:42:04,034 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:42:04,034 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:42:04,188 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:42:04,196 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:42:06,559 - father_agent.py:386 - Step: 805, Training loss: 7.973785877227783
2025-08-04 01:42:08,102 - father_agent.py:386 - Step: 810, Training loss: 4.923445701599121
2025-08-04 01:42:09,663 - father_agent.py:386 - Step: 815, Training loss: 8.697172164916992
2025-08-04 01:42:11,202 - father_agent.py:386 - Step: 820, Training loss: 3.9199230670928955
2025-08-04 01:42:12,752 - father_agent.py:386 - Step: 825, Training loss: 17.480844497680664
2025-08-04 01:42:14,310 - father_agent.py:386 - Step: 830, Training loss: 5.762115001678467
2025-08-04 01:42:15,862 - father_agent.py:386 - Step: 835, Training loss: 4.455330848693848
2025-08-04 01:42:17,411 - father_agent.py:386 - Step: 840, Training loss: 8.502988815307617
2025-08-04 01:42:18,953 - father_agent.py:386 - Step: 845, Training loss: 6.644558429718018
2025-08-04 01:42:20,514 - father_agent.py:386 - Step: 850, Training loss: 4.841068267822266
2025-08-04 01:42:22,072 - father_agent.py:386 - Step: 855, Training loss: 8.432689666748047
2025-08-04 01:42:23,624 - father_agent.py:386 - Step: 860, Training loss: 3.7259409427642822
2025-08-04 01:42:25,182 - father_agent.py:386 - Step: 865, Training loss: 9.713996887207031
2025-08-04 01:42:26,734 - father_agent.py:386 - Step: 870, Training loss: 6.478046417236328
2025-08-04 01:42:28,276 - father_agent.py:386 - Step: 875, Training loss: 5.474172115325928
2025-08-04 01:42:29,818 - father_agent.py:386 - Step: 880, Training loss: 26.027084350585938
2025-08-04 01:42:31,338 - father_agent.py:386 - Step: 885, Training loss: 4.943960666656494
2025-08-04 01:42:32,860 - father_agent.py:386 - Step: 890, Training loss: 4.974199295043945
2025-08-04 01:42:34,410 - father_agent.py:386 - Step: 895, Training loss: 8.479183197021484
2025-08-04 01:42:35,943 - father_agent.py:386 - Step: 900, Training loss: 4.756387710571289
2025-08-04 01:42:36,100 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:42:36,103 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:42:42,937 - evaluation_results_class.py:131 - Average Return = 7.080828666687012
2025-08-04 01:42:42,938 - evaluation_results_class.py:133 - Average Virtual Goal Value = 72.80828857421875
2025-08-04 01:42:42,938 - evaluation_results_class.py:135 - Average Discounted Reward = 44.23691940307617
2025-08-04 01:42:42,938 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:42:42,938 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:42:42,938 - evaluation_results_class.py:141 - Variance of Return = 21.69490623474121
2025-08-04 01:42:42,938 - evaluation_results_class.py:143 - Current Best Return = 7.080828666687012
2025-08-04 01:42:42,938 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:42:42,938 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:42:42,938 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:42:43,093 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:42:43,101 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:42:45,458 - father_agent.py:386 - Step: 905, Training loss: 8.015375137329102
2025-08-04 01:42:46,993 - father_agent.py:386 - Step: 910, Training loss: 3.971184730529785
2025-08-04 01:42:48,539 - father_agent.py:386 - Step: 915, Training loss: 8.861385345458984
2025-08-04 01:42:50,095 - father_agent.py:386 - Step: 920, Training loss: 3.9756526947021484
2025-08-04 01:42:51,658 - father_agent.py:386 - Step: 925, Training loss: 17.251419067382812
2025-08-04 01:42:53,212 - father_agent.py:386 - Step: 930, Training loss: 6.0713605880737305
2025-08-04 01:42:54,755 - father_agent.py:386 - Step: 935, Training loss: 5.301004886627197
2025-08-04 01:42:56,311 - father_agent.py:386 - Step: 940, Training loss: 8.938029289245605
2025-08-04 01:42:57,861 - father_agent.py:386 - Step: 945, Training loss: 6.318135738372803
2025-08-04 01:42:59,413 - father_agent.py:386 - Step: 950, Training loss: 4.823968410491943
2025-08-04 01:43:00,979 - father_agent.py:386 - Step: 955, Training loss: 8.38923454284668
2025-08-04 01:43:02,525 - father_agent.py:386 - Step: 960, Training loss: 3.7637674808502197
2025-08-04 01:43:04,082 - father_agent.py:386 - Step: 965, Training loss: 9.116562843322754
2025-08-04 01:43:05,638 - father_agent.py:386 - Step: 970, Training loss: 6.566771984100342
2025-08-04 01:43:07,180 - father_agent.py:386 - Step: 975, Training loss: 4.454960346221924
2025-08-04 01:43:08,743 - father_agent.py:386 - Step: 980, Training loss: 26.69468879699707
2025-08-04 01:43:10,285 - father_agent.py:386 - Step: 985, Training loss: 4.43866491317749
2025-08-04 01:43:11,824 - father_agent.py:386 - Step: 990, Training loss: 4.521157264709473
2025-08-04 01:43:13,369 - father_agent.py:386 - Step: 995, Training loss: 8.096141815185547
2025-08-04 01:43:14,907 - father_agent.py:386 - Step: 1000, Training loss: 4.950533866882324
2025-08-04 01:43:15,062 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:43:15,065 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:43:21,974 - evaluation_results_class.py:131 - Average Return = 7.115685939788818
2025-08-04 01:43:21,974 - evaluation_results_class.py:133 - Average Virtual Goal Value = 73.1568603515625
2025-08-04 01:43:21,974 - evaluation_results_class.py:135 - Average Discounted Reward = 44.588138580322266
2025-08-04 01:43:21,974 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:43:21,974 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:43:21,974 - evaluation_results_class.py:141 - Variance of Return = 21.28795623779297
2025-08-04 01:43:21,975 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:43:21,975 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:43:21,975 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:43:21,975 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:43:22,129 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:43:22,139 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:43:24,814 - father_agent.py:386 - Step: 1005, Training loss: 7.955477714538574
2025-08-04 01:43:26,356 - father_agent.py:386 - Step: 1010, Training loss: 4.217742919921875
2025-08-04 01:43:27,935 - father_agent.py:386 - Step: 1015, Training loss: 9.43401050567627
2025-08-04 01:43:29,482 - father_agent.py:386 - Step: 1020, Training loss: 3.914067506790161
2025-08-04 01:43:31,009 - father_agent.py:386 - Step: 1025, Training loss: 18.063047409057617
2025-08-04 01:43:32,528 - father_agent.py:386 - Step: 1030, Training loss: 6.078985214233398
2025-08-04 01:43:34,057 - father_agent.py:386 - Step: 1035, Training loss: 4.961230754852295
2025-08-04 01:43:35,599 - father_agent.py:386 - Step: 1040, Training loss: 7.5471577644348145
2025-08-04 01:43:37,126 - father_agent.py:386 - Step: 1045, Training loss: 6.546300888061523
2025-08-04 01:43:38,666 - father_agent.py:386 - Step: 1050, Training loss: 4.992739200592041
2025-08-04 01:43:40,218 - father_agent.py:386 - Step: 1055, Training loss: 8.370173454284668
2025-08-04 01:43:41,771 - father_agent.py:386 - Step: 1060, Training loss: 3.918513774871826
2025-08-04 01:43:43,319 - father_agent.py:386 - Step: 1065, Training loss: 9.137871742248535
2025-08-04 01:43:44,844 - father_agent.py:386 - Step: 1070, Training loss: 6.320713996887207
2025-08-04 01:43:46,363 - father_agent.py:386 - Step: 1075, Training loss: 4.401865482330322
2025-08-04 01:43:47,900 - father_agent.py:386 - Step: 1080, Training loss: 24.61760711669922
2025-08-04 01:43:49,446 - father_agent.py:386 - Step: 1085, Training loss: 4.456479549407959
2025-08-04 01:43:51,016 - father_agent.py:386 - Step: 1090, Training loss: 4.575422286987305
2025-08-04 01:43:52,595 - father_agent.py:386 - Step: 1095, Training loss: 8.078025817871094
2025-08-04 01:43:54,176 - father_agent.py:386 - Step: 1100, Training loss: 4.781339168548584
2025-08-04 01:43:54,337 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:43:54,340 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:44:01,201 - evaluation_results_class.py:131 - Average Return = 7.039151191711426
2025-08-04 01:44:01,201 - evaluation_results_class.py:133 - Average Virtual Goal Value = 72.39151000976562
2025-08-04 01:44:01,201 - evaluation_results_class.py:135 - Average Discounted Reward = 44.093048095703125
2025-08-04 01:44:01,201 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:44:01,201 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:44:01,202 - evaluation_results_class.py:141 - Variance of Return = 21.310667037963867
2025-08-04 01:44:01,202 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:44:01,202 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:44:01,202 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:44:01,202 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:44:01,355 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:44:01,364 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:44:03,766 - father_agent.py:386 - Step: 1105, Training loss: 7.979801177978516
2025-08-04 01:44:05,400 - father_agent.py:386 - Step: 1110, Training loss: 4.110084533691406
2025-08-04 01:44:07,044 - father_agent.py:386 - Step: 1115, Training loss: 9.439743041992188
2025-08-04 01:44:08,641 - father_agent.py:386 - Step: 1120, Training loss: 4.160921096801758
2025-08-04 01:44:10,176 - father_agent.py:386 - Step: 1125, Training loss: 17.466489791870117
2025-08-04 01:44:11,722 - father_agent.py:386 - Step: 1130, Training loss: 6.263816833496094
2025-08-04 01:44:13,246 - father_agent.py:386 - Step: 1135, Training loss: 4.552971363067627
2025-08-04 01:44:14,772 - father_agent.py:386 - Step: 1140, Training loss: 8.501410484313965
2025-08-04 01:44:16,286 - father_agent.py:386 - Step: 1145, Training loss: 6.2151899337768555
2025-08-04 01:44:17,824 - father_agent.py:386 - Step: 1150, Training loss: 4.595488548278809
2025-08-04 01:44:19,375 - father_agent.py:386 - Step: 1155, Training loss: 8.16843318939209
2025-08-04 01:44:20,919 - father_agent.py:386 - Step: 1160, Training loss: 3.6069467067718506
2025-08-04 01:44:22,442 - father_agent.py:386 - Step: 1165, Training loss: 9.138551712036133
2025-08-04 01:44:23,958 - father_agent.py:386 - Step: 1170, Training loss: 6.829479694366455
2025-08-04 01:44:25,474 - father_agent.py:386 - Step: 1175, Training loss: 4.342653274536133
2025-08-04 01:44:27,000 - father_agent.py:386 - Step: 1180, Training loss: 26.42935562133789
2025-08-04 01:44:28,517 - father_agent.py:386 - Step: 1185, Training loss: 4.541413307189941
2025-08-04 01:44:30,033 - father_agent.py:386 - Step: 1190, Training loss: 4.171260833740234
2025-08-04 01:44:31,564 - father_agent.py:386 - Step: 1195, Training loss: 7.751965522766113
2025-08-04 01:44:33,078 - father_agent.py:386 - Step: 1200, Training loss: 5.153995990753174
2025-08-04 01:44:33,235 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:44:33,238 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:44:40,041 - evaluation_results_class.py:131 - Average Return = 7.050265312194824
2025-08-04 01:44:40,041 - evaluation_results_class.py:133 - Average Virtual Goal Value = 72.50265502929688
2025-08-04 01:44:40,041 - evaluation_results_class.py:135 - Average Discounted Reward = 44.17142105102539
2025-08-04 01:44:40,041 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:44:40,041 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:44:40,041 - evaluation_results_class.py:141 - Variance of Return = 21.4491024017334
2025-08-04 01:44:40,042 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:44:40,042 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:44:40,042 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:44:40,042 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:44:40,195 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:44:40,204 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:44:42,535 - father_agent.py:386 - Step: 1205, Training loss: 7.938199996948242
2025-08-04 01:44:44,062 - father_agent.py:386 - Step: 1210, Training loss: 3.846207857131958
2025-08-04 01:44:45,600 - father_agent.py:386 - Step: 1215, Training loss: 8.31517219543457
2025-08-04 01:44:47,136 - father_agent.py:386 - Step: 1220, Training loss: 3.696096658706665
2025-08-04 01:44:48,669 - father_agent.py:386 - Step: 1225, Training loss: 16.663591384887695
2025-08-04 01:44:50,214 - father_agent.py:386 - Step: 1230, Training loss: 5.964085578918457
2025-08-04 01:44:51,753 - father_agent.py:386 - Step: 1235, Training loss: 4.78794002532959
2025-08-04 01:44:53,301 - father_agent.py:386 - Step: 1240, Training loss: 8.441182136535645
2025-08-04 01:44:54,843 - father_agent.py:386 - Step: 1245, Training loss: 6.1548752784729
2025-08-04 01:44:56,379 - father_agent.py:386 - Step: 1250, Training loss: 4.719407558441162
2025-08-04 01:44:57,913 - father_agent.py:386 - Step: 1255, Training loss: 8.23715877532959
2025-08-04 01:44:59,464 - father_agent.py:386 - Step: 1260, Training loss: 3.971121311187744
2025-08-04 01:45:01,045 - father_agent.py:386 - Step: 1265, Training loss: 9.189708709716797
2025-08-04 01:45:02,591 - father_agent.py:386 - Step: 1270, Training loss: 6.33173131942749
2025-08-04 01:45:04,133 - father_agent.py:386 - Step: 1275, Training loss: 4.878102779388428
2025-08-04 01:45:05,685 - father_agent.py:386 - Step: 1280, Training loss: 25.42669677734375
2025-08-04 01:45:07,211 - father_agent.py:386 - Step: 1285, Training loss: 4.586902618408203
2025-08-04 01:45:08,735 - father_agent.py:386 - Step: 1290, Training loss: 4.141887187957764
2025-08-04 01:45:10,298 - father_agent.py:386 - Step: 1295, Training loss: 7.895353317260742
2025-08-04 01:45:11,819 - father_agent.py:386 - Step: 1300, Training loss: 5.201075553894043
2025-08-04 01:45:11,979 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:45:11,982 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:45:18,760 - evaluation_results_class.py:131 - Average Return = 7.074008464813232
2025-08-04 01:45:18,760 - evaluation_results_class.py:133 - Average Virtual Goal Value = 72.7400894165039
2025-08-04 01:45:18,760 - evaluation_results_class.py:135 - Average Discounted Reward = 44.32765579223633
2025-08-04 01:45:18,760 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:45:18,760 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:45:18,760 - evaluation_results_class.py:141 - Variance of Return = 21.276411056518555
2025-08-04 01:45:18,760 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:45:18,760 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:45:18,760 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:45:18,760 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:45:18,915 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:45:18,923 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:45:21,282 - father_agent.py:386 - Step: 1305, Training loss: 7.527155876159668
2025-08-04 01:45:22,806 - father_agent.py:386 - Step: 1310, Training loss: 3.9219837188720703
2025-08-04 01:45:24,353 - father_agent.py:386 - Step: 1315, Training loss: 9.174166679382324
2025-08-04 01:45:25,912 - father_agent.py:386 - Step: 1320, Training loss: 4.140868186950684
2025-08-04 01:45:27,456 - father_agent.py:386 - Step: 1325, Training loss: 17.24014663696289
2025-08-04 01:45:29,006 - father_agent.py:386 - Step: 1330, Training loss: 5.697369575500488
2025-08-04 01:45:30,532 - father_agent.py:386 - Step: 1335, Training loss: 4.537062168121338
2025-08-04 01:45:32,065 - father_agent.py:386 - Step: 1340, Training loss: 8.28515338897705
2025-08-04 01:45:33,597 - father_agent.py:386 - Step: 1345, Training loss: 7.023104190826416
2025-08-04 01:45:35,139 - father_agent.py:386 - Step: 1350, Training loss: 4.812169551849365
2025-08-04 01:45:36,684 - father_agent.py:386 - Step: 1355, Training loss: 8.238702774047852
2025-08-04 01:45:38,219 - father_agent.py:386 - Step: 1360, Training loss: 4.503171920776367
2025-08-04 01:45:39,764 - father_agent.py:386 - Step: 1365, Training loss: 9.561517715454102
2025-08-04 01:45:41,294 - father_agent.py:386 - Step: 1370, Training loss: 6.523221492767334
2025-08-04 01:45:42,808 - father_agent.py:386 - Step: 1375, Training loss: 4.221332550048828
2025-08-04 01:45:44,338 - father_agent.py:386 - Step: 1380, Training loss: 25.654001235961914
2025-08-04 01:45:45,852 - father_agent.py:386 - Step: 1385, Training loss: 4.844359874725342
2025-08-04 01:45:47,361 - father_agent.py:386 - Step: 1390, Training loss: 4.502007961273193
2025-08-04 01:45:48,903 - father_agent.py:386 - Step: 1395, Training loss: 8.348024368286133
2025-08-04 01:45:50,470 - father_agent.py:386 - Step: 1400, Training loss: 4.559902667999268
2025-08-04 01:45:50,630 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:45:50,633 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:45:57,487 - evaluation_results_class.py:131 - Average Return = 7.065673351287842
2025-08-04 01:45:57,487 - evaluation_results_class.py:133 - Average Virtual Goal Value = 72.65673065185547
2025-08-04 01:45:57,487 - evaluation_results_class.py:135 - Average Discounted Reward = 44.25789260864258
2025-08-04 01:45:57,487 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:45:57,487 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:45:57,487 - evaluation_results_class.py:141 - Variance of Return = 21.225290298461914
2025-08-04 01:45:57,487 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:45:57,487 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:45:57,487 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:45:57,487 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:45:57,639 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:45:57,648 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:45:57,750 - father_agent.py:547 - Training finished.
2025-08-04 01:45:57,890 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:45:57,893 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:45:57,895 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 01:46:04,784 - evaluation_results_class.py:131 - Average Return = 7.034099578857422
2025-08-04 01:46:04,784 - evaluation_results_class.py:133 - Average Virtual Goal Value = 72.34099578857422
2025-08-04 01:46:04,784 - evaluation_results_class.py:135 - Average Discounted Reward = 44.00801086425781
2025-08-04 01:46:04,784 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:46:04,784 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:46:04,784 - evaluation_results_class.py:141 - Variance of Return = 21.56968879699707
2025-08-04 01:46:04,784 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:46:04,784 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:46:04,784 - evaluation_results_class.py:147 - Average Episode Length = 81.1242738065168
2025-08-04 01:46:04,784 - evaluation_results_class.py:149 - Counted Episodes = 3959
2025-08-04 01:46:04,938 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:46:04,940 - self_interpretable_extractor.py:286 - True
2025-08-04 01:46:04,949 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:46:17,306 - evaluation_results_class.py:131 - Average Return = 7.046827793121338
2025-08-04 01:46:17,306 - evaluation_results_class.py:133 - Average Virtual Goal Value = 72.46827697753906
2025-08-04 01:46:17,306 - evaluation_results_class.py:135 - Average Discounted Reward = 44.13024139404297
2025-08-04 01:46:17,306 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:46:17,306 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:46:17,307 - evaluation_results_class.py:141 - Variance of Return = 21.532047271728516
2025-08-04 01:46:17,307 - evaluation_results_class.py:143 - Current Best Return = 7.046827793121338
2025-08-04 01:46:17,307 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:46:17,307 - evaluation_results_class.py:147 - Average Episode Length = 80.92346424974824
2025-08-04 01:46:17,307 - evaluation_results_class.py:149 - Counted Episodes = 3972
2025-08-04 01:46:17,307 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 01:46:17,307 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 18352 trajectories
Learned trajectory lengths  {37, 157}
Buffer 1
2025-08-04 01:47:12,557 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 36704 trajectories
Learned trajectory lengths  {37, 157}
Buffer 2
2025-08-04 01:48:09,052 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 55056 trajectories
Learned trajectory lengths  {37, 157}
All trajectories collected
2025-08-04 01:49:05,200 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 01:49:05,200 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 55056 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 3
Model saved to fsc_0.dot.
Learned FSC of size 3
2025-08-04 01:49:17,905 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 01:49:29,622 - evaluation_results_class.py:131 - Average Return = 7.0108256340026855
2025-08-04 01:49:29,622 - evaluation_results_class.py:133 - Average Virtual Goal Value = 72.10826110839844
2025-08-04 01:49:29,622 - evaluation_results_class.py:135 - Average Discounted Reward = 43.873348236083984
2025-08-04 01:49:29,622 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:49:29,622 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:49:29,622 - evaluation_results_class.py:141 - Variance of Return = 21.270526885986328
2025-08-04 01:49:29,622 - evaluation_results_class.py:143 - Current Best Return = 7.0108256340026855
2025-08-04 01:49:29,622 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:49:29,622 - evaluation_results_class.py:147 - Average Episode Length = 80.92346424974824
2025-08-04 01:49:29,622 - evaluation_results_class.py:149 - Counted Episodes = 3972
FSC Result: {'best_episode_return': 72.10826, 'best_return': 7.0108256, 'goal_value': 0.0, 'returns_episodic': [72.10826], 'returns': [7.0108256], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [21.270527], 'each_episode_virtual_variance': [2127.053], 'combined_variance': [2573.7336], 'num_episodes': [3972], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [80.92346424974824], 'counted_episodes': [3972], 'discounted_rewards': [43.87335], 'new_pomdp_iteration_numbers': []}
[[6], [1, 4, 5], [3], [3], [3], [2], [2], [2], [6], [6], [6], [1, 5], [1, 4], [3], [2], [1], [0], [0], [0], [0]]
2025-08-04 01:49:32,474 - statistic.py:67 - synthesis initiated, design space: 140
2025-08-04 01:49:32,474 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 01:49:32,587 - synthesizer_ar.py:122 - value 16.2508 achieved after 792.2 seconds
2025-08-04 01:49:32,601 - synthesizer_ar.py:122 - value 8.9328 achieved after 792.21 seconds
2025-08-04 01:49:32,651 - synthesizer_ar.py:122 - value 8.609 achieved after 792.26 seconds
2025-08-04 01:49:32,692 - synthesizer_ar.py:122 - value 8.1685 achieved after 792.3 seconds
2025-08-04 01:49:32,726 - synthesizer_ar.py:122 - value 7.5614 achieved after 792.34 seconds
2025-08-04 01:49:32,754 - synthesizer_ar.py:122 - value 6.7158 achieved after 792.36 seconds
2025-08-04 01:49:32,766 - synthesizer_ar.py:122 - value 5.5249 achieved after 792.38 seconds
2025-08-04 01:49:32,771 - synthesizer_ar.py:122 - value 3.7746 achieved after 792.38 seconds
2025-08-04 01:49:32,771 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 01:49:32,772 - synthesizer.py:193 - SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 01:49:32,773 - synthesizer.py:198 - double-checking specification satisfiability:  : 3.7746132606522247
--------------------
Synthesis summary:
optimality objective: R{"packets_sent"}min=? [F "label_done"] 

method: AR, synthesis time: 0.3 s
number of holes: 4, family size: 140, quotient: 13042 states / 77741 actions
explored: 100 %
MDP stats: avg MDP size: 6484, iterations: 24

optimum: 3.774613
--------------------
2025-08-04 01:49:32,773 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 01:49:32,783 - robust_rl_trainer.py:432 - Iteration 2 of pure RL loop
2025-08-04 01:49:32,821 - storm_vec_env.py:70 - Computing row map
2025-08-04 01:49:32,827 - storm_vec_env.py:97 - Computing transitions
2025-08-04 01:49:32,838 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 01:49:32,838 - storm_vec_env.py:114 - Computing sinks
2025-08-04 01:49:32,838 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 01:49:32,842 - storm_vec_env.py:143 - Computing labels
2025-08-04 01:49:32,842 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 01:49:32,842 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 01:49:32,842 - storm_vec_env.py:175 - Computing observations
2025-08-04 01:49:32,940 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 01:49:32,940 - father_agent.py:540 - Before training evaluation.
2025-08-04 01:49:33,074 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:49:33,076 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:49:40,720 - evaluation_results_class.py:131 - Average Return = 4.403942108154297
2025-08-04 01:49:40,720 - evaluation_results_class.py:133 - Average Virtual Goal Value = 46.03942108154297
2025-08-04 01:49:40,720 - evaluation_results_class.py:135 - Average Discounted Reward = 34.062835693359375
2025-08-04 01:49:40,721 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:49:40,721 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:49:40,721 - evaluation_results_class.py:141 - Variance of Return = 8.406464576721191
2025-08-04 01:49:40,721 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:49:40,721 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:49:40,721 - evaluation_results_class.py:147 - Average Episode Length = 45.90844165119269
2025-08-04 01:49:40,721 - evaluation_results_class.py:149 - Counted Episodes = 7001
2025-08-04 01:49:40,879 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:49:40,887 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:49:40,991 - father_agent.py:436 - Training agent on-policy
2025-08-04 01:49:47,289 - father_agent.py:386 - Step: 0, Training loss: 12.38756275177002
2025-08-04 01:49:51,781 - father_agent.py:386 - Step: 5, Training loss: 6.417876720428467
2025-08-04 01:49:53,409 - father_agent.py:386 - Step: 10, Training loss: 16.203420639038086
2025-08-04 01:49:55,032 - father_agent.py:386 - Step: 15, Training loss: 4.9626688957214355
2025-08-04 01:49:56,653 - father_agent.py:386 - Step: 20, Training loss: 25.047170639038086
2025-08-04 01:49:58,265 - father_agent.py:386 - Step: 25, Training loss: 12.098066329956055
2025-08-04 01:49:59,863 - father_agent.py:386 - Step: 30, Training loss: 7.195621490478516
2025-08-04 01:50:01,471 - father_agent.py:386 - Step: 35, Training loss: 14.559027671813965
2025-08-04 01:50:03,077 - father_agent.py:386 - Step: 40, Training loss: 6.987435817718506
2025-08-04 01:50:04,689 - father_agent.py:386 - Step: 45, Training loss: 9.52330493927002
2025-08-04 01:50:06,293 - father_agent.py:386 - Step: 50, Training loss: 12.43138599395752
2025-08-04 01:50:07,878 - father_agent.py:386 - Step: 55, Training loss: 6.765566349029541
2025-08-04 01:50:09,495 - father_agent.py:386 - Step: 60, Training loss: 13.678986549377441
2025-08-04 01:50:11,104 - father_agent.py:386 - Step: 65, Training loss: 9.660581588745117
2025-08-04 01:50:12,727 - father_agent.py:386 - Step: 70, Training loss: 7.344988822937012
2025-08-04 01:50:14,356 - father_agent.py:386 - Step: 75, Training loss: 17.06846046447754
2025-08-04 01:50:15,975 - father_agent.py:386 - Step: 80, Training loss: 6.13948917388916
2025-08-04 01:50:17,593 - father_agent.py:386 - Step: 85, Training loss: 14.830805778503418
2025-08-04 01:50:19,210 - father_agent.py:386 - Step: 90, Training loss: 10.999381065368652
2025-08-04 01:50:20,818 - father_agent.py:386 - Step: 95, Training loss: 6.801144599914551
2025-08-04 01:50:22,427 - father_agent.py:386 - Step: 100, Training loss: 14.962964057922363
2025-08-04 01:50:22,595 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:50:22,598 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:50:30,190 - evaluation_results_class.py:131 - Average Return = 4.437223434448242
2025-08-04 01:50:30,190 - evaluation_results_class.py:133 - Average Virtual Goal Value = 46.37223434448242
2025-08-04 01:50:30,190 - evaluation_results_class.py:135 - Average Discounted Reward = 34.34308624267578
2025-08-04 01:50:30,190 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:50:30,190 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:50:30,190 - evaluation_results_class.py:141 - Variance of Return = 8.36889934539795
2025-08-04 01:50:30,190 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:50:30,190 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:50:30,190 - evaluation_results_class.py:147 - Average Episode Length = 45.90844165119269
2025-08-04 01:50:30,191 - evaluation_results_class.py:149 - Counted Episodes = 7001
2025-08-04 01:50:30,359 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:50:30,369 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:50:33,082 - father_agent.py:386 - Step: 105, Training loss: 9.382793426513672
2025-08-04 01:50:34,682 - father_agent.py:386 - Step: 110, Training loss: 8.128978729248047
2025-08-04 01:50:36,309 - father_agent.py:386 - Step: 115, Training loss: 9.367907524108887
2025-08-04 01:50:37,921 - father_agent.py:386 - Step: 120, Training loss: 5.896580696105957
2025-08-04 01:50:39,536 - father_agent.py:386 - Step: 125, Training loss: 12.005600929260254
2025-08-04 01:50:41,137 - father_agent.py:386 - Step: 130, Training loss: 8.911103248596191
2025-08-04 01:50:42,730 - father_agent.py:386 - Step: 135, Training loss: 7.873692035675049
2025-08-04 01:50:44,340 - father_agent.py:386 - Step: 140, Training loss: 10.279001235961914
2025-08-04 01:50:45,932 - father_agent.py:386 - Step: 145, Training loss: 8.417340278625488
2025-08-04 01:50:47,546 - father_agent.py:386 - Step: 150, Training loss: 7.905322074890137
2025-08-04 01:50:49,171 - father_agent.py:386 - Step: 155, Training loss: 10.496614456176758
2025-08-04 01:50:50,776 - father_agent.py:386 - Step: 160, Training loss: 6.433138370513916
2025-08-04 01:50:52,374 - father_agent.py:386 - Step: 165, Training loss: 11.035139083862305
2025-08-04 01:50:53,986 - father_agent.py:386 - Step: 170, Training loss: 7.589251518249512
2025-08-04 01:50:55,586 - father_agent.py:386 - Step: 175, Training loss: 8.495037078857422
2025-08-04 01:50:57,203 - father_agent.py:386 - Step: 180, Training loss: 10.643389701843262
2025-08-04 01:50:58,814 - father_agent.py:386 - Step: 185, Training loss: 6.639495372772217
2025-08-04 01:51:00,431 - father_agent.py:386 - Step: 190, Training loss: 6.734719276428223
2025-08-04 01:51:02,044 - father_agent.py:386 - Step: 195, Training loss: 9.715599060058594
2025-08-04 01:51:03,706 - father_agent.py:386 - Step: 200, Training loss: 9.113243103027344
2025-08-04 01:51:03,879 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:51:03,881 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:51:11,661 - evaluation_results_class.py:131 - Average Return = 4.478074550628662
2025-08-04 01:51:11,661 - evaluation_results_class.py:133 - Average Virtual Goal Value = 46.78074645996094
2025-08-04 01:51:11,661 - evaluation_results_class.py:135 - Average Discounted Reward = 34.634971618652344
2025-08-04 01:51:11,661 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:51:11,661 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:51:11,661 - evaluation_results_class.py:141 - Variance of Return = 8.33093547821045
2025-08-04 01:51:11,661 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:51:11,661 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:51:11,661 - evaluation_results_class.py:147 - Average Episode Length = 45.90844165119269
2025-08-04 01:51:11,661 - evaluation_results_class.py:149 - Counted Episodes = 7001
2025-08-04 01:51:11,833 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:51:11,842 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:51:14,674 - father_agent.py:386 - Step: 205, Training loss: 8.382497787475586
2025-08-04 01:51:16,376 - father_agent.py:386 - Step: 210, Training loss: 7.63386869430542
2025-08-04 01:51:18,085 - father_agent.py:386 - Step: 215, Training loss: 9.426125526428223
2025-08-04 01:51:19,825 - father_agent.py:386 - Step: 220, Training loss: 5.7563934326171875
2025-08-04 01:51:21,539 - father_agent.py:386 - Step: 225, Training loss: 10.45729923248291
2025-08-04 01:51:23,263 - father_agent.py:386 - Step: 230, Training loss: 7.93340539932251
2025-08-04 01:51:24,988 - father_agent.py:386 - Step: 235, Training loss: 7.432378768920898
2025-08-04 01:51:26,684 - father_agent.py:386 - Step: 240, Training loss: 8.887104988098145
2025-08-04 01:51:28,345 - father_agent.py:386 - Step: 245, Training loss: 7.892246723175049
2025-08-04 01:51:30,001 - father_agent.py:386 - Step: 250, Training loss: 8.012486457824707
2025-08-04 01:51:31,665 - father_agent.py:386 - Step: 255, Training loss: 8.936629295349121
2025-08-04 01:51:33,351 - father_agent.py:386 - Step: 260, Training loss: 6.011392116546631
2025-08-04 01:51:35,018 - father_agent.py:386 - Step: 265, Training loss: 9.939997673034668
2025-08-04 01:51:36,625 - father_agent.py:386 - Step: 270, Training loss: 7.9264373779296875
2025-08-04 01:51:38,226 - father_agent.py:386 - Step: 275, Training loss: 9.494595527648926
2025-08-04 01:51:39,840 - father_agent.py:386 - Step: 280, Training loss: 11.084484100341797
2025-08-04 01:51:41,435 - father_agent.py:386 - Step: 285, Training loss: 6.033105373382568
2025-08-04 01:51:43,036 - father_agent.py:386 - Step: 290, Training loss: 6.922336578369141
2025-08-04 01:51:44,650 - father_agent.py:386 - Step: 295, Training loss: 8.985939025878906
2025-08-04 01:51:46,246 - father_agent.py:386 - Step: 300, Training loss: 7.66467809677124
2025-08-04 01:51:46,422 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:51:46,425 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:51:54,068 - evaluation_results_class.py:131 - Average Return = 4.423939228057861
2025-08-04 01:51:54,068 - evaluation_results_class.py:133 - Average Virtual Goal Value = 46.23939514160156
2025-08-04 01:51:54,068 - evaluation_results_class.py:135 - Average Discounted Reward = 34.22550964355469
2025-08-04 01:51:54,068 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:51:54,068 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:51:54,069 - evaluation_results_class.py:141 - Variance of Return = 8.486466407775879
2025-08-04 01:51:54,069 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:51:54,069 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:51:54,069 - evaluation_results_class.py:147 - Average Episode Length = 45.90844165119269
2025-08-04 01:51:54,069 - evaluation_results_class.py:149 - Counted Episodes = 7001
2025-08-04 01:51:54,235 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:51:54,244 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:51:56,942 - father_agent.py:386 - Step: 305, Training loss: 8.672874450683594
2025-08-04 01:51:58,554 - father_agent.py:386 - Step: 310, Training loss: 7.474850177764893
2025-08-04 01:52:00,277 - father_agent.py:386 - Step: 315, Training loss: 8.812288284301758
2025-08-04 01:52:01,990 - father_agent.py:386 - Step: 320, Training loss: 5.8771185874938965
2025-08-04 01:52:03,758 - father_agent.py:386 - Step: 325, Training loss: 11.911898612976074
2025-08-04 01:52:05,467 - father_agent.py:386 - Step: 330, Training loss: 8.036971092224121
2025-08-04 01:52:07,135 - father_agent.py:386 - Step: 335, Training loss: 6.761069297790527
2025-08-04 01:52:08,751 - father_agent.py:386 - Step: 340, Training loss: 9.101456642150879
2025-08-04 01:52:10,329 - father_agent.py:386 - Step: 345, Training loss: 7.833388328552246
2025-08-04 01:52:11,927 - father_agent.py:386 - Step: 350, Training loss: 7.334747791290283
2025-08-04 01:52:13,531 - father_agent.py:386 - Step: 355, Training loss: 9.042901992797852
2025-08-04 01:52:15,159 - father_agent.py:386 - Step: 360, Training loss: 6.028038501739502
2025-08-04 01:52:16,790 - father_agent.py:386 - Step: 365, Training loss: 10.55138874053955
2025-08-04 01:52:18,439 - father_agent.py:386 - Step: 370, Training loss: 7.6354451179504395
2025-08-04 01:52:20,121 - father_agent.py:386 - Step: 375, Training loss: 8.686180114746094
2025-08-04 01:52:21,809 - father_agent.py:386 - Step: 380, Training loss: 10.596264839172363
2025-08-04 01:52:23,460 - father_agent.py:386 - Step: 385, Training loss: 6.355492115020752
2025-08-04 01:52:25,111 - father_agent.py:386 - Step: 390, Training loss: 7.398764133453369
2025-08-04 01:52:26,764 - father_agent.py:386 - Step: 395, Training loss: 8.855416297912598
2025-08-04 01:52:28,431 - father_agent.py:386 - Step: 400, Training loss: 7.425912380218506
2025-08-04 01:52:28,657 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:52:28,660 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:52:36,792 - evaluation_results_class.py:131 - Average Return = 4.444793701171875
2025-08-04 01:52:36,792 - evaluation_results_class.py:133 - Average Virtual Goal Value = 46.44793701171875
2025-08-04 01:52:36,792 - evaluation_results_class.py:135 - Average Discounted Reward = 34.38071823120117
2025-08-04 01:52:36,792 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:52:36,792 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:52:36,792 - evaluation_results_class.py:141 - Variance of Return = 8.360077857971191
2025-08-04 01:52:36,792 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:52:36,792 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:52:36,792 - evaluation_results_class.py:147 - Average Episode Length = 45.90844165119269
2025-08-04 01:52:36,792 - evaluation_results_class.py:149 - Counted Episodes = 7001
2025-08-04 01:52:36,959 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:52:36,968 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:52:39,685 - father_agent.py:386 - Step: 405, Training loss: 8.219520568847656
2025-08-04 01:52:41,331 - father_agent.py:386 - Step: 410, Training loss: 7.338738918304443
2025-08-04 01:52:42,992 - father_agent.py:386 - Step: 415, Training loss: 8.361550331115723
2025-08-04 01:52:44,652 - father_agent.py:386 - Step: 420, Training loss: 5.261867523193359
2025-08-04 01:52:46,310 - father_agent.py:386 - Step: 425, Training loss: 10.405523300170898
2025-08-04 01:52:48,035 - father_agent.py:386 - Step: 430, Training loss: 7.694761276245117
2025-08-04 01:52:49,743 - father_agent.py:386 - Step: 435, Training loss: 6.7988810539245605
2025-08-04 01:52:51,422 - father_agent.py:386 - Step: 440, Training loss: 8.218293190002441
2025-08-04 01:52:53,091 - father_agent.py:386 - Step: 445, Training loss: 7.609635829925537
2025-08-04 01:52:54,763 - father_agent.py:386 - Step: 450, Training loss: 7.166176795959473
2025-08-04 01:52:56,455 - father_agent.py:386 - Step: 455, Training loss: 8.463540077209473
2025-08-04 01:52:58,158 - father_agent.py:386 - Step: 460, Training loss: 5.837388038635254
2025-08-04 01:53:00,005 - father_agent.py:386 - Step: 465, Training loss: 9.831298828125
2025-08-04 01:53:01,749 - father_agent.py:386 - Step: 470, Training loss: 6.92857027053833
2025-08-04 01:53:03,485 - father_agent.py:386 - Step: 475, Training loss: 9.784326553344727
2025-08-04 01:53:05,222 - father_agent.py:386 - Step: 480, Training loss: 10.191575050354004
2025-08-04 01:53:06,936 - father_agent.py:386 - Step: 485, Training loss: 5.774395942687988
2025-08-04 01:53:08,703 - father_agent.py:386 - Step: 490, Training loss: 6.944167137145996
2025-08-04 01:53:10,401 - father_agent.py:386 - Step: 495, Training loss: 8.261350631713867
2025-08-04 01:53:12,070 - father_agent.py:386 - Step: 500, Training loss: 7.382389545440674
2025-08-04 01:53:12,253 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:53:12,256 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:53:20,301 - evaluation_results_class.py:131 - Average Return = 4.480931282043457
2025-08-04 01:53:20,301 - evaluation_results_class.py:133 - Average Virtual Goal Value = 46.8093147277832
2025-08-04 01:53:20,301 - evaluation_results_class.py:135 - Average Discounted Reward = 34.646156311035156
2025-08-04 01:53:20,301 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:53:20,301 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:53:20,301 - evaluation_results_class.py:141 - Variance of Return = 8.429895401000977
2025-08-04 01:53:20,301 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:53:20,301 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:53:20,301 - evaluation_results_class.py:147 - Average Episode Length = 45.90844165119269
2025-08-04 01:53:20,301 - evaluation_results_class.py:149 - Counted Episodes = 7001
2025-08-04 01:53:20,482 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:53:20,492 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:53:20,602 - father_agent.py:547 - Training finished.
2025-08-04 01:53:20,749 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:53:20,751 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:53:20,754 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 01:53:28,600 - evaluation_results_class.py:131 - Average Return = 4.484787940979004
2025-08-04 01:53:28,600 - evaluation_results_class.py:133 - Average Virtual Goal Value = 46.847877502441406
2025-08-04 01:53:28,600 - evaluation_results_class.py:135 - Average Discounted Reward = 34.6951904296875
2025-08-04 01:53:28,600 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:53:28,601 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:53:28,601 - evaluation_results_class.py:141 - Variance of Return = 8.139212608337402
2025-08-04 01:53:28,601 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:53:28,601 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:53:28,601 - evaluation_results_class.py:147 - Average Episode Length = 45.90844165119269
2025-08-04 01:53:28,601 - evaluation_results_class.py:149 - Counted Episodes = 7001
2025-08-04 01:53:28,775 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:53:28,777 - self_interpretable_extractor.py:286 - True
2025-08-04 01:53:28,788 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:53:43,143 - evaluation_results_class.py:131 - Average Return = 4.41665506362915
2025-08-04 01:53:43,143 - evaluation_results_class.py:133 - Average Virtual Goal Value = 46.16654968261719
2025-08-04 01:53:43,143 - evaluation_results_class.py:135 - Average Discounted Reward = 34.273048400878906
2025-08-04 01:53:43,143 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:53:43,143 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:53:43,143 - evaluation_results_class.py:141 - Variance of Return = 8.125357627868652
2025-08-04 01:53:43,143 - evaluation_results_class.py:143 - Current Best Return = 4.41665506362915
2025-08-04 01:53:43,143 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:53:43,143 - evaluation_results_class.py:147 - Average Episode Length = 45.625121443442055
2025-08-04 01:53:43,143 - evaluation_results_class.py:149 - Counted Episodes = 7205
2025-08-04 01:53:43,143 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 01:53:43,144 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 32392 trajectories
Learned trajectory lengths  {37, 157}
Buffer 1
2025-08-04 01:54:43,972 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 64784 trajectories
Learned trajectory lengths  {37, 157}
Buffer 2
2025-08-04 01:55:43,534 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 97176 trajectories
Learned trajectory lengths  {37, 157}
All trajectories collected
2025-08-04 01:56:42,889 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 01:56:42,889 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 97176 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 3
Model saved to fsc_1.dot.
Learned FSC of size 3
2025-08-04 01:56:50,359 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 01:57:03,024 - evaluation_results_class.py:131 - Average Return = 4.435947418212891
2025-08-04 01:57:03,024 - evaluation_results_class.py:133 - Average Virtual Goal Value = 46.359474182128906
2025-08-04 01:57:03,025 - evaluation_results_class.py:135 - Average Discounted Reward = 34.45440673828125
2025-08-04 01:57:03,025 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:57:03,025 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:57:03,025 - evaluation_results_class.py:141 - Variance of Return = 7.917513847351074
2025-08-04 01:57:03,025 - evaluation_results_class.py:143 - Current Best Return = 4.435947418212891
2025-08-04 01:57:03,025 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:57:03,025 - evaluation_results_class.py:147 - Average Episode Length = 45.625121443442055
2025-08-04 01:57:03,025 - evaluation_results_class.py:149 - Counted Episodes = 7205
FSC Result: {'best_episode_return': 46.359474, 'best_return': 4.4359474, 'goal_value': 0.0, 'returns_episodic': [46.359474], 'returns': [4.4359474], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [7.917514], 'each_episode_virtual_variance': [791.7514], 'combined_variance': [958.01917], 'num_episodes': [7205], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [45.625121443442055], 'counted_episodes': [7205], 'discounted_rewards': [34.454407], 'new_pomdp_iteration_numbers': []}
[[6], [1, 4, 5], [3], [3], [3], [2], [2], [2], [6], [6], [6], [1, 5], [1, 4], [3], [2], [1], [0], [0], [0], [0]]
2025-08-04 01:57:03,257 - statistic.py:67 - synthesis initiated, design space: 140
2025-08-04 01:57:03,258 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 01:57:03,372 - synthesizer_ar.py:122 - value 16.2603 achieved after 1242.98 seconds
2025-08-04 01:57:03,387 - synthesizer_ar.py:122 - value 8.9374 achieved after 1243.0 seconds
2025-08-04 01:57:03,439 - synthesizer_ar.py:122 - value 8.6144 achieved after 1243.05 seconds
2025-08-04 01:57:03,481 - synthesizer_ar.py:122 - value 8.1744 achieved after 1243.09 seconds
2025-08-04 01:57:03,515 - synthesizer_ar.py:122 - value 7.5673 achieved after 1243.13 seconds
2025-08-04 01:57:03,544 - synthesizer_ar.py:122 - value 6.7204 achieved after 1243.15 seconds
2025-08-04 01:57:03,556 - synthesizer_ar.py:122 - value 5.5263 achieved after 1243.17 seconds
2025-08-04 01:57:03,562 - synthesizer_ar.py:122 - value 3.7678 achieved after 1243.17 seconds
2025-08-04 01:57:03,562 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 01:57:03,562 - synthesizer.py:193 - SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 01:57:03,564 - synthesizer.py:198 - double-checking specification satisfiability:  : 3.7678377210585503
--------------------
Synthesis summary:
optimality objective: R{"packets_sent"}min=? [F "label_done"] 

method: AR, synthesis time: 0.3 s
number of holes: 4, family size: 140, quotient: 13042 states / 77741 actions
explored: 100 %
MDP stats: avg MDP size: 6484, iterations: 24

optimum: 3.767838
--------------------
2025-08-04 01:57:03,564 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 01:57:03,567 - robust_rl_trainer.py:432 - Iteration 3 of pure RL loop
2025-08-04 01:57:03,613 - storm_vec_env.py:70 - Computing row map
2025-08-04 01:57:03,619 - storm_vec_env.py:97 - Computing transitions
2025-08-04 01:57:03,631 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 01:57:03,631 - storm_vec_env.py:114 - Computing sinks
2025-08-04 01:57:03,631 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 01:57:03,634 - storm_vec_env.py:143 - Computing labels
2025-08-04 01:57:03,634 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 01:57:03,634 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 01:57:03,634 - storm_vec_env.py:175 - Computing observations
2025-08-04 01:57:03,753 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 01:57:03,755 - father_agent.py:540 - Before training evaluation.
2025-08-04 01:57:03,894 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:57:03,896 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:57:12,191 - evaluation_results_class.py:131 - Average Return = 4.048919677734375
2025-08-04 01:57:12,191 - evaluation_results_class.py:133 - Average Virtual Goal Value = 42.48919677734375
2025-08-04 01:57:12,191 - evaluation_results_class.py:135 - Average Discounted Reward = 33.12118911743164
2025-08-04 01:57:12,191 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:57:12,191 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:57:12,191 - evaluation_results_class.py:141 - Variance of Return = 4.573966026306152
2025-08-04 01:57:12,191 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:57:12,191 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:57:12,191 - evaluation_results_class.py:147 - Average Episode Length = 39.91631487459647
2025-08-04 01:57:12,191 - evaluation_results_class.py:149 - Counted Episodes = 8054
2025-08-04 01:57:12,372 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:57:12,382 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:57:12,491 - father_agent.py:436 - Training agent on-policy
2025-08-04 01:57:19,549 - father_agent.py:386 - Step: 0, Training loss: 7.985179424285889
2025-08-04 01:57:24,383 - father_agent.py:386 - Step: 5, Training loss: 8.026209831237793
2025-08-04 01:57:26,156 - father_agent.py:386 - Step: 10, Training loss: 8.97091007232666
2025-08-04 01:57:27,882 - father_agent.py:386 - Step: 15, Training loss: 6.702341556549072
2025-08-04 01:57:29,630 - father_agent.py:386 - Step: 20, Training loss: 12.238347053527832
2025-08-04 01:57:31,349 - father_agent.py:386 - Step: 25, Training loss: 7.87360954284668
2025-08-04 01:57:33,097 - father_agent.py:386 - Step: 30, Training loss: 7.600316524505615
2025-08-04 01:57:34,821 - father_agent.py:386 - Step: 35, Training loss: 9.27884578704834
2025-08-04 01:57:36,556 - father_agent.py:386 - Step: 40, Training loss: 6.2266645431518555
2025-08-04 01:57:38,294 - father_agent.py:386 - Step: 45, Training loss: 9.839570045471191
2025-08-04 01:57:39,986 - father_agent.py:386 - Step: 50, Training loss: 8.748648643493652
2025-08-04 01:57:41,666 - father_agent.py:386 - Step: 55, Training loss: 8.02119255065918
2025-08-04 01:57:43,374 - father_agent.py:386 - Step: 60, Training loss: 9.189713478088379
2025-08-04 01:57:45,065 - father_agent.py:386 - Step: 65, Training loss: 7.612025260925293
2025-08-04 01:57:46,742 - father_agent.py:386 - Step: 70, Training loss: 9.218679428100586
2025-08-04 01:57:48,459 - father_agent.py:386 - Step: 75, Training loss: 9.084508895874023
2025-08-04 01:57:50,206 - father_agent.py:386 - Step: 80, Training loss: 6.149250030517578
2025-08-04 01:57:51,870 - father_agent.py:386 - Step: 85, Training loss: 11.894145011901855
2025-08-04 01:57:53,564 - father_agent.py:386 - Step: 90, Training loss: 8.172625541687012
2025-08-04 01:57:55,247 - father_agent.py:386 - Step: 95, Training loss: 9.402008056640625
2025-08-04 01:57:56,945 - father_agent.py:386 - Step: 100, Training loss: 8.051250457763672
2025-08-04 01:57:57,137 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:57:57,139 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:58:05,512 - evaluation_results_class.py:131 - Average Return = 3.9914329051971436
2025-08-04 01:58:05,513 - evaluation_results_class.py:133 - Average Virtual Goal Value = 41.914329528808594
2025-08-04 01:58:05,513 - evaluation_results_class.py:135 - Average Discounted Reward = 32.6192626953125
2025-08-04 01:58:05,513 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:58:05,513 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:58:05,513 - evaluation_results_class.py:141 - Variance of Return = 4.7850022315979
2025-08-04 01:58:05,513 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:58:05,513 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:58:05,513 - evaluation_results_class.py:147 - Average Episode Length = 39.91631487459647
2025-08-04 01:58:05,513 - evaluation_results_class.py:149 - Counted Episodes = 8054
2025-08-04 01:58:05,703 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:58:05,714 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:58:08,861 - father_agent.py:386 - Step: 105, Training loss: 6.210322380065918
2025-08-04 01:58:10,575 - father_agent.py:386 - Step: 110, Training loss: 8.884469032287598
2025-08-04 01:58:12,296 - father_agent.py:386 - Step: 115, Training loss: 6.565694332122803
2025-08-04 01:58:13,991 - father_agent.py:386 - Step: 120, Training loss: 6.460468769073486
2025-08-04 01:58:15,698 - father_agent.py:386 - Step: 125, Training loss: 10.165428161621094
2025-08-04 01:58:17,409 - father_agent.py:386 - Step: 130, Training loss: 6.331076145172119
2025-08-04 01:58:19,090 - father_agent.py:386 - Step: 135, Training loss: 8.547256469726562
2025-08-04 01:58:20,772 - father_agent.py:386 - Step: 140, Training loss: 7.07229471206665
2025-08-04 01:58:22,456 - father_agent.py:386 - Step: 145, Training loss: 6.819034576416016
2025-08-04 01:58:24,132 - father_agent.py:386 - Step: 150, Training loss: 7.736717224121094
2025-08-04 01:58:25,807 - father_agent.py:386 - Step: 155, Training loss: 7.241245746612549
2025-08-04 01:58:27,489 - father_agent.py:386 - Step: 160, Training loss: 7.387612819671631
2025-08-04 01:58:29,214 - father_agent.py:386 - Step: 165, Training loss: 8.524864196777344
2025-08-04 01:58:30,906 - father_agent.py:386 - Step: 170, Training loss: 6.389388561248779
2025-08-04 01:58:32,589 - father_agent.py:386 - Step: 175, Training loss: 10.0
2025-08-04 01:58:34,284 - father_agent.py:386 - Step: 180, Training loss: 7.521238327026367
2025-08-04 01:58:35,970 - father_agent.py:386 - Step: 185, Training loss: 6.496154308319092
2025-08-04 01:58:37,662 - father_agent.py:386 - Step: 190, Training loss: 8.01513957977295
2025-08-04 01:58:39,370 - father_agent.py:386 - Step: 195, Training loss: 7.47514533996582
2025-08-04 01:58:41,074 - father_agent.py:386 - Step: 200, Training loss: 8.689620971679688
2025-08-04 01:58:41,268 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:58:41,270 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:58:49,750 - evaluation_results_class.py:131 - Average Return = 4.0743727684021
2025-08-04 01:58:49,750 - evaluation_results_class.py:133 - Average Virtual Goal Value = 42.74372863769531
2025-08-04 01:58:49,750 - evaluation_results_class.py:135 - Average Discounted Reward = 33.27379608154297
2025-08-04 01:58:49,750 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:58:49,750 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:58:49,750 - evaluation_results_class.py:141 - Variance of Return = 4.672020435333252
2025-08-04 01:58:49,750 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:58:49,750 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:58:49,750 - evaluation_results_class.py:147 - Average Episode Length = 39.91631487459647
2025-08-04 01:58:49,750 - evaluation_results_class.py:149 - Counted Episodes = 8054
2025-08-04 01:58:49,945 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:58:49,956 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:58:53,116 - father_agent.py:386 - Step: 205, Training loss: 7.093098163604736
2025-08-04 01:58:54,830 - father_agent.py:386 - Step: 210, Training loss: 9.75420093536377
2025-08-04 01:58:56,557 - father_agent.py:386 - Step: 215, Training loss: 7.354057788848877
2025-08-04 01:58:58,263 - father_agent.py:386 - Step: 220, Training loss: 5.848879337310791
2025-08-04 01:58:59,974 - father_agent.py:386 - Step: 225, Training loss: 10.23741626739502
2025-08-04 01:59:01,684 - father_agent.py:386 - Step: 230, Training loss: 6.5014142990112305
2025-08-04 01:59:03,398 - father_agent.py:386 - Step: 235, Training loss: 6.372105598449707
2025-08-04 01:59:05,129 - father_agent.py:386 - Step: 240, Training loss: 7.794630527496338
2025-08-04 01:59:06,827 - father_agent.py:386 - Step: 245, Training loss: 7.944447040557861
2025-08-04 01:59:08,528 - father_agent.py:386 - Step: 250, Training loss: 8.757734298706055
2025-08-04 01:59:10,205 - father_agent.py:386 - Step: 255, Training loss: 7.821774482727051
2025-08-04 01:59:11,887 - father_agent.py:386 - Step: 260, Training loss: 7.623211860656738
2025-08-04 01:59:13,577 - father_agent.py:386 - Step: 265, Training loss: 9.328943252563477
2025-08-04 01:59:15,232 - father_agent.py:386 - Step: 270, Training loss: 5.769943714141846
2025-08-04 01:59:16,896 - father_agent.py:386 - Step: 275, Training loss: 9.523579597473145
2025-08-04 01:59:18,588 - father_agent.py:386 - Step: 280, Training loss: 6.921686172485352
2025-08-04 01:59:20,290 - father_agent.py:386 - Step: 285, Training loss: 5.569825649261475
2025-08-04 01:59:21,975 - father_agent.py:386 - Step: 290, Training loss: 7.518280029296875
2025-08-04 01:59:23,662 - father_agent.py:386 - Step: 295, Training loss: 7.700876712799072
2025-08-04 01:59:25,368 - father_agent.py:386 - Step: 300, Training loss: 9.598238945007324
2025-08-04 01:59:25,582 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:59:25,585 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:59:34,066 - evaluation_results_class.py:131 - Average Return = 4.074000358581543
2025-08-04 01:59:34,067 - evaluation_results_class.py:133 - Average Virtual Goal Value = 42.74000549316406
2025-08-04 01:59:34,067 - evaluation_results_class.py:135 - Average Discounted Reward = 33.3250617980957
2025-08-04 01:59:34,067 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 01:59:34,067 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 01:59:34,067 - evaluation_results_class.py:141 - Variance of Return = 4.66822624206543
2025-08-04 01:59:34,067 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 01:59:34,067 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 01:59:34,067 - evaluation_results_class.py:147 - Average Episode Length = 39.91631487459647
2025-08-04 01:59:34,067 - evaluation_results_class.py:149 - Counted Episodes = 8054
2025-08-04 01:59:34,270 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:59:34,280 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 01:59:37,400 - father_agent.py:386 - Step: 305, Training loss: 6.195636749267578
2025-08-04 01:59:39,088 - father_agent.py:386 - Step: 310, Training loss: 8.27463150024414
2025-08-04 01:59:40,823 - father_agent.py:386 - Step: 315, Training loss: 7.5262250900268555
2025-08-04 01:59:42,528 - father_agent.py:386 - Step: 320, Training loss: 5.3919291496276855
2025-08-04 01:59:44,217 - father_agent.py:386 - Step: 325, Training loss: 10.675625801086426
2025-08-04 01:59:45,910 - father_agent.py:386 - Step: 330, Training loss: 6.288567066192627
2025-08-04 01:59:47,596 - father_agent.py:386 - Step: 335, Training loss: 7.014297008514404
2025-08-04 01:59:49,311 - father_agent.py:386 - Step: 340, Training loss: 7.5953755378723145
2025-08-04 01:59:50,995 - father_agent.py:386 - Step: 345, Training loss: 7.079802989959717
2025-08-04 01:59:52,683 - father_agent.py:386 - Step: 350, Training loss: 10.130539894104004
2025-08-04 01:59:54,381 - father_agent.py:386 - Step: 355, Training loss: 7.57766580581665
2025-08-04 01:59:56,066 - father_agent.py:386 - Step: 360, Training loss: 6.6741251945495605
2025-08-04 01:59:57,751 - father_agent.py:386 - Step: 365, Training loss: 8.09565258026123
2025-08-04 01:59:59,433 - father_agent.py:386 - Step: 370, Training loss: 5.850675582885742
2025-08-04 02:00:01,090 - father_agent.py:386 - Step: 375, Training loss: 9.489513397216797
2025-08-04 02:00:02,774 - father_agent.py:386 - Step: 380, Training loss: 8.15880012512207
2025-08-04 02:00:04,434 - father_agent.py:386 - Step: 385, Training loss: 5.290923118591309
2025-08-04 02:00:06,111 - father_agent.py:386 - Step: 390, Training loss: 7.855042934417725
2025-08-04 02:00:07,774 - father_agent.py:386 - Step: 395, Training loss: 7.58971643447876
2025-08-04 02:00:09,517 - father_agent.py:386 - Step: 400, Training loss: 6.753902435302734
2025-08-04 02:00:09,717 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:00:09,720 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:00:19,578 - evaluation_results_class.py:131 - Average Return = 4.009063720703125
2025-08-04 02:00:19,578 - evaluation_results_class.py:133 - Average Virtual Goal Value = 42.09063720703125
2025-08-04 02:00:19,578 - evaluation_results_class.py:135 - Average Discounted Reward = 32.820701599121094
2025-08-04 02:00:19,578 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:00:19,578 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:00:19,578 - evaluation_results_class.py:141 - Variance of Return = 4.741288185119629
2025-08-04 02:00:19,578 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:00:19,578 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:00:19,578 - evaluation_results_class.py:147 - Average Episode Length = 39.91631487459647
2025-08-04 02:00:19,578 - evaluation_results_class.py:149 - Counted Episodes = 8054
2025-08-04 02:00:19,773 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:00:19,783 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:00:22,913 - father_agent.py:386 - Step: 405, Training loss: 6.779077053070068
2025-08-04 02:00:24,613 - father_agent.py:386 - Step: 410, Training loss: 8.675451278686523
2025-08-04 02:00:26,307 - father_agent.py:386 - Step: 415, Training loss: 6.697527885437012
2025-08-04 02:00:28,003 - father_agent.py:386 - Step: 420, Training loss: 5.763476848602295
2025-08-04 02:00:29,683 - father_agent.py:386 - Step: 425, Training loss: 10.470711708068848
2025-08-04 02:00:31,389 - father_agent.py:386 - Step: 430, Training loss: 6.306999683380127
2025-08-04 02:00:33,105 - father_agent.py:386 - Step: 435, Training loss: 7.702497482299805
2025-08-04 02:00:34,836 - father_agent.py:386 - Step: 440, Training loss: 7.610500335693359
2025-08-04 02:00:36,513 - father_agent.py:386 - Step: 445, Training loss: 6.435463905334473
2025-08-04 02:00:38,147 - father_agent.py:386 - Step: 450, Training loss: 8.162418365478516
2025-08-04 02:00:39,788 - father_agent.py:386 - Step: 455, Training loss: 7.51401948928833
2025-08-04 02:00:41,445 - father_agent.py:386 - Step: 460, Training loss: 6.773531913757324
2025-08-04 02:00:43,135 - father_agent.py:386 - Step: 465, Training loss: 8.10786247253418
2025-08-04 02:00:44,783 - father_agent.py:386 - Step: 470, Training loss: 6.112651824951172
2025-08-04 02:00:46,449 - father_agent.py:386 - Step: 475, Training loss: 9.991646766662598
2025-08-04 02:00:48,091 - father_agent.py:386 - Step: 480, Training loss: 7.536590099334717
2025-08-04 02:00:49,739 - father_agent.py:386 - Step: 485, Training loss: 5.723441123962402
2025-08-04 02:00:51,377 - father_agent.py:386 - Step: 490, Training loss: 7.877964973449707
2025-08-04 02:00:53,021 - father_agent.py:386 - Step: 495, Training loss: 8.090744018554688
2025-08-04 02:00:54,703 - father_agent.py:386 - Step: 500, Training loss: 8.610665321350098
2025-08-04 02:00:54,896 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:00:54,899 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:01:03,287 - evaluation_results_class.py:131 - Average Return = 4.030667781829834
2025-08-04 02:01:03,290 - evaluation_results_class.py:133 - Average Virtual Goal Value = 42.306678771972656
2025-08-04 02:01:03,290 - evaluation_results_class.py:135 - Average Discounted Reward = 32.90964889526367
2025-08-04 02:01:03,290 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:01:03,290 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:01:03,290 - evaluation_results_class.py:141 - Variance of Return = 4.739933490753174
2025-08-04 02:01:03,290 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:01:03,290 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:01:03,290 - evaluation_results_class.py:147 - Average Episode Length = 39.91631487459647
2025-08-04 02:01:03,290 - evaluation_results_class.py:149 - Counted Episodes = 8054
2025-08-04 02:01:03,471 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:01:03,482 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:01:03,593 - father_agent.py:547 - Training finished.
2025-08-04 02:01:03,735 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:01:03,738 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:01:03,740 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 02:01:12,059 - evaluation_results_class.py:131 - Average Return = 4.058852672576904
2025-08-04 02:01:12,059 - evaluation_results_class.py:133 - Average Virtual Goal Value = 42.58852767944336
2025-08-04 02:01:12,059 - evaluation_results_class.py:135 - Average Discounted Reward = 33.1875
2025-08-04 02:01:12,059 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:01:12,059 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:01:12,059 - evaluation_results_class.py:141 - Variance of Return = 4.681413173675537
2025-08-04 02:01:12,059 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:01:12,059 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:01:12,059 - evaluation_results_class.py:147 - Average Episode Length = 39.91631487459647
2025-08-04 02:01:12,059 - evaluation_results_class.py:149 - Counted Episodes = 8054
2025-08-04 02:01:12,243 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:01:12,246 - self_interpretable_extractor.py:286 - True
2025-08-04 02:01:12,256 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:01:26,687 - evaluation_results_class.py:131 - Average Return = 4.036571979522705
2025-08-04 02:01:26,687 - evaluation_results_class.py:133 - Average Virtual Goal Value = 42.36572265625
2025-08-04 02:01:26,687 - evaluation_results_class.py:135 - Average Discounted Reward = 33.04642105102539
2025-08-04 02:01:26,687 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:01:26,688 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:01:26,688 - evaluation_results_class.py:141 - Variance of Return = 4.722259521484375
2025-08-04 02:01:26,688 - evaluation_results_class.py:143 - Current Best Return = 4.036571979522705
2025-08-04 02:01:26,688 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:01:26,688 - evaluation_results_class.py:147 - Average Episode Length = 39.865902232951115
2025-08-04 02:01:26,688 - evaluation_results_class.py:149 - Counted Episodes = 8285
2025-08-04 02:01:26,688 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 02:01:26,688 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 37192 trajectories
Learned trajectory lengths  {37, 157}
Buffer 1
2025-08-04 02:02:27,818 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 74384 trajectories
Learned trajectory lengths  {37, 157}
Buffer 2
2025-08-04 02:03:29,284 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 111576 trajectories
Learned trajectory lengths  {37, 157}
All trajectories collected
2025-08-04 02:04:32,008 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 02:04:32,009 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 111576 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 4
Model saved to fsc_2.dot.
Learned FSC of size 4
2025-08-04 02:04:35,689 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 02:04:49,401 - evaluation_results_class.py:131 - Average Return = 4.01448392868042
2025-08-04 02:04:49,401 - evaluation_results_class.py:133 - Average Virtual Goal Value = 42.144840240478516
2025-08-04 02:04:49,401 - evaluation_results_class.py:135 - Average Discounted Reward = 32.83651351928711
2025-08-04 02:04:49,401 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:04:49,401 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:04:49,401 - evaluation_results_class.py:141 - Variance of Return = 4.652294158935547
2025-08-04 02:04:49,401 - evaluation_results_class.py:143 - Current Best Return = 4.01448392868042
2025-08-04 02:04:49,401 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:04:49,401 - evaluation_results_class.py:147 - Average Episode Length = 39.865902232951115
2025-08-04 02:04:49,401 - evaluation_results_class.py:149 - Counted Episodes = 8285
FSC Result: {'best_episode_return': 42.14484, 'best_return': 4.014484, 'goal_value': 0.0, 'returns_episodic': [42.14484], 'returns': [4.014484], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [4.652294], 'each_episode_virtual_variance': [465.2295], 'combined_variance': [562.9277], 'num_episodes': [8285], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [39.865902232951115], 'counted_episodes': [8285], 'discounted_rewards': [32.836514], 'new_pomdp_iteration_numbers': []}
[[6], [1, 4, 5], [3], [3], [3], [2], [2], [2], [6], [6], [6], [1, 5], [1, 4], [3], [2], [1], [0], [0], [0], [0]]
2025-08-04 02:04:50,436 - statistic.py:67 - synthesis initiated, design space: 140
2025-08-04 02:04:50,438 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 02:04:50,556 - synthesizer_ar.py:122 - value 16.2607 achieved after 1710.17 seconds
2025-08-04 02:04:50,571 - synthesizer_ar.py:122 - value 8.9372 achieved after 1710.18 seconds
2025-08-04 02:04:50,624 - synthesizer_ar.py:122 - value 8.6156 achieved after 1710.23 seconds
2025-08-04 02:04:50,667 - synthesizer_ar.py:122 - value 8.1782 achieved after 1710.28 seconds
2025-08-04 02:04:50,703 - synthesizer_ar.py:122 - value 7.5758 achieved after 1710.31 seconds
2025-08-04 02:04:50,734 - synthesizer_ar.py:122 - value 6.7372 achieved after 1710.34 seconds
2025-08-04 02:04:50,747 - synthesizer_ar.py:122 - value 5.5567 achieved after 1710.36 seconds
2025-08-04 02:04:50,753 - synthesizer_ar.py:122 - value 3.8057 achieved after 1710.36 seconds
2025-08-04 02:04:50,753 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 02:04:50,753 - synthesizer.py:193 - SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 02:04:50,755 - synthesizer.py:198 - double-checking specification satisfiability:  : 3.8056547296639236
--------------------
Synthesis summary:
optimality objective: R{"packets_sent"}min=? [F "label_done"] 

method: AR, synthesis time: 0.32 s
number of holes: 4, family size: 140, quotient: 13482 states / 78181 actions
explored: 100 %
MDP stats: avg MDP size: 6639, iterations: 24

optimum: 3.805655
--------------------
2025-08-04 02:04:50,755 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 02:04:50,759 - robust_rl_trainer.py:432 - Iteration 4 of pure RL loop
2025-08-04 02:04:50,804 - storm_vec_env.py:70 - Computing row map
2025-08-04 02:04:50,810 - storm_vec_env.py:97 - Computing transitions
2025-08-04 02:04:50,821 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 02:04:50,822 - storm_vec_env.py:114 - Computing sinks
2025-08-04 02:04:50,822 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 02:04:50,825 - storm_vec_env.py:143 - Computing labels
2025-08-04 02:04:50,825 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 02:04:50,825 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 02:04:50,825 - storm_vec_env.py:175 - Computing observations
2025-08-04 02:04:50,951 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 02:04:50,952 - father_agent.py:540 - Before training evaluation.
2025-08-04 02:04:51,094 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:04:51,097 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:05:00,013 - evaluation_results_class.py:131 - Average Return = 3.9421780109405518
2025-08-04 02:05:00,013 - evaluation_results_class.py:133 - Average Virtual Goal Value = 41.42177963256836
2025-08-04 02:05:00,013 - evaluation_results_class.py:135 - Average Discounted Reward = 32.848609924316406
2025-08-04 02:05:00,013 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:05:00,013 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:05:00,013 - evaluation_results_class.py:141 - Variance of Return = 3.4135894775390625
2025-08-04 02:05:00,013 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:05:00,013 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:05:00,014 - evaluation_results_class.py:147 - Average Episode Length = 38.01785503133499
2025-08-04 02:05:00,014 - evaluation_results_class.py:149 - Counted Episodes = 8457
2025-08-04 02:05:00,207 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:05:00,217 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:05:00,326 - father_agent.py:436 - Training agent on-policy
2025-08-04 02:05:07,540 - father_agent.py:386 - Step: 0, Training loss: 6.826718330383301
2025-08-04 02:05:12,362 - father_agent.py:386 - Step: 5, Training loss: 8.330248832702637
2025-08-04 02:05:14,255 - father_agent.py:386 - Step: 10, Training loss: 7.667562484741211
2025-08-04 02:05:16,083 - father_agent.py:386 - Step: 15, Training loss: 5.734113693237305
2025-08-04 02:05:17,911 - father_agent.py:386 - Step: 20, Training loss: 9.751160621643066
2025-08-04 02:05:19,749 - father_agent.py:386 - Step: 25, Training loss: 6.745364665985107
2025-08-04 02:05:21,582 - father_agent.py:386 - Step: 30, Training loss: 6.994411468505859
2025-08-04 02:05:23,435 - father_agent.py:386 - Step: 35, Training loss: 7.404919147491455
2025-08-04 02:05:25,320 - father_agent.py:386 - Step: 40, Training loss: 5.2286834716796875
2025-08-04 02:05:27,167 - father_agent.py:386 - Step: 45, Training loss: 9.438570022583008
2025-08-04 02:05:29,023 - father_agent.py:386 - Step: 50, Training loss: 6.5734124183654785
2025-08-04 02:05:30,837 - father_agent.py:386 - Step: 55, Training loss: 7.225346088409424
2025-08-04 02:05:32,717 - father_agent.py:386 - Step: 60, Training loss: 7.668219566345215
2025-08-04 02:05:34,533 - father_agent.py:386 - Step: 65, Training loss: 7.026156425476074
2025-08-04 02:05:36,406 - father_agent.py:386 - Step: 70, Training loss: 9.412274360656738
2025-08-04 02:05:38,240 - father_agent.py:386 - Step: 75, Training loss: 7.868618011474609
2025-08-04 02:05:39,971 - father_agent.py:386 - Step: 80, Training loss: 6.283116340637207
2025-08-04 02:05:41,695 - father_agent.py:386 - Step: 85, Training loss: 10.367999076843262
2025-08-04 02:05:43,452 - father_agent.py:386 - Step: 90, Training loss: 8.1660737991333
2025-08-04 02:05:45,246 - father_agent.py:386 - Step: 95, Training loss: 6.719328880310059
2025-08-04 02:05:47,013 - father_agent.py:386 - Step: 100, Training loss: 6.6920318603515625
2025-08-04 02:05:47,215 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:05:47,217 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:05:56,250 - evaluation_results_class.py:131 - Average Return = 3.9321272373199463
2025-08-04 02:05:56,250 - evaluation_results_class.py:133 - Average Virtual Goal Value = 41.32127380371094
2025-08-04 02:05:56,250 - evaluation_results_class.py:135 - Average Discounted Reward = 32.75306701660156
2025-08-04 02:05:56,250 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:05:56,250 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:05:56,250 - evaluation_results_class.py:141 - Variance of Return = 3.518155574798584
2025-08-04 02:05:56,250 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:05:56,250 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:05:56,250 - evaluation_results_class.py:147 - Average Episode Length = 38.01785503133499
2025-08-04 02:05:56,250 - evaluation_results_class.py:149 - Counted Episodes = 8457
2025-08-04 02:05:56,454 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:05:56,465 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:05:59,938 - father_agent.py:386 - Step: 105, Training loss: 6.22281551361084
2025-08-04 02:06:01,745 - father_agent.py:386 - Step: 110, Training loss: 7.873537540435791
2025-08-04 02:06:03,550 - father_agent.py:386 - Step: 115, Training loss: 5.946705341339111
2025-08-04 02:06:05,314 - father_agent.py:386 - Step: 120, Training loss: 5.0041913986206055
2025-08-04 02:06:07,109 - father_agent.py:386 - Step: 125, Training loss: 8.537503242492676
2025-08-04 02:06:08,860 - father_agent.py:386 - Step: 130, Training loss: 5.8376898765563965
2025-08-04 02:06:10,573 - father_agent.py:386 - Step: 135, Training loss: 7.453362941741943
2025-08-04 02:06:12,289 - father_agent.py:386 - Step: 140, Training loss: 6.6148786544799805
2025-08-04 02:06:13,989 - father_agent.py:386 - Step: 145, Training loss: 6.023221015930176
2025-08-04 02:06:15,720 - father_agent.py:386 - Step: 150, Training loss: 9.769789695739746
2025-08-04 02:06:17,428 - father_agent.py:386 - Step: 155, Training loss: 6.181936264038086
2025-08-04 02:06:19,144 - father_agent.py:386 - Step: 160, Training loss: 7.790012359619141
2025-08-04 02:06:20,861 - father_agent.py:386 - Step: 165, Training loss: 7.726555824279785
2025-08-04 02:06:22,548 - father_agent.py:386 - Step: 170, Training loss: 5.2803544998168945
2025-08-04 02:06:24,262 - father_agent.py:386 - Step: 175, Training loss: 9.969145774841309
2025-08-04 02:06:25,968 - father_agent.py:386 - Step: 180, Training loss: 5.932159900665283
2025-08-04 02:06:27,691 - father_agent.py:386 - Step: 185, Training loss: 4.8499250411987305
2025-08-04 02:06:29,434 - father_agent.py:386 - Step: 190, Training loss: 8.831583976745605
2025-08-04 02:06:31,182 - father_agent.py:386 - Step: 195, Training loss: 6.735345363616943
2025-08-04 02:06:32,894 - father_agent.py:386 - Step: 200, Training loss: 7.867538928985596
2025-08-04 02:06:33,098 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:06:33,101 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:06:41,985 - evaluation_results_class.py:131 - Average Return = 3.935792922973633
2025-08-04 02:06:41,986 - evaluation_results_class.py:133 - Average Virtual Goal Value = 41.35792922973633
2025-08-04 02:06:41,986 - evaluation_results_class.py:135 - Average Discounted Reward = 32.833377838134766
2025-08-04 02:06:41,986 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:06:41,986 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:06:41,986 - evaluation_results_class.py:141 - Variance of Return = 3.5007848739624023
2025-08-04 02:06:41,986 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:06:41,986 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:06:41,986 - evaluation_results_class.py:147 - Average Episode Length = 38.01785503133499
2025-08-04 02:06:41,986 - evaluation_results_class.py:149 - Counted Episodes = 8457
2025-08-04 02:06:42,185 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:06:42,195 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:06:45,615 - father_agent.py:386 - Step: 205, Training loss: 5.077755928039551
2025-08-04 02:06:47,330 - father_agent.py:386 - Step: 210, Training loss: 7.607669353485107
2025-08-04 02:06:49,084 - father_agent.py:386 - Step: 215, Training loss: 5.772989749908447
2025-08-04 02:06:50,800 - father_agent.py:386 - Step: 220, Training loss: 5.2806243896484375
2025-08-04 02:06:52,516 - father_agent.py:386 - Step: 225, Training loss: 9.180622100830078
2025-08-04 02:06:54,235 - father_agent.py:386 - Step: 230, Training loss: 5.7948150634765625
2025-08-04 02:06:55,942 - father_agent.py:386 - Step: 235, Training loss: 7.540974140167236
2025-08-04 02:06:57,669 - father_agent.py:386 - Step: 240, Training loss: 6.343990325927734
2025-08-04 02:06:59,382 - father_agent.py:386 - Step: 245, Training loss: 6.433342933654785
2025-08-04 02:07:01,104 - father_agent.py:386 - Step: 250, Training loss: 9.63245964050293
2025-08-04 02:07:02,828 - father_agent.py:386 - Step: 255, Training loss: 6.604213714599609
2025-08-04 02:07:04,576 - father_agent.py:386 - Step: 260, Training loss: 8.124626159667969
2025-08-04 02:07:06,319 - father_agent.py:386 - Step: 265, Training loss: 7.223119258880615
2025-08-04 02:07:08,043 - father_agent.py:386 - Step: 270, Training loss: 5.552745342254639
2025-08-04 02:07:09,763 - father_agent.py:386 - Step: 275, Training loss: 9.888229370117188
2025-08-04 02:07:11,499 - father_agent.py:386 - Step: 280, Training loss: 6.342864513397217
2025-08-04 02:07:13,219 - father_agent.py:386 - Step: 285, Training loss: 5.0029401779174805
2025-08-04 02:07:14,933 - father_agent.py:386 - Step: 290, Training loss: 8.751822471618652
2025-08-04 02:07:16,640 - father_agent.py:386 - Step: 295, Training loss: 6.898906707763672
2025-08-04 02:07:18,346 - father_agent.py:386 - Step: 300, Training loss: 8.154959678649902
2025-08-04 02:07:18,550 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:07:18,553 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:07:27,826 - evaluation_results_class.py:131 - Average Return = 3.879153251647949
2025-08-04 02:07:27,826 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.791534423828125
2025-08-04 02:07:27,826 - evaluation_results_class.py:135 - Average Discounted Reward = 32.334228515625
2025-08-04 02:07:27,826 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:07:27,826 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:07:27,826 - evaluation_results_class.py:141 - Variance of Return = 3.5214016437530518
2025-08-04 02:07:27,827 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:07:27,827 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:07:27,827 - evaluation_results_class.py:147 - Average Episode Length = 38.01785503133499
2025-08-04 02:07:27,827 - evaluation_results_class.py:149 - Counted Episodes = 8457
2025-08-04 02:07:28,024 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:07:28,035 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:07:31,444 - father_agent.py:386 - Step: 305, Training loss: 6.125875949859619
2025-08-04 02:07:33,174 - father_agent.py:386 - Step: 310, Training loss: 7.287238597869873
2025-08-04 02:07:34,895 - father_agent.py:386 - Step: 315, Training loss: 6.001137733459473
2025-08-04 02:07:36,606 - father_agent.py:386 - Step: 320, Training loss: 4.853942394256592
2025-08-04 02:07:38,325 - father_agent.py:386 - Step: 325, Training loss: 8.878273010253906
2025-08-04 02:07:40,020 - father_agent.py:386 - Step: 330, Training loss: 6.0000996589660645
2025-08-04 02:07:41,718 - father_agent.py:386 - Step: 335, Training loss: 7.525752067565918
2025-08-04 02:07:43,441 - father_agent.py:386 - Step: 340, Training loss: 7.080361843109131
2025-08-04 02:07:45,139 - father_agent.py:386 - Step: 345, Training loss: 6.89122200012207
2025-08-04 02:07:46,846 - father_agent.py:386 - Step: 350, Training loss: 8.717740058898926
2025-08-04 02:07:48,544 - father_agent.py:386 - Step: 355, Training loss: 7.460745334625244
2025-08-04 02:07:50,265 - father_agent.py:386 - Step: 360, Training loss: 6.482461452484131
2025-08-04 02:07:51,977 - father_agent.py:386 - Step: 365, Training loss: 8.110943794250488
2025-08-04 02:07:53,677 - father_agent.py:386 - Step: 370, Training loss: 5.6157636642456055
2025-08-04 02:07:55,385 - father_agent.py:386 - Step: 375, Training loss: 12.211925506591797
2025-08-04 02:07:57,140 - father_agent.py:386 - Step: 380, Training loss: 5.824305057525635
2025-08-04 02:07:58,932 - father_agent.py:386 - Step: 385, Training loss: 4.507104873657227
2025-08-04 02:08:00,704 - father_agent.py:386 - Step: 390, Training loss: 9.704183578491211
2025-08-04 02:08:02,466 - father_agent.py:386 - Step: 395, Training loss: 6.544467449188232
2025-08-04 02:08:04,302 - father_agent.py:386 - Step: 400, Training loss: 8.368202209472656
2025-08-04 02:08:04,516 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:08:04,520 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:08:13,796 - evaluation_results_class.py:131 - Average Return = 3.9191203117370605
2025-08-04 02:08:13,796 - evaluation_results_class.py:133 - Average Virtual Goal Value = 41.19120407104492
2025-08-04 02:08:13,796 - evaluation_results_class.py:135 - Average Discounted Reward = 32.64487075805664
2025-08-04 02:08:13,796 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:08:13,796 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:08:13,796 - evaluation_results_class.py:141 - Variance of Return = 3.542234420776367
2025-08-04 02:08:13,796 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:08:13,796 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:08:13,797 - evaluation_results_class.py:147 - Average Episode Length = 38.01785503133499
2025-08-04 02:08:13,797 - evaluation_results_class.py:149 - Counted Episodes = 8457
2025-08-04 02:08:13,997 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:08:14,007 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:08:17,417 - father_agent.py:386 - Step: 405, Training loss: 5.858969211578369
2025-08-04 02:08:19,235 - father_agent.py:386 - Step: 410, Training loss: 8.149328231811523
2025-08-04 02:08:21,031 - father_agent.py:386 - Step: 415, Training loss: 6.334781646728516
2025-08-04 02:08:22,835 - father_agent.py:386 - Step: 420, Training loss: 4.299221038818359
2025-08-04 02:08:24,623 - father_agent.py:386 - Step: 425, Training loss: 9.87716007232666
2025-08-04 02:08:26,397 - father_agent.py:386 - Step: 430, Training loss: 6.174161911010742
2025-08-04 02:08:28,166 - father_agent.py:386 - Step: 435, Training loss: 7.52056360244751
2025-08-04 02:08:29,943 - father_agent.py:386 - Step: 440, Training loss: 6.9818925857543945
2025-08-04 02:08:31,727 - father_agent.py:386 - Step: 445, Training loss: 6.128872394561768
2025-08-04 02:08:33,512 - father_agent.py:386 - Step: 450, Training loss: 9.505453109741211
2025-08-04 02:08:35,291 - father_agent.py:386 - Step: 455, Training loss: 6.47317361831665
2025-08-04 02:08:37,084 - father_agent.py:386 - Step: 460, Training loss: 6.27708101272583
2025-08-04 02:08:38,879 - father_agent.py:386 - Step: 465, Training loss: 8.060705184936523
2025-08-04 02:08:40,661 - father_agent.py:386 - Step: 470, Training loss: 5.340506553649902
2025-08-04 02:08:42,419 - father_agent.py:386 - Step: 475, Training loss: 9.202858924865723
2025-08-04 02:08:44,212 - father_agent.py:386 - Step: 480, Training loss: 6.53425931930542
2025-08-04 02:08:46,043 - father_agent.py:386 - Step: 485, Training loss: 3.921677350997925
2025-08-04 02:08:47,894 - father_agent.py:386 - Step: 490, Training loss: 9.016637802124023
2025-08-04 02:08:49,742 - father_agent.py:386 - Step: 495, Training loss: 6.602774143218994
2025-08-04 02:08:51,519 - father_agent.py:386 - Step: 500, Training loss: 8.70166015625
2025-08-04 02:08:51,727 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:08:51,729 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:09:00,810 - evaluation_results_class.py:131 - Average Return = 3.8842380046844482
2025-08-04 02:09:00,810 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.84238052368164
2025-08-04 02:09:00,810 - evaluation_results_class.py:135 - Average Discounted Reward = 32.39542770385742
2025-08-04 02:09:00,810 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:09:00,810 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:09:00,810 - evaluation_results_class.py:141 - Variance of Return = 3.4231605529785156
2025-08-04 02:09:00,810 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:09:00,810 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:09:00,810 - evaluation_results_class.py:147 - Average Episode Length = 38.01785503133499
2025-08-04 02:09:00,810 - evaluation_results_class.py:149 - Counted Episodes = 8457
2025-08-04 02:09:01,013 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:09:01,023 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:09:01,133 - father_agent.py:547 - Training finished.
2025-08-04 02:09:01,285 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:09:01,287 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:09:01,290 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 02:09:10,417 - evaluation_results_class.py:131 - Average Return = 3.9386308193206787
2025-08-04 02:09:10,418 - evaluation_results_class.py:133 - Average Virtual Goal Value = 41.38630676269531
2025-08-04 02:09:10,418 - evaluation_results_class.py:135 - Average Discounted Reward = 32.81618881225586
2025-08-04 02:09:10,418 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:09:10,418 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:09:10,418 - evaluation_results_class.py:141 - Variance of Return = 3.542053699493408
2025-08-04 02:09:10,418 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:09:10,418 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:09:10,418 - evaluation_results_class.py:147 - Average Episode Length = 38.01785503133499
2025-08-04 02:09:10,418 - evaluation_results_class.py:149 - Counted Episodes = 8457
2025-08-04 02:09:10,615 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:09:10,617 - self_interpretable_extractor.py:286 - True
2025-08-04 02:09:10,628 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:09:26,232 - evaluation_results_class.py:131 - Average Return = 3.9046690464019775
2025-08-04 02:09:26,232 - evaluation_results_class.py:133 - Average Virtual Goal Value = 41.04669189453125
2025-08-04 02:09:26,232 - evaluation_results_class.py:135 - Average Discounted Reward = 32.57957458496094
2025-08-04 02:09:26,232 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:09:26,232 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:09:26,232 - evaluation_results_class.py:141 - Variance of Return = 3.3066165447235107
2025-08-04 02:09:26,232 - evaluation_results_class.py:143 - Current Best Return = 3.9046690464019775
2025-08-04 02:09:26,232 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:09:26,232 - evaluation_results_class.py:147 - Average Episode Length = 37.961798783985316
2025-08-04 02:09:26,232 - evaluation_results_class.py:149 - Counted Episodes = 8717
2025-08-04 02:09:26,233 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 02:09:26,233 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 39112 trajectories
Learned trajectory lengths  {37, 157}
Buffer 1
2025-08-04 02:10:30,628 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 78224 trajectories
Learned trajectory lengths  {37, 157}
Buffer 2
2025-08-04 02:11:35,198 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 117336 trajectories
Learned trajectory lengths  {37, 157}
All trajectories collected
2025-08-04 02:12:39,618 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 02:12:39,619 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 117336 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 4
Model saved to fsc_3.dot.
Learned FSC of size 4
2025-08-04 02:12:42,132 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 02:12:56,339 - evaluation_results_class.py:131 - Average Return = 3.862338066101074
2025-08-04 02:12:56,339 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.62337875366211
2025-08-04 02:12:56,340 - evaluation_results_class.py:135 - Average Discounted Reward = 32.25027847290039
2025-08-04 02:12:56,340 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:12:56,340 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:12:56,340 - evaluation_results_class.py:141 - Variance of Return = 3.4237473011016846
2025-08-04 02:12:56,340 - evaluation_results_class.py:143 - Current Best Return = 3.862338066101074
2025-08-04 02:12:56,340 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:12:56,340 - evaluation_results_class.py:147 - Average Episode Length = 37.961798783985316
2025-08-04 02:12:56,340 - evaluation_results_class.py:149 - Counted Episodes = 8717
FSC Result: {'best_episode_return': 40.62338, 'best_return': 3.862338, 'goal_value': 0.0, 'returns_episodic': [40.62338], 'returns': [3.862338], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [3.4237473], 'each_episode_virtual_variance': [342.3747], 'combined_variance': [414.27347], 'num_episodes': [8717], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [37.961798783985316], 'counted_episodes': [8717], 'discounted_rewards': [32.25028], 'new_pomdp_iteration_numbers': []}
[[6], [1, 4, 5], [3], [3], [3], [2], [2], [2], [6], [6], [6], [1, 5], [1, 4], [3], [2], [1], [0], [0], [0], [0]]
2025-08-04 02:12:57,161 - statistic.py:67 - synthesis initiated, design space: 140
2025-08-04 02:12:57,163 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 02:12:57,280 - synthesizer_ar.py:122 - value 16.2583 achieved after 2196.89 seconds
2025-08-04 02:12:57,296 - synthesizer_ar.py:122 - value 8.9357 achieved after 2196.91 seconds
2025-08-04 02:12:57,349 - synthesizer_ar.py:122 - value 8.6129 achieved after 2196.96 seconds
2025-08-04 02:12:57,392 - synthesizer_ar.py:122 - value 8.1734 achieved after 2197.0 seconds
2025-08-04 02:12:57,428 - synthesizer_ar.py:122 - value 7.5668 achieved after 2197.04 seconds
2025-08-04 02:12:57,458 - synthesizer_ar.py:122 - value 6.7211 achieved after 2197.07 seconds
2025-08-04 02:12:57,471 - synthesizer_ar.py:122 - value 5.5282 achieved after 2197.08 seconds
2025-08-04 02:12:57,478 - synthesizer_ar.py:122 - value 3.7544 achieved after 2197.09 seconds
2025-08-04 02:12:57,478 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 02:12:57,478 - synthesizer.py:193 - SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 02:12:57,479 - synthesizer.py:198 - double-checking specification satisfiability:  : 3.754425786950337
--------------------
Synthesis summary:
optimality objective: R{"packets_sent"}min=? [F "label_done"] 

method: AR, synthesis time: 0.31 s
number of holes: 4, family size: 140, quotient: 13482 states / 78181 actions
explored: 100 %
MDP stats: avg MDP size: 6639, iterations: 24

optimum: 3.754426
--------------------
2025-08-04 02:12:57,480 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 02:12:57,483 - robust_rl_trainer.py:432 - Iteration 5 of pure RL loop
2025-08-04 02:12:57,528 - storm_vec_env.py:70 - Computing row map
2025-08-04 02:12:57,535 - storm_vec_env.py:97 - Computing transitions
2025-08-04 02:12:57,546 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 02:12:57,546 - storm_vec_env.py:114 - Computing sinks
2025-08-04 02:12:57,546 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 02:12:57,549 - storm_vec_env.py:143 - Computing labels
2025-08-04 02:12:57,549 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 02:12:57,549 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 02:12:57,549 - storm_vec_env.py:175 - Computing observations
2025-08-04 02:12:57,682 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 02:12:57,683 - father_agent.py:540 - Before training evaluation.
2025-08-04 02:12:57,832 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:12:57,834 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:13:07,311 - evaluation_results_class.py:131 - Average Return = 3.869964122772217
2025-08-04 02:13:07,311 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.69963836669922
2025-08-04 02:13:07,311 - evaluation_results_class.py:135 - Average Discounted Reward = 32.49467849731445
2025-08-04 02:13:07,311 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:13:07,311 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:13:07,311 - evaluation_results_class.py:141 - Variance of Return = 3.091647148132324
2025-08-04 02:13:07,311 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:13:07,312 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:13:07,312 - evaluation_results_class.py:147 - Average Episode Length = 37.33066295135261
2025-08-04 02:13:07,312 - evaluation_results_class.py:149 - Counted Episodes = 8613
2025-08-04 02:13:07,510 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:13:07,521 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:13:07,636 - father_agent.py:436 - Training agent on-policy
2025-08-04 02:13:15,318 - father_agent.py:386 - Step: 0, Training loss: 5.184844970703125
2025-08-04 02:13:20,190 - father_agent.py:386 - Step: 5, Training loss: 9.366878509521484
2025-08-04 02:13:22,016 - father_agent.py:386 - Step: 10, Training loss: 5.833259105682373
2025-08-04 02:13:23,804 - father_agent.py:386 - Step: 15, Training loss: 4.755753517150879
2025-08-04 02:13:25,593 - father_agent.py:386 - Step: 20, Training loss: 8.745116233825684
2025-08-04 02:13:27,373 - father_agent.py:386 - Step: 25, Training loss: 5.227350234985352
2025-08-04 02:13:29,182 - father_agent.py:386 - Step: 30, Training loss: 6.3989715576171875
2025-08-04 02:13:31,076 - father_agent.py:386 - Step: 35, Training loss: 5.956223011016846
2025-08-04 02:13:32,930 - father_agent.py:386 - Step: 40, Training loss: 3.975506544113159
2025-08-04 02:13:34,796 - father_agent.py:386 - Step: 45, Training loss: 10.906702995300293
2025-08-04 02:13:36,666 - father_agent.py:386 - Step: 50, Training loss: 4.813185214996338
2025-08-04 02:13:38,538 - father_agent.py:386 - Step: 55, Training loss: 8.650592803955078
2025-08-04 02:13:40,452 - father_agent.py:386 - Step: 60, Training loss: 6.7321858406066895
2025-08-04 02:13:42,387 - father_agent.py:386 - Step: 65, Training loss: 5.3427414894104
2025-08-04 02:13:44,249 - father_agent.py:386 - Step: 70, Training loss: 9.437533378601074
2025-08-04 02:13:46,114 - father_agent.py:386 - Step: 75, Training loss: 5.478023052215576
2025-08-04 02:13:47,993 - father_agent.py:386 - Step: 80, Training loss: 6.211095333099365
2025-08-04 02:13:49,901 - father_agent.py:386 - Step: 85, Training loss: 9.192536354064941
2025-08-04 02:13:51,777 - father_agent.py:386 - Step: 90, Training loss: 5.547519207000732
2025-08-04 02:13:53,670 - father_agent.py:386 - Step: 95, Training loss: 7.799657821655273
2025-08-04 02:13:55,622 - father_agent.py:386 - Step: 100, Training loss: 5.4142608642578125
2025-08-04 02:13:55,902 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:13:55,906 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:14:05,681 - evaluation_results_class.py:131 - Average Return = 3.8455822467803955
2025-08-04 02:14:05,681 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.4558219909668
2025-08-04 02:14:05,681 - evaluation_results_class.py:135 - Average Discounted Reward = 32.28487777709961
2025-08-04 02:14:05,681 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:14:05,681 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:14:05,681 - evaluation_results_class.py:141 - Variance of Return = 3.011566638946533
2025-08-04 02:14:05,681 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:14:05,681 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:14:05,681 - evaluation_results_class.py:147 - Average Episode Length = 37.33066295135261
2025-08-04 02:14:05,681 - evaluation_results_class.py:149 - Counted Episodes = 8613
2025-08-04 02:14:05,893 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:14:05,903 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:14:09,685 - father_agent.py:386 - Step: 105, Training loss: 2.9011805057525635
2025-08-04 02:14:11,509 - father_agent.py:386 - Step: 110, Training loss: 7.80238676071167
2025-08-04 02:14:13,331 - father_agent.py:386 - Step: 115, Training loss: 4.169583320617676
2025-08-04 02:14:15,132 - father_agent.py:386 - Step: 120, Training loss: 2.7911078929901123
2025-08-04 02:14:16,947 - father_agent.py:386 - Step: 125, Training loss: 9.409956932067871
2025-08-04 02:14:18,775 - father_agent.py:386 - Step: 130, Training loss: 3.3733582496643066
2025-08-04 02:14:20,563 - father_agent.py:386 - Step: 135, Training loss: 9.517197608947754
2025-08-04 02:14:22,346 - father_agent.py:386 - Step: 140, Training loss: 4.967364311218262
2025-08-04 02:14:24,127 - father_agent.py:386 - Step: 145, Training loss: 4.004377841949463
2025-08-04 02:14:25,912 - father_agent.py:386 - Step: 150, Training loss: 10.998913764953613
2025-08-04 02:14:27,695 - father_agent.py:386 - Step: 155, Training loss: 4.829784870147705
2025-08-04 02:14:29,501 - father_agent.py:386 - Step: 160, Training loss: 9.666074752807617
2025-08-04 02:14:31,303 - father_agent.py:386 - Step: 165, Training loss: 5.57592248916626
2025-08-04 02:14:33,123 - father_agent.py:386 - Step: 170, Training loss: 3.555432081222534
2025-08-04 02:14:34,933 - father_agent.py:386 - Step: 175, Training loss: 14.292140007019043
2025-08-04 02:14:36,749 - father_agent.py:386 - Step: 180, Training loss: 4.856604099273682
2025-08-04 02:14:38,555 - father_agent.py:386 - Step: 185, Training loss: 2.7976388931274414
2025-08-04 02:14:40,354 - father_agent.py:386 - Step: 190, Training loss: 12.187124252319336
2025-08-04 02:14:42,179 - father_agent.py:386 - Step: 195, Training loss: 4.073938846588135
2025-08-04 02:14:44,001 - father_agent.py:386 - Step: 200, Training loss: 14.387188911437988
2025-08-04 02:14:44,219 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:14:44,222 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:14:53,700 - evaluation_results_class.py:131 - Average Return = 3.8607919216156006
2025-08-04 02:14:53,700 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.60791778564453
2025-08-04 02:14:53,700 - evaluation_results_class.py:135 - Average Discounted Reward = 32.43046188354492
2025-08-04 02:14:53,700 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:14:53,700 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:14:53,700 - evaluation_results_class.py:141 - Variance of Return = 3.0321710109710693
2025-08-04 02:14:53,700 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:14:53,700 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:14:53,700 - evaluation_results_class.py:147 - Average Episode Length = 37.33066295135261
2025-08-04 02:14:53,700 - evaluation_results_class.py:149 - Counted Episodes = 8613
2025-08-04 02:14:53,912 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:14:53,923 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:14:57,647 - father_agent.py:386 - Step: 205, Training loss: 2.9904403686523438
2025-08-04 02:14:59,438 - father_agent.py:386 - Step: 210, Training loss: 9.822875022888184
2025-08-04 02:15:01,235 - father_agent.py:386 - Step: 215, Training loss: 4.812005043029785
2025-08-04 02:15:03,023 - father_agent.py:386 - Step: 220, Training loss: 2.3430700302124023
2025-08-04 02:15:04,819 - father_agent.py:386 - Step: 225, Training loss: 10.102492332458496
2025-08-04 02:15:06,621 - father_agent.py:386 - Step: 230, Training loss: 3.1692776679992676
2025-08-04 02:15:08,415 - father_agent.py:386 - Step: 235, Training loss: 10.038187026977539
2025-08-04 02:15:10,220 - father_agent.py:386 - Step: 240, Training loss: 4.662831783294678
2025-08-04 02:15:12,036 - father_agent.py:386 - Step: 245, Training loss: 3.9765148162841797
2025-08-04 02:15:13,864 - father_agent.py:386 - Step: 250, Training loss: 9.329374313354492
2025-08-04 02:15:15,678 - father_agent.py:386 - Step: 255, Training loss: 4.139962196350098
2025-08-04 02:15:17,468 - father_agent.py:386 - Step: 260, Training loss: 7.552330493927002
2025-08-04 02:15:19,308 - father_agent.py:386 - Step: 265, Training loss: 6.462860107421875
2025-08-04 02:15:21,113 - father_agent.py:386 - Step: 270, Training loss: 2.9297749996185303
2025-08-04 02:15:22,912 - father_agent.py:386 - Step: 275, Training loss: 12.691600799560547
2025-08-04 02:15:24,715 - father_agent.py:386 - Step: 280, Training loss: 3.8157546520233154
2025-08-04 02:15:26,529 - father_agent.py:386 - Step: 285, Training loss: 2.114419460296631
2025-08-04 02:15:28,319 - father_agent.py:386 - Step: 290, Training loss: 12.268378257751465
2025-08-04 02:15:30,110 - father_agent.py:386 - Step: 295, Training loss: 3.5489799976348877
2025-08-04 02:15:31,897 - father_agent.py:386 - Step: 300, Training loss: 15.073895454406738
2025-08-04 02:15:32,118 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:15:32,121 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:15:41,647 - evaluation_results_class.py:131 - Average Return = 3.923719882965088
2025-08-04 02:15:41,647 - evaluation_results_class.py:133 - Average Virtual Goal Value = 41.23719787597656
2025-08-04 02:15:41,647 - evaluation_results_class.py:135 - Average Discounted Reward = 32.89854049682617
2025-08-04 02:15:41,647 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:15:41,647 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:15:41,647 - evaluation_results_class.py:141 - Variance of Return = 3.0698812007904053
2025-08-04 02:15:41,647 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:15:41,647 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:15:41,647 - evaluation_results_class.py:147 - Average Episode Length = 37.33066295135261
2025-08-04 02:15:41,647 - evaluation_results_class.py:149 - Counted Episodes = 8613
2025-08-04 02:15:41,861 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:15:41,872 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:15:45,938 - father_agent.py:386 - Step: 305, Training loss: 2.527456760406494
2025-08-04 02:15:47,745 - father_agent.py:386 - Step: 310, Training loss: 8.277128219604492
2025-08-04 02:15:49,560 - father_agent.py:386 - Step: 315, Training loss: 4.121854782104492
2025-08-04 02:15:51,357 - father_agent.py:386 - Step: 320, Training loss: 2.4464833736419678
2025-08-04 02:15:53,153 - father_agent.py:386 - Step: 325, Training loss: 13.046262741088867
2025-08-04 02:15:54,953 - father_agent.py:386 - Step: 330, Training loss: 2.4523465633392334
2025-08-04 02:15:56,733 - father_agent.py:386 - Step: 335, Training loss: 12.222023963928223
2025-08-04 02:15:58,541 - father_agent.py:386 - Step: 340, Training loss: 4.241297721862793
2025-08-04 02:16:00,319 - father_agent.py:386 - Step: 345, Training loss: 3.4636948108673096
2025-08-04 02:16:02,102 - father_agent.py:386 - Step: 350, Training loss: 9.648477554321289
2025-08-04 02:16:03,894 - father_agent.py:386 - Step: 355, Training loss: 3.472379684448242
2025-08-04 02:16:05,676 - father_agent.py:386 - Step: 360, Training loss: 7.516140937805176
2025-08-04 02:16:07,479 - father_agent.py:386 - Step: 365, Training loss: 5.217021942138672
2025-08-04 02:16:09,266 - father_agent.py:386 - Step: 370, Training loss: 2.9890971183776855
2025-08-04 02:16:11,051 - father_agent.py:386 - Step: 375, Training loss: 12.251391410827637
2025-08-04 02:16:12,852 - father_agent.py:386 - Step: 380, Training loss: 3.6179561614990234
2025-08-04 02:16:14,636 - father_agent.py:386 - Step: 385, Training loss: 2.0787513256073
2025-08-04 02:16:16,422 - father_agent.py:386 - Step: 390, Training loss: 12.12830638885498
2025-08-04 02:16:18,207 - father_agent.py:386 - Step: 395, Training loss: 3.140648603439331
2025-08-04 02:16:20,008 - father_agent.py:386 - Step: 400, Training loss: 12.570854187011719
2025-08-04 02:16:20,229 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:16:20,232 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:16:29,784 - evaluation_results_class.py:131 - Average Return = 3.8587019443511963
2025-08-04 02:16:29,784 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.58702087402344
2025-08-04 02:16:29,784 - evaluation_results_class.py:135 - Average Discounted Reward = 32.40769577026367
2025-08-04 02:16:29,784 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:16:29,784 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:16:29,784 - evaluation_results_class.py:141 - Variance of Return = 3.181126356124878
2025-08-04 02:16:29,784 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:16:29,784 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:16:29,784 - evaluation_results_class.py:147 - Average Episode Length = 37.33066295135261
2025-08-04 02:16:29,784 - evaluation_results_class.py:149 - Counted Episodes = 8613
2025-08-04 02:16:29,996 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:16:30,007 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:16:33,683 - father_agent.py:386 - Step: 405, Training loss: 2.2490339279174805
2025-08-04 02:16:35,496 - father_agent.py:386 - Step: 410, Training loss: 7.409260272979736
2025-08-04 02:16:37,326 - father_agent.py:386 - Step: 415, Training loss: 3.806365728378296
2025-08-04 02:16:39,147 - father_agent.py:386 - Step: 420, Training loss: 2.187223196029663
2025-08-04 02:16:40,971 - father_agent.py:386 - Step: 425, Training loss: 11.015110969543457
2025-08-04 02:16:42,773 - father_agent.py:386 - Step: 430, Training loss: 2.1715614795684814
2025-08-04 02:16:44,562 - father_agent.py:386 - Step: 435, Training loss: 10.905207633972168
2025-08-04 02:16:46,370 - father_agent.py:386 - Step: 440, Training loss: 4.442923545837402
2025-08-04 02:16:48,157 - father_agent.py:386 - Step: 445, Training loss: 2.9340732097625732
2025-08-04 02:16:49,965 - father_agent.py:386 - Step: 450, Training loss: 8.02698802947998
2025-08-04 02:16:51,769 - father_agent.py:386 - Step: 455, Training loss: 2.725099563598633
2025-08-04 02:16:53,567 - father_agent.py:386 - Step: 460, Training loss: 7.526796340942383
2025-08-04 02:16:55,365 - father_agent.py:386 - Step: 465, Training loss: 5.347140312194824
2025-08-04 02:16:57,164 - father_agent.py:386 - Step: 470, Training loss: 3.0113606452941895
2025-08-04 02:16:58,969 - father_agent.py:386 - Step: 475, Training loss: 12.77774715423584
2025-08-04 02:17:00,775 - father_agent.py:386 - Step: 480, Training loss: 3.580697774887085
2025-08-04 02:17:02,558 - father_agent.py:386 - Step: 485, Training loss: 2.219528913497925
2025-08-04 02:17:04,340 - father_agent.py:386 - Step: 490, Training loss: 11.760579109191895
2025-08-04 02:17:06,139 - father_agent.py:386 - Step: 495, Training loss: 2.345916748046875
2025-08-04 02:17:07,925 - father_agent.py:386 - Step: 500, Training loss: 12.877084732055664
2025-08-04 02:17:08,147 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:17:08,150 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:17:17,656 - evaluation_results_class.py:131 - Average Return = 3.912574052810669
2025-08-04 02:17:17,657 - evaluation_results_class.py:133 - Average Virtual Goal Value = 41.12574005126953
2025-08-04 02:17:17,657 - evaluation_results_class.py:135 - Average Discounted Reward = 32.84074020385742
2025-08-04 02:17:17,657 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:17:17,657 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:17:17,657 - evaluation_results_class.py:141 - Variance of Return = 3.000483989715576
2025-08-04 02:17:17,657 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:17:17,657 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:17:17,657 - evaluation_results_class.py:147 - Average Episode Length = 37.33066295135261
2025-08-04 02:17:17,657 - evaluation_results_class.py:149 - Counted Episodes = 8613
2025-08-04 02:17:17,870 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:17:17,881 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:17:17,993 - father_agent.py:547 - Training finished.
2025-08-04 02:17:18,140 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:17:18,143 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:17:18,145 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 02:17:27,604 - evaluation_results_class.py:131 - Average Return = 3.861720561981201
2025-08-04 02:17:27,604 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.61720657348633
2025-08-04 02:17:27,604 - evaluation_results_class.py:135 - Average Discounted Reward = 32.41928482055664
2025-08-04 02:17:27,604 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:17:27,604 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:17:27,604 - evaluation_results_class.py:141 - Variance of Return = 3.0760838985443115
2025-08-04 02:17:27,604 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:17:27,604 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:17:27,604 - evaluation_results_class.py:147 - Average Episode Length = 37.33066295135261
2025-08-04 02:17:27,604 - evaluation_results_class.py:149 - Counted Episodes = 8613
2025-08-04 02:17:27,818 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:17:27,820 - self_interpretable_extractor.py:286 - True
2025-08-04 02:17:27,832 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:17:44,280 - evaluation_results_class.py:131 - Average Return = 3.8204753398895264
2025-08-04 02:17:44,280 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.20475387573242
2025-08-04 02:17:44,280 - evaluation_results_class.py:135 - Average Discounted Reward = 32.09250259399414
2025-08-04 02:17:44,280 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:17:44,280 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:17:44,280 - evaluation_results_class.py:141 - Variance of Return = 2.9899580478668213
2025-08-04 02:17:44,280 - evaluation_results_class.py:143 - Current Best Return = 3.8204753398895264
2025-08-04 02:17:44,281 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:17:44,281 - evaluation_results_class.py:147 - Average Episode Length = 37.29552877576304
2025-08-04 02:17:44,281 - evaluation_results_class.py:149 - Counted Episodes = 8879
2025-08-04 02:17:44,281 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 02:17:44,281 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 39832 trajectories
Learned trajectory lengths  {37, 157}
Buffer 1
2025-08-04 02:18:52,753 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 79664 trajectories
Learned trajectory lengths  {37, 157}
Buffer 2
2025-08-04 02:20:00,782 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 119496 trajectories
Learned trajectory lengths  {37, 157}
All trajectories collected
2025-08-04 02:21:08,666 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 02:21:08,666 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 119496 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 4
Model saved to fsc_4.dot.
Learned FSC of size 4
2025-08-04 02:21:10,397 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 02:21:26,357 - evaluation_results_class.py:131 - Average Return = 3.7723841667175293
2025-08-04 02:21:26,357 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.72384262084961
2025-08-04 02:21:26,357 - evaluation_results_class.py:135 - Average Discounted Reward = 31.738265991210938
2025-08-04 02:21:26,357 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:21:26,357 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:21:26,357 - evaluation_results_class.py:141 - Variance of Return = 3.1063170433044434
2025-08-04 02:21:26,357 - evaluation_results_class.py:143 - Current Best Return = 3.7723841667175293
2025-08-04 02:21:26,357 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:21:26,357 - evaluation_results_class.py:147 - Average Episode Length = 37.29552877576304
2025-08-04 02:21:26,357 - evaluation_results_class.py:149 - Counted Episodes = 8879
FSC Result: {'best_episode_return': 39.723843, 'best_return': 3.7723842, 'goal_value': 0.0, 'returns_episodic': [39.723843], 'returns': [3.7723842], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [3.106317], 'each_episode_virtual_variance': [310.6317], 'combined_variance': [375.86435], 'num_episodes': [8879], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [37.29552877576304], 'counted_episodes': [8879], 'discounted_rewards': [31.738266], 'new_pomdp_iteration_numbers': []}
[[6], [1, 4, 5], [3], [3], [3], [2], [2], [2], [6], [6], [6], [1, 5], [1, 4], [3], [2], [1], [0], [0], [0], [0]]
2025-08-04 02:21:26,624 - statistic.py:67 - synthesis initiated, design space: 140
2025-08-04 02:21:26,625 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 02:21:26,742 - synthesizer_ar.py:122 - value 16.2579 achieved after 2706.35 seconds
2025-08-04 02:21:26,756 - synthesizer_ar.py:122 - value 8.9356 achieved after 2706.37 seconds
2025-08-04 02:21:26,809 - synthesizer_ar.py:122 - value 8.6133 achieved after 2706.42 seconds
2025-08-04 02:21:26,853 - synthesizer_ar.py:122 - value 8.1748 achieved after 2706.46 seconds
2025-08-04 02:21:26,888 - synthesizer_ar.py:122 - value 7.5705 achieved after 2706.5 seconds
2025-08-04 02:21:26,919 - synthesizer_ar.py:122 - value 6.7285 achieved after 2706.53 seconds
2025-08-04 02:21:26,932 - synthesizer_ar.py:122 - value 5.5417 achieved after 2706.54 seconds
2025-08-04 02:21:26,938 - synthesizer_ar.py:122 - value 3.7768 achieved after 2706.55 seconds
2025-08-04 02:21:26,938 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 02:21:26,938 - synthesizer.py:193 - SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 02:21:26,940 - synthesizer.py:198 - double-checking specification satisfiability:  : 3.776782549310915
--------------------
Synthesis summary:
optimality objective: R{"packets_sent"}min=? [F "label_done"] 

method: AR, synthesis time: 0.31 s
number of holes: 4, family size: 140, quotient: 13482 states / 78181 actions
explored: 100 %
MDP stats: avg MDP size: 6639, iterations: 24

optimum: 3.776783
--------------------
2025-08-04 02:21:26,940 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 02:21:26,944 - robust_rl_trainer.py:432 - Iteration 6 of pure RL loop
2025-08-04 02:21:26,988 - storm_vec_env.py:70 - Computing row map
2025-08-04 02:21:26,995 - storm_vec_env.py:97 - Computing transitions
2025-08-04 02:21:27,006 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 02:21:27,006 - storm_vec_env.py:114 - Computing sinks
2025-08-04 02:21:27,006 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 02:21:27,009 - storm_vec_env.py:143 - Computing labels
2025-08-04 02:21:27,010 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 02:21:27,010 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 02:21:27,010 - storm_vec_env.py:175 - Computing observations
2025-08-04 02:21:27,160 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 02:21:27,162 - father_agent.py:540 - Before training evaluation.
2025-08-04 02:21:27,308 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:21:27,310 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:21:37,778 - evaluation_results_class.py:131 - Average Return = 3.7994229793548584
2025-08-04 02:21:37,778 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.99422836303711
2025-08-04 02:21:37,778 - evaluation_results_class.py:135 - Average Discounted Reward = 32.007354736328125
2025-08-04 02:21:37,778 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:21:37,778 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:21:37,778 - evaluation_results_class.py:141 - Variance of Return = 2.8527867794036865
2025-08-04 02:21:37,778 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:21:37,778 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:21:37,778 - evaluation_results_class.py:147 - Average Episode Length = 37.107097518753605
2025-08-04 02:21:37,778 - evaluation_results_class.py:149 - Counted Episodes = 8665
2025-08-04 02:21:37,999 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:21:38,011 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:21:38,122 - father_agent.py:436 - Training agent on-policy
2025-08-04 02:21:46,205 - father_agent.py:386 - Step: 0, Training loss: 3.0719552040100098
2025-08-04 02:21:51,187 - father_agent.py:386 - Step: 5, Training loss: 10.187420845031738
2025-08-04 02:21:53,075 - father_agent.py:386 - Step: 10, Training loss: 2.722442150115967
2025-08-04 02:21:54,964 - father_agent.py:386 - Step: 15, Training loss: 3.170745849609375
2025-08-04 02:21:56,834 - father_agent.py:386 - Step: 20, Training loss: 5.987428188323975
2025-08-04 02:21:58,710 - father_agent.py:386 - Step: 25, Training loss: 4.149923801422119
2025-08-04 02:22:00,567 - father_agent.py:386 - Step: 30, Training loss: 7.623318195343018
2025-08-04 02:22:02,423 - father_agent.py:386 - Step: 35, Training loss: 4.01612663269043
2025-08-04 02:22:04,291 - father_agent.py:386 - Step: 40, Training loss: 1.9851418733596802
2025-08-04 02:22:06,162 - father_agent.py:386 - Step: 45, Training loss: 10.31527328491211
2025-08-04 02:22:08,022 - father_agent.py:386 - Step: 50, Training loss: 2.4069907665252686
2025-08-04 02:22:09,912 - father_agent.py:386 - Step: 55, Training loss: 10.531530380249023
2025-08-04 02:22:11,762 - father_agent.py:386 - Step: 60, Training loss: 4.507774829864502
2025-08-04 02:22:13,598 - father_agent.py:386 - Step: 65, Training loss: 3.3808224201202393
2025-08-04 02:22:15,454 - father_agent.py:386 - Step: 70, Training loss: 7.750626564025879
2025-08-04 02:22:17,327 - father_agent.py:386 - Step: 75, Training loss: 2.2791359424591064
2025-08-04 02:22:19,211 - father_agent.py:386 - Step: 80, Training loss: 2.8084237575531006
2025-08-04 02:22:21,084 - father_agent.py:386 - Step: 85, Training loss: 5.525795936584473
2025-08-04 02:22:22,977 - father_agent.py:386 - Step: 90, Training loss: 2.3808186054229736
2025-08-04 02:22:24,825 - father_agent.py:386 - Step: 95, Training loss: 7.531071662902832
2025-08-04 02:22:26,690 - father_agent.py:386 - Step: 100, Training loss: 3.794133186340332
2025-08-04 02:22:26,918 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:22:26,921 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:22:37,060 - evaluation_results_class.py:131 - Average Return = 3.8244662284851074
2025-08-04 02:22:37,060 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.24466323852539
2025-08-04 02:22:37,060 - evaluation_results_class.py:135 - Average Discounted Reward = 32.16522979736328
2025-08-04 02:22:37,060 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:22:37,060 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:22:37,060 - evaluation_results_class.py:141 - Variance of Return = 2.8884029388427734
2025-08-04 02:22:37,060 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:22:37,060 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:22:37,060 - evaluation_results_class.py:147 - Average Episode Length = 37.107097518753605
2025-08-04 02:22:37,060 - evaluation_results_class.py:149 - Counted Episodes = 8665
2025-08-04 02:22:37,282 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:22:37,293 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:22:41,247 - father_agent.py:386 - Step: 105, Training loss: 1.7234326601028442
2025-08-04 02:22:43,109 - father_agent.py:386 - Step: 110, Training loss: 7.43731164932251
2025-08-04 02:22:44,981 - father_agent.py:386 - Step: 115, Training loss: 3.899744987487793
2025-08-04 02:22:46,851 - father_agent.py:386 - Step: 120, Training loss: 2.182933807373047
2025-08-04 02:22:48,713 - father_agent.py:386 - Step: 125, Training loss: 10.208123207092285
2025-08-04 02:22:50,571 - father_agent.py:386 - Step: 130, Training loss: 2.192234992980957
2025-08-04 02:22:52,415 - father_agent.py:386 - Step: 135, Training loss: 11.12224292755127
2025-08-04 02:22:54,267 - father_agent.py:386 - Step: 140, Training loss: 4.118497371673584
2025-08-04 02:22:56,092 - father_agent.py:386 - Step: 145, Training loss: 3.2408154010772705
2025-08-04 02:22:57,925 - father_agent.py:386 - Step: 150, Training loss: 6.624578952789307
2025-08-04 02:22:59,785 - father_agent.py:386 - Step: 155, Training loss: 3.1423227787017822
2025-08-04 02:23:01,642 - father_agent.py:386 - Step: 160, Training loss: 6.0345458984375
2025-08-04 02:23:03,502 - father_agent.py:386 - Step: 165, Training loss: 4.488063812255859
2025-08-04 02:23:05,338 - father_agent.py:386 - Step: 170, Training loss: 3.414140224456787
2025-08-04 02:23:07,170 - father_agent.py:386 - Step: 175, Training loss: 13.898149490356445
2025-08-04 02:23:09,046 - father_agent.py:386 - Step: 180, Training loss: 4.749277591705322
2025-08-04 02:23:10,910 - father_agent.py:386 - Step: 185, Training loss: 2.6006100177764893
2025-08-04 02:23:12,763 - father_agent.py:386 - Step: 190, Training loss: 10.369091033935547
2025-08-04 02:23:14,633 - father_agent.py:386 - Step: 195, Training loss: 3.2039341926574707
2025-08-04 02:23:16,496 - father_agent.py:386 - Step: 200, Training loss: 11.122496604919434
2025-08-04 02:23:16,725 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:23:16,727 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:23:26,792 - evaluation_results_class.py:131 - Average Return = 3.8024234771728516
2025-08-04 02:23:26,792 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.024234771728516
2025-08-04 02:23:26,792 - evaluation_results_class.py:135 - Average Discounted Reward = 32.0158576965332
2025-08-04 02:23:26,792 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:23:26,792 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:23:26,792 - evaluation_results_class.py:141 - Variance of Return = 2.875908374786377
2025-08-04 02:23:26,792 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:23:26,792 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:23:26,792 - evaluation_results_class.py:147 - Average Episode Length = 37.107097518753605
2025-08-04 02:23:26,792 - evaluation_results_class.py:149 - Counted Episodes = 8665
2025-08-04 02:23:27,027 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:23:27,038 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:23:30,964 - father_agent.py:386 - Step: 205, Training loss: 3.05324387550354
2025-08-04 02:23:32,823 - father_agent.py:386 - Step: 210, Training loss: 5.667359828948975
2025-08-04 02:23:34,688 - father_agent.py:386 - Step: 215, Training loss: 3.371492862701416
2025-08-04 02:23:36,546 - father_agent.py:386 - Step: 220, Training loss: 2.5275940895080566
2025-08-04 02:23:38,406 - father_agent.py:386 - Step: 225, Training loss: 11.199350357055664
2025-08-04 02:23:40,270 - father_agent.py:386 - Step: 230, Training loss: 2.365687131881714
2025-08-04 02:23:42,104 - father_agent.py:386 - Step: 235, Training loss: 10.335440635681152
2025-08-04 02:23:44,017 - father_agent.py:386 - Step: 240, Training loss: 3.8401737213134766
2025-08-04 02:23:45,904 - father_agent.py:386 - Step: 245, Training loss: 4.029098033905029
2025-08-04 02:23:47,769 - father_agent.py:386 - Step: 250, Training loss: 5.855014324188232
2025-08-04 02:23:49,653 - father_agent.py:386 - Step: 255, Training loss: 2.824605703353882
2025-08-04 02:23:51,523 - father_agent.py:386 - Step: 260, Training loss: 5.255957126617432
2025-08-04 02:23:53,371 - father_agent.py:386 - Step: 265, Training loss: 4.979446887969971
2025-08-04 02:23:55,216 - father_agent.py:386 - Step: 270, Training loss: 4.367362976074219
2025-08-04 02:23:57,061 - father_agent.py:386 - Step: 275, Training loss: 13.508987426757812
2025-08-04 02:23:58,945 - father_agent.py:386 - Step: 280, Training loss: 4.268010139465332
2025-08-04 02:24:00,803 - father_agent.py:386 - Step: 285, Training loss: 2.6173501014709473
2025-08-04 02:24:02,664 - father_agent.py:386 - Step: 290, Training loss: 12.315828323364258
2025-08-04 02:24:04,515 - father_agent.py:386 - Step: 295, Training loss: 1.8914101123809814
2025-08-04 02:24:06,367 - father_agent.py:386 - Step: 300, Training loss: 11.858185768127441
2025-08-04 02:24:06,602 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:24:06,604 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:24:17,067 - evaluation_results_class.py:131 - Average Return = 3.892671585083008
2025-08-04 02:24:17,068 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.92671585083008
2025-08-04 02:24:17,068 - evaluation_results_class.py:135 - Average Discounted Reward = 32.79137420654297
2025-08-04 02:24:17,068 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:24:17,068 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:24:17,068 - evaluation_results_class.py:141 - Variance of Return = 2.8985788822174072
2025-08-04 02:24:17,068 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:24:17,068 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:24:17,068 - evaluation_results_class.py:147 - Average Episode Length = 37.107097518753605
2025-08-04 02:24:17,068 - evaluation_results_class.py:149 - Counted Episodes = 8665
2025-08-04 02:24:17,298 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:24:17,310 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:24:21,261 - father_agent.py:386 - Step: 305, Training loss: 1.6118640899658203
2025-08-04 02:24:23,117 - father_agent.py:386 - Step: 310, Training loss: 6.784079074859619
2025-08-04 02:24:24,986 - father_agent.py:386 - Step: 315, Training loss: 3.2925257682800293
2025-08-04 02:24:26,845 - father_agent.py:386 - Step: 320, Training loss: 3.0252492427825928
2025-08-04 02:24:28,726 - father_agent.py:386 - Step: 325, Training loss: 9.046988487243652
2025-08-04 02:24:30,570 - father_agent.py:386 - Step: 330, Training loss: 2.787785530090332
2025-08-04 02:24:32,395 - father_agent.py:386 - Step: 335, Training loss: 11.32198715209961
2025-08-04 02:24:34,267 - father_agent.py:386 - Step: 340, Training loss: 3.531095266342163
2025-08-04 02:24:36,092 - father_agent.py:386 - Step: 345, Training loss: 3.324106454849243
2025-08-04 02:24:37,944 - father_agent.py:386 - Step: 350, Training loss: 5.30120849609375
2025-08-04 02:24:39,785 - father_agent.py:386 - Step: 355, Training loss: 2.636777877807617
2025-08-04 02:24:41,635 - father_agent.py:386 - Step: 360, Training loss: 5.0140700340271
2025-08-04 02:24:43,505 - father_agent.py:386 - Step: 365, Training loss: 4.038641452789307
2025-08-04 02:24:45,329 - father_agent.py:386 - Step: 370, Training loss: 4.016721725463867
2025-08-04 02:24:47,157 - father_agent.py:386 - Step: 375, Training loss: 12.353010177612305
2025-08-04 02:24:49,007 - father_agent.py:386 - Step: 380, Training loss: 4.745486259460449
2025-08-04 02:24:50,887 - father_agent.py:386 - Step: 385, Training loss: 2.8323051929473877
2025-08-04 02:24:52,777 - father_agent.py:386 - Step: 390, Training loss: 11.753619194030762
2025-08-04 02:24:54,698 - father_agent.py:386 - Step: 395, Training loss: 1.864595651626587
2025-08-04 02:24:56,600 - father_agent.py:386 - Step: 400, Training loss: 12.34022331237793
2025-08-04 02:24:56,832 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:24:56,834 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:25:07,016 - evaluation_results_class.py:131 - Average Return = 3.8650894165039062
2025-08-04 02:25:07,016 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.65089416503906
2025-08-04 02:25:07,016 - evaluation_results_class.py:135 - Average Discounted Reward = 32.56306838989258
2025-08-04 02:25:07,016 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:25:07,016 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:25:07,016 - evaluation_results_class.py:141 - Variance of Return = 2.923403024673462
2025-08-04 02:25:07,016 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:25:07,016 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:25:07,016 - evaluation_results_class.py:147 - Average Episode Length = 37.107097518753605
2025-08-04 02:25:07,016 - evaluation_results_class.py:149 - Counted Episodes = 8665
2025-08-04 02:25:07,246 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:25:07,257 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:25:11,267 - father_agent.py:386 - Step: 405, Training loss: 1.6296924352645874
2025-08-04 02:25:13,124 - father_agent.py:386 - Step: 410, Training loss: 5.834266185760498
2025-08-04 02:25:14,975 - father_agent.py:386 - Step: 415, Training loss: 3.2990689277648926
2025-08-04 02:25:16,799 - father_agent.py:386 - Step: 420, Training loss: 2.7108311653137207
2025-08-04 02:25:18,637 - father_agent.py:386 - Step: 425, Training loss: 10.561236381530762
2025-08-04 02:25:20,494 - father_agent.py:386 - Step: 430, Training loss: 2.2804105281829834
2025-08-04 02:25:22,320 - father_agent.py:386 - Step: 435, Training loss: 9.920533180236816
2025-08-04 02:25:24,176 - father_agent.py:386 - Step: 440, Training loss: 3.1620330810546875
2025-08-04 02:25:26,017 - father_agent.py:386 - Step: 445, Training loss: 3.7134437561035156
2025-08-04 02:25:27,859 - father_agent.py:386 - Step: 450, Training loss: 5.297243595123291
2025-08-04 02:25:29,696 - father_agent.py:386 - Step: 455, Training loss: 2.8032703399658203
2025-08-04 02:25:31,532 - father_agent.py:386 - Step: 460, Training loss: 5.255821228027344
2025-08-04 02:25:33,401 - father_agent.py:386 - Step: 465, Training loss: 4.19777250289917
2025-08-04 02:25:35,237 - father_agent.py:386 - Step: 470, Training loss: 4.638354778289795
2025-08-04 02:25:37,065 - father_agent.py:386 - Step: 475, Training loss: 12.550248146057129
2025-08-04 02:25:38,916 - father_agent.py:386 - Step: 480, Training loss: 4.353201389312744
2025-08-04 02:25:40,774 - father_agent.py:386 - Step: 485, Training loss: 2.3764891624450684
2025-08-04 02:25:42,636 - father_agent.py:386 - Step: 490, Training loss: 10.192273139953613
2025-08-04 02:25:44,515 - father_agent.py:386 - Step: 495, Training loss: 1.5773277282714844
2025-08-04 02:25:46,364 - father_agent.py:386 - Step: 500, Training loss: 13.884888648986816
2025-08-04 02:25:46,594 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:25:46,596 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:25:56,587 - evaluation_results_class.py:131 - Average Return = 3.880669355392456
2025-08-04 02:25:56,587 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.80669403076172
2025-08-04 02:25:56,587 - evaluation_results_class.py:135 - Average Discounted Reward = 32.649742126464844
2025-08-04 02:25:56,588 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:25:56,588 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:25:56,588 - evaluation_results_class.py:141 - Variance of Return = 3.0345773696899414
2025-08-04 02:25:56,588 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:25:56,588 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:25:56,588 - evaluation_results_class.py:147 - Average Episode Length = 37.107097518753605
2025-08-04 02:25:56,588 - evaluation_results_class.py:149 - Counted Episodes = 8665
2025-08-04 02:25:56,815 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:25:56,826 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:25:56,940 - father_agent.py:547 - Training finished.
2025-08-04 02:25:57,090 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:25:57,093 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:25:57,095 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 02:26:07,427 - evaluation_results_class.py:131 - Average Return = 3.861396312713623
2025-08-04 02:26:07,427 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.61396408081055
2025-08-04 02:26:07,427 - evaluation_results_class.py:135 - Average Discounted Reward = 32.5076789855957
2025-08-04 02:26:07,427 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:26:07,427 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:26:07,427 - evaluation_results_class.py:141 - Variance of Return = 3.038261651992798
2025-08-04 02:26:07,427 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:26:07,427 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:26:07,427 - evaluation_results_class.py:147 - Average Episode Length = 37.107097518753605
2025-08-04 02:26:07,427 - evaluation_results_class.py:149 - Counted Episodes = 8665
2025-08-04 02:26:07,656 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:26:07,659 - self_interpretable_extractor.py:286 - True
2025-08-04 02:26:07,671 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:26:24,592 - evaluation_results_class.py:131 - Average Return = 3.7983880043029785
2025-08-04 02:26:24,592 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.98387908935547
2025-08-04 02:26:24,592 - evaluation_results_class.py:135 - Average Discounted Reward = 32.044403076171875
2025-08-04 02:26:24,592 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:26:24,592 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:26:24,592 - evaluation_results_class.py:141 - Variance of Return = 2.865095615386963
2025-08-04 02:26:24,592 - evaluation_results_class.py:143 - Current Best Return = 3.7983880043029785
2025-08-04 02:26:24,592 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:26:24,592 - evaluation_results_class.py:147 - Average Episode Length = 37.07880891078025
2025-08-04 02:26:24,592 - evaluation_results_class.py:149 - Counted Episodes = 8933
2025-08-04 02:26:24,593 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 02:26:24,593 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 40072 trajectories
Learned trajectory lengths  {37, 157}
Buffer 1
2025-08-04 02:27:34,850 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 80144 trajectories
Learned trajectory lengths  {37, 157}
Buffer 2
2025-08-04 02:28:45,028 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 120216 trajectories
Learned trajectory lengths  {37, 157}
All trajectories collected
2025-08-04 02:29:55,061 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 02:29:55,061 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 120216 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 4
Model saved to fsc_5.dot.
Learned FSC of size 4
2025-08-04 02:29:57,303 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 02:30:13,670 - evaluation_results_class.py:131 - Average Return = 3.7768945693969727
2025-08-04 02:30:13,670 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.76894760131836
2025-08-04 02:30:13,670 - evaluation_results_class.py:135 - Average Discounted Reward = 31.82832908630371
2025-08-04 02:30:13,670 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:30:13,670 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:30:13,670 - evaluation_results_class.py:141 - Variance of Return = 2.83917498588562
2025-08-04 02:30:13,670 - evaluation_results_class.py:143 - Current Best Return = 3.7768945693969727
2025-08-04 02:30:13,670 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:30:13,670 - evaluation_results_class.py:147 - Average Episode Length = 37.07880891078025
2025-08-04 02:30:13,670 - evaluation_results_class.py:149 - Counted Episodes = 8933
FSC Result: {'best_episode_return': 39.768948, 'best_return': 3.7768946, 'goal_value': 0.0, 'returns_episodic': [39.768948], 'returns': [3.7768946], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [2.839175], 'each_episode_virtual_variance': [283.9175], 'combined_variance': [343.54016], 'num_episodes': [8933], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [37.07880891078025], 'counted_episodes': [8933], 'discounted_rewards': [31.82833], 'new_pomdp_iteration_numbers': []}
[[6], [1, 4, 5], [3], [3], [3], [2], [2], [2], [6], [6], [6], [1, 5], [1, 4], [3], [2], [1], [0], [0], [0], [0]]
2025-08-04 02:30:14,393 - statistic.py:67 - synthesis initiated, design space: 140
2025-08-04 02:30:14,395 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 02:30:14,514 - synthesizer_ar.py:122 - value 16.2579 achieved after 3234.13 seconds
2025-08-04 02:30:14,529 - synthesizer_ar.py:122 - value 8.9353 achieved after 3234.14 seconds
2025-08-04 02:30:14,582 - synthesizer_ar.py:122 - value 8.6118 achieved after 3234.19 seconds
2025-08-04 02:30:14,625 - synthesizer_ar.py:122 - value 8.1709 achieved after 3234.24 seconds
2025-08-04 02:30:14,661 - synthesizer_ar.py:122 - value 7.562 achieved after 3234.27 seconds
2025-08-04 02:30:14,691 - synthesizer_ar.py:122 - value 6.7121 achieved after 3234.3 seconds
2025-08-04 02:30:14,704 - synthesizer_ar.py:122 - value 5.5117 achieved after 3234.32 seconds
2025-08-04 02:30:14,710 - synthesizer_ar.py:122 - value 3.7216 achieved after 3234.32 seconds
2025-08-04 02:30:14,711 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 02:30:14,711 - synthesizer.py:193 - SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 02:30:14,712 - synthesizer.py:198 - double-checking specification satisfiability:  : 3.721624875113317
--------------------
Synthesis summary:
optimality objective: R{"packets_sent"}min=? [F "label_done"] 

method: AR, synthesis time: 0.32 s
number of holes: 4, family size: 140, quotient: 13482 states / 78181 actions
explored: 100 %
MDP stats: avg MDP size: 6639, iterations: 24

optimum: 3.721625
--------------------
2025-08-04 02:30:14,713 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 02:30:14,717 - robust_rl_trainer.py:432 - Iteration 7 of pure RL loop
2025-08-04 02:30:14,762 - storm_vec_env.py:70 - Computing row map
2025-08-04 02:30:14,769 - storm_vec_env.py:97 - Computing transitions
2025-08-04 02:30:14,780 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 02:30:14,780 - storm_vec_env.py:114 - Computing sinks
2025-08-04 02:30:14,780 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 02:30:14,783 - storm_vec_env.py:143 - Computing labels
2025-08-04 02:30:14,784 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 02:30:14,784 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 02:30:14,784 - storm_vec_env.py:175 - Computing observations
2025-08-04 02:30:14,949 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 02:30:14,949 - father_agent.py:540 - Before training evaluation.
2025-08-04 02:30:15,102 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:30:15,104 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:30:25,699 - evaluation_results_class.py:131 - Average Return = 3.839949369430542
2025-08-04 02:30:25,699 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.39949417114258
2025-08-04 02:30:25,699 - evaluation_results_class.py:135 - Average Discounted Reward = 32.35537338256836
2025-08-04 02:30:25,699 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:30:25,699 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:30:25,699 - evaluation_results_class.py:141 - Variance of Return = 2.8116867542266846
2025-08-04 02:30:25,699 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:30:25,699 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:30:25,699 - evaluation_results_class.py:147 - Average Episode Length = 36.99631803014613
2025-08-04 02:30:25,699 - evaluation_results_class.py:149 - Counted Episodes = 8691
2025-08-04 02:30:25,932 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:30:25,944 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:30:26,056 - father_agent.py:436 - Training agent on-policy
2025-08-04 02:30:34,047 - father_agent.py:386 - Step: 0, Training loss: 4.351035118103027
2025-08-04 02:30:39,078 - father_agent.py:386 - Step: 5, Training loss: 7.762302875518799
2025-08-04 02:30:40,946 - father_agent.py:386 - Step: 10, Training loss: 3.0993940830230713
2025-08-04 02:30:42,798 - father_agent.py:386 - Step: 15, Training loss: 4.141723155975342
2025-08-04 02:30:44,637 - father_agent.py:386 - Step: 20, Training loss: 13.483725547790527
2025-08-04 02:30:46,481 - father_agent.py:386 - Step: 25, Training loss: 7.055055141448975
2025-08-04 02:30:48,334 - father_agent.py:386 - Step: 30, Training loss: 6.958075523376465
2025-08-04 02:30:50,184 - father_agent.py:386 - Step: 35, Training loss: 4.366670608520508
2025-08-04 02:30:52,046 - father_agent.py:386 - Step: 40, Training loss: 3.3665308952331543
2025-08-04 02:30:53,889 - father_agent.py:386 - Step: 45, Training loss: 14.611316680908203
2025-08-04 02:30:55,738 - father_agent.py:386 - Step: 50, Training loss: 4.839386463165283
2025-08-04 02:30:57,591 - father_agent.py:386 - Step: 55, Training loss: 2.4558610916137695
2025-08-04 02:30:59,441 - father_agent.py:386 - Step: 60, Training loss: 11.86650276184082
2025-08-04 02:31:01,288 - father_agent.py:386 - Step: 65, Training loss: 1.7124996185302734
2025-08-04 02:31:03,117 - father_agent.py:386 - Step: 70, Training loss: 13.9921293258667
2025-08-04 02:31:04,978 - father_agent.py:386 - Step: 75, Training loss: 5.0304341316223145
2025-08-04 02:31:06,820 - father_agent.py:386 - Step: 80, Training loss: 2.4430525302886963
2025-08-04 02:31:08,655 - father_agent.py:386 - Step: 85, Training loss: 10.067699432373047
2025-08-04 02:31:10,520 - father_agent.py:386 - Step: 90, Training loss: 3.1197173595428467
2025-08-04 02:31:12,363 - father_agent.py:386 - Step: 95, Training loss: 2.4358372688293457
2025-08-04 02:31:14,202 - father_agent.py:386 - Step: 100, Training loss: 8.582883834838867
2025-08-04 02:31:14,435 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:31:14,437 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:31:24,940 - evaluation_results_class.py:131 - Average Return = 3.814635753631592
2025-08-04 02:31:24,940 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.146358489990234
2025-08-04 02:31:24,940 - evaluation_results_class.py:135 - Average Discounted Reward = 32.14222717285156
2025-08-04 02:31:24,941 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:31:24,941 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:31:24,941 - evaluation_results_class.py:141 - Variance of Return = 2.8611643314361572
2025-08-04 02:31:24,941 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:31:24,941 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:31:24,941 - evaluation_results_class.py:147 - Average Episode Length = 36.99631803014613
2025-08-04 02:31:24,941 - evaluation_results_class.py:149 - Counted Episodes = 8691
2025-08-04 02:31:25,179 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:31:25,190 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:31:29,164 - father_agent.py:386 - Step: 105, Training loss: 1.4764988422393799
2025-08-04 02:31:30,991 - father_agent.py:386 - Step: 110, Training loss: 10.533841133117676
2025-08-04 02:31:32,828 - father_agent.py:386 - Step: 115, Training loss: 3.332817792892456
2025-08-04 02:31:34,651 - father_agent.py:386 - Step: 120, Training loss: 3.2956466674804688
2025-08-04 02:31:36,475 - father_agent.py:386 - Step: 125, Training loss: 6.594254970550537
2025-08-04 02:31:38,313 - father_agent.py:386 - Step: 130, Training loss: 2.0587899684906006
2025-08-04 02:31:40,140 - father_agent.py:386 - Step: 135, Training loss: 3.097724199295044
2025-08-04 02:31:41,975 - father_agent.py:386 - Step: 140, Training loss: 5.531781196594238
2025-08-04 02:31:43,810 - father_agent.py:386 - Step: 145, Training loss: 1.865691065788269
2025-08-04 02:31:45,644 - father_agent.py:386 - Step: 150, Training loss: 6.0790696144104
2025-08-04 02:31:47,476 - father_agent.py:386 - Step: 155, Training loss: 3.321206569671631
2025-08-04 02:31:49,310 - father_agent.py:386 - Step: 160, Training loss: 6.710293292999268
2025-08-04 02:31:51,149 - father_agent.py:386 - Step: 165, Training loss: 6.259959697723389
2025-08-04 02:31:52,983 - father_agent.py:386 - Step: 170, Training loss: 2.4101908206939697
2025-08-04 02:31:54,791 - father_agent.py:386 - Step: 175, Training loss: 2.441179037094116
2025-08-04 02:31:56,628 - father_agent.py:386 - Step: 180, Training loss: 5.868897438049316
2025-08-04 02:31:58,475 - father_agent.py:386 - Step: 185, Training loss: 3.8482282161712646
2025-08-04 02:32:00,312 - father_agent.py:386 - Step: 190, Training loss: 6.5640716552734375
2025-08-04 02:32:02,147 - father_agent.py:386 - Step: 195, Training loss: 3.353949546813965
2025-08-04 02:32:03,981 - father_agent.py:386 - Step: 200, Training loss: 3.5827982425689697
2025-08-04 02:32:04,215 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:32:04,218 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:32:14,848 - evaluation_results_class.py:131 - Average Return = 3.841330051422119
2025-08-04 02:32:14,848 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.413299560546875
2025-08-04 02:32:14,848 - evaluation_results_class.py:135 - Average Discounted Reward = 32.37933349609375
2025-08-04 02:32:14,848 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:32:14,848 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:32:14,848 - evaluation_results_class.py:141 - Variance of Return = 2.9264979362487793
2025-08-04 02:32:14,848 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:32:14,848 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:32:14,848 - evaluation_results_class.py:147 - Average Episode Length = 36.99631803014613
2025-08-04 02:32:14,848 - evaluation_results_class.py:149 - Counted Episodes = 8691
2025-08-04 02:32:15,089 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:32:15,100 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:32:19,057 - father_agent.py:386 - Step: 205, Training loss: 1.884299874305725
2025-08-04 02:32:20,949 - father_agent.py:386 - Step: 210, Training loss: 10.92070198059082
2025-08-04 02:32:22,847 - father_agent.py:386 - Step: 215, Training loss: 3.2644007205963135
2025-08-04 02:32:24,744 - father_agent.py:386 - Step: 220, Training loss: 3.203831911087036
2025-08-04 02:32:26,606 - father_agent.py:386 - Step: 225, Training loss: 6.462731838226318
2025-08-04 02:32:28,486 - father_agent.py:386 - Step: 230, Training loss: 2.1504132747650146
2025-08-04 02:32:30,459 - father_agent.py:386 - Step: 235, Training loss: 3.5920844078063965
2025-08-04 02:32:32,399 - father_agent.py:386 - Step: 240, Training loss: 6.532120227813721
2025-08-04 02:32:34,329 - father_agent.py:386 - Step: 245, Training loss: 1.8902981281280518
2025-08-04 02:32:36,299 - father_agent.py:386 - Step: 250, Training loss: 8.181938171386719
2025-08-04 02:32:38,258 - father_agent.py:386 - Step: 255, Training loss: 3.254882574081421
2025-08-04 02:32:40,278 - father_agent.py:386 - Step: 260, Training loss: 6.065718173980713
2025-08-04 02:32:42,213 - father_agent.py:386 - Step: 265, Training loss: 7.387580871582031
2025-08-04 02:32:44,195 - father_agent.py:386 - Step: 270, Training loss: 2.1737990379333496
2025-08-04 02:32:46,231 - father_agent.py:386 - Step: 275, Training loss: 2.9508514404296875
2025-08-04 02:32:48,168 - father_agent.py:386 - Step: 280, Training loss: 6.051645755767822
2025-08-04 02:32:50,153 - father_agent.py:386 - Step: 285, Training loss: 3.1119422912597656
2025-08-04 02:32:52,137 - father_agent.py:386 - Step: 290, Training loss: 7.01229190826416
2025-08-04 02:32:54,139 - father_agent.py:386 - Step: 295, Training loss: 2.9443836212158203
2025-08-04 02:32:56,062 - father_agent.py:386 - Step: 300, Training loss: 3.251077890396118
2025-08-04 02:32:56,300 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:32:56,302 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:33:07,767 - evaluation_results_class.py:131 - Average Return = 3.7968013286590576
2025-08-04 02:33:07,767 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.968013763427734
2025-08-04 02:33:07,767 - evaluation_results_class.py:135 - Average Discounted Reward = 32.014373779296875
2025-08-04 02:33:07,767 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:33:07,767 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:33:07,767 - evaluation_results_class.py:141 - Variance of Return = 2.845374584197998
2025-08-04 02:33:07,767 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:33:07,767 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:33:07,768 - evaluation_results_class.py:147 - Average Episode Length = 36.99631803014613
2025-08-04 02:33:07,768 - evaluation_results_class.py:149 - Counted Episodes = 8691
2025-08-04 02:33:08,011 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:33:08,023 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:33:12,139 - father_agent.py:386 - Step: 305, Training loss: 1.9199610948562622
2025-08-04 02:33:14,115 - father_agent.py:386 - Step: 310, Training loss: 9.202116966247559
2025-08-04 02:33:16,022 - father_agent.py:386 - Step: 315, Training loss: 3.3946142196655273
2025-08-04 02:33:17,936 - father_agent.py:386 - Step: 320, Training loss: 2.981001853942871
2025-08-04 02:33:19,952 - father_agent.py:386 - Step: 325, Training loss: 6.183464050292969
2025-08-04 02:33:21,898 - father_agent.py:386 - Step: 330, Training loss: 1.935246229171753
2025-08-04 02:33:23,801 - father_agent.py:386 - Step: 335, Training loss: 2.8356082439422607
2025-08-04 02:33:25,713 - father_agent.py:386 - Step: 340, Training loss: 4.720474720001221
2025-08-04 02:33:27,633 - father_agent.py:386 - Step: 345, Training loss: 2.13576602935791
2025-08-04 02:33:29,573 - father_agent.py:386 - Step: 350, Training loss: 6.501455783843994
2025-08-04 02:33:31,579 - father_agent.py:386 - Step: 355, Training loss: 3.2487545013427734
2025-08-04 02:33:33,512 - father_agent.py:386 - Step: 360, Training loss: 5.498256683349609
2025-08-04 02:33:35,409 - father_agent.py:386 - Step: 365, Training loss: 5.850615978240967
2025-08-04 02:33:37,316 - father_agent.py:386 - Step: 370, Training loss: 2.193014621734619
2025-08-04 02:33:39,272 - father_agent.py:386 - Step: 375, Training loss: 2.5325684547424316
2025-08-04 02:33:41,269 - father_agent.py:386 - Step: 380, Training loss: 5.278538227081299
2025-08-04 02:33:43,176 - father_agent.py:386 - Step: 385, Training loss: 3.509160041809082
2025-08-04 02:33:45,098 - father_agent.py:386 - Step: 390, Training loss: 6.478369235992432
2025-08-04 02:33:47,054 - father_agent.py:386 - Step: 395, Training loss: 2.803030490875244
2025-08-04 02:33:49,004 - father_agent.py:386 - Step: 400, Training loss: 3.116391181945801
2025-08-04 02:33:49,258 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:33:49,261 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:34:00,513 - evaluation_results_class.py:131 - Average Return = 3.8259117603302
2025-08-04 02:34:00,513 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.259117126464844
2025-08-04 02:34:00,513 - evaluation_results_class.py:135 - Average Discounted Reward = 32.24671936035156
2025-08-04 02:34:00,513 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:34:00,513 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:34:00,513 - evaluation_results_class.py:141 - Variance of Return = 2.91538405418396
2025-08-04 02:34:00,513 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:34:00,513 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:34:00,513 - evaluation_results_class.py:147 - Average Episode Length = 36.99631803014613
2025-08-04 02:34:00,513 - evaluation_results_class.py:149 - Counted Episodes = 8691
2025-08-04 02:34:00,762 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:34:00,773 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:34:04,908 - father_agent.py:386 - Step: 405, Training loss: 1.9010354280471802
2025-08-04 02:34:06,799 - father_agent.py:386 - Step: 410, Training loss: 9.008338928222656
2025-08-04 02:34:08,671 - father_agent.py:386 - Step: 415, Training loss: 3.1708271503448486
2025-08-04 02:34:10,516 - father_agent.py:386 - Step: 420, Training loss: 3.2235348224639893
2025-08-04 02:34:12,365 - father_agent.py:386 - Step: 425, Training loss: 7.316220760345459
2025-08-04 02:34:14,197 - father_agent.py:386 - Step: 430, Training loss: 1.9656522274017334
2025-08-04 02:34:16,025 - father_agent.py:386 - Step: 435, Training loss: 4.051978588104248
2025-08-04 02:34:17,870 - father_agent.py:386 - Step: 440, Training loss: 6.862582206726074
2025-08-04 02:34:19,688 - father_agent.py:386 - Step: 445, Training loss: 2.045027494430542
2025-08-04 02:34:21,552 - father_agent.py:386 - Step: 450, Training loss: 8.094766616821289
2025-08-04 02:34:23,388 - father_agent.py:386 - Step: 455, Training loss: 2.8704731464385986
2025-08-04 02:34:25,218 - father_agent.py:386 - Step: 460, Training loss: 4.842562675476074
2025-08-04 02:34:27,032 - father_agent.py:386 - Step: 465, Training loss: 7.381604194641113
2025-08-04 02:34:28,866 - father_agent.py:386 - Step: 470, Training loss: 2.3139169216156006
2025-08-04 02:34:30,697 - father_agent.py:386 - Step: 475, Training loss: 2.542762041091919
2025-08-04 02:34:32,554 - father_agent.py:386 - Step: 480, Training loss: 4.304431438446045
2025-08-04 02:34:34,387 - father_agent.py:386 - Step: 485, Training loss: 2.9253175258636475
2025-08-04 02:34:36,216 - father_agent.py:386 - Step: 490, Training loss: 6.877511024475098
2025-08-04 02:34:38,051 - father_agent.py:386 - Step: 495, Training loss: 2.92153263092041
2025-08-04 02:34:39,867 - father_agent.py:386 - Step: 500, Training loss: 2.857797861099243
2025-08-04 02:34:40,108 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:34:40,111 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:34:50,759 - evaluation_results_class.py:131 - Average Return = 3.857208490371704
2025-08-04 02:34:50,760 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.572086334228516
2025-08-04 02:34:50,760 - evaluation_results_class.py:135 - Average Discounted Reward = 32.47652816772461
2025-08-04 02:34:50,760 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:34:50,760 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:34:50,760 - evaluation_results_class.py:141 - Variance of Return = 2.783085346221924
2025-08-04 02:34:50,760 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:34:50,760 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:34:50,760 - evaluation_results_class.py:147 - Average Episode Length = 36.99631803014613
2025-08-04 02:34:50,760 - evaluation_results_class.py:149 - Counted Episodes = 8691
2025-08-04 02:34:51,002 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:34:51,013 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:34:51,129 - father_agent.py:547 - Training finished.
2025-08-04 02:34:51,282 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:34:51,285 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:34:51,288 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 02:35:02,301 - evaluation_results_class.py:131 - Average Return = 3.838223457336426
2025-08-04 02:35:02,301 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.382232666015625
2025-08-04 02:35:02,301 - evaluation_results_class.py:135 - Average Discounted Reward = 32.35893630981445
2025-08-04 02:35:02,301 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:35:02,301 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:35:02,301 - evaluation_results_class.py:141 - Variance of Return = 2.866245746612549
2025-08-04 02:35:02,301 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:35:02,301 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:35:02,301 - evaluation_results_class.py:147 - Average Episode Length = 36.99631803014613
2025-08-04 02:35:02,301 - evaluation_results_class.py:149 - Counted Episodes = 8691
2025-08-04 02:35:02,544 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:35:02,547 - self_interpretable_extractor.py:286 - True
2025-08-04 02:35:02,559 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:35:19,450 - evaluation_results_class.py:131 - Average Return = 3.807142972946167
2025-08-04 02:35:19,450 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.07143020629883
2025-08-04 02:35:19,450 - evaluation_results_class.py:135 - Average Discounted Reward = 32.09839630126953
2025-08-04 02:35:19,450 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:35:19,450 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:35:19,450 - evaluation_results_class.py:141 - Variance of Return = 2.8654847145080566
2025-08-04 02:35:19,450 - evaluation_results_class.py:143 - Current Best Return = 3.807142972946167
2025-08-04 02:35:19,450 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:35:19,450 - evaluation_results_class.py:147 - Average Episode Length = 36.97142857142857
2025-08-04 02:35:19,450 - evaluation_results_class.py:149 - Counted Episodes = 8960
2025-08-04 02:35:19,451 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 02:35:19,451 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 40192 trajectories
Learned trajectory lengths  {37}
Buffer 1
2025-08-04 02:36:29,168 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 80384 trajectories
Learned trajectory lengths  {37}
Buffer 2
2025-08-04 02:37:38,794 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 120576 trajectories
Learned trajectory lengths  {37}
All trajectories collected
2025-08-04 02:38:48,137 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 02:38:48,138 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 120576 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 5
Model saved to fsc_6.dot.
Learned FSC of size 5
2025-08-04 02:38:50,344 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 02:39:06,342 - evaluation_results_class.py:131 - Average Return = 3.8623883724212646
2025-08-04 02:39:06,343 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.62388229370117
2025-08-04 02:39:06,343 - evaluation_results_class.py:135 - Average Discounted Reward = 32.54190444946289
2025-08-04 02:39:06,343 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:39:06,343 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:39:06,343 - evaluation_results_class.py:141 - Variance of Return = 2.781843900680542
2025-08-04 02:39:06,343 - evaluation_results_class.py:143 - Current Best Return = 3.8623883724212646
2025-08-04 02:39:06,343 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:39:06,343 - evaluation_results_class.py:147 - Average Episode Length = 36.97142857142857
2025-08-04 02:39:06,343 - evaluation_results_class.py:149 - Counted Episodes = 8960
FSC Result: {'best_episode_return': 40.623882, 'best_return': 3.8623884, 'goal_value': 0.0, 'returns_episodic': [40.623882], 'returns': [3.8623884], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [2.781844], 'each_episode_virtual_variance': [278.18445], 'combined_variance': [336.60315], 'num_episodes': [8960], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [36.97142857142857], 'counted_episodes': [8960], 'discounted_rewards': [32.541904], 'new_pomdp_iteration_numbers': []}
[[6], [1, 4, 5], [3], [3], [3], [2], [2], [2], [6], [6], [6], [1, 5], [1, 4], [3], [2], [1], [0], [0], [0], [0]]
2025-08-04 02:39:06,620 - statistic.py:67 - synthesis initiated, design space: 140
2025-08-04 02:39:06,621 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 02:39:06,755 - synthesizer_ar.py:122 - value 16.2639 achieved after 3766.37 seconds
2025-08-04 02:39:06,772 - synthesizer_ar.py:122 - value 8.9388 achieved after 3766.38 seconds
2025-08-04 02:39:06,832 - synthesizer_ar.py:122 - value 8.6181 achieved after 3766.44 seconds
2025-08-04 02:39:06,881 - synthesizer_ar.py:122 - value 8.1823 achieved after 3766.49 seconds
2025-08-04 02:39:06,921 - synthesizer_ar.py:122 - value 7.5826 achieved after 3766.53 seconds
2025-08-04 02:39:06,955 - synthesizer_ar.py:122 - value 6.7486 achieved after 3766.57 seconds
2025-08-04 02:39:06,970 - synthesizer_ar.py:122 - value 5.5758 achieved after 3766.58 seconds
2025-08-04 02:39:06,977 - synthesizer_ar.py:122 - value 3.8354 achieved after 3766.59 seconds
2025-08-04 02:39:06,977 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 02:39:06,977 - synthesizer.py:193 - SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 02:39:06,979 - synthesizer.py:198 - double-checking specification satisfiability:  : 3.83538283968006
--------------------
Synthesis summary:
optimality objective: R{"packets_sent"}min=? [F "label_done"] 

method: AR, synthesis time: 0.36 s
number of holes: 4, family size: 140, quotient: 15158 states / 91872 actions
explored: 100 %
MDP stats: avg MDP size: 7243, iterations: 24

optimum: 3.835383
--------------------
2025-08-04 02:39:06,979 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 02:39:06,984 - robust_rl_trainer.py:432 - Iteration 8 of pure RL loop
2025-08-04 02:39:07,030 - storm_vec_env.py:70 - Computing row map
2025-08-04 02:39:07,037 - storm_vec_env.py:97 - Computing transitions
2025-08-04 02:39:07,048 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 02:39:07,048 - storm_vec_env.py:114 - Computing sinks
2025-08-04 02:39:07,048 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 02:39:07,051 - storm_vec_env.py:143 - Computing labels
2025-08-04 02:39:07,051 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 02:39:07,051 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 02:39:07,051 - storm_vec_env.py:175 - Computing observations
2025-08-04 02:39:07,215 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 02:39:07,215 - father_agent.py:540 - Before training evaluation.
2025-08-04 02:39:07,372 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:39:07,787 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:39:18,493 - evaluation_results_class.py:131 - Average Return = 3.8816635608673096
2025-08-04 02:39:18,493 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.81663513183594
2025-08-04 02:39:18,493 - evaluation_results_class.py:135 - Average Discounted Reward = 32.696224212646484
2025-08-04 02:39:18,493 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:39:18,493 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:39:18,494 - evaluation_results_class.py:141 - Variance of Return = 2.8026325702667236
2025-08-04 02:39:18,494 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:39:18,494 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:39:18,494 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:39:18,494 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:39:18,732 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:39:18,744 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:39:18,860 - father_agent.py:436 - Training agent on-policy
2025-08-04 02:39:27,026 - father_agent.py:386 - Step: 0, Training loss: 3.339686632156372
2025-08-04 02:39:32,135 - father_agent.py:386 - Step: 5, Training loss: 6.091845989227295
2025-08-04 02:39:33,995 - father_agent.py:386 - Step: 10, Training loss: 2.975306987762451
2025-08-04 02:39:35,866 - father_agent.py:386 - Step: 15, Training loss: 2.874542236328125
2025-08-04 02:39:37,719 - father_agent.py:386 - Step: 20, Training loss: 12.467548370361328
2025-08-04 02:39:39,604 - father_agent.py:386 - Step: 25, Training loss: 6.303670406341553
2025-08-04 02:39:41,437 - father_agent.py:386 - Step: 30, Training loss: 5.2053704261779785
2025-08-04 02:39:43,301 - father_agent.py:386 - Step: 35, Training loss: 4.006241321563721
2025-08-04 02:39:45,152 - father_agent.py:386 - Step: 40, Training loss: 3.2556958198547363
2025-08-04 02:39:46,986 - father_agent.py:386 - Step: 45, Training loss: 11.870099067687988
2025-08-04 02:39:48,835 - father_agent.py:386 - Step: 50, Training loss: 3.2202563285827637
2025-08-04 02:39:50,679 - father_agent.py:386 - Step: 55, Training loss: 2.126706123352051
2025-08-04 02:39:52,511 - father_agent.py:386 - Step: 60, Training loss: 11.05693531036377
2025-08-04 02:39:54,366 - father_agent.py:386 - Step: 65, Training loss: 2.0939278602600098
2025-08-04 02:39:56,192 - father_agent.py:386 - Step: 70, Training loss: 10.66547966003418
2025-08-04 02:39:58,034 - father_agent.py:386 - Step: 75, Training loss: 5.602288722991943
2025-08-04 02:39:59,864 - father_agent.py:386 - Step: 80, Training loss: 2.3250741958618164
2025-08-04 02:40:01,691 - father_agent.py:386 - Step: 85, Training loss: 12.14225959777832
2025-08-04 02:40:03,528 - father_agent.py:386 - Step: 90, Training loss: 2.3026278018951416
2025-08-04 02:40:05,362 - father_agent.py:386 - Step: 95, Training loss: 2.6466708183288574
2025-08-04 02:40:07,187 - father_agent.py:386 - Step: 100, Training loss: 8.553017616271973
2025-08-04 02:40:07,430 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:40:07,433 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:40:17,969 - evaluation_results_class.py:131 - Average Return = 3.853860378265381
2025-08-04 02:40:17,969 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.538604736328125
2025-08-04 02:40:17,969 - evaluation_results_class.py:135 - Average Discounted Reward = 32.46595764160156
2025-08-04 02:40:17,969 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:40:17,969 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:40:17,969 - evaluation_results_class.py:141 - Variance of Return = 2.8573198318481445
2025-08-04 02:40:17,969 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:40:17,969 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:40:17,969 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:40:17,969 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:40:18,213 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:40:18,224 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:40:22,212 - father_agent.py:386 - Step: 105, Training loss: 1.4778220653533936
2025-08-04 02:40:24,051 - father_agent.py:386 - Step: 110, Training loss: 8.372349739074707
2025-08-04 02:40:25,908 - father_agent.py:386 - Step: 115, Training loss: 3.434147834777832
2025-08-04 02:40:27,755 - father_agent.py:386 - Step: 120, Training loss: 2.454637289047241
2025-08-04 02:40:29,588 - father_agent.py:386 - Step: 125, Training loss: 7.3394012451171875
2025-08-04 02:40:31,433 - father_agent.py:386 - Step: 130, Training loss: 1.850752592086792
2025-08-04 02:40:33,258 - father_agent.py:386 - Step: 135, Training loss: 3.023620843887329
2025-08-04 02:40:35,101 - father_agent.py:386 - Step: 140, Training loss: 7.101358413696289
2025-08-04 02:40:36,939 - father_agent.py:386 - Step: 145, Training loss: 1.958263635635376
2025-08-04 02:40:38,779 - father_agent.py:386 - Step: 150, Training loss: 8.353581428527832
2025-08-04 02:40:40,614 - father_agent.py:386 - Step: 155, Training loss: 3.042348861694336
2025-08-04 02:40:42,451 - father_agent.py:386 - Step: 160, Training loss: 4.254422664642334
2025-08-04 02:40:44,286 - father_agent.py:386 - Step: 165, Training loss: 7.711719989776611
2025-08-04 02:40:46,140 - father_agent.py:386 - Step: 170, Training loss: 2.134114980697632
2025-08-04 02:40:47,977 - father_agent.py:386 - Step: 175, Training loss: 2.671074867248535
2025-08-04 02:40:49,841 - father_agent.py:386 - Step: 180, Training loss: 4.393424034118652
2025-08-04 02:40:51,665 - father_agent.py:386 - Step: 185, Training loss: 3.099900484085083
2025-08-04 02:40:53,498 - father_agent.py:386 - Step: 190, Training loss: 7.367100715637207
2025-08-04 02:40:55,335 - father_agent.py:386 - Step: 195, Training loss: 2.8814697265625
2025-08-04 02:40:57,161 - father_agent.py:386 - Step: 200, Training loss: 2.761329174041748
2025-08-04 02:40:57,402 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:40:57,404 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:41:07,970 - evaluation_results_class.py:131 - Average Return = 3.859145164489746
2025-08-04 02:41:07,970 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.591453552246094
2025-08-04 02:41:07,970 - evaluation_results_class.py:135 - Average Discounted Reward = 32.498905181884766
2025-08-04 02:41:07,970 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:41:07,970 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:41:07,970 - evaluation_results_class.py:141 - Variance of Return = 2.8275864124298096
2025-08-04 02:41:07,970 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:41:07,970 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:41:07,970 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:41:07,970 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:41:08,213 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:41:08,224 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:41:12,646 - father_agent.py:386 - Step: 205, Training loss: 1.905538558959961
2025-08-04 02:41:14,537 - father_agent.py:386 - Step: 210, Training loss: 9.92701530456543
2025-08-04 02:41:16,473 - father_agent.py:386 - Step: 215, Training loss: 2.9975533485412598
2025-08-04 02:41:18,383 - father_agent.py:386 - Step: 220, Training loss: 3.1029083728790283
2025-08-04 02:41:20,259 - father_agent.py:386 - Step: 225, Training loss: 10.172311782836914
2025-08-04 02:41:22,151 - father_agent.py:386 - Step: 230, Training loss: 2.242030620574951
2025-08-04 02:41:24,024 - father_agent.py:386 - Step: 235, Training loss: 2.733243703842163
2025-08-04 02:41:25,942 - father_agent.py:386 - Step: 240, Training loss: 5.353206157684326
2025-08-04 02:41:27,849 - father_agent.py:386 - Step: 245, Training loss: 2.4664249420166016
2025-08-04 02:41:29,724 - father_agent.py:386 - Step: 250, Training loss: 5.877459526062012
2025-08-04 02:41:31,619 - father_agent.py:386 - Step: 255, Training loss: 3.09617280960083
2025-08-04 02:41:33,502 - father_agent.py:386 - Step: 260, Training loss: 4.611827850341797
2025-08-04 02:41:35,393 - father_agent.py:386 - Step: 265, Training loss: 7.6818528175354
2025-08-04 02:41:37,282 - father_agent.py:386 - Step: 270, Training loss: 1.8640865087509155
2025-08-04 02:41:39,188 - father_agent.py:386 - Step: 275, Training loss: 2.6533732414245605
2025-08-04 02:41:41,088 - father_agent.py:386 - Step: 280, Training loss: 4.630881309509277
2025-08-04 02:41:42,959 - father_agent.py:386 - Step: 285, Training loss: 3.553647994995117
2025-08-04 02:41:44,823 - father_agent.py:386 - Step: 290, Training loss: 7.323150634765625
2025-08-04 02:41:46,717 - father_agent.py:386 - Step: 295, Training loss: 3.2856993675231934
2025-08-04 02:41:48,648 - father_agent.py:386 - Step: 300, Training loss: 2.661482334136963
2025-08-04 02:41:48,985 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:41:48,988 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:41:59,826 - evaluation_results_class.py:131 - Average Return = 3.8096277713775635
2025-08-04 02:41:59,827 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.09627914428711
2025-08-04 02:41:59,827 - evaluation_results_class.py:135 - Average Discounted Reward = 32.1450080871582
2025-08-04 02:41:59,827 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:41:59,827 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:41:59,827 - evaluation_results_class.py:141 - Variance of Return = 2.750178337097168
2025-08-04 02:41:59,827 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:41:59,827 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:41:59,827 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:41:59,827 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:42:00,073 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:42:00,084 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:42:04,090 - father_agent.py:386 - Step: 305, Training loss: 2.198657989501953
2025-08-04 02:42:05,936 - father_agent.py:386 - Step: 310, Training loss: 7.465720176696777
2025-08-04 02:42:07,820 - father_agent.py:386 - Step: 315, Training loss: 3.7119176387786865
2025-08-04 02:42:09,667 - father_agent.py:386 - Step: 320, Training loss: 3.4236159324645996
2025-08-04 02:42:11,532 - father_agent.py:386 - Step: 325, Training loss: 6.1355671882629395
2025-08-04 02:42:13,376 - father_agent.py:386 - Step: 330, Training loss: 2.0257012844085693
2025-08-04 02:42:15,224 - father_agent.py:386 - Step: 335, Training loss: 3.1152124404907227
2025-08-04 02:42:17,091 - father_agent.py:386 - Step: 340, Training loss: 6.705038547515869
2025-08-04 02:42:18,922 - father_agent.py:386 - Step: 345, Training loss: 1.9561158418655396
2025-08-04 02:42:20,748 - father_agent.py:386 - Step: 350, Training loss: 7.635452747344971
2025-08-04 02:42:22,594 - father_agent.py:386 - Step: 355, Training loss: 3.16263747215271
2025-08-04 02:42:24,416 - father_agent.py:386 - Step: 360, Training loss: 4.724668979644775
2025-08-04 02:42:26,244 - father_agent.py:386 - Step: 365, Training loss: 7.359150409698486
2025-08-04 02:42:28,098 - father_agent.py:386 - Step: 370, Training loss: 1.872562050819397
2025-08-04 02:42:29,955 - father_agent.py:386 - Step: 375, Training loss: 3.3910412788391113
2025-08-04 02:42:31,784 - father_agent.py:386 - Step: 380, Training loss: 6.002757549285889
2025-08-04 02:42:33,617 - father_agent.py:386 - Step: 385, Training loss: 2.760039806365967
2025-08-04 02:42:35,444 - father_agent.py:386 - Step: 390, Training loss: 7.030352592468262
2025-08-04 02:42:37,280 - father_agent.py:386 - Step: 395, Training loss: 2.9017789363861084
2025-08-04 02:42:39,102 - father_agent.py:386 - Step: 400, Training loss: 3.0438616275787354
2025-08-04 02:42:39,343 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:42:39,346 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:42:49,877 - evaluation_results_class.py:131 - Average Return = 3.8466222286224365
2025-08-04 02:42:49,877 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.46622085571289
2025-08-04 02:42:49,877 - evaluation_results_class.py:135 - Average Discounted Reward = 32.406009674072266
2025-08-04 02:42:49,877 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:42:49,877 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:42:49,877 - evaluation_results_class.py:141 - Variance of Return = 2.804715156555176
2025-08-04 02:42:49,877 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:42:49,877 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:42:49,877 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:42:49,877 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:42:50,131 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:42:50,142 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:42:54,162 - father_agent.py:386 - Step: 405, Training loss: 2.0689094066619873
2025-08-04 02:42:55,967 - father_agent.py:386 - Step: 410, Training loss: 9.60531997680664
2025-08-04 02:42:57,785 - father_agent.py:386 - Step: 415, Training loss: 3.338715076446533
2025-08-04 02:42:59,618 - father_agent.py:386 - Step: 420, Training loss: 2.7540719509124756
2025-08-04 02:43:01,444 - father_agent.py:386 - Step: 425, Training loss: 7.6281609535217285
2025-08-04 02:43:03,288 - father_agent.py:386 - Step: 430, Training loss: 1.7081959247589111
2025-08-04 02:43:05,115 - father_agent.py:386 - Step: 435, Training loss: 2.937425374984741
2025-08-04 02:43:06,959 - father_agent.py:386 - Step: 440, Training loss: 6.904670715332031
2025-08-04 02:43:08,794 - father_agent.py:386 - Step: 445, Training loss: 2.0758419036865234
2025-08-04 02:43:10,617 - father_agent.py:386 - Step: 450, Training loss: 7.204975128173828
2025-08-04 02:43:12,457 - father_agent.py:386 - Step: 455, Training loss: 2.5588788986206055
2025-08-04 02:43:14,315 - father_agent.py:386 - Step: 460, Training loss: 4.821878910064697
2025-08-04 02:43:16,152 - father_agent.py:386 - Step: 465, Training loss: 6.065908908843994
2025-08-04 02:43:18,001 - father_agent.py:386 - Step: 470, Training loss: 2.3314502239227295
2025-08-04 02:43:19,825 - father_agent.py:386 - Step: 475, Training loss: 2.610057830810547
2025-08-04 02:43:21,688 - father_agent.py:386 - Step: 480, Training loss: 4.337525844573975
2025-08-04 02:43:23,527 - father_agent.py:386 - Step: 485, Training loss: 3.9324100017547607
2025-08-04 02:43:25,373 - father_agent.py:386 - Step: 490, Training loss: 5.88637113571167
2025-08-04 02:43:27,219 - father_agent.py:386 - Step: 495, Training loss: 2.4900970458984375
2025-08-04 02:43:29,072 - father_agent.py:386 - Step: 500, Training loss: 2.731175661087036
2025-08-04 02:43:29,317 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:43:29,320 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:43:40,209 - evaluation_results_class.py:131 - Average Return = 3.845588207244873
2025-08-04 02:43:40,209 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.45588302612305
2025-08-04 02:43:40,209 - evaluation_results_class.py:135 - Average Discounted Reward = 32.41742706298828
2025-08-04 02:43:40,209 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:43:40,209 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:43:40,209 - evaluation_results_class.py:141 - Variance of Return = 2.752581834793091
2025-08-04 02:43:40,209 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:43:40,209 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:43:40,209 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:43:40,209 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:43:40,459 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:43:40,471 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:43:40,587 - father_agent.py:547 - Training finished.
2025-08-04 02:43:40,740 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:43:40,743 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:43:40,745 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 02:43:51,263 - evaluation_results_class.py:131 - Average Return = 3.8232996463775635
2025-08-04 02:43:51,263 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.23299789428711
2025-08-04 02:43:51,263 - evaluation_results_class.py:135 - Average Discounted Reward = 32.233802795410156
2025-08-04 02:43:51,263 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:43:51,263 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:43:51,263 - evaluation_results_class.py:141 - Variance of Return = 2.766111373901367
2025-08-04 02:43:51,263 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:43:51,263 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:43:51,263 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:43:51,264 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:43:51,511 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:43:51,514 - self_interpretable_extractor.py:286 - True
2025-08-04 02:43:51,526 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:44:08,479 - evaluation_results_class.py:131 - Average Return = 3.8306920528411865
2025-08-04 02:44:08,479 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.30691909790039
2025-08-04 02:44:08,479 - evaluation_results_class.py:135 - Average Discounted Reward = 32.28352355957031
2025-08-04 02:44:08,479 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:44:08,479 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:44:08,480 - evaluation_results_class.py:141 - Variance of Return = 2.7232322692871094
2025-08-04 02:44:08,480 - evaluation_results_class.py:143 - Current Best Return = 3.8306920528411865
2025-08-04 02:44:08,480 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:44:08,480 - evaluation_results_class.py:147 - Average Episode Length = 36.97142857142857
2025-08-04 02:44:08,480 - evaluation_results_class.py:149 - Counted Episodes = 8960
2025-08-04 02:44:08,480 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 02:44:08,480 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 40192 trajectories
Learned trajectory lengths  {37}
Buffer 1
2025-08-04 02:45:18,431 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 80384 trajectories
Learned trajectory lengths  {37}
Buffer 2
2025-08-04 02:46:28,173 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 120576 trajectories
Learned trajectory lengths  {37}
All trajectories collected
2025-08-04 02:47:37,994 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 02:47:37,994 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 120576 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 5
Model saved to fsc_7.dot.
Learned FSC of size 5
2025-08-04 02:47:40,227 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 02:47:56,255 - evaluation_results_class.py:131 - Average Return = 3.806919574737549
2025-08-04 02:47:56,258 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.06919479370117
2025-08-04 02:47:56,258 - evaluation_results_class.py:135 - Average Discounted Reward = 32.079185485839844
2025-08-04 02:47:56,258 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:47:56,258 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:47:56,258 - evaluation_results_class.py:141 - Variance of Return = 2.795085906982422
2025-08-04 02:47:56,258 - evaluation_results_class.py:143 - Current Best Return = 3.806919574737549
2025-08-04 02:47:56,258 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:47:56,258 - evaluation_results_class.py:147 - Average Episode Length = 36.97142857142857
2025-08-04 02:47:56,258 - evaluation_results_class.py:149 - Counted Episodes = 8960
FSC Result: {'best_episode_return': 40.069195, 'best_return': 3.8069196, 'goal_value': 0.0, 'returns_episodic': [40.069195], 'returns': [3.8069196], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [2.795086], 'each_episode_virtual_variance': [279.5086], 'combined_variance': [338.2054], 'num_episodes': [8960], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [36.97142857142857], 'counted_episodes': [8960], 'discounted_rewards': [32.079185], 'new_pomdp_iteration_numbers': []}
[[6], [1, 4, 5], [3], [3], [3], [2], [2], [2], [6], [6], [6], [1, 5], [1, 4], [3], [2], [1], [0], [0], [0], [0]]
2025-08-04 02:47:57,012 - statistic.py:67 - synthesis initiated, design space: 140
2025-08-04 02:47:57,014 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 02:47:57,151 - synthesizer_ar.py:122 - value 16.2649 achieved after 4296.76 seconds
2025-08-04 02:47:57,168 - synthesizer_ar.py:122 - value 8.9394 achieved after 4296.78 seconds
2025-08-04 02:47:57,229 - synthesizer_ar.py:122 - value 8.6187 achieved after 4296.84 seconds
2025-08-04 02:47:57,278 - synthesizer_ar.py:122 - value 8.1829 achieved after 4296.89 seconds
2025-08-04 02:47:57,319 - synthesizer_ar.py:122 - value 7.5832 achieved after 4296.93 seconds
2025-08-04 02:47:57,354 - synthesizer_ar.py:122 - value 6.7491 achieved after 4296.96 seconds
2025-08-04 02:47:57,368 - synthesizer_ar.py:122 - value 5.5763 achieved after 4296.98 seconds
2025-08-04 02:47:57,375 - synthesizer_ar.py:122 - value 3.8355 achieved after 4296.99 seconds
2025-08-04 02:47:57,375 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 02:47:57,375 - synthesizer.py:193 - SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 02:47:57,377 - synthesizer.py:198 - double-checking specification satisfiability:  : 3.8354580153400044
--------------------
Synthesis summary:
optimality objective: R{"packets_sent"}min=? [F "label_done"] 

method: AR, synthesis time: 0.36 s
number of holes: 4, family size: 140, quotient: 15158 states / 91872 actions
explored: 100 %
MDP stats: avg MDP size: 7243, iterations: 24

optimum: 3.835458
--------------------
2025-08-04 02:47:57,377 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 02:47:57,382 - robust_rl_trainer.py:432 - Iteration 9 of pure RL loop
2025-08-04 02:47:57,427 - storm_vec_env.py:70 - Computing row map
2025-08-04 02:47:57,434 - storm_vec_env.py:97 - Computing transitions
2025-08-04 02:47:57,445 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 02:47:57,445 - storm_vec_env.py:114 - Computing sinks
2025-08-04 02:47:57,445 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 02:47:57,448 - storm_vec_env.py:143 - Computing labels
2025-08-04 02:47:57,449 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 02:47:57,449 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 02:47:57,449 - storm_vec_env.py:175 - Computing observations
2025-08-04 02:47:57,616 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 02:47:57,617 - father_agent.py:540 - Before training evaluation.
2025-08-04 02:47:57,778 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:47:57,780 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:48:08,336 - evaluation_results_class.py:131 - Average Return = 3.8130743503570557
2025-08-04 02:48:08,337 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.13074493408203
2025-08-04 02:48:08,337 - evaluation_results_class.py:135 - Average Discounted Reward = 32.14689254760742
2025-08-04 02:48:08,337 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:48:08,337 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:48:08,337 - evaluation_results_class.py:141 - Variance of Return = 2.8151278495788574
2025-08-04 02:48:08,337 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:48:08,337 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:48:08,337 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:48:08,337 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:48:08,585 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:48:08,597 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:48:08,711 - father_agent.py:436 - Training agent on-policy
2025-08-04 02:48:16,982 - father_agent.py:386 - Step: 0, Training loss: 3.075714111328125
2025-08-04 02:48:22,227 - father_agent.py:386 - Step: 5, Training loss: 7.18493127822876
2025-08-04 02:48:24,075 - father_agent.py:386 - Step: 10, Training loss: 2.850421667098999
2025-08-04 02:48:25,931 - father_agent.py:386 - Step: 15, Training loss: 2.878507137298584
2025-08-04 02:48:27,780 - father_agent.py:386 - Step: 20, Training loss: 9.255040168762207
2025-08-04 02:48:29,630 - father_agent.py:386 - Step: 25, Training loss: 4.926477432250977
2025-08-04 02:48:31,464 - father_agent.py:386 - Step: 30, Training loss: 7.1120219230651855
2025-08-04 02:48:33,304 - father_agent.py:386 - Step: 35, Training loss: 5.008702754974365
2025-08-04 02:48:35,125 - father_agent.py:386 - Step: 40, Training loss: 2.4684805870056152
2025-08-04 02:48:36,951 - father_agent.py:386 - Step: 45, Training loss: 11.59588623046875
2025-08-04 02:48:38,809 - father_agent.py:386 - Step: 50, Training loss: 2.861881971359253
2025-08-04 02:48:40,675 - father_agent.py:386 - Step: 55, Training loss: 1.9208173751831055
2025-08-04 02:48:42,526 - father_agent.py:386 - Step: 60, Training loss: 11.151726722717285
2025-08-04 02:48:44,395 - father_agent.py:386 - Step: 65, Training loss: 1.6069893836975098
2025-08-04 02:48:46,234 - father_agent.py:386 - Step: 70, Training loss: 10.957202911376953
2025-08-04 02:48:48,099 - father_agent.py:386 - Step: 75, Training loss: 5.338050842285156
2025-08-04 02:48:49,960 - father_agent.py:386 - Step: 80, Training loss: 2.2251527309417725
2025-08-04 02:48:51,813 - father_agent.py:386 - Step: 85, Training loss: 12.652589797973633
2025-08-04 02:48:53,668 - father_agent.py:386 - Step: 90, Training loss: 2.495358943939209
2025-08-04 02:48:55,538 - father_agent.py:386 - Step: 95, Training loss: 2.934440851211548
2025-08-04 02:48:57,397 - father_agent.py:386 - Step: 100, Training loss: 8.462738990783691
2025-08-04 02:48:57,643 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:48:57,645 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:49:08,269 - evaluation_results_class.py:131 - Average Return = 3.806410789489746
2025-08-04 02:49:08,269 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.064109802246094
2025-08-04 02:49:08,269 - evaluation_results_class.py:135 - Average Discounted Reward = 32.102664947509766
2025-08-04 02:49:08,269 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:49:08,269 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:49:08,269 - evaluation_results_class.py:141 - Variance of Return = 2.80684757232666
2025-08-04 02:49:08,269 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:49:08,269 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:49:08,270 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:49:08,270 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:49:08,517 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:49:08,528 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:49:12,520 - father_agent.py:386 - Step: 105, Training loss: 1.5137460231781006
2025-08-04 02:49:14,351 - father_agent.py:386 - Step: 110, Training loss: 9.11426830291748
2025-08-04 02:49:16,204 - father_agent.py:386 - Step: 115, Training loss: 3.148897647857666
2025-08-04 02:49:18,064 - father_agent.py:386 - Step: 120, Training loss: 2.93658185005188
2025-08-04 02:49:19,929 - father_agent.py:386 - Step: 125, Training loss: 9.106851577758789
2025-08-04 02:49:21,776 - father_agent.py:386 - Step: 130, Training loss: 1.9351190328598022
2025-08-04 02:49:23,608 - father_agent.py:386 - Step: 135, Training loss: 3.3156838417053223
2025-08-04 02:49:25,464 - father_agent.py:386 - Step: 140, Training loss: 6.511909008026123
2025-08-04 02:49:27,314 - father_agent.py:386 - Step: 145, Training loss: 1.935013771057129
2025-08-04 02:49:29,167 - father_agent.py:386 - Step: 150, Training loss: 7.3025360107421875
2025-08-04 02:49:31,001 - father_agent.py:386 - Step: 155, Training loss: 2.965163469314575
2025-08-04 02:49:32,833 - father_agent.py:386 - Step: 160, Training loss: 4.75372314453125
2025-08-04 02:49:34,648 - father_agent.py:386 - Step: 165, Training loss: 8.155389785766602
2025-08-04 02:49:36,507 - father_agent.py:386 - Step: 170, Training loss: 2.140310764312744
2025-08-04 02:49:38,338 - father_agent.py:386 - Step: 175, Training loss: 2.7767083644866943
2025-08-04 02:49:40,177 - father_agent.py:386 - Step: 180, Training loss: 5.237601280212402
2025-08-04 02:49:42,019 - father_agent.py:386 - Step: 185, Training loss: 3.501744270324707
2025-08-04 02:49:43,865 - father_agent.py:386 - Step: 190, Training loss: 6.639151096343994
2025-08-04 02:49:45,721 - father_agent.py:386 - Step: 195, Training loss: 2.9247326850891113
2025-08-04 02:49:47,566 - father_agent.py:386 - Step: 200, Training loss: 2.677396774291992
2025-08-04 02:49:47,811 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:49:47,814 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:49:58,373 - evaluation_results_class.py:131 - Average Return = 3.829733371734619
2025-08-04 02:49:58,373 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.297332763671875
2025-08-04 02:49:58,373 - evaluation_results_class.py:135 - Average Discounted Reward = 32.315128326416016
2025-08-04 02:49:58,373 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:49:58,373 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:49:58,373 - evaluation_results_class.py:141 - Variance of Return = 2.854740858078003
2025-08-04 02:49:58,373 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:49:58,373 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:49:58,373 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:49:58,373 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:49:58,624 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:49:58,636 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:50:03,087 - father_agent.py:386 - Step: 205, Training loss: 2.105529308319092
2025-08-04 02:50:04,906 - father_agent.py:386 - Step: 210, Training loss: 9.213653564453125
2025-08-04 02:50:06,753 - father_agent.py:386 - Step: 215, Training loss: 2.9111552238464355
2025-08-04 02:50:08,585 - father_agent.py:386 - Step: 220, Training loss: 2.9085330963134766
2025-08-04 02:50:10,421 - father_agent.py:386 - Step: 225, Training loss: 5.5488972663879395
2025-08-04 02:50:12,254 - father_agent.py:386 - Step: 230, Training loss: 1.7617506980895996
2025-08-04 02:50:14,073 - father_agent.py:386 - Step: 235, Training loss: 3.1761691570281982
2025-08-04 02:50:15,919 - father_agent.py:386 - Step: 240, Training loss: 6.378402233123779
2025-08-04 02:50:17,737 - father_agent.py:386 - Step: 245, Training loss: 2.130831480026245
2025-08-04 02:50:19,576 - father_agent.py:386 - Step: 250, Training loss: 8.297378540039062
2025-08-04 02:50:21,413 - father_agent.py:386 - Step: 255, Training loss: 2.957550048828125
2025-08-04 02:50:23,241 - father_agent.py:386 - Step: 260, Training loss: 5.239718437194824
2025-08-04 02:50:25,072 - father_agent.py:386 - Step: 265, Training loss: 6.810388088226318
2025-08-04 02:50:26,926 - father_agent.py:386 - Step: 270, Training loss: 1.9899557828903198
2025-08-04 02:50:28,762 - father_agent.py:386 - Step: 275, Training loss: 2.6376192569732666
2025-08-04 02:50:30,609 - father_agent.py:386 - Step: 280, Training loss: 6.300259590148926
2025-08-04 02:50:32,447 - father_agent.py:386 - Step: 285, Training loss: 2.460141181945801
2025-08-04 02:50:34,289 - father_agent.py:386 - Step: 290, Training loss: 6.729541778564453
2025-08-04 02:50:36,195 - father_agent.py:386 - Step: 295, Training loss: 2.781092882156372
2025-08-04 02:50:38,094 - father_agent.py:386 - Step: 300, Training loss: 2.8620171546936035
2025-08-04 02:50:38,349 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:50:38,352 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:50:49,287 - evaluation_results_class.py:131 - Average Return = 3.8329503536224365
2025-08-04 02:50:49,287 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.32950210571289
2025-08-04 02:50:49,287 - evaluation_results_class.py:135 - Average Discounted Reward = 32.33428192138672
2025-08-04 02:50:49,287 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:50:49,287 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:50:49,287 - evaluation_results_class.py:141 - Variance of Return = 2.8294014930725098
2025-08-04 02:50:49,287 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:50:49,287 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:50:49,287 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:50:49,287 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:50:49,546 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:50:49,558 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:50:53,646 - father_agent.py:386 - Step: 305, Training loss: 1.9633312225341797
2025-08-04 02:50:55,521 - father_agent.py:386 - Step: 310, Training loss: 7.531116962432861
2025-08-04 02:50:57,420 - father_agent.py:386 - Step: 315, Training loss: 3.0791587829589844
2025-08-04 02:50:59,337 - father_agent.py:386 - Step: 320, Training loss: 3.120920419692993
2025-08-04 02:51:01,227 - father_agent.py:386 - Step: 325, Training loss: 7.868830680847168
2025-08-04 02:51:03,132 - father_agent.py:386 - Step: 330, Training loss: 1.9314298629760742
2025-08-04 02:51:05,029 - father_agent.py:386 - Step: 335, Training loss: 3.1821796894073486
2025-08-04 02:51:07,017 - father_agent.py:386 - Step: 340, Training loss: 7.432191371917725
2025-08-04 02:51:08,934 - father_agent.py:386 - Step: 345, Training loss: 2.1333348751068115
2025-08-04 02:51:10,836 - father_agent.py:386 - Step: 350, Training loss: 7.5290985107421875
2025-08-04 02:51:12,731 - father_agent.py:386 - Step: 355, Training loss: 3.2461044788360596
2025-08-04 02:51:14,611 - father_agent.py:386 - Step: 360, Training loss: 4.796077728271484
2025-08-04 02:51:16,497 - father_agent.py:386 - Step: 365, Training loss: 6.443077564239502
2025-08-04 02:51:18,383 - father_agent.py:386 - Step: 370, Training loss: 1.8372430801391602
2025-08-04 02:51:20,294 - father_agent.py:386 - Step: 375, Training loss: 2.907008171081543
2025-08-04 02:51:22,206 - father_agent.py:386 - Step: 380, Training loss: 5.939875602722168
2025-08-04 02:51:24,108 - father_agent.py:386 - Step: 385, Training loss: 2.5283424854278564
2025-08-04 02:51:25,983 - father_agent.py:386 - Step: 390, Training loss: 7.295657157897949
2025-08-04 02:51:27,886 - father_agent.py:386 - Step: 395, Training loss: 2.7584869861602783
2025-08-04 02:51:29,774 - father_agent.py:386 - Step: 400, Training loss: 2.9145424365997314
2025-08-04 02:51:30,021 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:51:30,024 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:51:40,814 - evaluation_results_class.py:131 - Average Return = 3.816176414489746
2025-08-04 02:51:40,814 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.161766052246094
2025-08-04 02:51:40,814 - evaluation_results_class.py:135 - Average Discounted Reward = 32.201847076416016
2025-08-04 02:51:40,814 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:51:40,814 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:51:40,814 - evaluation_results_class.py:141 - Variance of Return = 2.793414831161499
2025-08-04 02:51:40,814 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:51:40,814 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:51:40,814 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:51:40,814 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:51:41,141 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:51:41,152 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:51:45,175 - father_agent.py:386 - Step: 405, Training loss: 2.138516426086426
2025-08-04 02:51:47,009 - father_agent.py:386 - Step: 410, Training loss: 7.815556526184082
2025-08-04 02:51:48,867 - father_agent.py:386 - Step: 415, Training loss: 3.496044635772705
2025-08-04 02:51:50,694 - father_agent.py:386 - Step: 420, Training loss: 3.3243658542633057
2025-08-04 02:51:52,546 - father_agent.py:386 - Step: 425, Training loss: 7.0143961906433105
2025-08-04 02:51:54,413 - father_agent.py:386 - Step: 430, Training loss: 1.7809702157974243
2025-08-04 02:51:56,253 - father_agent.py:386 - Step: 435, Training loss: 3.0725810527801514
2025-08-04 02:51:58,109 - father_agent.py:386 - Step: 440, Training loss: 6.2109375
2025-08-04 02:51:59,940 - father_agent.py:386 - Step: 445, Training loss: 1.9671781063079834
2025-08-04 02:52:01,789 - father_agent.py:386 - Step: 450, Training loss: 8.177705764770508
2025-08-04 02:52:03,652 - father_agent.py:386 - Step: 455, Training loss: 2.873392105102539
2025-08-04 02:52:05,499 - father_agent.py:386 - Step: 460, Training loss: 4.653843879699707
2025-08-04 02:52:07,310 - father_agent.py:386 - Step: 465, Training loss: 8.189990043640137
2025-08-04 02:52:09,137 - father_agent.py:386 - Step: 470, Training loss: 1.8379294872283936
2025-08-04 02:52:10,951 - father_agent.py:386 - Step: 475, Training loss: 3.020094394683838
2025-08-04 02:52:12,793 - father_agent.py:386 - Step: 480, Training loss: 5.241842746734619
2025-08-04 02:52:14,611 - father_agent.py:386 - Step: 485, Training loss: 3.0599541664123535
2025-08-04 02:52:16,441 - father_agent.py:386 - Step: 490, Training loss: 7.535806179046631
2025-08-04 02:52:18,283 - father_agent.py:386 - Step: 495, Training loss: 2.3767685890197754
2025-08-04 02:52:20,107 - father_agent.py:386 - Step: 500, Training loss: 2.502225875854492
2025-08-04 02:52:20,357 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:52:20,359 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:52:31,018 - evaluation_results_class.py:131 - Average Return = 3.8038833141326904
2025-08-04 02:52:31,018 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.03883361816406
2025-08-04 02:52:31,018 - evaluation_results_class.py:135 - Average Discounted Reward = 32.10183334350586
2025-08-04 02:52:31,018 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:52:31,018 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:52:31,018 - evaluation_results_class.py:141 - Variance of Return = 2.79965877532959
2025-08-04 02:52:31,018 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:52:31,018 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:52:31,018 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:52:31,018 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:52:31,272 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:52:31,284 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:52:31,400 - father_agent.py:547 - Training finished.
2025-08-04 02:52:31,555 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:52:31,558 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:52:31,561 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 02:52:42,552 - evaluation_results_class.py:131 - Average Return = 3.8073298931121826
2025-08-04 02:52:42,552 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.073299407958984
2025-08-04 02:52:42,552 - evaluation_results_class.py:135 - Average Discounted Reward = 32.13193130493164
2025-08-04 02:52:42,552 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:52:42,552 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:52:42,552 - evaluation_results_class.py:141 - Variance of Return = 2.8007686138153076
2025-08-04 02:52:42,552 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:52:42,552 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:52:42,552 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:52:42,552 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:52:42,806 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:52:42,808 - self_interpretable_extractor.py:286 - True
2025-08-04 02:52:42,821 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:52:59,781 - evaluation_results_class.py:131 - Average Return = 3.8555803298950195
2025-08-04 02:52:59,781 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.55580520629883
2025-08-04 02:52:59,781 - evaluation_results_class.py:135 - Average Discounted Reward = 32.531246185302734
2025-08-04 02:52:59,781 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:52:59,781 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:52:59,781 - evaluation_results_class.py:141 - Variance of Return = 2.776017904281616
2025-08-04 02:52:59,781 - evaluation_results_class.py:143 - Current Best Return = 3.8555803298950195
2025-08-04 02:52:59,781 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:52:59,781 - evaluation_results_class.py:147 - Average Episode Length = 36.97142857142857
2025-08-04 02:52:59,781 - evaluation_results_class.py:149 - Counted Episodes = 8960
2025-08-04 02:52:59,781 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 02:52:59,781 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 40192 trajectories
Learned trajectory lengths  {37}
Buffer 1
2025-08-04 02:54:09,919 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 80384 trajectories
Learned trajectory lengths  {37}
Buffer 2
2025-08-04 02:55:19,946 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 120576 trajectories
Learned trajectory lengths  {37}
All trajectories collected
2025-08-04 02:56:29,971 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 02:56:29,973 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 120576 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 1
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 5
Model saved to fsc_8.dot.
Learned FSC of size 5
2025-08-04 02:56:32,284 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 02:56:48,384 - evaluation_results_class.py:131 - Average Return = 3.7811384201049805
2025-08-04 02:56:48,384 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.81138229370117
2025-08-04 02:56:48,385 - evaluation_results_class.py:135 - Average Discounted Reward = 31.895360946655273
2025-08-04 02:56:48,385 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:56:48,385 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:56:48,385 - evaluation_results_class.py:141 - Variance of Return = 2.9053361415863037
2025-08-04 02:56:48,385 - evaluation_results_class.py:143 - Current Best Return = 3.7811384201049805
2025-08-04 02:56:48,385 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:56:48,385 - evaluation_results_class.py:147 - Average Episode Length = 36.97142857142857
2025-08-04 02:56:48,385 - evaluation_results_class.py:149 - Counted Episodes = 8960
FSC Result: {'best_episode_return': 39.811382, 'best_return': 3.7811384, 'goal_value': 0.0, 'returns_episodic': [39.811382], 'returns': [3.7811384], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [2.9053361], 'each_episode_virtual_variance': [290.5336], 'combined_variance': [351.5457], 'num_episodes': [8960], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [36.97142857142857], 'counted_episodes': [8960], 'discounted_rewards': [31.89536], 'new_pomdp_iteration_numbers': []}
[[6], [1, 4, 5], [3], [3], [3], [2], [2], [2], [6], [6], [6], [1, 5], [1, 4], [3], [2], [1], [0], [0], [0], [0]]
2025-08-04 02:56:49,185 - statistic.py:67 - synthesis initiated, design space: 140
2025-08-04 02:56:49,187 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 02:56:49,324 - synthesizer_ar.py:122 - value 16.263 achieved after 4828.93 seconds
2025-08-04 02:56:49,341 - synthesizer_ar.py:122 - value 8.9384 achieved after 4828.95 seconds
2025-08-04 02:56:49,402 - synthesizer_ar.py:122 - value 8.6173 achieved after 4829.01 seconds
2025-08-04 02:56:49,451 - synthesizer_ar.py:122 - value 8.181 achieved after 4829.06 seconds
2025-08-04 02:56:49,492 - synthesizer_ar.py:122 - value 7.5805 achieved after 4829.1 seconds
2025-08-04 02:56:49,527 - synthesizer_ar.py:122 - value 6.7457 achieved after 4829.14 seconds
2025-08-04 02:56:49,542 - synthesizer_ar.py:122 - value 5.5721 achieved after 4829.15 seconds
2025-08-04 02:56:49,549 - synthesizer_ar.py:122 - value 3.8318 achieved after 4829.16 seconds
2025-08-04 02:56:49,549 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 02:56:49,549 - synthesizer.py:193 - SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 02:56:49,551 - synthesizer.py:198 - double-checking specification satisfiability:  : 3.8317568061776424
--------------------
Synthesis summary:
optimality objective: R{"packets_sent"}min=? [F "label_done"] 

method: AR, synthesis time: 0.36 s
number of holes: 4, family size: 140, quotient: 15158 states / 91872 actions
explored: 100 %
MDP stats: avg MDP size: 7243, iterations: 24

optimum: 3.831757
--------------------
2025-08-04 02:56:49,551 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 02:56:49,556 - robust_rl_trainer.py:432 - Iteration 10 of pure RL loop
2025-08-04 02:56:49,600 - storm_vec_env.py:70 - Computing row map
2025-08-04 02:56:49,606 - storm_vec_env.py:97 - Computing transitions
2025-08-04 02:56:49,617 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 02:56:49,617 - storm_vec_env.py:114 - Computing sinks
2025-08-04 02:56:49,617 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 02:56:49,621 - storm_vec_env.py:143 - Computing labels
2025-08-04 02:56:49,621 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 02:56:49,621 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 02:56:49,621 - storm_vec_env.py:175 - Computing observations
2025-08-04 02:56:49,795 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 02:56:49,796 - father_agent.py:540 - Before training evaluation.
2025-08-04 02:56:49,955 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:56:49,958 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:57:00,674 - evaluation_results_class.py:131 - Average Return = 3.802504539489746
2025-08-04 02:57:00,674 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.025047302246094
2025-08-04 02:57:00,675 - evaluation_results_class.py:135 - Average Discounted Reward = 32.064453125
2025-08-04 02:57:00,675 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:57:00,675 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:57:00,675 - evaluation_results_class.py:141 - Variance of Return = 2.792912006378174
2025-08-04 02:57:00,675 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:57:00,675 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:57:00,675 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:57:00,675 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:57:00,929 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:57:00,941 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:57:01,059 - father_agent.py:436 - Training agent on-policy
2025-08-04 02:57:09,436 - father_agent.py:386 - Step: 0, Training loss: 2.5179526805877686
2025-08-04 02:57:14,819 - father_agent.py:386 - Step: 5, Training loss: 7.5916595458984375
2025-08-04 02:57:16,782 - father_agent.py:386 - Step: 10, Training loss: 2.5390944480895996
2025-08-04 02:57:18,724 - father_agent.py:386 - Step: 15, Training loss: 2.4292802810668945
2025-08-04 02:57:20,613 - father_agent.py:386 - Step: 20, Training loss: 10.41596508026123
2025-08-04 02:57:22,514 - father_agent.py:386 - Step: 25, Training loss: 4.570313930511475
2025-08-04 02:57:24,393 - father_agent.py:386 - Step: 30, Training loss: 5.81519889831543
2025-08-04 02:57:26,274 - father_agent.py:386 - Step: 35, Training loss: 4.653468132019043
2025-08-04 02:57:28,247 - father_agent.py:386 - Step: 40, Training loss: 3.3900561332702637
2025-08-04 02:57:30,205 - father_agent.py:386 - Step: 45, Training loss: 10.402019500732422
2025-08-04 02:57:32,074 - father_agent.py:386 - Step: 50, Training loss: 2.187908411026001
2025-08-04 02:57:33,932 - father_agent.py:386 - Step: 55, Training loss: 1.9441949129104614
2025-08-04 02:57:35,796 - father_agent.py:386 - Step: 60, Training loss: 10.445590019226074
2025-08-04 02:57:37,650 - father_agent.py:386 - Step: 65, Training loss: 1.6088111400604248
2025-08-04 02:57:39,496 - father_agent.py:386 - Step: 70, Training loss: 8.218683242797852
2025-08-04 02:57:41,346 - father_agent.py:386 - Step: 75, Training loss: 5.561556339263916
2025-08-04 02:57:43,174 - father_agent.py:386 - Step: 80, Training loss: 2.424544095993042
2025-08-04 02:57:45,015 - father_agent.py:386 - Step: 85, Training loss: 12.106766700744629
2025-08-04 02:57:46,859 - father_agent.py:386 - Step: 90, Training loss: 1.9250452518463135
2025-08-04 02:57:48,676 - father_agent.py:386 - Step: 95, Training loss: 2.938490152359009
2025-08-04 02:57:50,540 - father_agent.py:386 - Step: 100, Training loss: 9.707918167114258
2025-08-04 02:57:50,794 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:57:50,798 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:58:01,309 - evaluation_results_class.py:131 - Average Return = 3.859375
2025-08-04 02:58:01,309 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.59375
2025-08-04 02:58:01,309 - evaluation_results_class.py:135 - Average Discounted Reward = 32.525936126708984
2025-08-04 02:58:01,309 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:58:01,309 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:58:01,309 - evaluation_results_class.py:141 - Variance of Return = 2.792494773864746
2025-08-04 02:58:01,309 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:58:01,309 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:58:01,309 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:58:01,309 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:58:01,559 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:58:01,571 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:58:05,628 - father_agent.py:386 - Step: 105, Training loss: 1.2622615098953247
2025-08-04 02:58:07,450 - father_agent.py:386 - Step: 110, Training loss: 9.297030448913574
2025-08-04 02:58:09,281 - father_agent.py:386 - Step: 115, Training loss: 3.0892882347106934
2025-08-04 02:58:11,102 - father_agent.py:386 - Step: 120, Training loss: 2.761456251144409
2025-08-04 02:58:12,927 - father_agent.py:386 - Step: 125, Training loss: 7.85115385055542
2025-08-04 02:58:14,772 - father_agent.py:386 - Step: 130, Training loss: 1.7479004859924316
2025-08-04 02:58:16,596 - father_agent.py:386 - Step: 135, Training loss: 3.3677780628204346
2025-08-04 02:58:18,449 - father_agent.py:386 - Step: 140, Training loss: 7.881444931030273
2025-08-04 02:58:20,295 - father_agent.py:386 - Step: 145, Training loss: 1.872943639755249
2025-08-04 02:58:22,156 - father_agent.py:386 - Step: 150, Training loss: 7.332067012786865
2025-08-04 02:58:24,024 - father_agent.py:386 - Step: 155, Training loss: 2.6547608375549316
2025-08-04 02:58:25,859 - father_agent.py:386 - Step: 160, Training loss: 4.728217124938965
2025-08-04 02:58:27,722 - father_agent.py:386 - Step: 165, Training loss: 8.350406646728516
2025-08-04 02:58:29,585 - father_agent.py:386 - Step: 170, Training loss: 1.8960810899734497
2025-08-04 02:58:31,418 - father_agent.py:386 - Step: 175, Training loss: 4.061440467834473
2025-08-04 02:58:33,270 - father_agent.py:386 - Step: 180, Training loss: 6.86021614074707
2025-08-04 02:58:35,105 - father_agent.py:386 - Step: 185, Training loss: 2.8439559936523438
2025-08-04 02:58:36,939 - father_agent.py:386 - Step: 190, Training loss: 8.645142555236816
2025-08-04 02:58:38,789 - father_agent.py:386 - Step: 195, Training loss: 2.592388868331909
2025-08-04 02:58:40,626 - father_agent.py:386 - Step: 200, Training loss: 3.147996425628662
2025-08-04 02:58:40,881 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:58:40,884 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:58:51,363 - evaluation_results_class.py:131 - Average Return = 3.8546645641326904
2025-08-04 02:58:51,364 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.54664611816406
2025-08-04 02:58:51,364 - evaluation_results_class.py:135 - Average Discounted Reward = 32.5009765625
2025-08-04 02:58:51,364 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:58:51,364 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:58:51,364 - evaluation_results_class.py:141 - Variance of Return = 2.9233856201171875
2025-08-04 02:58:51,364 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:58:51,364 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:58:51,364 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:58:51,364 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:58:51,621 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:58:51,633 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:58:55,674 - father_agent.py:386 - Step: 205, Training loss: 2.185615301132202
2025-08-04 02:58:57,524 - father_agent.py:386 - Step: 210, Training loss: 9.501500129699707
2025-08-04 02:58:59,395 - father_agent.py:386 - Step: 215, Training loss: 3.034507989883423
2025-08-04 02:59:01,225 - father_agent.py:386 - Step: 220, Training loss: 2.9086930751800537
2025-08-04 02:59:03,052 - father_agent.py:386 - Step: 225, Training loss: 9.454779624938965
2025-08-04 02:59:04,905 - father_agent.py:386 - Step: 230, Training loss: 1.8189197778701782
2025-08-04 02:59:06,737 - father_agent.py:386 - Step: 235, Training loss: 4.113461494445801
2025-08-04 02:59:08,584 - father_agent.py:386 - Step: 240, Training loss: 5.8660502433776855
2025-08-04 02:59:10,435 - father_agent.py:386 - Step: 245, Training loss: 2.6902835369110107
2025-08-04 02:59:12,259 - father_agent.py:386 - Step: 250, Training loss: 8.328633308410645
2025-08-04 02:59:14,097 - father_agent.py:386 - Step: 255, Training loss: 2.771390676498413
2025-08-04 02:59:15,920 - father_agent.py:386 - Step: 260, Training loss: 4.50369119644165
2025-08-04 02:59:17,728 - father_agent.py:386 - Step: 265, Training loss: 7.779959201812744
2025-08-04 02:59:19,565 - father_agent.py:386 - Step: 270, Training loss: 2.0658318996429443
2025-08-04 02:59:21,378 - father_agent.py:386 - Step: 275, Training loss: 3.3295507431030273
2025-08-04 02:59:23,209 - father_agent.py:386 - Step: 280, Training loss: 4.873960971832275
2025-08-04 02:59:25,022 - father_agent.py:386 - Step: 285, Training loss: 3.5089499950408936
2025-08-04 02:59:26,842 - father_agent.py:386 - Step: 290, Training loss: 7.412741661071777
2025-08-04 02:59:28,678 - father_agent.py:386 - Step: 295, Training loss: 2.681668758392334
2025-08-04 02:59:30,511 - father_agent.py:386 - Step: 300, Training loss: 2.710465908050537
2025-08-04 02:59:30,767 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:59:30,769 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:59:41,889 - evaluation_results_class.py:131 - Average Return = 3.835822582244873
2025-08-04 02:59:41,889 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.35822677612305
2025-08-04 02:59:41,889 - evaluation_results_class.py:135 - Average Discounted Reward = 32.30865478515625
2025-08-04 02:59:41,889 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 02:59:41,889 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 02:59:41,889 - evaluation_results_class.py:141 - Variance of Return = 2.816680908203125
2025-08-04 02:59:41,889 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 02:59:41,890 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 02:59:41,890 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 02:59:41,890 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 02:59:42,147 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:59:42,159 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 02:59:46,197 - father_agent.py:386 - Step: 305, Training loss: 1.7914280891418457
2025-08-04 02:59:48,027 - father_agent.py:386 - Step: 310, Training loss: 7.519181728363037
2025-08-04 02:59:49,881 - father_agent.py:386 - Step: 315, Training loss: 3.6231026649475098
2025-08-04 02:59:51,718 - father_agent.py:386 - Step: 320, Training loss: 3.0531280040740967
2025-08-04 02:59:53,540 - father_agent.py:386 - Step: 325, Training loss: 9.552677154541016
2025-08-04 02:59:55,386 - father_agent.py:386 - Step: 330, Training loss: 1.722190260887146
2025-08-04 02:59:57,220 - father_agent.py:386 - Step: 335, Training loss: 3.058628559112549
2025-08-04 02:59:59,063 - father_agent.py:386 - Step: 340, Training loss: 6.879865646362305
2025-08-04 03:00:00,912 - father_agent.py:386 - Step: 345, Training loss: 1.9873430728912354
2025-08-04 03:00:02,747 - father_agent.py:386 - Step: 350, Training loss: 7.242784023284912
2025-08-04 03:00:04,579 - father_agent.py:386 - Step: 355, Training loss: 2.531538248062134
2025-08-04 03:00:06,405 - father_agent.py:386 - Step: 360, Training loss: 4.4447197914123535
2025-08-04 03:00:08,232 - father_agent.py:386 - Step: 365, Training loss: 7.195532321929932
2025-08-04 03:00:10,115 - father_agent.py:386 - Step: 370, Training loss: 2.0231361389160156
2025-08-04 03:00:11,937 - father_agent.py:386 - Step: 375, Training loss: 2.7803890705108643
2025-08-04 03:00:13,765 - father_agent.py:386 - Step: 380, Training loss: 5.058902263641357
2025-08-04 03:00:15,580 - father_agent.py:386 - Step: 385, Training loss: 2.735546350479126
2025-08-04 03:00:17,433 - father_agent.py:386 - Step: 390, Training loss: 7.436685562133789
2025-08-04 03:00:19,312 - father_agent.py:386 - Step: 395, Training loss: 2.526890993118286
2025-08-04 03:00:21,150 - father_agent.py:386 - Step: 400, Training loss: 2.657656192779541
2025-08-04 03:00:21,411 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:00:21,414 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:00:31,989 - evaluation_results_class.py:131 - Average Return = 3.8681066036224365
2025-08-04 03:00:31,990 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.68106460571289
2025-08-04 03:00:31,990 - evaluation_results_class.py:135 - Average Discounted Reward = 32.624603271484375
2025-08-04 03:00:31,990 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:00:31,990 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:00:31,990 - evaluation_results_class.py:141 - Variance of Return = 2.7934954166412354
2025-08-04 03:00:31,990 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:00:31,990 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:00:31,990 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:00:31,990 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:00:32,248 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:00:32,260 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:00:36,269 - father_agent.py:386 - Step: 405, Training loss: 1.817090630531311
2025-08-04 03:00:38,102 - father_agent.py:386 - Step: 410, Training loss: 7.532382488250732
2025-08-04 03:00:39,955 - father_agent.py:386 - Step: 415, Training loss: 2.8647074699401855
2025-08-04 03:00:41,805 - father_agent.py:386 - Step: 420, Training loss: 2.856619358062744
2025-08-04 03:00:43,658 - father_agent.py:386 - Step: 425, Training loss: 8.472614288330078
2025-08-04 03:00:45,510 - father_agent.py:386 - Step: 430, Training loss: 1.6273393630981445
2025-08-04 03:00:47,358 - father_agent.py:386 - Step: 435, Training loss: 3.372295379638672
2025-08-04 03:00:49,195 - father_agent.py:386 - Step: 440, Training loss: 7.5395941734313965
2025-08-04 03:00:51,019 - father_agent.py:386 - Step: 445, Training loss: 2.210836887359619
2025-08-04 03:00:52,858 - father_agent.py:386 - Step: 450, Training loss: 8.938122749328613
2025-08-04 03:00:54,708 - father_agent.py:386 - Step: 455, Training loss: 3.361198663711548
2025-08-04 03:00:56,522 - father_agent.py:386 - Step: 460, Training loss: 5.140048980712891
2025-08-04 03:00:58,360 - father_agent.py:386 - Step: 465, Training loss: 7.147316932678223
2025-08-04 03:01:00,307 - father_agent.py:386 - Step: 470, Training loss: 1.8344212770462036
2025-08-04 03:01:02,202 - father_agent.py:386 - Step: 475, Training loss: 2.985969066619873
2025-08-04 03:01:04,099 - father_agent.py:386 - Step: 480, Training loss: 6.212823867797852
2025-08-04 03:01:06,024 - father_agent.py:386 - Step: 485, Training loss: 2.5919361114501953
2025-08-04 03:01:07,909 - father_agent.py:386 - Step: 490, Training loss: 8.219646453857422
2025-08-04 03:01:09,797 - father_agent.py:386 - Step: 495, Training loss: 2.5470242500305176
2025-08-04 03:01:11,690 - father_agent.py:386 - Step: 500, Training loss: 3.1382830142974854
2025-08-04 03:01:12,102 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:01:12,106 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:01:22,940 - evaluation_results_class.py:131 - Average Return = 3.7999770641326904
2025-08-04 03:01:22,940 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.99977111816406
2025-08-04 03:01:22,940 - evaluation_results_class.py:135 - Average Discounted Reward = 32.05741500854492
2025-08-04 03:01:22,940 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:01:22,940 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:01:22,940 - evaluation_results_class.py:141 - Variance of Return = 2.846364974975586
2025-08-04 03:01:22,940 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:01:22,940 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:01:22,940 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:01:22,940 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:01:23,200 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:01:23,211 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:01:23,327 - father_agent.py:547 - Training finished.
2025-08-04 03:01:23,485 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:01:23,487 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:01:23,489 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 03:01:34,852 - evaluation_results_class.py:131 - Average Return = 3.851792335510254
2025-08-04 03:01:34,852 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.517921447753906
2025-08-04 03:01:34,852 - evaluation_results_class.py:135 - Average Discounted Reward = 32.455596923828125
2025-08-04 03:01:34,852 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:01:34,853 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:01:34,853 - evaluation_results_class.py:141 - Variance of Return = 2.7852494716644287
2025-08-04 03:01:34,853 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:01:34,853 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:01:34,853 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:01:34,853 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:01:35,210 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:01:35,213 - self_interpretable_extractor.py:286 - True
2025-08-04 03:01:35,231 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:01:52,414 - evaluation_results_class.py:131 - Average Return = 3.8101563453674316
2025-08-04 03:01:52,414 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.1015625
2025-08-04 03:01:52,414 - evaluation_results_class.py:135 - Average Discounted Reward = 32.1009407043457
2025-08-04 03:01:52,414 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:01:52,414 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:01:52,414 - evaluation_results_class.py:141 - Variance of Return = 2.7312583923339844
2025-08-04 03:01:52,414 - evaluation_results_class.py:143 - Current Best Return = 3.8101563453674316
2025-08-04 03:01:52,414 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:01:52,414 - evaluation_results_class.py:147 - Average Episode Length = 36.97142857142857
2025-08-04 03:01:52,414 - evaluation_results_class.py:149 - Counted Episodes = 8960
2025-08-04 03:01:52,415 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 03:01:52,415 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 40192 trajectories
Learned trajectory lengths  {37}
Buffer 1
2025-08-04 03:03:02,348 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 80384 trajectories
Learned trajectory lengths  {37}
Buffer 2
2025-08-04 03:04:12,615 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 120576 trajectories
Learned trajectory lengths  {37}
All trajectories collected
2025-08-04 03:05:23,025 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 03:05:23,025 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 120576 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 5
Model saved to fsc_9.dot.
Learned FSC of size 5
2025-08-04 03:05:25,316 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 03:05:41,388 - evaluation_results_class.py:131 - Average Return = 3.862053632736206
2025-08-04 03:05:41,388 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.62053680419922
2025-08-04 03:05:41,388 - evaluation_results_class.py:135 - Average Discounted Reward = 32.522762298583984
2025-08-04 03:05:41,388 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:05:41,388 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:05:41,388 - evaluation_results_class.py:141 - Variance of Return = 2.729185104370117
2025-08-04 03:05:41,388 - evaluation_results_class.py:143 - Current Best Return = 3.862053632736206
2025-08-04 03:05:41,388 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:05:41,388 - evaluation_results_class.py:147 - Average Episode Length = 36.97142857142857
2025-08-04 03:05:41,388 - evaluation_results_class.py:149 - Counted Episodes = 8960
FSC Result: {'best_episode_return': 40.620537, 'best_return': 3.8620536, 'goal_value': 0.0, 'returns_episodic': [40.620537], 'returns': [3.8620536], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [2.729185], 'each_episode_virtual_variance': [272.9185], 'combined_variance': [330.23135], 'num_episodes': [8960], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [36.97142857142857], 'counted_episodes': [8960], 'discounted_rewards': [32.522762], 'new_pomdp_iteration_numbers': []}
[[6], [1, 4, 5], [3], [3], [3], [2], [2], [2], [6], [6], [6], [1, 5], [1, 4], [3], [2], [1], [0], [0], [0], [0]]
2025-08-04 03:05:41,684 - statistic.py:67 - synthesis initiated, design space: 140
2025-08-04 03:05:41,685 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 03:05:41,822 - synthesizer_ar.py:122 - value 16.2648 achieved after 5361.43 seconds
2025-08-04 03:05:41,839 - synthesizer_ar.py:122 - value 8.9393 achieved after 5361.45 seconds
2025-08-04 03:05:41,901 - synthesizer_ar.py:122 - value 8.6186 achieved after 5361.51 seconds
2025-08-04 03:05:41,951 - synthesizer_ar.py:122 - value 8.1826 achieved after 5361.56 seconds
2025-08-04 03:05:41,992 - synthesizer_ar.py:122 - value 7.5827 achieved after 5361.6 seconds
2025-08-04 03:05:42,027 - synthesizer_ar.py:122 - value 6.7485 achieved after 5361.64 seconds
2025-08-04 03:05:42,041 - synthesizer_ar.py:122 - value 5.5755 achieved after 5361.65 seconds
2025-08-04 03:05:42,049 - synthesizer_ar.py:122 - value 3.8348 achieved after 5361.66 seconds
2025-08-04 03:05:42,049 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 03:05:42,049 - synthesizer.py:193 - SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 03:05:42,051 - synthesizer.py:198 - double-checking specification satisfiability:  : 3.8348099332015533
--------------------
Synthesis summary:
optimality objective: R{"packets_sent"}min=? [F "label_done"] 

method: AR, synthesis time: 0.36 s
number of holes: 4, family size: 140, quotient: 15158 states / 91872 actions
explored: 100 %
MDP stats: avg MDP size: 7243, iterations: 24

optimum: 3.83481
--------------------
2025-08-04 03:05:42,051 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 03:05:42,056 - robust_rl_trainer.py:432 - Iteration 11 of pure RL loop
2025-08-04 03:05:42,100 - storm_vec_env.py:70 - Computing row map
2025-08-04 03:05:42,106 - storm_vec_env.py:97 - Computing transitions
2025-08-04 03:05:42,117 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 03:05:42,117 - storm_vec_env.py:114 - Computing sinks
2025-08-04 03:05:42,117 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 03:05:42,121 - storm_vec_env.py:143 - Computing labels
2025-08-04 03:05:42,121 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 03:05:42,121 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 03:05:42,121 - storm_vec_env.py:175 - Computing observations
2025-08-04 03:05:42,292 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 03:05:42,292 - father_agent.py:540 - Before training evaluation.
2025-08-04 03:05:42,455 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:05:42,459 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:05:53,495 - evaluation_results_class.py:131 - Average Return = 3.833754539489746
2025-08-04 03:05:53,495 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.337547302246094
2025-08-04 03:05:53,495 - evaluation_results_class.py:135 - Average Discounted Reward = 32.31354522705078
2025-08-04 03:05:53,495 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:05:53,495 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:05:53,495 - evaluation_results_class.py:141 - Variance of Return = 2.829554557800293
2025-08-04 03:05:53,495 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:05:53,495 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:05:53,495 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:05:53,496 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:05:53,756 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:05:53,769 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:05:53,889 - father_agent.py:436 - Training agent on-policy
2025-08-04 03:06:02,410 - father_agent.py:386 - Step: 0, Training loss: 2.9119198322296143
2025-08-04 03:06:07,819 - father_agent.py:386 - Step: 5, Training loss: 9.276389122009277
2025-08-04 03:06:09,716 - father_agent.py:386 - Step: 10, Training loss: 3.2014570236206055
2025-08-04 03:06:11,624 - father_agent.py:386 - Step: 15, Training loss: 3.318302631378174
2025-08-04 03:06:13,487 - father_agent.py:386 - Step: 20, Training loss: 11.778739929199219
2025-08-04 03:06:15,375 - father_agent.py:386 - Step: 25, Training loss: 3.6140084266662598
2025-08-04 03:06:17,262 - father_agent.py:386 - Step: 30, Training loss: 6.631209850311279
2025-08-04 03:06:19,217 - father_agent.py:386 - Step: 35, Training loss: 4.115622043609619
2025-08-04 03:06:21,086 - father_agent.py:386 - Step: 40, Training loss: 3.6297292709350586
2025-08-04 03:06:22,957 - father_agent.py:386 - Step: 45, Training loss: 11.369105339050293
2025-08-04 03:06:24,943 - father_agent.py:386 - Step: 50, Training loss: 2.3905608654022217
2025-08-04 03:06:26,851 - father_agent.py:386 - Step: 55, Training loss: 2.8052053451538086
2025-08-04 03:06:28,738 - father_agent.py:386 - Step: 60, Training loss: 11.114618301391602
2025-08-04 03:06:30,601 - father_agent.py:386 - Step: 65, Training loss: 2.062610387802124
2025-08-04 03:06:32,419 - father_agent.py:386 - Step: 70, Training loss: 8.414782524108887
2025-08-04 03:06:34,257 - father_agent.py:386 - Step: 75, Training loss: 4.252863883972168
2025-08-04 03:06:36,095 - father_agent.py:386 - Step: 80, Training loss: 3.3891336917877197
2025-08-04 03:06:37,933 - father_agent.py:386 - Step: 85, Training loss: 10.002708435058594
2025-08-04 03:06:39,793 - father_agent.py:386 - Step: 90, Training loss: 2.2446014881134033
2025-08-04 03:06:41,622 - father_agent.py:386 - Step: 95, Training loss: 3.335132360458374
2025-08-04 03:06:43,474 - father_agent.py:386 - Step: 100, Training loss: 8.493612289428711
2025-08-04 03:06:43,734 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:06:43,737 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:06:54,258 - evaluation_results_class.py:131 - Average Return = 3.871208667755127
2025-08-04 03:06:54,259 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.71208572387695
2025-08-04 03:06:54,259 - evaluation_results_class.py:135 - Average Discounted Reward = 32.64164733886719
2025-08-04 03:06:54,259 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:06:54,259 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:06:54,259 - evaluation_results_class.py:141 - Variance of Return = 2.768453598022461
2025-08-04 03:06:54,259 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:06:54,259 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:06:54,259 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:06:54,259 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:06:54,516 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:06:54,528 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:06:58,576 - father_agent.py:386 - Step: 105, Training loss: 2.4941728115081787
2025-08-04 03:07:00,435 - father_agent.py:386 - Step: 110, Training loss: 6.506911754608154
2025-08-04 03:07:02,334 - father_agent.py:386 - Step: 115, Training loss: 3.349405288696289
2025-08-04 03:07:04,201 - father_agent.py:386 - Step: 120, Training loss: 3.337557077407837
2025-08-04 03:07:06,082 - father_agent.py:386 - Step: 125, Training loss: 7.48399543762207
2025-08-04 03:07:07,963 - father_agent.py:386 - Step: 130, Training loss: 1.6149095296859741
2025-08-04 03:07:09,955 - father_agent.py:386 - Step: 135, Training loss: 2.6841394901275635
2025-08-04 03:07:11,995 - father_agent.py:386 - Step: 140, Training loss: 6.152385711669922
2025-08-04 03:07:13,942 - father_agent.py:386 - Step: 145, Training loss: 1.8217779397964478
2025-08-04 03:07:15,834 - father_agent.py:386 - Step: 150, Training loss: 9.402318954467773
2025-08-04 03:07:17,728 - father_agent.py:386 - Step: 155, Training loss: 2.874377727508545
2025-08-04 03:07:19,631 - father_agent.py:386 - Step: 160, Training loss: 4.463714122772217
2025-08-04 03:07:21,555 - father_agent.py:386 - Step: 165, Training loss: 7.001596450805664
2025-08-04 03:07:23,482 - father_agent.py:386 - Step: 170, Training loss: 1.86375093460083
2025-08-04 03:07:25,392 - father_agent.py:386 - Step: 175, Training loss: 3.3539493083953857
2025-08-04 03:07:27,336 - father_agent.py:386 - Step: 180, Training loss: 5.810507297515869
2025-08-04 03:07:29,261 - father_agent.py:386 - Step: 185, Training loss: 3.0086331367492676
2025-08-04 03:07:31,201 - father_agent.py:386 - Step: 190, Training loss: 7.208395957946777
2025-08-04 03:07:33,092 - father_agent.py:386 - Step: 195, Training loss: 2.818432569503784
2025-08-04 03:07:34,970 - father_agent.py:386 - Step: 200, Training loss: 2.7934579849243164
2025-08-04 03:07:35,236 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:07:35,239 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:07:45,816 - evaluation_results_class.py:131 - Average Return = 3.857192039489746
2025-08-04 03:07:45,816 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.571922302246094
2025-08-04 03:07:45,816 - evaluation_results_class.py:135 - Average Discounted Reward = 32.51026916503906
2025-08-04 03:07:45,816 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:07:45,816 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:07:45,816 - evaluation_results_class.py:141 - Variance of Return = 2.845989227294922
2025-08-04 03:07:45,816 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:07:45,817 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:07:45,817 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:07:45,817 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:07:46,071 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:07:46,083 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:07:50,519 - father_agent.py:386 - Step: 205, Training loss: 1.7171934843063354
2025-08-04 03:07:52,358 - father_agent.py:386 - Step: 210, Training loss: 9.043685913085938
2025-08-04 03:07:54,209 - father_agent.py:386 - Step: 215, Training loss: 2.9012625217437744
2025-08-04 03:07:56,068 - father_agent.py:386 - Step: 220, Training loss: 2.6223061084747314
2025-08-04 03:07:57,893 - father_agent.py:386 - Step: 225, Training loss: 6.9807233810424805
2025-08-04 03:07:59,722 - father_agent.py:386 - Step: 230, Training loss: 1.7227003574371338
2025-08-04 03:08:01,562 - father_agent.py:386 - Step: 235, Training loss: 4.048462867736816
2025-08-04 03:08:03,396 - father_agent.py:386 - Step: 240, Training loss: 6.758408546447754
2025-08-04 03:08:05,219 - father_agent.py:386 - Step: 245, Training loss: 2.5253703594207764
2025-08-04 03:08:07,065 - father_agent.py:386 - Step: 250, Training loss: 8.935576438903809
2025-08-04 03:08:08,920 - father_agent.py:386 - Step: 255, Training loss: 2.6584677696228027
2025-08-04 03:08:10,745 - father_agent.py:386 - Step: 260, Training loss: 5.462985038757324
2025-08-04 03:08:12,554 - father_agent.py:386 - Step: 265, Training loss: 7.583872318267822
2025-08-04 03:08:14,381 - father_agent.py:386 - Step: 270, Training loss: 2.19783878326416
2025-08-04 03:08:16,213 - father_agent.py:386 - Step: 275, Training loss: 4.004909038543701
2025-08-04 03:08:18,041 - father_agent.py:386 - Step: 280, Training loss: 4.900020599365234
2025-08-04 03:08:19,888 - father_agent.py:386 - Step: 285, Training loss: 3.150129795074463
2025-08-04 03:08:21,714 - father_agent.py:386 - Step: 290, Training loss: 6.957213401794434
2025-08-04 03:08:23,551 - father_agent.py:386 - Step: 295, Training loss: 2.6635899543762207
2025-08-04 03:08:25,377 - father_agent.py:386 - Step: 300, Training loss: 3.1891963481903076
2025-08-04 03:08:25,634 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:08:25,637 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:08:36,185 - evaluation_results_class.py:131 - Average Return = 3.829733371734619
2025-08-04 03:08:36,185 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.297332763671875
2025-08-04 03:08:36,185 - evaluation_results_class.py:135 - Average Discounted Reward = 32.29405975341797
2025-08-04 03:08:36,185 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:08:36,185 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:08:36,185 - evaluation_results_class.py:141 - Variance of Return = 2.8110828399658203
2025-08-04 03:08:36,185 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:08:36,185 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:08:36,185 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:08:36,185 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:08:36,448 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:08:36,460 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:08:40,567 - father_agent.py:386 - Step: 305, Training loss: 1.9841746091842651
2025-08-04 03:08:42,406 - father_agent.py:386 - Step: 310, Training loss: 10.088465690612793
2025-08-04 03:08:44,260 - father_agent.py:386 - Step: 315, Training loss: 3.2030792236328125
2025-08-04 03:08:46,128 - father_agent.py:386 - Step: 320, Training loss: 3.1858174800872803
2025-08-04 03:08:47,980 - father_agent.py:386 - Step: 325, Training loss: 6.857501983642578
2025-08-04 03:08:49,823 - father_agent.py:386 - Step: 330, Training loss: 2.0114355087280273
2025-08-04 03:08:51,664 - father_agent.py:386 - Step: 335, Training loss: 5.379680156707764
2025-08-04 03:08:53,520 - father_agent.py:386 - Step: 340, Training loss: 5.520293235778809
2025-08-04 03:08:55,349 - father_agent.py:386 - Step: 345, Training loss: 3.5554308891296387
2025-08-04 03:08:57,185 - father_agent.py:386 - Step: 350, Training loss: 6.653943061828613
2025-08-04 03:08:59,018 - father_agent.py:386 - Step: 355, Training loss: 2.9794087409973145
2025-08-04 03:09:00,851 - father_agent.py:386 - Step: 360, Training loss: 4.232909202575684
2025-08-04 03:09:02,699 - father_agent.py:386 - Step: 365, Training loss: 8.624074935913086
2025-08-04 03:09:04,587 - father_agent.py:386 - Step: 370, Training loss: 2.1032211780548096
2025-08-04 03:09:06,435 - father_agent.py:386 - Step: 375, Training loss: 4.133941173553467
2025-08-04 03:09:08,296 - father_agent.py:386 - Step: 380, Training loss: 4.267813682556152
2025-08-04 03:09:10,143 - father_agent.py:386 - Step: 385, Training loss: 3.702080726623535
2025-08-04 03:09:11,961 - father_agent.py:386 - Step: 390, Training loss: 7.101923942565918
2025-08-04 03:09:13,824 - father_agent.py:386 - Step: 395, Training loss: 2.5513949394226074
2025-08-04 03:09:15,669 - father_agent.py:386 - Step: 400, Training loss: 3.1057281494140625
2025-08-04 03:09:15,929 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:09:15,931 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:09:26,458 - evaluation_results_class.py:131 - Average Return = 3.7842371463775635
2025-08-04 03:09:26,458 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.84237289428711
2025-08-04 03:09:26,458 - evaluation_results_class.py:135 - Average Discounted Reward = 31.910144805908203
2025-08-04 03:09:26,458 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:09:26,458 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:09:26,458 - evaluation_results_class.py:141 - Variance of Return = 2.7549171447753906
2025-08-04 03:09:26,458 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:09:26,458 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:09:26,458 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:09:26,458 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:09:26,727 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:09:26,739 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:09:30,777 - father_agent.py:386 - Step: 405, Training loss: 1.8663994073867798
2025-08-04 03:09:32,602 - father_agent.py:386 - Step: 410, Training loss: 7.943571090698242
2025-08-04 03:09:34,430 - father_agent.py:386 - Step: 415, Training loss: 2.911432981491089
2025-08-04 03:09:36,253 - father_agent.py:386 - Step: 420, Training loss: 3.161376476287842
2025-08-04 03:09:38,098 - father_agent.py:386 - Step: 425, Training loss: 7.672493934631348
2025-08-04 03:09:39,945 - father_agent.py:386 - Step: 430, Training loss: 1.8906046152114868
2025-08-04 03:09:41,787 - father_agent.py:386 - Step: 435, Training loss: 2.320671319961548
2025-08-04 03:09:43,625 - father_agent.py:386 - Step: 440, Training loss: 5.062077045440674
2025-08-04 03:09:45,445 - father_agent.py:386 - Step: 445, Training loss: 2.8456709384918213
2025-08-04 03:09:47,273 - father_agent.py:386 - Step: 450, Training loss: 6.602761745452881
2025-08-04 03:09:49,115 - father_agent.py:386 - Step: 455, Training loss: 2.8040812015533447
2025-08-04 03:09:50,953 - father_agent.py:386 - Step: 460, Training loss: 5.291407108306885
2025-08-04 03:09:52,789 - father_agent.py:386 - Step: 465, Training loss: 7.485123634338379
2025-08-04 03:09:54,629 - father_agent.py:386 - Step: 470, Training loss: 1.8130362033843994
2025-08-04 03:09:56,458 - father_agent.py:386 - Step: 475, Training loss: 2.92183256149292
2025-08-04 03:09:58,303 - father_agent.py:386 - Step: 480, Training loss: 5.437462329864502
2025-08-04 03:10:00,122 - father_agent.py:386 - Step: 485, Training loss: 3.502734661102295
2025-08-04 03:10:01,944 - father_agent.py:386 - Step: 490, Training loss: 7.980164527893066
2025-08-04 03:10:03,788 - father_agent.py:386 - Step: 495, Training loss: 2.5399973392486572
2025-08-04 03:10:05,612 - father_agent.py:386 - Step: 500, Training loss: 2.7525758743286133
2025-08-04 03:10:05,882 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:10:05,884 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:10:16,500 - evaluation_results_class.py:131 - Average Return = 3.8255975246429443
2025-08-04 03:10:16,501 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.25597381591797
2025-08-04 03:10:16,501 - evaluation_results_class.py:135 - Average Discounted Reward = 32.24603271484375
2025-08-04 03:10:16,501 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:10:16,501 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:10:16,501 - evaluation_results_class.py:141 - Variance of Return = 2.829418182373047
2025-08-04 03:10:16,501 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:10:16,501 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:10:16,501 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:10:16,501 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:10:16,771 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:10:16,789 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:10:16,929 - father_agent.py:547 - Training finished.
2025-08-04 03:10:17,116 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:10:17,119 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:10:17,122 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 03:10:28,135 - evaluation_results_class.py:131 - Average Return = 3.8196232318878174
2025-08-04 03:10:28,135 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.196231842041016
2025-08-04 03:10:28,135 - evaluation_results_class.py:135 - Average Discounted Reward = 32.219478607177734
2025-08-04 03:10:28,135 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:10:28,135 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:10:28,136 - evaluation_results_class.py:141 - Variance of Return = 2.7505524158477783
2025-08-04 03:10:28,136 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:10:28,136 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:10:28,136 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:10:28,136 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:10:28,415 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:10:28,418 - self_interpretable_extractor.py:286 - True
2025-08-04 03:10:28,436 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:10:45,552 - evaluation_results_class.py:131 - Average Return = 3.7858259677886963
2025-08-04 03:10:45,552 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.85825729370117
2025-08-04 03:10:45,552 - evaluation_results_class.py:135 - Average Discounted Reward = 31.930335998535156
2025-08-04 03:10:45,552 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:10:45,552 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:10:45,552 - evaluation_results_class.py:141 - Variance of Return = 2.705357074737549
2025-08-04 03:10:45,552 - evaluation_results_class.py:143 - Current Best Return = 3.7858259677886963
2025-08-04 03:10:45,552 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:10:45,552 - evaluation_results_class.py:147 - Average Episode Length = 36.97142857142857
2025-08-04 03:10:45,552 - evaluation_results_class.py:149 - Counted Episodes = 8960
2025-08-04 03:10:45,552 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 03:10:45,552 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 40192 trajectories
Learned trajectory lengths  {37}
Buffer 1
2025-08-04 03:11:55,626 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 80384 trajectories
Learned trajectory lengths  {37}
Buffer 2
2025-08-04 03:13:04,710 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 120576 trajectories
Learned trajectory lengths  {37}
All trajectories collected
2025-08-04 03:14:14,505 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 03:14:14,505 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 120576 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 5
Model saved to fsc_10.dot.
Learned FSC of size 5
2025-08-04 03:14:16,795 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 03:14:33,135 - evaluation_results_class.py:131 - Average Return = 3.8274552822113037
2025-08-04 03:14:33,135 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.27455520629883
2025-08-04 03:14:33,135 - evaluation_results_class.py:135 - Average Discounted Reward = 32.244422912597656
2025-08-04 03:14:33,135 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:14:33,135 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:14:33,135 - evaluation_results_class.py:141 - Variance of Return = 2.8164334297180176
2025-08-04 03:14:33,135 - evaluation_results_class.py:143 - Current Best Return = 3.8274552822113037
2025-08-04 03:14:33,135 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:14:33,135 - evaluation_results_class.py:147 - Average Episode Length = 36.97142857142857
2025-08-04 03:14:33,135 - evaluation_results_class.py:149 - Counted Episodes = 8960
FSC Result: {'best_episode_return': 40.274555, 'best_return': 3.8274553, 'goal_value': 0.0, 'returns_episodic': [40.274555], 'returns': [3.8274553], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [2.8164334], 'each_episode_virtual_variance': [281.6434], 'combined_variance': [340.7885], 'num_episodes': [8960], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [36.97142857142857], 'counted_episodes': [8960], 'discounted_rewards': [32.244423], 'new_pomdp_iteration_numbers': []}
[[6], [1, 4, 5], [3], [3], [3], [2], [2], [2], [6], [6], [6], [1, 5], [1, 4], [3], [2], [1], [0], [0], [0], [0]]
2025-08-04 03:14:33,938 - statistic.py:67 - synthesis initiated, design space: 140
2025-08-04 03:14:33,941 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 03:14:34,079 - synthesizer_ar.py:122 - value 16.2652 achieved after 5893.69 seconds
2025-08-04 03:14:34,096 - synthesizer_ar.py:122 - value 8.9395 achieved after 5893.71 seconds
2025-08-04 03:14:34,157 - synthesizer_ar.py:122 - value 8.6188 achieved after 5893.77 seconds
2025-08-04 03:14:34,207 - synthesizer_ar.py:122 - value 8.1828 achieved after 5893.82 seconds
2025-08-04 03:14:34,248 - synthesizer_ar.py:122 - value 7.5829 achieved after 5893.86 seconds
2025-08-04 03:14:34,283 - synthesizer_ar.py:122 - value 6.7487 achieved after 5893.89 seconds
2025-08-04 03:14:34,298 - synthesizer_ar.py:122 - value 5.5757 achieved after 5893.91 seconds
2025-08-04 03:14:34,305 - synthesizer_ar.py:122 - value 3.8352 achieved after 5893.92 seconds
2025-08-04 03:14:34,305 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 03:14:34,305 - synthesizer.py:193 - SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 03:14:34,307 - synthesizer.py:198 - double-checking specification satisfiability:  : 3.835241477564578
--------------------
Synthesis summary:
optimality objective: R{"packets_sent"}min=? [F "label_done"] 

method: AR, synthesis time: 0.36 s
number of holes: 4, family size: 140, quotient: 15158 states / 91872 actions
explored: 100 %
MDP stats: avg MDP size: 7243, iterations: 24

optimum: 3.835241
--------------------
2025-08-04 03:14:34,307 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 03:14:34,312 - robust_rl_trainer.py:432 - Iteration 12 of pure RL loop
2025-08-04 03:14:34,358 - storm_vec_env.py:70 - Computing row map
2025-08-04 03:14:34,365 - storm_vec_env.py:97 - Computing transitions
2025-08-04 03:14:34,376 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 03:14:34,376 - storm_vec_env.py:114 - Computing sinks
2025-08-04 03:14:34,376 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 03:14:34,379 - storm_vec_env.py:143 - Computing labels
2025-08-04 03:14:34,379 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 03:14:34,380 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 03:14:34,380 - storm_vec_env.py:175 - Computing observations
2025-08-04 03:14:34,559 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 03:14:34,559 - father_agent.py:540 - Before training evaluation.
2025-08-04 03:14:34,723 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:14:34,726 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:14:45,285 - evaluation_results_class.py:131 - Average Return = 3.849264621734619
2025-08-04 03:14:45,288 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.492645263671875
2025-08-04 03:14:45,288 - evaluation_results_class.py:135 - Average Discounted Reward = 32.47004318237305
2025-08-04 03:14:45,288 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:14:45,288 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:14:45,288 - evaluation_results_class.py:141 - Variance of Return = 2.805403470993042
2025-08-04 03:14:45,288 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:14:45,288 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:14:45,288 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:14:45,288 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:14:45,541 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:14:45,554 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:14:45,672 - father_agent.py:436 - Training agent on-policy
2025-08-04 03:14:54,132 - father_agent.py:386 - Step: 0, Training loss: 2.5685296058654785
2025-08-04 03:14:59,555 - father_agent.py:386 - Step: 5, Training loss: 9.270694732666016
2025-08-04 03:15:01,401 - father_agent.py:386 - Step: 10, Training loss: 2.509066581726074
2025-08-04 03:15:03,257 - father_agent.py:386 - Step: 15, Training loss: 2.678582191467285
2025-08-04 03:15:05,119 - father_agent.py:386 - Step: 20, Training loss: 8.219481468200684
2025-08-04 03:15:06,960 - father_agent.py:386 - Step: 25, Training loss: 4.069088935852051
2025-08-04 03:15:08,805 - father_agent.py:386 - Step: 30, Training loss: 6.255424976348877
2025-08-04 03:15:10,679 - father_agent.py:386 - Step: 35, Training loss: 5.79219388961792
2025-08-04 03:15:12,516 - father_agent.py:386 - Step: 40, Training loss: 2.3004119396209717
2025-08-04 03:15:14,374 - father_agent.py:386 - Step: 45, Training loss: 12.109490394592285
2025-08-04 03:15:16,244 - father_agent.py:386 - Step: 50, Training loss: 2.634922981262207
2025-08-04 03:15:18,078 - father_agent.py:386 - Step: 55, Training loss: 2.212395429611206
2025-08-04 03:15:19,917 - father_agent.py:386 - Step: 60, Training loss: 10.857016563415527
2025-08-04 03:15:21,773 - father_agent.py:386 - Step: 65, Training loss: 1.7351999282836914
2025-08-04 03:15:23,618 - father_agent.py:386 - Step: 70, Training loss: 9.913840293884277
2025-08-04 03:15:25,465 - father_agent.py:386 - Step: 75, Training loss: 4.176126956939697
2025-08-04 03:15:27,285 - father_agent.py:386 - Step: 80, Training loss: 2.228334665298462
2025-08-04 03:15:29,128 - father_agent.py:386 - Step: 85, Training loss: 9.700729370117188
2025-08-04 03:15:30,980 - father_agent.py:386 - Step: 90, Training loss: 2.567323684692383
2025-08-04 03:15:32,816 - father_agent.py:386 - Step: 95, Training loss: 4.48264217376709
2025-08-04 03:15:34,647 - father_agent.py:386 - Step: 100, Training loss: 9.373969078063965
2025-08-04 03:15:34,906 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:15:34,909 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:15:45,488 - evaluation_results_class.py:131 - Average Return = 3.826401710510254
2025-08-04 03:15:45,488 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.264015197753906
2025-08-04 03:15:45,488 - evaluation_results_class.py:135 - Average Discounted Reward = 32.264060974121094
2025-08-04 03:15:45,488 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:15:45,488 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:15:45,488 - evaluation_results_class.py:141 - Variance of Return = 2.7978737354278564
2025-08-04 03:15:45,488 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:15:45,488 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:15:45,488 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:15:45,488 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:15:45,748 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:15:45,760 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:15:49,787 - father_agent.py:386 - Step: 105, Training loss: 1.7967345714569092
2025-08-04 03:15:51,607 - father_agent.py:386 - Step: 110, Training loss: 9.988525390625
2025-08-04 03:15:53,456 - father_agent.py:386 - Step: 115, Training loss: 2.923233985900879
2025-08-04 03:15:55,290 - father_agent.py:386 - Step: 120, Training loss: 3.342719793319702
2025-08-04 03:15:57,123 - father_agent.py:386 - Step: 125, Training loss: 8.063470840454102
2025-08-04 03:15:58,962 - father_agent.py:386 - Step: 130, Training loss: 1.9007138013839722
2025-08-04 03:16:00,813 - father_agent.py:386 - Step: 135, Training loss: 4.093021869659424
2025-08-04 03:16:02,656 - father_agent.py:386 - Step: 140, Training loss: 7.065999507904053
2025-08-04 03:16:04,479 - father_agent.py:386 - Step: 145, Training loss: 2.4631237983703613
2025-08-04 03:16:06,306 - father_agent.py:386 - Step: 150, Training loss: 8.817376136779785
2025-08-04 03:16:08,154 - father_agent.py:386 - Step: 155, Training loss: 2.5999982357025146
2025-08-04 03:16:10,058 - father_agent.py:386 - Step: 160, Training loss: 4.693004608154297
2025-08-04 03:16:11,969 - father_agent.py:386 - Step: 165, Training loss: 8.30479621887207
2025-08-04 03:16:13,916 - father_agent.py:386 - Step: 170, Training loss: 2.071131944656372
2025-08-04 03:16:15,857 - father_agent.py:386 - Step: 175, Training loss: 4.518749237060547
2025-08-04 03:16:17,816 - father_agent.py:386 - Step: 180, Training loss: 5.091583251953125
2025-08-04 03:16:19,793 - father_agent.py:386 - Step: 185, Training loss: 3.37147855758667
2025-08-04 03:16:21,770 - father_agent.py:386 - Step: 190, Training loss: 8.845818519592285
2025-08-04 03:16:23,719 - father_agent.py:386 - Step: 195, Training loss: 2.779404401779175
2025-08-04 03:16:25,680 - father_agent.py:386 - Step: 200, Training loss: 3.381937026977539
2025-08-04 03:16:25,944 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:16:25,947 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:16:37,347 - evaluation_results_class.py:131 - Average Return = 3.788717746734619
2025-08-04 03:16:37,347 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.887176513671875
2025-08-04 03:16:37,347 - evaluation_results_class.py:135 - Average Discounted Reward = 31.960494995117188
2025-08-04 03:16:37,347 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:16:37,347 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:16:37,348 - evaluation_results_class.py:141 - Variance of Return = 2.7020280361175537
2025-08-04 03:16:37,348 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:16:37,348 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:16:37,348 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:16:37,348 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:16:37,618 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:16:37,631 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:16:41,821 - father_agent.py:386 - Step: 205, Training loss: 2.0966131687164307
2025-08-04 03:16:43,793 - father_agent.py:386 - Step: 210, Training loss: 6.016366004943848
2025-08-04 03:16:45,669 - father_agent.py:386 - Step: 215, Training loss: 3.078190326690674
2025-08-04 03:16:47,513 - father_agent.py:386 - Step: 220, Training loss: 3.5043747425079346
2025-08-04 03:16:49,364 - father_agent.py:386 - Step: 225, Training loss: 6.34509801864624
2025-08-04 03:16:51,218 - father_agent.py:386 - Step: 230, Training loss: 1.834415078163147
2025-08-04 03:16:53,067 - father_agent.py:386 - Step: 235, Training loss: 2.561635732650757
2025-08-04 03:16:54,911 - father_agent.py:386 - Step: 240, Training loss: 4.779938697814941
2025-08-04 03:16:56,759 - father_agent.py:386 - Step: 245, Training loss: 2.6626596450805664
2025-08-04 03:16:58,594 - father_agent.py:386 - Step: 250, Training loss: 5.381217002868652
2025-08-04 03:17:00,457 - father_agent.py:386 - Step: 255, Training loss: 3.214794635772705
2025-08-04 03:17:02,295 - father_agent.py:386 - Step: 260, Training loss: 5.229091167449951
2025-08-04 03:17:04,124 - father_agent.py:386 - Step: 265, Training loss: 8.381858825683594
2025-08-04 03:17:05,972 - father_agent.py:386 - Step: 270, Training loss: 1.9399311542510986
2025-08-04 03:17:07,797 - father_agent.py:386 - Step: 275, Training loss: 3.1731202602386475
2025-08-04 03:17:09,648 - father_agent.py:386 - Step: 280, Training loss: 5.899733543395996
2025-08-04 03:17:11,489 - father_agent.py:386 - Step: 285, Training loss: 2.7586615085601807
2025-08-04 03:17:13,347 - father_agent.py:386 - Step: 290, Training loss: 9.366435050964355
2025-08-04 03:17:15,180 - father_agent.py:386 - Step: 295, Training loss: 2.46785044670105
2025-08-04 03:17:17,016 - father_agent.py:386 - Step: 300, Training loss: 2.7645022869110107
2025-08-04 03:17:17,283 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:17:17,285 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:17:28,209 - evaluation_results_class.py:131 - Average Return = 3.841911792755127
2025-08-04 03:17:28,210 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.41911697387695
2025-08-04 03:17:28,210 - evaluation_results_class.py:135 - Average Discounted Reward = 32.422367095947266
2025-08-04 03:17:28,210 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:17:28,210 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:17:28,210 - evaluation_results_class.py:141 - Variance of Return = 2.7700445652008057
2025-08-04 03:17:28,210 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:17:28,210 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:17:28,210 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:17:28,210 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:17:28,478 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:17:28,490 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:17:32,579 - father_agent.py:386 - Step: 305, Training loss: 1.877767562866211
2025-08-04 03:17:34,461 - father_agent.py:386 - Step: 310, Training loss: 7.061864376068115
2025-08-04 03:17:36,368 - father_agent.py:386 - Step: 315, Training loss: 3.0628292560577393
2025-08-04 03:17:38,364 - father_agent.py:386 - Step: 320, Training loss: 3.1557843685150146
2025-08-04 03:17:40,372 - father_agent.py:386 - Step: 325, Training loss: 6.383800029754639
2025-08-04 03:17:42,411 - father_agent.py:386 - Step: 330, Training loss: 1.7845004796981812
2025-08-04 03:17:44,306 - father_agent.py:386 - Step: 335, Training loss: 3.8177297115325928
2025-08-04 03:17:46,224 - father_agent.py:386 - Step: 340, Training loss: 6.831672191619873
2025-08-04 03:17:48,132 - father_agent.py:386 - Step: 345, Training loss: 2.105618715286255
2025-08-04 03:17:50,051 - father_agent.py:386 - Step: 350, Training loss: 6.172521591186523
2025-08-04 03:17:51,993 - father_agent.py:386 - Step: 355, Training loss: 2.709872007369995
2025-08-04 03:17:53,919 - father_agent.py:386 - Step: 360, Training loss: 4.7349419593811035
2025-08-04 03:17:55,833 - father_agent.py:386 - Step: 365, Training loss: 6.981454372406006
2025-08-04 03:17:57,818 - father_agent.py:386 - Step: 370, Training loss: 2.0611839294433594
2025-08-04 03:17:59,715 - father_agent.py:386 - Step: 375, Training loss: 4.291133880615234
2025-08-04 03:18:01,623 - father_agent.py:386 - Step: 380, Training loss: 5.549395561218262
2025-08-04 03:18:03,519 - father_agent.py:386 - Step: 385, Training loss: 2.9187545776367188
2025-08-04 03:18:05,439 - father_agent.py:386 - Step: 390, Training loss: 8.271099090576172
2025-08-04 03:18:07,356 - father_agent.py:386 - Step: 395, Training loss: 2.760244369506836
2025-08-04 03:18:09,266 - father_agent.py:386 - Step: 400, Training loss: 3.3658432960510254
2025-08-04 03:18:09,542 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:18:09,544 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:18:20,251 - evaluation_results_class.py:131 - Average Return = 3.7898666858673096
2025-08-04 03:18:20,251 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.89866638183594
2025-08-04 03:18:20,251 - evaluation_results_class.py:135 - Average Discounted Reward = 31.936128616333008
2025-08-04 03:18:20,251 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:18:20,251 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:18:20,251 - evaluation_results_class.py:141 - Variance of Return = 2.7052695751190186
2025-08-04 03:18:20,251 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:18:20,251 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:18:20,251 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:18:20,251 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:18:20,524 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:18:20,536 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:18:24,640 - father_agent.py:386 - Step: 405, Training loss: 2.2024314403533936
2025-08-04 03:18:26,517 - father_agent.py:386 - Step: 410, Training loss: 8.993056297302246
2025-08-04 03:18:28,416 - father_agent.py:386 - Step: 415, Training loss: 2.9789624214172363
2025-08-04 03:18:30,285 - father_agent.py:386 - Step: 420, Training loss: 3.7423694133758545
2025-08-04 03:18:32,154 - father_agent.py:386 - Step: 425, Training loss: 8.817747116088867
2025-08-04 03:18:34,009 - father_agent.py:386 - Step: 430, Training loss: 1.7997355461120605
2025-08-04 03:18:35,855 - father_agent.py:386 - Step: 435, Training loss: 4.2452168464660645
2025-08-04 03:18:37,706 - father_agent.py:386 - Step: 440, Training loss: 4.831036567687988
2025-08-04 03:18:39,558 - father_agent.py:386 - Step: 445, Training loss: 3.1631338596343994
2025-08-04 03:18:41,406 - father_agent.py:386 - Step: 450, Training loss: 6.552071571350098
2025-08-04 03:18:43,273 - father_agent.py:386 - Step: 455, Training loss: 2.4721572399139404
2025-08-04 03:18:45,118 - father_agent.py:386 - Step: 460, Training loss: 4.688177108764648
2025-08-04 03:18:46,965 - father_agent.py:386 - Step: 465, Training loss: 8.046647071838379
2025-08-04 03:18:48,833 - father_agent.py:386 - Step: 470, Training loss: 2.2245638370513916
2025-08-04 03:18:50,700 - father_agent.py:386 - Step: 475, Training loss: 4.368278980255127
2025-08-04 03:18:52,587 - father_agent.py:386 - Step: 480, Training loss: 4.497190952301025
2025-08-04 03:18:54,431 - father_agent.py:386 - Step: 485, Training loss: 3.910534143447876
2025-08-04 03:18:56,278 - father_agent.py:386 - Step: 490, Training loss: 7.492663383483887
2025-08-04 03:18:58,135 - father_agent.py:386 - Step: 495, Training loss: 2.45289945602417
2025-08-04 03:18:59,999 - father_agent.py:386 - Step: 500, Training loss: 3.0645995140075684
2025-08-04 03:19:00,272 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:19:00,275 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:19:10,965 - evaluation_results_class.py:131 - Average Return = 3.8640854358673096
2025-08-04 03:19:10,965 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.64085388183594
2025-08-04 03:19:10,965 - evaluation_results_class.py:135 - Average Discounted Reward = 32.56708908081055
2025-08-04 03:19:10,965 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:19:10,965 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:19:10,965 - evaluation_results_class.py:141 - Variance of Return = 2.866063117980957
2025-08-04 03:19:10,965 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:19:10,965 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:19:10,965 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:19:10,965 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:19:11,239 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:19:11,251 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:19:11,370 - father_agent.py:547 - Training finished.
2025-08-04 03:19:11,533 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:19:11,535 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:19:11,538 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 03:19:22,824 - evaluation_results_class.py:131 - Average Return = 3.83984375
2025-08-04 03:19:22,824 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.3984375
2025-08-04 03:19:22,824 - evaluation_results_class.py:135 - Average Discounted Reward = 32.37538146972656
2025-08-04 03:19:22,824 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:19:22,824 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:19:22,824 - evaluation_results_class.py:141 - Variance of Return = 2.8105173110961914
2025-08-04 03:19:22,824 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:19:22,824 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:19:22,824 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:19:22,824 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:19:23,095 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:19:23,098 - self_interpretable_extractor.py:286 - True
2025-08-04 03:19:23,111 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:19:40,310 - evaluation_results_class.py:131 - Average Return = 3.8348214626312256
2025-08-04 03:19:40,310 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.34821319580078
2025-08-04 03:19:40,310 - evaluation_results_class.py:135 - Average Discounted Reward = 32.30670166015625
2025-08-04 03:19:40,310 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:19:40,310 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:19:40,310 - evaluation_results_class.py:141 - Variance of Return = 2.782090902328491
2025-08-04 03:19:40,310 - evaluation_results_class.py:143 - Current Best Return = 3.8348214626312256
2025-08-04 03:19:40,310 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:19:40,310 - evaluation_results_class.py:147 - Average Episode Length = 36.97142857142857
2025-08-04 03:19:40,310 - evaluation_results_class.py:149 - Counted Episodes = 8960
2025-08-04 03:19:40,310 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 03:19:40,311 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 40192 trajectories
Learned trajectory lengths  {37}
Buffer 1
2025-08-04 03:20:50,581 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 80384 trajectories
Learned trajectory lengths  {37}
Buffer 2
2025-08-04 03:22:01,240 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 120576 trajectories
Learned trajectory lengths  {37}
All trajectories collected
2025-08-04 03:23:11,566 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 03:23:11,567 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 120576 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 1
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 3
Model saved to fsc_11.dot.
Learned FSC of size 3
2025-08-04 03:23:13,867 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 03:23:30,050 - evaluation_results_class.py:131 - Average Return = 3.6907365322113037
2025-08-04 03:23:30,050 - evaluation_results_class.py:133 - Average Virtual Goal Value = 38.90736770629883
2025-08-04 03:23:30,050 - evaluation_results_class.py:135 - Average Discounted Reward = 31.171003341674805
2025-08-04 03:23:30,050 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:23:30,050 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:23:30,051 - evaluation_results_class.py:141 - Variance of Return = 2.777682065963745
2025-08-04 03:23:30,051 - evaluation_results_class.py:143 - Current Best Return = 3.6907365322113037
2025-08-04 03:23:30,051 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:23:30,051 - evaluation_results_class.py:147 - Average Episode Length = 36.97142857142857
2025-08-04 03:23:30,051 - evaluation_results_class.py:149 - Counted Episodes = 8960
FSC Result: {'best_episode_return': 38.907368, 'best_return': 3.6907365, 'goal_value': 0.0, 'returns_episodic': [38.907368], 'returns': [3.6907365], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [2.777682], 'each_episode_virtual_variance': [277.7682], 'combined_variance': [336.09952], 'num_episodes': [8960], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [36.97142857142857], 'counted_episodes': [8960], 'discounted_rewards': [31.171003], 'new_pomdp_iteration_numbers': []}
[[6], [1, 4, 5], [3], [3], [3], [2], [2], [2], [6], [6], [6], [1, 5], [1, 4], [3], [2], [1], [0], [0], [0], [0]]
2025-08-04 03:23:30,360 - statistic.py:67 - synthesis initiated, design space: 140
2025-08-04 03:23:30,361 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 03:23:30,500 - synthesizer_ar.py:122 - value 16.2616 achieved after 6430.11 seconds
2025-08-04 03:23:30,517 - synthesizer_ar.py:122 - value 8.9374 achieved after 6430.13 seconds
2025-08-04 03:23:30,578 - synthesizer_ar.py:122 - value 8.6146 achieved after 6430.19 seconds
2025-08-04 03:23:30,627 - synthesizer_ar.py:122 - value 8.1746 achieved after 6430.24 seconds
2025-08-04 03:23:30,668 - synthesizer_ar.py:122 - value 7.567 achieved after 6430.28 seconds
2025-08-04 03:23:30,702 - synthesizer_ar.py:122 - value 6.7189 achieved after 6430.31 seconds
2025-08-04 03:23:30,717 - synthesizer_ar.py:122 - value 5.5226 achieved after 6430.33 seconds
2025-08-04 03:23:30,723 - synthesizer_ar.py:122 - value 3.7418 achieved after 6430.33 seconds
2025-08-04 03:23:30,724 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 03:23:30,724 - synthesizer.py:193 - SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 03:23:30,726 - synthesizer.py:198 - double-checking specification satisfiability:  : 3.7418467327972085
--------------------
Synthesis summary:
optimality objective: R{"packets_sent"}min=? [F "label_done"] 

method: AR, synthesis time: 0.36 s
number of holes: 4, family size: 140, quotient: 14422 states / 91136 actions
explored: 100 %
MDP stats: avg MDP size: 7050, iterations: 24

optimum: 3.741847
--------------------
2025-08-04 03:23:30,726 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 03:23:30,731 - robust_rl_trainer.py:432 - Iteration 13 of pure RL loop
2025-08-04 03:23:30,777 - storm_vec_env.py:70 - Computing row map
2025-08-04 03:23:30,783 - storm_vec_env.py:97 - Computing transitions
2025-08-04 03:23:30,795 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 03:23:30,795 - storm_vec_env.py:114 - Computing sinks
2025-08-04 03:23:30,795 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 03:23:30,798 - storm_vec_env.py:143 - Computing labels
2025-08-04 03:23:30,798 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 03:23:30,798 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 03:23:30,798 - storm_vec_env.py:175 - Computing observations
2025-08-04 03:23:30,981 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 03:23:30,982 - father_agent.py:540 - Before training evaluation.
2025-08-04 03:23:31,149 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:23:31,152 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:23:42,210 - evaluation_results_class.py:131 - Average Return = 3.7806756496429443
2025-08-04 03:23:42,210 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.80675506591797
2025-08-04 03:23:42,210 - evaluation_results_class.py:135 - Average Discounted Reward = 31.933996200561523
2025-08-04 03:23:42,210 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:23:42,210 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:23:42,210 - evaluation_results_class.py:141 - Variance of Return = 2.736708402633667
2025-08-04 03:23:42,210 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:23:42,210 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:23:42,210 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:23:42,210 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:23:42,483 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:23:42,496 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:23:42,617 - father_agent.py:436 - Training agent on-policy
2025-08-04 03:23:51,222 - father_agent.py:386 - Step: 0, Training loss: 2.892845630645752
2025-08-04 03:23:56,762 - father_agent.py:386 - Step: 5, Training loss: 10.30966567993164
2025-08-04 03:23:58,642 - father_agent.py:386 - Step: 10, Training loss: 2.6062638759613037
2025-08-04 03:24:00,518 - father_agent.py:386 - Step: 15, Training loss: 3.1413731575012207
2025-08-04 03:24:02,390 - father_agent.py:386 - Step: 20, Training loss: 12.197035789489746
2025-08-04 03:24:04,259 - father_agent.py:386 - Step: 25, Training loss: 2.8088159561157227
2025-08-04 03:24:06,123 - father_agent.py:386 - Step: 30, Training loss: 5.803823471069336
2025-08-04 03:24:07,988 - father_agent.py:386 - Step: 35, Training loss: 3.928253412246704
2025-08-04 03:24:09,846 - father_agent.py:386 - Step: 40, Training loss: 3.6226649284362793
2025-08-04 03:24:11,704 - father_agent.py:386 - Step: 45, Training loss: 9.968116760253906
2025-08-04 03:24:13,576 - father_agent.py:386 - Step: 50, Training loss: 2.510371208190918
2025-08-04 03:24:15,436 - father_agent.py:386 - Step: 55, Training loss: 2.5732288360595703
2025-08-04 03:24:17,311 - father_agent.py:386 - Step: 60, Training loss: 8.938006401062012
2025-08-04 03:24:19,181 - father_agent.py:386 - Step: 65, Training loss: 2.103659152984619
2025-08-04 03:24:21,046 - father_agent.py:386 - Step: 70, Training loss: 7.087220191955566
2025-08-04 03:24:22,929 - father_agent.py:386 - Step: 75, Training loss: 4.809662342071533
2025-08-04 03:24:24,793 - father_agent.py:386 - Step: 80, Training loss: 3.181227684020996
2025-08-04 03:24:26,647 - father_agent.py:386 - Step: 85, Training loss: 9.981780052185059
2025-08-04 03:24:28,522 - father_agent.py:386 - Step: 90, Training loss: 2.195923089981079
2025-08-04 03:24:30,375 - father_agent.py:386 - Step: 95, Training loss: 3.453861713409424
2025-08-04 03:24:32,213 - father_agent.py:386 - Step: 100, Training loss: 8.562695503234863
2025-08-04 03:24:32,492 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:24:32,495 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:24:43,230 - evaluation_results_class.py:131 - Average Return = 3.793428421020508
2025-08-04 03:24:43,231 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.93428421020508
2025-08-04 03:24:43,231 - evaluation_results_class.py:135 - Average Discounted Reward = 31.97650146484375
2025-08-04 03:24:43,231 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:24:43,231 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:24:43,231 - evaluation_results_class.py:141 - Variance of Return = 2.7429440021514893
2025-08-04 03:24:43,231 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:24:43,231 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:24:43,231 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:24:43,231 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:24:43,500 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:24:43,513 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:24:47,601 - father_agent.py:386 - Step: 105, Training loss: 1.6376492977142334
2025-08-04 03:24:49,441 - father_agent.py:386 - Step: 110, Training loss: 8.718240737915039
2025-08-04 03:24:51,279 - father_agent.py:386 - Step: 115, Training loss: 3.2325451374053955
2025-08-04 03:24:53,100 - father_agent.py:386 - Step: 120, Training loss: 3.162255048751831
2025-08-04 03:24:54,938 - father_agent.py:386 - Step: 125, Training loss: 8.64883804321289
2025-08-04 03:24:56,776 - father_agent.py:386 - Step: 130, Training loss: 1.8017399311065674
2025-08-04 03:24:58,610 - father_agent.py:386 - Step: 135, Training loss: 2.6475818157196045
2025-08-04 03:25:00,467 - father_agent.py:386 - Step: 140, Training loss: 5.462102890014648
2025-08-04 03:25:02,307 - father_agent.py:386 - Step: 145, Training loss: 2.418290376663208
2025-08-04 03:25:04,143 - father_agent.py:386 - Step: 150, Training loss: 6.767095565795898
2025-08-04 03:25:05,986 - father_agent.py:386 - Step: 155, Training loss: 3.0072386264801025
2025-08-04 03:25:07,818 - father_agent.py:386 - Step: 160, Training loss: 4.536225318908691
2025-08-04 03:25:09,670 - father_agent.py:386 - Step: 165, Training loss: 7.539671897888184
2025-08-04 03:25:11,519 - father_agent.py:386 - Step: 170, Training loss: 1.768500804901123
2025-08-04 03:25:13,358 - father_agent.py:386 - Step: 175, Training loss: 3.2313594818115234
2025-08-04 03:25:15,199 - father_agent.py:386 - Step: 180, Training loss: 6.119612693786621
2025-08-04 03:25:17,026 - father_agent.py:386 - Step: 185, Training loss: 2.939176559448242
2025-08-04 03:25:18,872 - father_agent.py:386 - Step: 190, Training loss: 8.563477516174316
2025-08-04 03:25:20,729 - father_agent.py:386 - Step: 195, Training loss: 2.7800660133361816
2025-08-04 03:25:22,588 - father_agent.py:386 - Step: 200, Training loss: 2.7363297939300537
2025-08-04 03:25:22,868 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:25:22,870 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:25:33,630 - evaluation_results_class.py:131 - Average Return = 3.8563878536224365
2025-08-04 03:25:33,630 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.56387710571289
2025-08-04 03:25:33,630 - evaluation_results_class.py:135 - Average Discounted Reward = 32.52762222290039
2025-08-04 03:25:33,630 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:25:33,630 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:25:33,630 - evaluation_results_class.py:141 - Variance of Return = 2.7796971797943115
2025-08-04 03:25:33,630 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:25:33,630 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:25:33,630 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:25:33,630 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:25:33,904 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:25:33,917 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:25:38,423 - father_agent.py:386 - Step: 205, Training loss: 1.9286246299743652
2025-08-04 03:25:40,284 - father_agent.py:386 - Step: 210, Training loss: 6.337158679962158
2025-08-04 03:25:42,132 - father_agent.py:386 - Step: 215, Training loss: 3.2228944301605225
2025-08-04 03:25:43,969 - father_agent.py:386 - Step: 220, Training loss: 2.8691956996917725
2025-08-04 03:25:45,815 - father_agent.py:386 - Step: 225, Training loss: 8.110013008117676
2025-08-04 03:25:47,671 - father_agent.py:386 - Step: 230, Training loss: 1.9653722047805786
2025-08-04 03:25:49,566 - father_agent.py:386 - Step: 235, Training loss: 3.010180711746216
2025-08-04 03:25:51,469 - father_agent.py:386 - Step: 240, Training loss: 6.367502212524414
2025-08-04 03:25:53,359 - father_agent.py:386 - Step: 245, Training loss: 2.2128520011901855
2025-08-04 03:25:55,277 - father_agent.py:386 - Step: 250, Training loss: 6.897539138793945
2025-08-04 03:25:57,156 - father_agent.py:386 - Step: 255, Training loss: 2.955329179763794
2025-08-04 03:25:59,030 - father_agent.py:386 - Step: 260, Training loss: 4.742700099945068
2025-08-04 03:26:00,891 - father_agent.py:386 - Step: 265, Training loss: 7.284110069274902
2025-08-04 03:26:02,750 - father_agent.py:386 - Step: 270, Training loss: 1.945235252380371
2025-08-04 03:26:04,606 - father_agent.py:386 - Step: 275, Training loss: 3.6653032302856445
2025-08-04 03:26:06,477 - father_agent.py:386 - Step: 280, Training loss: 6.422576904296875
2025-08-04 03:26:08,338 - father_agent.py:386 - Step: 285, Training loss: 2.639817953109741
2025-08-04 03:26:10,196 - father_agent.py:386 - Step: 290, Training loss: 8.134844779968262
2025-08-04 03:26:12,070 - father_agent.py:386 - Step: 295, Training loss: 2.5405967235565186
2025-08-04 03:26:13,932 - father_agent.py:386 - Step: 300, Training loss: 3.0165112018585205
2025-08-04 03:26:14,215 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:26:14,218 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:26:25,092 - evaluation_results_class.py:131 - Average Return = 3.855009078979492
2025-08-04 03:26:25,092 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.55009078979492
2025-08-04 03:26:25,092 - evaluation_results_class.py:135 - Average Discounted Reward = 32.46334457397461
2025-08-04 03:26:25,092 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:26:25,092 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:26:25,092 - evaluation_results_class.py:141 - Variance of Return = 2.787801742553711
2025-08-04 03:26:25,093 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:26:25,093 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:26:25,093 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:26:25,093 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:26:25,456 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:26:25,469 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:26:29,611 - father_agent.py:386 - Step: 305, Training loss: 1.907104253768921
2025-08-04 03:26:31,558 - father_agent.py:386 - Step: 310, Training loss: 8.81090259552002
2025-08-04 03:26:33,525 - father_agent.py:386 - Step: 315, Training loss: 2.6967179775238037
2025-08-04 03:26:35,494 - father_agent.py:386 - Step: 320, Training loss: 3.225560426712036
2025-08-04 03:26:37,379 - father_agent.py:386 - Step: 325, Training loss: 7.589245319366455
2025-08-04 03:26:39,292 - father_agent.py:386 - Step: 330, Training loss: 1.8352774381637573
2025-08-04 03:26:41,200 - father_agent.py:386 - Step: 335, Training loss: 5.643152713775635
2025-08-04 03:26:43,156 - father_agent.py:386 - Step: 340, Training loss: 5.182854652404785
2025-08-04 03:26:45,062 - father_agent.py:386 - Step: 345, Training loss: 3.0263447761535645
2025-08-04 03:26:46,987 - father_agent.py:386 - Step: 350, Training loss: 6.738466262817383
2025-08-04 03:26:48,972 - father_agent.py:386 - Step: 355, Training loss: 3.004432439804077
2025-08-04 03:26:50,909 - father_agent.py:386 - Step: 360, Training loss: 4.766778469085693
2025-08-04 03:26:52,896 - father_agent.py:386 - Step: 365, Training loss: 6.725231170654297
2025-08-04 03:26:54,846 - father_agent.py:386 - Step: 370, Training loss: 2.134427547454834
2025-08-04 03:26:56,772 - father_agent.py:386 - Step: 375, Training loss: 3.7181766033172607
2025-08-04 03:26:58,706 - father_agent.py:386 - Step: 380, Training loss: 4.229954719543457
2025-08-04 03:27:00,691 - father_agent.py:386 - Step: 385, Training loss: 3.661616563796997
2025-08-04 03:27:02,647 - father_agent.py:386 - Step: 390, Training loss: 9.211181640625
2025-08-04 03:27:04,608 - father_agent.py:386 - Step: 395, Training loss: 2.50479793548584
2025-08-04 03:27:06,532 - father_agent.py:386 - Step: 400, Training loss: 3.8516080379486084
2025-08-04 03:27:06,811 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:27:06,813 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:27:17,947 - evaluation_results_class.py:131 - Average Return = 3.837660789489746
2025-08-04 03:27:17,948 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.376609802246094
2025-08-04 03:27:17,948 - evaluation_results_class.py:135 - Average Discounted Reward = 32.36517333984375
2025-08-04 03:27:17,948 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:27:17,948 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:27:17,948 - evaluation_results_class.py:141 - Variance of Return = 2.8875937461853027
2025-08-04 03:27:17,948 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:27:17,948 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:27:17,948 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:27:17,948 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:27:18,231 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:27:18,244 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:27:22,428 - father_agent.py:386 - Step: 405, Training loss: 2.221212148666382
2025-08-04 03:27:24,356 - father_agent.py:386 - Step: 410, Training loss: 7.470772743225098
2025-08-04 03:27:26,300 - father_agent.py:386 - Step: 415, Training loss: 3.0371341705322266
2025-08-04 03:27:28,245 - father_agent.py:386 - Step: 420, Training loss: 3.758090019226074
2025-08-04 03:27:30,243 - father_agent.py:386 - Step: 425, Training loss: 8.265425682067871
2025-08-04 03:27:32,245 - father_agent.py:386 - Step: 430, Training loss: 1.9116926193237305
2025-08-04 03:27:34,211 - father_agent.py:386 - Step: 435, Training loss: 4.811936378479004
2025-08-04 03:27:36,152 - father_agent.py:386 - Step: 440, Training loss: 4.597585201263428
2025-08-04 03:27:38,058 - father_agent.py:386 - Step: 445, Training loss: 3.385554075241089
2025-08-04 03:27:39,918 - father_agent.py:386 - Step: 450, Training loss: 7.548033237457275
2025-08-04 03:27:41,809 - father_agent.py:386 - Step: 455, Training loss: 2.9153292179107666
2025-08-04 03:27:43,727 - father_agent.py:386 - Step: 460, Training loss: 5.25887393951416
2025-08-04 03:27:45,649 - father_agent.py:386 - Step: 465, Training loss: 8.360776901245117
2025-08-04 03:27:47,557 - father_agent.py:386 - Step: 470, Training loss: 2.24931263923645
2025-08-04 03:27:49,499 - father_agent.py:386 - Step: 475, Training loss: 3.781651258468628
2025-08-04 03:27:51,480 - father_agent.py:386 - Step: 480, Training loss: 4.068367004394531
2025-08-04 03:27:53,407 - father_agent.py:386 - Step: 485, Training loss: 4.063801288604736
2025-08-04 03:27:55,353 - father_agent.py:386 - Step: 490, Training loss: 7.721217155456543
2025-08-04 03:27:57,326 - father_agent.py:386 - Step: 495, Training loss: 2.510927677154541
2025-08-04 03:27:59,311 - father_agent.py:386 - Step: 500, Training loss: 4.313070774078369
2025-08-04 03:27:59,589 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:27:59,592 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:28:10,396 - evaluation_results_class.py:131 - Average Return = 3.878676414489746
2025-08-04 03:28:10,396 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.786766052246094
2025-08-04 03:28:10,396 - evaluation_results_class.py:135 - Average Discounted Reward = 32.709259033203125
2025-08-04 03:28:10,396 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:28:10,396 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:28:10,396 - evaluation_results_class.py:141 - Variance of Return = 2.902330160140991
2025-08-04 03:28:10,396 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:28:10,396 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:28:10,396 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:28:10,396 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:28:10,671 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:28:10,684 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:28:10,809 - father_agent.py:547 - Training finished.
2025-08-04 03:28:10,971 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:28:10,974 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:28:10,977 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 03:28:22,209 - evaluation_results_class.py:131 - Average Return = 3.8524816036224365
2025-08-04 03:28:22,210 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.52481460571289
2025-08-04 03:28:22,210 - evaluation_results_class.py:135 - Average Discounted Reward = 32.45633316040039
2025-08-04 03:28:22,210 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:28:22,210 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:28:22,210 - evaluation_results_class.py:141 - Variance of Return = 2.786372661590576
2025-08-04 03:28:22,210 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:28:22,210 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:28:22,210 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:28:22,210 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:28:22,490 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:28:22,492 - self_interpretable_extractor.py:286 - True
2025-08-04 03:28:22,505 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:28:39,795 - evaluation_results_class.py:131 - Average Return = 3.843526840209961
2025-08-04 03:28:39,796 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.43526840209961
2025-08-04 03:28:39,796 - evaluation_results_class.py:135 - Average Discounted Reward = 32.36046600341797
2025-08-04 03:28:39,796 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:28:39,796 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:28:39,796 - evaluation_results_class.py:141 - Variance of Return = 2.831319570541382
2025-08-04 03:28:39,796 - evaluation_results_class.py:143 - Current Best Return = 3.843526840209961
2025-08-04 03:28:39,796 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:28:39,796 - evaluation_results_class.py:147 - Average Episode Length = 36.97142857142857
2025-08-04 03:28:39,796 - evaluation_results_class.py:149 - Counted Episodes = 8960
2025-08-04 03:28:39,797 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 03:28:39,797 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 40192 trajectories
Learned trajectory lengths  {37}
Buffer 1
2025-08-04 03:29:49,086 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 80384 trajectories
Learned trajectory lengths  {37}
Buffer 2
2025-08-04 03:30:58,399 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 120576 trajectories
Learned trajectory lengths  {37}
All trajectories collected
2025-08-04 03:32:07,774 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 03:32:07,774 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 120576 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 5
Model saved to fsc_12.dot.
Learned FSC of size 5
2025-08-04 03:32:09,420 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 03:32:26,270 - evaluation_results_class.py:131 - Average Return = 3.845200777053833
2025-08-04 03:32:26,271 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.45200729370117
2025-08-04 03:32:26,271 - evaluation_results_class.py:135 - Average Discounted Reward = 32.39807891845703
2025-08-04 03:32:26,271 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:32:26,271 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:32:26,271 - evaluation_results_class.py:141 - Variance of Return = 2.783961296081543
2025-08-04 03:32:26,271 - evaluation_results_class.py:143 - Current Best Return = 3.845200777053833
2025-08-04 03:32:26,271 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:32:26,271 - evaluation_results_class.py:147 - Average Episode Length = 36.97142857142857
2025-08-04 03:32:26,271 - evaluation_results_class.py:149 - Counted Episodes = 8960
FSC Result: {'best_episode_return': 40.452007, 'best_return': 3.8452008, 'goal_value': 0.0, 'returns_episodic': [40.452007], 'returns': [3.8452008], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [2.7839613], 'each_episode_virtual_variance': [278.39618], 'combined_variance': [336.8593], 'num_episodes': [8960], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [36.97142857142857], 'counted_episodes': [8960], 'discounted_rewards': [32.39808], 'new_pomdp_iteration_numbers': []}
[[6], [1, 4, 5], [3], [3], [3], [2], [2], [2], [6], [6], [6], [1, 5], [1, 4], [3], [2], [1], [0], [0], [0], [0]]
2025-08-04 03:32:26,587 - statistic.py:67 - synthesis initiated, design space: 140
2025-08-04 03:32:26,588 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 03:32:26,725 - synthesizer_ar.py:122 - value 16.2645 achieved after 6966.34 seconds
2025-08-04 03:32:26,742 - synthesizer_ar.py:122 - value 8.9391 achieved after 6966.35 seconds
2025-08-04 03:32:26,803 - synthesizer_ar.py:122 - value 8.6186 achieved after 6966.41 seconds
2025-08-04 03:32:26,852 - synthesizer_ar.py:122 - value 8.183 achieved after 6966.46 seconds
2025-08-04 03:32:26,893 - synthesizer_ar.py:122 - value 7.5835 achieved after 6966.5 seconds
2025-08-04 03:32:26,927 - synthesizer_ar.py:122 - value 6.7499 achieved after 6966.54 seconds
2025-08-04 03:32:26,942 - synthesizer_ar.py:122 - value 5.5777 achieved after 6966.55 seconds
2025-08-04 03:32:26,949 - synthesizer_ar.py:122 - value 3.8374 achieved after 6966.56 seconds
2025-08-04 03:32:26,949 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 03:32:26,949 - synthesizer.py:193 - SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 03:32:26,951 - synthesizer.py:198 - double-checking specification satisfiability:  : 3.837429907044528
--------------------
Synthesis summary:
optimality objective: R{"packets_sent"}min=? [F "label_done"] 

method: AR, synthesis time: 0.36 s
number of holes: 4, family size: 140, quotient: 15158 states / 91872 actions
explored: 100 %
MDP stats: avg MDP size: 7243, iterations: 24

optimum: 3.83743
--------------------
2025-08-04 03:32:26,951 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: SLOTS=2, PERIODS=6, chan1_init=0, chan2_init=0
2025-08-04 03:32:26,957 - robust_rl_trainer.py:432 - Iteration 14 of pure RL loop
2025-08-04 03:32:27,001 - storm_vec_env.py:70 - Computing row map
2025-08-04 03:32:27,007 - storm_vec_env.py:97 - Computing transitions
2025-08-04 03:32:27,019 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 03:32:27,019 - storm_vec_env.py:114 - Computing sinks
2025-08-04 03:32:27,019 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 03:32:27,022 - storm_vec_env.py:143 - Computing labels
2025-08-04 03:32:27,022 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 03:32:27,022 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 03:32:27,022 - storm_vec_env.py:175 - Computing observations
2025-08-04 03:32:27,199 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 03:32:27,199 - father_agent.py:540 - Before training evaluation.
2025-08-04 03:32:27,368 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:32:27,370 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:32:38,465 - evaluation_results_class.py:131 - Average Return = 3.791245460510254
2025-08-04 03:32:38,466 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.912452697753906
2025-08-04 03:32:38,466 - evaluation_results_class.py:135 - Average Discounted Reward = 31.986127853393555
2025-08-04 03:32:38,466 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:32:38,466 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:32:38,466 - evaluation_results_class.py:141 - Variance of Return = 2.881168842315674
2025-08-04 03:32:38,466 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:32:38,466 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:32:38,466 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:32:38,466 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:32:38,727 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:32:38,740 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:32:38,857 - father_agent.py:436 - Training agent on-policy
2025-08-04 03:32:47,347 - father_agent.py:386 - Step: 0, Training loss: 2.9992618560791016
2025-08-04 03:32:52,844 - father_agent.py:386 - Step: 5, Training loss: 8.235310554504395
2025-08-04 03:32:54,708 - father_agent.py:386 - Step: 10, Training loss: 2.668881893157959
2025-08-04 03:32:56,552 - father_agent.py:386 - Step: 15, Training loss: 4.287040710449219
2025-08-04 03:32:58,403 - father_agent.py:386 - Step: 20, Training loss: 8.500064849853516
2025-08-04 03:33:00,258 - father_agent.py:386 - Step: 25, Training loss: 3.886871337890625
2025-08-04 03:33:02,112 - father_agent.py:386 - Step: 30, Training loss: 5.604959487915039
2025-08-04 03:33:03,987 - father_agent.py:386 - Step: 35, Training loss: 3.905719757080078
2025-08-04 03:33:05,822 - father_agent.py:386 - Step: 40, Training loss: 4.609639644622803
2025-08-04 03:33:07,671 - father_agent.py:386 - Step: 45, Training loss: 11.225821495056152
2025-08-04 03:33:09,530 - father_agent.py:386 - Step: 50, Training loss: 2.3242440223693848
2025-08-04 03:33:11,384 - father_agent.py:386 - Step: 55, Training loss: 3.778480291366577
2025-08-04 03:33:13,233 - father_agent.py:386 - Step: 60, Training loss: 9.850653648376465
2025-08-04 03:33:15,083 - father_agent.py:386 - Step: 65, Training loss: 2.4704477787017822
2025-08-04 03:33:16,935 - father_agent.py:386 - Step: 70, Training loss: 8.597042083740234
2025-08-04 03:33:18,801 - father_agent.py:386 - Step: 75, Training loss: 4.580101013183594
2025-08-04 03:33:20,663 - father_agent.py:386 - Step: 80, Training loss: 3.9199295043945312
2025-08-04 03:33:22,509 - father_agent.py:386 - Step: 85, Training loss: 9.310284614562988
2025-08-04 03:33:24,369 - father_agent.py:386 - Step: 90, Training loss: 2.0948119163513184
2025-08-04 03:33:26,226 - father_agent.py:386 - Step: 95, Training loss: 3.9115335941314697
2025-08-04 03:33:28,068 - father_agent.py:386 - Step: 100, Training loss: 10.366399765014648
2025-08-04 03:33:28,335 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:33:28,337 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:33:38,954 - evaluation_results_class.py:131 - Average Return = 3.838005542755127
2025-08-04 03:33:38,955 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.38005447387695
2025-08-04 03:33:38,955 - evaluation_results_class.py:135 - Average Discounted Reward = 32.359920501708984
2025-08-04 03:33:38,955 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:33:38,955 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:33:38,955 - evaluation_results_class.py:141 - Variance of Return = 2.89011812210083
2025-08-04 03:33:38,955 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:33:38,955 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:33:38,955 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:33:38,955 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:33:39,234 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:33:39,250 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:33:43,365 - father_agent.py:386 - Step: 105, Training loss: 2.1798346042633057
2025-08-04 03:33:45,199 - father_agent.py:386 - Step: 110, Training loss: 7.644306659698486
2025-08-04 03:33:47,049 - father_agent.py:386 - Step: 115, Training loss: 3.054002523422241
2025-08-04 03:33:48,893 - father_agent.py:386 - Step: 120, Training loss: 3.7030937671661377
2025-08-04 03:33:50,714 - father_agent.py:386 - Step: 125, Training loss: 7.105411529541016
2025-08-04 03:33:52,568 - father_agent.py:386 - Step: 130, Training loss: 1.9015889167785645
2025-08-04 03:33:54,390 - father_agent.py:386 - Step: 135, Training loss: 3.0564393997192383
2025-08-04 03:33:56,223 - father_agent.py:386 - Step: 140, Training loss: 5.668569564819336
2025-08-04 03:33:58,055 - father_agent.py:386 - Step: 145, Training loss: 2.5723323822021484
2025-08-04 03:33:59,875 - father_agent.py:386 - Step: 150, Training loss: 6.748780727386475
2025-08-04 03:34:01,731 - father_agent.py:386 - Step: 155, Training loss: 3.2329342365264893
2025-08-04 03:34:03,632 - father_agent.py:386 - Step: 160, Training loss: 5.094907283782959
2025-08-04 03:34:05,541 - father_agent.py:386 - Step: 165, Training loss: 7.164921760559082
2025-08-04 03:34:07,444 - father_agent.py:386 - Step: 170, Training loss: 2.0695276260375977
2025-08-04 03:34:09,334 - father_agent.py:386 - Step: 175, Training loss: 3.8826096057891846
2025-08-04 03:34:11,224 - father_agent.py:386 - Step: 180, Training loss: 5.049933910369873
2025-08-04 03:34:13,158 - father_agent.py:386 - Step: 185, Training loss: 3.924740791320801
2025-08-04 03:34:15,065 - father_agent.py:386 - Step: 190, Training loss: 6.98721170425415
2025-08-04 03:34:16,960 - father_agent.py:386 - Step: 195, Training loss: 2.5920588970184326
2025-08-04 03:34:18,859 - father_agent.py:386 - Step: 200, Training loss: 3.0906248092651367
2025-08-04 03:34:19,133 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:34:19,136 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:34:29,841 - evaluation_results_class.py:131 - Average Return = 3.8353631496429443
2025-08-04 03:34:29,842 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.35363006591797
2025-08-04 03:34:29,842 - evaluation_results_class.py:135 - Average Discounted Reward = 32.34152603149414
2025-08-04 03:34:29,842 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:34:29,842 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:34:29,842 - evaluation_results_class.py:141 - Variance of Return = 2.8592686653137207
2025-08-04 03:34:29,842 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:34:29,842 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:34:29,842 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:34:29,842 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:34:30,119 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:34:30,131 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:34:34,673 - father_agent.py:386 - Step: 205, Training loss: 2.309492588043213
2025-08-04 03:34:36,565 - father_agent.py:386 - Step: 210, Training loss: 6.9053850173950195
2025-08-04 03:34:38,452 - father_agent.py:386 - Step: 215, Training loss: 2.5264289379119873
2025-08-04 03:34:40,323 - father_agent.py:386 - Step: 220, Training loss: 3.2673189640045166
2025-08-04 03:34:42,207 - father_agent.py:386 - Step: 225, Training loss: 6.60481595993042
2025-08-04 03:34:44,069 - father_agent.py:386 - Step: 230, Training loss: 1.6896717548370361
2025-08-04 03:34:45,931 - father_agent.py:386 - Step: 235, Training loss: 3.4741902351379395
2025-08-04 03:34:47,836 - father_agent.py:386 - Step: 240, Training loss: 6.178483963012695
2025-08-04 03:34:49,719 - father_agent.py:386 - Step: 245, Training loss: 2.6187634468078613
2025-08-04 03:34:51,575 - father_agent.py:386 - Step: 250, Training loss: 7.913905620574951
2025-08-04 03:34:53,459 - father_agent.py:386 - Step: 255, Training loss: 2.643554449081421
2025-08-04 03:34:55,355 - father_agent.py:386 - Step: 260, Training loss: 4.8528618812561035
2025-08-04 03:34:57,227 - father_agent.py:386 - Step: 265, Training loss: 7.169557571411133
2025-08-04 03:34:59,157 - father_agent.py:386 - Step: 270, Training loss: 2.16561222076416
2025-08-04 03:35:01,061 - father_agent.py:386 - Step: 275, Training loss: 4.543408393859863
2025-08-04 03:35:02,944 - father_agent.py:386 - Step: 280, Training loss: 5.787985801696777
2025-08-04 03:35:04,847 - father_agent.py:386 - Step: 285, Training loss: 3.465158700942993
2025-08-04 03:35:06,755 - father_agent.py:386 - Step: 290, Training loss: 7.898748874664307
2025-08-04 03:35:08,667 - father_agent.py:386 - Step: 295, Training loss: 2.723541736602783
2025-08-04 03:35:10,543 - father_agent.py:386 - Step: 300, Training loss: 3.3030219078063965
2025-08-04 03:35:10,822 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:35:10,825 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:35:21,557 - evaluation_results_class.py:131 - Average Return = 3.8523666858673096
2025-08-04 03:35:21,557 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.52366638183594
2025-08-04 03:35:21,557 - evaluation_results_class.py:135 - Average Discounted Reward = 32.475914001464844
2025-08-04 03:35:21,557 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:35:21,557 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:35:21,557 - evaluation_results_class.py:141 - Variance of Return = 2.820920705795288
2025-08-04 03:35:21,557 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:35:21,557 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:35:21,557 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:35:21,557 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:35:21,830 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:35:21,843 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:35:25,993 - father_agent.py:386 - Step: 305, Training loss: 2.168210744857788
2025-08-04 03:35:27,855 - father_agent.py:386 - Step: 310, Training loss: 6.107001781463623
2025-08-04 03:35:29,797 - father_agent.py:386 - Step: 315, Training loss: 3.0613582134246826
2025-08-04 03:35:31,696 - father_agent.py:386 - Step: 320, Training loss: 3.215277671813965
2025-08-04 03:35:33,571 - father_agent.py:386 - Step: 325, Training loss: 9.051335334777832
2025-08-04 03:35:35,466 - father_agent.py:386 - Step: 330, Training loss: 1.8097574710845947
2025-08-04 03:35:37,346 - father_agent.py:386 - Step: 335, Training loss: 3.023037910461426
2025-08-04 03:35:39,277 - father_agent.py:386 - Step: 340, Training loss: 5.158415794372559
2025-08-04 03:35:41,167 - father_agent.py:386 - Step: 345, Training loss: 2.6347696781158447
2025-08-04 03:35:43,031 - father_agent.py:386 - Step: 350, Training loss: 5.618186950683594
2025-08-04 03:35:44,892 - father_agent.py:386 - Step: 355, Training loss: 2.827810287475586
2025-08-04 03:35:46,738 - father_agent.py:386 - Step: 360, Training loss: 4.467956066131592
2025-08-04 03:35:48,595 - father_agent.py:386 - Step: 365, Training loss: 7.585238933563232
2025-08-04 03:35:50,442 - father_agent.py:386 - Step: 370, Training loss: 2.099247932434082
2025-08-04 03:35:52,269 - father_agent.py:386 - Step: 375, Training loss: 3.326240301132202
2025-08-04 03:35:54,134 - father_agent.py:386 - Step: 380, Training loss: 4.461455345153809
2025-08-04 03:35:55,959 - father_agent.py:386 - Step: 385, Training loss: 4.380317211151123
2025-08-04 03:35:57,784 - father_agent.py:386 - Step: 390, Training loss: 7.470366477966309
2025-08-04 03:35:59,651 - father_agent.py:386 - Step: 395, Training loss: 2.5806713104248047
2025-08-04 03:36:01,473 - father_agent.py:386 - Step: 400, Training loss: 3.1872544288635254
2025-08-04 03:36:01,748 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:36:01,751 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:36:12,272 - evaluation_results_class.py:131 - Average Return = 3.8034236431121826
2025-08-04 03:36:12,272 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.034236907958984
2025-08-04 03:36:12,272 - evaluation_results_class.py:135 - Average Discounted Reward = 32.07727813720703
2025-08-04 03:36:12,272 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:36:12,272 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:36:12,272 - evaluation_results_class.py:141 - Variance of Return = 2.8160223960876465
2025-08-04 03:36:12,273 - evaluation_results_class.py:143 - Current Best Return = 7.115685939788818
2025-08-04 03:36:12,273 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:36:12,273 - evaluation_results_class.py:147 - Average Episode Length = 36.94117647058823
2025-08-04 03:36:12,273 - evaluation_results_class.py:149 - Counted Episodes = 8704
2025-08-04 03:36:12,549 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:36:12,561 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:36:16,613 - father_agent.py:386 - Step: 405, Training loss: 1.7848963737487793

2025-08-04 03:36:18.439305: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-04 03:36:18.441139: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-04 03:36:18.470701: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-04 03:36:18.470745: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-04 03:36:18.472156: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-04 03:36:18.477733: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-04 03:36:18.477911: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-04 03:36:19.002984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/models_robust_subset/obstacles-10-2/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/models_robust_subset/obstacles-10-2/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"penalty"}min=? [F ((x = 10) & (y = 10))] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 326 states and 829 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"rewardmodel_penalty"}min=? [F ((x = 10) & (y = 10))] 
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-10-2...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Iteration 1 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -456.903564453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -415.0464172363281
INFO:tools.evaluation_results_class:Average Discounted Reward = -91.93956756591797
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.5232142857142857
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 77211.0234375
INFO:tools.evaluation_results_class:Current Best Return = -456.903564453125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.5232142857142857
INFO:tools.evaluation_results_class:Average Episode Length = 494.9875
INFO:tools.evaluation_results_class:Counted Episodes = 560
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 821.4912719726562
INFO:agents.father_agent:Step: 5, Training loss: 3.0214102268218994
INFO:agents.father_agent:Step: 10, Training loss: 3.9139227867126465
INFO:agents.father_agent:Step: 15, Training loss: 3.3443312644958496
INFO:agents.father_agent:Step: 20, Training loss: 3.6727378368377686
INFO:agents.father_agent:Step: 25, Training loss: 2.8836522102355957
INFO:agents.father_agent:Step: 30, Training loss: 4.145596027374268
INFO:agents.father_agent:Step: 35, Training loss: 4.468220233917236
INFO:agents.father_agent:Step: 40, Training loss: 2.764862537384033
INFO:agents.father_agent:Step: 45, Training loss: 2.9878547191619873
INFO:agents.father_agent:Step: 50, Training loss: 4.522129058837891
INFO:agents.father_agent:Step: 55, Training loss: 5.023496627807617
INFO:agents.father_agent:Step: 60, Training loss: 5.065096378326416
INFO:agents.father_agent:Step: 65, Training loss: 4.868790149688721
INFO:agents.father_agent:Step: 70, Training loss: 3.83225154876709
INFO:agents.father_agent:Step: 75, Training loss: 2.34279203414917
INFO:agents.father_agent:Step: 80, Training loss: 2.8010449409484863
INFO:agents.father_agent:Step: 85, Training loss: 1.6876325607299805
INFO:agents.father_agent:Step: 90, Training loss: 2.31612491607666
INFO:agents.father_agent:Step: 95, Training loss: 1.5779807567596436
INFO:agents.father_agent:Step: 100, Training loss: 1.9602490663528442
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -186.83912658691406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -155.24862670898438
INFO:tools.evaluation_results_class:Average Discounted Reward = -30.780536651611328
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.39488117001828155
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6658.43505859375
INFO:tools.evaluation_results_class:Current Best Return = -186.83912658691406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.5232142857142857
INFO:tools.evaluation_results_class:Average Episode Length = 508.18098720292505
INFO:tools.evaluation_results_class:Counted Episodes = 547
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 2.3242809772491455
INFO:agents.father_agent:Step: 110, Training loss: 3.062309503555298
INFO:agents.father_agent:Step: 115, Training loss: 1.894438624382019
INFO:agents.father_agent:Step: 120, Training loss: 1.6216646432876587
INFO:agents.father_agent:Step: 125, Training loss: 1.7114770412445068
INFO:agents.father_agent:Step: 130, Training loss: 2.401435613632202
INFO:agents.father_agent:Step: 135, Training loss: 2.0192887783050537
INFO:agents.father_agent:Step: 140, Training loss: 1.962235689163208
INFO:agents.father_agent:Step: 145, Training loss: 2.0133581161499023
INFO:agents.father_agent:Step: 150, Training loss: 2.043372392654419
INFO:agents.father_agent:Step: 155, Training loss: 2.167464256286621
INFO:agents.father_agent:Step: 160, Training loss: 1.7956650257110596
INFO:agents.father_agent:Step: 165, Training loss: 2.0221054553985596
INFO:agents.father_agent:Step: 170, Training loss: 2.170037031173706
INFO:agents.father_agent:Step: 175, Training loss: 2.7425425052642822
INFO:agents.father_agent:Step: 180, Training loss: 4.335172176361084
INFO:agents.father_agent:Step: 185, Training loss: 3.7007288932800293
INFO:agents.father_agent:Step: 190, Training loss: 3.1554770469665527
INFO:agents.father_agent:Step: 195, Training loss: 2.9577255249023438
INFO:agents.father_agent:Step: 200, Training loss: 2.942009210586548
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -42.42390823364258
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.57609176635742
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.99871826171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1870.708740234375
INFO:tools.evaluation_results_class:Current Best Return = -42.42390823364258
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.960991126736985
INFO:tools.evaluation_results_class:Counted Episodes = 5973
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 2.7764387130737305
INFO:agents.father_agent:Step: 210, Training loss: 2.6915950775146484
INFO:agents.father_agent:Step: 215, Training loss: 3.2278831005096436
INFO:agents.father_agent:Step: 220, Training loss: 3.7765583992004395
INFO:agents.father_agent:Step: 225, Training loss: 3.0926496982574463
INFO:agents.father_agent:Step: 230, Training loss: 2.8483355045318604
INFO:agents.father_agent:Step: 235, Training loss: 1.959585428237915
INFO:agents.father_agent:Step: 240, Training loss: 3.4218790531158447
INFO:agents.father_agent:Step: 245, Training loss: 2.9802656173706055
INFO:agents.father_agent:Step: 250, Training loss: 2.9387147426605225
INFO:agents.father_agent:Step: 255, Training loss: 3.1863958835601807
INFO:agents.father_agent:Step: 260, Training loss: 3.7790913581848145
INFO:agents.father_agent:Step: 265, Training loss: 3.176362991333008
INFO:agents.father_agent:Step: 270, Training loss: 3.3919003009796143
INFO:agents.father_agent:Step: 275, Training loss: 2.8083395957946777
INFO:agents.father_agent:Step: 280, Training loss: 3.151308298110962
INFO:agents.father_agent:Step: 285, Training loss: 2.1654584407806396
INFO:agents.father_agent:Step: 290, Training loss: 2.7942862510681152
INFO:agents.father_agent:Step: 295, Training loss: 1.8846819400787354
INFO:agents.father_agent:Step: 300, Training loss: 3.1383213996887207
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -47.478721618652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 32.521278381347656
INFO:tools.evaluation_results_class:Average Discounted Reward = 10.97391128540039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2105.032470703125
INFO:tools.evaluation_results_class:Current Best Return = -42.42390823364258
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.03687943262411
INFO:tools.evaluation_results_class:Counted Episodes = 5640
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 3.1246743202209473
INFO:agents.father_agent:Step: 310, Training loss: 2.4418749809265137
INFO:agents.father_agent:Step: 315, Training loss: 2.915898084640503
INFO:agents.father_agent:Step: 320, Training loss: 3.397451162338257
INFO:agents.father_agent:Step: 325, Training loss: 2.473215103149414
INFO:agents.father_agent:Step: 330, Training loss: 2.07234787940979
INFO:agents.father_agent:Step: 335, Training loss: 2.4428954124450684
INFO:agents.father_agent:Step: 340, Training loss: 2.956813335418701
INFO:agents.father_agent:Step: 345, Training loss: 1.5982152223587036
INFO:agents.father_agent:Step: 350, Training loss: 2.2987356185913086
INFO:agents.father_agent:Step: 355, Training loss: 2.4115655422210693
INFO:agents.father_agent:Step: 360, Training loss: 1.2821694612503052
INFO:agents.father_agent:Step: 365, Training loss: 1.8690438270568848
INFO:agents.father_agent:Step: 370, Training loss: 2.044766426086426
INFO:agents.father_agent:Step: 375, Training loss: 1.911030650138855
INFO:agents.father_agent:Step: 380, Training loss: 1.7753751277923584
INFO:agents.father_agent:Step: 385, Training loss: 1.6494512557983398
INFO:agents.father_agent:Step: 390, Training loss: 1.2277222871780396
INFO:agents.father_agent:Step: 395, Training loss: 0.823540449142456
INFO:agents.father_agent:Step: 400, Training loss: 1.7572553157806396
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -26.13637351989746
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 53.86362838745117
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.116214752197266
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 442.71685791015625
INFO:tools.evaluation_results_class:Current Best Return = -26.13637351989746
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 63.80943811237753
INFO:tools.evaluation_results_class:Counted Episodes = 5001
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 0.9146985411643982
INFO:agents.father_agent:Step: 410, Training loss: 2.1802589893341064
INFO:agents.father_agent:Step: 415, Training loss: 1.700276494026184
INFO:agents.father_agent:Step: 420, Training loss: 1.8520243167877197
INFO:agents.father_agent:Step: 425, Training loss: 1.429463267326355
INFO:agents.father_agent:Step: 430, Training loss: 1.7113327980041504
INFO:agents.father_agent:Step: 435, Training loss: 1.9980788230895996
INFO:agents.father_agent:Step: 440, Training loss: 1.1029856204986572
INFO:agents.father_agent:Step: 445, Training loss: 1.1210592985153198
INFO:agents.father_agent:Step: 450, Training loss: 2.329023599624634
INFO:agents.father_agent:Step: 455, Training loss: 1.0843405723571777
INFO:agents.father_agent:Step: 460, Training loss: 0.9028455018997192
INFO:agents.father_agent:Step: 465, Training loss: 1.2193682193756104
INFO:agents.father_agent:Step: 470, Training loss: 1.5251128673553467
INFO:agents.father_agent:Step: 475, Training loss: 1.038661241531372
INFO:agents.father_agent:Step: 480, Training loss: 1.5240802764892578
INFO:agents.father_agent:Step: 485, Training loss: 0.7092015147209167
INFO:agents.father_agent:Step: 490, Training loss: 0.7121933102607727
INFO:agents.father_agent:Step: 495, Training loss: 1.4252054691314697
INFO:agents.father_agent:Step: 500, Training loss: 1.504014015197754
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -34.789955139160156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.210044860839844
INFO:tools.evaluation_results_class:Average Discounted Reward = 8.618751525878906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 334.34619140625
INFO:tools.evaluation_results_class:Current Best Return = -26.13637351989746
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.08158295281584
INFO:tools.evaluation_results_class:Counted Episodes = 3285
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 505, Training loss: 1.052660584449768
INFO:agents.father_agent:Step: 510, Training loss: 1.2574067115783691
INFO:agents.father_agent:Step: 515, Training loss: 1.0274244546890259
INFO:agents.father_agent:Step: 520, Training loss: 1.6159160137176514
INFO:agents.father_agent:Step: 525, Training loss: 0.7404887676239014
INFO:agents.father_agent:Step: 530, Training loss: 0.7449870705604553
INFO:agents.father_agent:Step: 535, Training loss: 0.8847125768661499
INFO:agents.father_agent:Step: 540, Training loss: 0.9414108991622925
INFO:agents.father_agent:Step: 545, Training loss: 0.5504487752914429
INFO:agents.father_agent:Step: 550, Training loss: 1.3383030891418457
INFO:agents.father_agent:Step: 555, Training loss: 0.737942636013031
INFO:agents.father_agent:Step: 560, Training loss: 0.8565859198570251
INFO:agents.father_agent:Step: 565, Training loss: 0.8770067691802979
INFO:agents.father_agent:Step: 570, Training loss: 0.8565278053283691
INFO:agents.father_agent:Step: 575, Training loss: 0.9571089744567871
INFO:agents.father_agent:Step: 580, Training loss: 0.7942892909049988
INFO:agents.father_agent:Step: 585, Training loss: 0.9803413152694702
INFO:agents.father_agent:Step: 590, Training loss: 0.5932608842849731
INFO:agents.father_agent:Step: 595, Training loss: 0.9356982111930847
INFO:agents.father_agent:Step: 600, Training loss: 1.204413652420044
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -31.593732833862305
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.40626525878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.714205741882324
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 275.10113525390625
INFO:tools.evaluation_results_class:Current Best Return = -26.13637351989746
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.7125340599455
INFO:tools.evaluation_results_class:Counted Episodes = 3670
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 605, Training loss: 1.0122188329696655
INFO:agents.father_agent:Step: 610, Training loss: 1.065401554107666
INFO:agents.father_agent:Step: 615, Training loss: 0.27576977014541626
INFO:agents.father_agent:Step: 620, Training loss: 1.1544914245605469
INFO:agents.father_agent:Step: 625, Training loss: 0.7790568470954895
INFO:agents.father_agent:Step: 630, Training loss: 0.8852759003639221
INFO:agents.father_agent:Step: 635, Training loss: 1.0465295314788818
INFO:agents.father_agent:Step: 640, Training loss: 1.4874463081359863
INFO:agents.father_agent:Step: 645, Training loss: 0.7171218395233154
INFO:agents.father_agent:Step: 650, Training loss: 1.2960959672927856
INFO:agents.father_agent:Step: 655, Training loss: 0.9284986853599548
INFO:agents.father_agent:Step: 660, Training loss: 0.33941197395324707
INFO:agents.father_agent:Step: 665, Training loss: 0.708771288394928
INFO:agents.father_agent:Step: 670, Training loss: 0.5613738894462585
INFO:agents.father_agent:Step: 675, Training loss: 0.7075448036193848
INFO:agents.father_agent:Step: 680, Training loss: 0.8208730816841125
INFO:agents.father_agent:Step: 685, Training loss: 1.0267164707183838
INFO:agents.father_agent:Step: 690, Training loss: 0.31068167090415955
INFO:agents.father_agent:Step: 695, Training loss: 0.8082998991012573
INFO:agents.father_agent:Step: 700, Training loss: 0.7705374956130981
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -34.85066223144531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.14933776855469
INFO:tools.evaluation_results_class:Average Discounted Reward = 8.296334266662598
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 279.3630676269531
INFO:tools.evaluation_results_class:Current Best Return = -26.13637351989746
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 95.34094415303919
INFO:tools.evaluation_results_class:Counted Episodes = 3241
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 705, Training loss: 0.6967599391937256
INFO:agents.father_agent:Step: 710, Training loss: 1.654590368270874
INFO:agents.father_agent:Step: 715, Training loss: 0.6645647287368774
INFO:agents.father_agent:Step: 720, Training loss: 0.5483865737915039
INFO:agents.father_agent:Step: 725, Training loss: 1.0535106658935547
INFO:agents.father_agent:Step: 730, Training loss: 0.7705332040786743
INFO:agents.father_agent:Step: 735, Training loss: 0.6243098378181458
INFO:agents.father_agent:Step: 740, Training loss: 1.6262378692626953
INFO:agents.father_agent:Step: 745, Training loss: 0.760197639465332
INFO:agents.father_agent:Step: 750, Training loss: 1.07864511013031
INFO:agents.father_agent:Step: 755, Training loss: 0.9427209496498108
INFO:agents.father_agent:Step: 760, Training loss: 0.761974036693573
INFO:agents.father_agent:Step: 765, Training loss: 1.0880228281021118
INFO:agents.father_agent:Step: 770, Training loss: 0.6979993581771851
INFO:agents.father_agent:Step: 775, Training loss: 1.0718857049942017
INFO:agents.father_agent:Step: 780, Training loss: 0.4456765055656433
INFO:agents.father_agent:Step: 785, Training loss: 0.7432185411453247
INFO:agents.father_agent:Step: 790, Training loss: 0.647698700428009
INFO:agents.father_agent:Step: 795, Training loss: 0.9301589131355286
INFO:agents.father_agent:Step: 800, Training loss: 0.8538400530815125
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -29.150049209594727
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.84994888305664
INFO:tools.evaluation_results_class:Average Discounted Reward = 16.11651039123535
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 251.94467163085938
INFO:tools.evaluation_results_class:Current Best Return = -26.13637351989746
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 79.10746492985972
INFO:tools.evaluation_results_class:Counted Episodes = 3992
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 805, Training loss: 1.470744252204895
INFO:agents.father_agent:Step: 810, Training loss: 0.9363499283790588
INFO:agents.father_agent:Step: 815, Training loss: 0.5063703656196594
INFO:agents.father_agent:Step: 820, Training loss: 0.9960811734199524
INFO:agents.father_agent:Step: 825, Training loss: 0.6032091975212097
INFO:agents.father_agent:Step: 830, Training loss: 0.4170677661895752
INFO:agents.father_agent:Step: 835, Training loss: 0.9318838119506836
INFO:agents.father_agent:Step: 840, Training loss: 1.1153855323791504
INFO:agents.father_agent:Step: 845, Training loss: 0.78374844789505
INFO:agents.father_agent:Step: 850, Training loss: 0.5924923419952393
INFO:agents.father_agent:Step: 855, Training loss: 0.8646931052207947
INFO:agents.father_agent:Step: 860, Training loss: 0.6485036611557007
INFO:agents.father_agent:Step: 865, Training loss: 0.44857290387153625
INFO:agents.father_agent:Step: 870, Training loss: 1.19978928565979
INFO:agents.father_agent:Step: 875, Training loss: 0.47636717557907104
INFO:agents.father_agent:Step: 880, Training loss: 0.9857136607170105
INFO:agents.father_agent:Step: 885, Training loss: 1.1088807582855225
INFO:agents.father_agent:Step: 890, Training loss: 0.3836032450199127
INFO:agents.father_agent:Step: 895, Training loss: 0.6290953159332275
INFO:agents.father_agent:Step: 900, Training loss: 0.7602649331092834
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -36.39439392089844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.60560607910156
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.525973796844482
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 307.1894836425781
INFO:tools.evaluation_results_class:Current Best Return = -26.13637351989746
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.32303281299777
INFO:tools.evaluation_results_class:Counted Episodes = 3139
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 905, Training loss: 1.0976206064224243
INFO:agents.father_agent:Step: 910, Training loss: 0.7035269737243652
INFO:agents.father_agent:Step: 915, Training loss: 1.1239290237426758
INFO:agents.father_agent:Step: 920, Training loss: 0.7741888165473938
INFO:agents.father_agent:Step: 925, Training loss: 1.0696840286254883
INFO:agents.father_agent:Step: 930, Training loss: 0.6892836093902588
INFO:agents.father_agent:Step: 935, Training loss: 0.20484063029289246
INFO:agents.father_agent:Step: 940, Training loss: 0.6319203972816467
INFO:agents.father_agent:Step: 945, Training loss: 1.504528522491455
INFO:agents.father_agent:Step: 950, Training loss: 0.4402570128440857
INFO:agents.father_agent:Step: 955, Training loss: 0.6177151203155518
INFO:agents.father_agent:Step: 960, Training loss: 0.4672442376613617
INFO:agents.father_agent:Step: 965, Training loss: 0.46701210737228394
INFO:agents.father_agent:Step: 970, Training loss: 0.4966133236885071
INFO:agents.father_agent:Step: 975, Training loss: 0.2723139524459839
INFO:agents.father_agent:Step: 980, Training loss: 0.5775010585784912
INFO:agents.father_agent:Step: 985, Training loss: 0.8618367910385132
INFO:agents.father_agent:Step: 990, Training loss: 0.4614229202270508
INFO:agents.father_agent:Step: 995, Training loss: 0.6436258554458618
INFO:agents.father_agent:Step: 1000, Training loss: 0.5245781540870667
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -33.04679870605469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.95320129394531
INFO:tools.evaluation_results_class:Average Discounted Reward = 10.486032485961914
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 241.2484588623047
INFO:tools.evaluation_results_class:Current Best Return = -26.13637351989746
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.23427902895584
INFO:tools.evaluation_results_class:Counted Episodes = 3419
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1005, Training loss: 0.76268470287323
INFO:agents.father_agent:Step: 1010, Training loss: 0.6529328227043152
INFO:agents.father_agent:Step: 1015, Training loss: 0.46688830852508545
INFO:agents.father_agent:Step: 1020, Training loss: 0.3231843411922455
INFO:agents.father_agent:Step: 1025, Training loss: 0.8091237545013428
INFO:agents.father_agent:Step: 1030, Training loss: 0.5071648359298706
INFO:agents.father_agent:Step: 1035, Training loss: 0.5254585146903992
INFO:agents.father_agent:Step: 1040, Training loss: 1.012128472328186
INFO:agents.father_agent:Step: 1045, Training loss: 0.8342239856719971
INFO:agents.father_agent:Step: 1050, Training loss: 0.6566317081451416
INFO:agents.father_agent:Step: 1055, Training loss: 0.6299517154693604
INFO:agents.father_agent:Step: 1060, Training loss: 0.3117360472679138
INFO:agents.father_agent:Step: 1065, Training loss: 0.9373279809951782
INFO:agents.father_agent:Step: 1070, Training loss: 0.6757712364196777
INFO:agents.father_agent:Step: 1075, Training loss: 0.5133483409881592
INFO:agents.father_agent:Step: 1080, Training loss: 0.4364948570728302
INFO:agents.father_agent:Step: 1085, Training loss: 1.135042667388916
INFO:agents.father_agent:Step: 1090, Training loss: 0.6302093267440796
INFO:agents.father_agent:Step: 1095, Training loss: 0.6698460578918457
INFO:agents.father_agent:Step: 1100, Training loss: 0.6668384075164795
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -31.289430618286133
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.710567474365234
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.941533088684082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 247.70973205566406
INFO:tools.evaluation_results_class:Current Best Return = -26.13637351989746
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.65636856368563
INFO:tools.evaluation_results_class:Counted Episodes = 3690
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1105, Training loss: 0.8580142259597778
INFO:agents.father_agent:Step: 1110, Training loss: 0.350737988948822
INFO:agents.father_agent:Step: 1115, Training loss: 1.0411157608032227
INFO:agents.father_agent:Step: 1120, Training loss: 0.29192882776260376
INFO:agents.father_agent:Step: 1125, Training loss: 0.9416301250457764
INFO:agents.father_agent:Step: 1130, Training loss: 0.5220678448677063
INFO:agents.father_agent:Step: 1135, Training loss: 0.769143283367157
INFO:agents.father_agent:Step: 1140, Training loss: 0.6724891066551208
INFO:agents.father_agent:Step: 1145, Training loss: 0.6516963243484497
INFO:agents.father_agent:Step: 1150, Training loss: 0.44208815693855286
INFO:agents.father_agent:Step: 1155, Training loss: 0.7073726654052734
INFO:agents.father_agent:Step: 1160, Training loss: 0.5382922887802124
INFO:agents.father_agent:Step: 1165, Training loss: 0.9732933640480042
INFO:agents.father_agent:Step: 1170, Training loss: 0.2520190477371216
INFO:agents.father_agent:Step: 1175, Training loss: 0.7121171951293945
INFO:agents.father_agent:Step: 1180, Training loss: 0.36728471517562866
INFO:agents.father_agent:Step: 1185, Training loss: 0.5438033938407898
INFO:agents.father_agent:Step: 1190, Training loss: 0.10005998611450195
INFO:agents.father_agent:Step: 1195, Training loss: 0.5065471529960632
INFO:agents.father_agent:Step: 1200, Training loss: 0.6741078495979309
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -31.40583610534668
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.59416198730469
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.937702178955078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 272.431884765625
INFO:tools.evaluation_results_class:Current Best Return = -26.13637351989746
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.21642799243448
INFO:tools.evaluation_results_class:Counted Episodes = 3701
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1205, Training loss: 0.869452714920044
INFO:agents.father_agent:Step: 1210, Training loss: 0.4420675039291382
INFO:agents.father_agent:Step: 1215, Training loss: 0.3154870569705963
INFO:agents.father_agent:Step: 1220, Training loss: 0.45126795768737793
INFO:agents.father_agent:Step: 1225, Training loss: 0.5870457291603088
INFO:agents.father_agent:Step: 1230, Training loss: 0.22024983167648315
INFO:agents.father_agent:Step: 1235, Training loss: 0.7151726484298706
INFO:agents.father_agent:Step: 1240, Training loss: 0.3985612094402313
INFO:agents.father_agent:Step: 1245, Training loss: 0.5529044270515442
INFO:agents.father_agent:Step: 1250, Training loss: 0.7520943880081177
INFO:agents.father_agent:Step: 1255, Training loss: 1.1062911748886108
INFO:agents.father_agent:Step: 1260, Training loss: 0.18297415971755981
INFO:agents.father_agent:Step: 1265, Training loss: 1.035939335823059
INFO:agents.father_agent:Step: 1270, Training loss: 0.45911285281181335
INFO:agents.father_agent:Step: 1275, Training loss: 0.38746902346611023
INFO:agents.father_agent:Step: 1280, Training loss: 0.5736092329025269
INFO:agents.father_agent:Step: 1285, Training loss: 0.9745641946792603
INFO:agents.father_agent:Step: 1290, Training loss: 0.36730852723121643
INFO:agents.father_agent:Step: 1295, Training loss: 0.8820961713790894
INFO:agents.father_agent:Step: 1300, Training loss: 0.6877776384353638
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -31.11370277404785
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.88629913330078
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.553126335144043
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 308.0260314941406
INFO:tools.evaluation_results_class:Current Best Return = -26.13637351989746
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.30002650410813
INFO:tools.evaluation_results_class:Counted Episodes = 3773
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1305, Training loss: 0.4376998841762543
INFO:agents.father_agent:Step: 1310, Training loss: 0.6756040453910828
INFO:agents.father_agent:Step: 1315, Training loss: 0.49504661560058594
INFO:agents.father_agent:Step: 1320, Training loss: 0.27816349267959595
INFO:agents.father_agent:Step: 1325, Training loss: 0.8467023372650146
INFO:agents.father_agent:Step: 1330, Training loss: 0.4299667775630951
INFO:agents.father_agent:Step: 1335, Training loss: 0.593269407749176
INFO:agents.father_agent:Step: 1340, Training loss: 0.7625445127487183
INFO:agents.father_agent:Step: 1345, Training loss: 0.6456772089004517
INFO:agents.father_agent:Step: 1350, Training loss: 0.2687259912490845
INFO:agents.father_agent:Step: 1355, Training loss: 0.8701675534248352
INFO:agents.father_agent:Step: 1360, Training loss: 0.6414409279823303
INFO:agents.father_agent:Step: 1365, Training loss: 0.709823727607727
INFO:agents.father_agent:Step: 1370, Training loss: 1.0095930099487305
INFO:agents.father_agent:Step: 1375, Training loss: 0.8265507221221924
INFO:agents.father_agent:Step: 1380, Training loss: 0.5970216989517212
INFO:agents.father_agent:Step: 1385, Training loss: 1.0955557823181152
INFO:agents.father_agent:Step: 1390, Training loss: 0.5684275031089783
INFO:agents.father_agent:Step: 1395, Training loss: 0.4691044092178345
INFO:agents.father_agent:Step: 1400, Training loss: 0.7203890681266785
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -31.388019561767578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.61198043823242
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.569693565368652
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 214.93051147460938
INFO:tools.evaluation_results_class:Current Best Return = -26.13637351989746
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.0052212146194
INFO:tools.evaluation_results_class:Counted Episodes = 3639
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -31.64809799194336
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.35190200805664
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.299276351928711
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 228.76585388183594
INFO:tools.evaluation_results_class:Current Best Return = -26.13637351989746
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.26696083838941
INFO:tools.evaluation_results_class:Counted Episodes = 3626
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -31.880685806274414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.0975227355957
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.064054489135742
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9997275946608554
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 245.91932678222656
INFO:tools.evaluation_results_class:Current Best Return = -31.880685806274414
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9997275946608554
INFO:tools.evaluation_results_class:Average Episode Length = 87.70798147643694
INFO:tools.evaluation_results_class:Counted Episodes = 3671
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.10845184326172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 29.89154624938965
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.5597752332687378
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1594.7034912109375
INFO:tools.evaluation_results_class:Current Best Return = -50.10845184326172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.28241369937483
INFO:tools.evaluation_results_class:Counted Episodes = 3679
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 120.6467 achieved after 804.0 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 130.7542 achieved after 804.01 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 146.5208 achieved after 804.02 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 147.0217 achieved after 804.04 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 147.354 achieved after 804.07 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 147.35403944280887
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:robust_rl.robust_rl_trainer:Iteration 2 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -86.97844696044922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -6.978444576263428
INFO:tools.evaluation_results_class:Average Discounted Reward = -34.959510803222656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3341.884033203125
INFO:tools.evaluation_results_class:Current Best Return = -26.13637351989746
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.42592087312414
INFO:tools.evaluation_results_class:Counted Episodes = 3665
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.567180156707764
INFO:agents.father_agent:Step: 5, Training loss: 1.8351260423660278
INFO:agents.father_agent:Step: 10, Training loss: 1.641623616218567
INFO:agents.father_agent:Step: 15, Training loss: 0.8677101731300354
INFO:agents.father_agent:Step: 20, Training loss: 0.6471579670906067
INFO:agents.father_agent:Step: 25, Training loss: 0.3719930648803711
INFO:agents.father_agent:Step: 30, Training loss: 0.4058099687099457
INFO:agents.father_agent:Step: 35, Training loss: 0.6723889708518982
INFO:agents.father_agent:Step: 40, Training loss: 0.7178596258163452
INFO:agents.father_agent:Step: 45, Training loss: 0.3692004680633545
INFO:agents.father_agent:Step: 50, Training loss: 0.5697238445281982
INFO:agents.father_agent:Step: 55, Training loss: 0.1695096492767334
INFO:agents.father_agent:Step: 60, Training loss: 0.8646591901779175
INFO:agents.father_agent:Step: 65, Training loss: 0.9094987511634827
INFO:agents.father_agent:Step: 70, Training loss: 0.7023414969444275
INFO:agents.father_agent:Step: 75, Training loss: 0.46521809697151184
INFO:agents.father_agent:Step: 80, Training loss: 0.7092491388320923
INFO:agents.father_agent:Step: 85, Training loss: 0.6899308562278748
INFO:agents.father_agent:Step: 90, Training loss: 0.6179941296577454
INFO:agents.father_agent:Step: 95, Training loss: 0.7482850551605225
INFO:agents.father_agent:Step: 100, Training loss: 0.7549356818199158
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -28.69614028930664
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 51.30385971069336
INFO:tools.evaluation_results_class:Average Discounted Reward = 18.80882453918457
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 468.80517578125
INFO:tools.evaluation_results_class:Current Best Return = -26.13637351989746
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 70.28974641675855
INFO:tools.evaluation_results_class:Counted Episodes = 4535
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 1.1068159341812134
INFO:agents.father_agent:Step: 110, Training loss: 1.0421234369277954
INFO:agents.father_agent:Step: 115, Training loss: 0.6439429521560669
INFO:agents.father_agent:Step: 120, Training loss: 0.46966230869293213
INFO:agents.father_agent:Step: 125, Training loss: 0.6064272522926331
INFO:agents.father_agent:Step: 130, Training loss: 0.5265712141990662
INFO:agents.father_agent:Step: 135, Training loss: 0.5014077425003052
INFO:agents.father_agent:Step: 140, Training loss: 0.9797109365463257
INFO:agents.father_agent:Step: 145, Training loss: 1.039454460144043
INFO:agents.father_agent:Step: 150, Training loss: 0.9312633872032166
INFO:agents.father_agent:Step: 155, Training loss: 0.909500241279602
INFO:agents.father_agent:Step: 160, Training loss: 0.24888183176517487
INFO:agents.father_agent:Step: 165, Training loss: 0.7422457337379456
INFO:agents.father_agent:Step: 170, Training loss: 0.5762810707092285
INFO:agents.father_agent:Step: 175, Training loss: 0.9858123064041138
INFO:agents.father_agent:Step: 180, Training loss: 0.5128993391990662
INFO:agents.father_agent:Step: 185, Training loss: 0.5827155113220215
INFO:agents.father_agent:Step: 190, Training loss: 0.2591018080711365
INFO:agents.father_agent:Step: 195, Training loss: 0.5452839136123657
INFO:agents.father_agent:Step: 200, Training loss: 0.5930113792419434
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -25.66155433654785
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.324344635009766
INFO:tools.evaluation_results_class:Average Discounted Reward = 25.69943618774414
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9998237264234091
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 598.9163208007812
INFO:tools.evaluation_results_class:Current Best Return = -25.66155433654785
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 56.751277983430285
INFO:tools.evaluation_results_class:Counted Episodes = 5673
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 1.3941993713378906
INFO:agents.father_agent:Step: 210, Training loss: 0.77767413854599
INFO:agents.father_agent:Step: 215, Training loss: 0.3105565309524536
INFO:agents.father_agent:Step: 220, Training loss: 0.5685692429542542
INFO:agents.father_agent:Step: 225, Training loss: 0.5864629745483398
INFO:agents.father_agent:Step: 230, Training loss: 0.38908320665359497
INFO:agents.father_agent:Step: 235, Training loss: 0.9326236844062805
INFO:agents.father_agent:Step: 240, Training loss: 0.5957992076873779
INFO:agents.father_agent:Step: 245, Training loss: 0.7531166672706604
INFO:agents.father_agent:Step: 250, Training loss: 0.8481775522232056
INFO:agents.father_agent:Step: 255, Training loss: 1.0042146444320679
INFO:agents.father_agent:Step: 260, Training loss: 0.66734379529953
INFO:agents.father_agent:Step: 265, Training loss: 0.5928881168365479
INFO:agents.father_agent:Step: 270, Training loss: 0.6014263033866882
INFO:agents.father_agent:Step: 275, Training loss: 0.3716817796230316
INFO:agents.father_agent:Step: 280, Training loss: 0.9362181425094604
INFO:agents.father_agent:Step: 285, Training loss: 0.8758100867271423
INFO:agents.father_agent:Step: 290, Training loss: 0.6909381747245789
INFO:agents.father_agent:Step: 295, Training loss: 0.656492292881012
INFO:agents.father_agent:Step: 300, Training loss: 0.7839717268943787
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -25.038421630859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.961578369140625
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.987564086914062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 605.3135375976562
INFO:tools.evaluation_results_class:Current Best Return = -25.038421630859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.58039945836154
INFO:tools.evaluation_results_class:Counted Episodes = 5908
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.2725039720535278
INFO:agents.father_agent:Step: 310, Training loss: 1.5029362440109253
INFO:agents.father_agent:Step: 315, Training loss: 0.44823944568634033
INFO:agents.father_agent:Step: 320, Training loss: 0.3962676525115967
INFO:agents.father_agent:Step: 325, Training loss: 0.9192372560501099
INFO:agents.father_agent:Step: 330, Training loss: 1.06813645362854
INFO:agents.father_agent:Step: 335, Training loss: 0.6562293767929077
INFO:agents.father_agent:Step: 340, Training loss: 0.7201664447784424
INFO:agents.father_agent:Step: 345, Training loss: 0.5635547637939453
INFO:agents.father_agent:Step: 350, Training loss: 0.5497996807098389
INFO:agents.father_agent:Step: 355, Training loss: 0.8939631581306458
INFO:agents.father_agent:Step: 360, Training loss: 0.8597396016120911
INFO:agents.father_agent:Step: 365, Training loss: 1.1154381036758423
INFO:agents.father_agent:Step: 370, Training loss: 1.080445647239685
INFO:agents.father_agent:Step: 375, Training loss: 0.8049513101577759
INFO:agents.father_agent:Step: 380, Training loss: 0.6782037019729614
INFO:agents.father_agent:Step: 385, Training loss: 1.065622091293335
INFO:agents.father_agent:Step: 390, Training loss: 1.2369863986968994
INFO:agents.father_agent:Step: 395, Training loss: 0.65807044506073
INFO:agents.father_agent:Step: 400, Training loss: 0.9744332432746887
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -23.299257278442383
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.700740814208984
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.815153121948242
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 560.06787109375
INFO:tools.evaluation_results_class:Current Best Return = -23.299257278442383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.851951335124035
INFO:tools.evaluation_results_class:Counted Episodes = 6329
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.0725927352905273
INFO:agents.father_agent:Step: 410, Training loss: 0.9634861946105957
INFO:agents.father_agent:Step: 415, Training loss: 0.9203646779060364
INFO:agents.father_agent:Step: 420, Training loss: 0.9433776140213013
INFO:agents.father_agent:Step: 425, Training loss: 0.828937828540802
INFO:agents.father_agent:Step: 430, Training loss: 0.5436557531356812
INFO:agents.father_agent:Step: 435, Training loss: 1.284661889076233
INFO:agents.father_agent:Step: 440, Training loss: 0.9171848893165588
INFO:agents.father_agent:Step: 445, Training loss: 0.9509739279747009
INFO:agents.father_agent:Step: 450, Training loss: 0.5793873071670532
INFO:agents.father_agent:Step: 455, Training loss: 1.0647268295288086
INFO:agents.father_agent:Step: 460, Training loss: 0.5602774024009705
INFO:agents.father_agent:Step: 465, Training loss: 0.6111209392547607
INFO:agents.father_agent:Step: 470, Training loss: 0.8091838955879211
INFO:agents.father_agent:Step: 475, Training loss: 0.7993966341018677
INFO:agents.father_agent:Step: 480, Training loss: 0.6414588689804077
INFO:agents.father_agent:Step: 485, Training loss: 0.608406126499176
INFO:agents.father_agent:Step: 490, Training loss: 0.7101970911026001
INFO:agents.father_agent:Step: 495, Training loss: 0.8781863451004028
INFO:agents.father_agent:Step: 500, Training loss: 0.7071856260299683
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -23.910497665405273
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.08950424194336
INFO:tools.evaluation_results_class:Average Discounted Reward = 28.447317123413086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 548.685302734375
INFO:tools.evaluation_results_class:Current Best Return = -23.299257278442383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.1250411319513
INFO:tools.evaluation_results_class:Counted Episodes = 6078
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -24.34024429321289
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.65975570678711
INFO:tools.evaluation_results_class:Average Discounted Reward = 28.122146606445312
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 590.0142211914062
INFO:tools.evaluation_results_class:Current Best Return = -23.299257278442383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.03224744981902
INFO:tools.evaluation_results_class:Counted Episodes = 6078
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -24.324840545654297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.6751594543457
INFO:tools.evaluation_results_class:Average Discounted Reward = 28.047821044921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 583.7374877929688
INFO:tools.evaluation_results_class:Current Best Return = -24.324840545654297
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.314388371713214
INFO:tools.evaluation_results_class:Counted Episodes = 6123
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.86067962646484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 7.139317035675049
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.461804389953613
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4035.777587890625
INFO:tools.evaluation_results_class:Current Best Return = -72.86067962646484
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.729403884795715
INFO:tools.evaluation_results_class:Counted Episodes = 5972
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 61.2941 achieved after 1252.75 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 68.7342 achieved after 1252.78 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 75.4439 achieved after 1252.8 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 75.9595 achieved after 1252.81 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 82.4796 achieved after 1252.86 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 83.9458 achieved after 1252.89 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 91.0734 achieved after 1252.91 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=8
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 91.07340897558599
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
INFO:robust_rl.robust_rl_trainer:Iteration 3 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -25.037616729736328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.96238327026367
INFO:tools.evaluation_results_class:Average Discounted Reward = 27.593801498413086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 644.015869140625
INFO:tools.evaluation_results_class:Current Best Return = -23.299257278442383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.14073585216961
INFO:tools.evaluation_results_class:Counted Episodes = 6061
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.4464908540248871
INFO:agents.father_agent:Step: 5, Training loss: 0.7412898540496826
INFO:agents.father_agent:Step: 10, Training loss: 1.0807381868362427
INFO:agents.father_agent:Step: 15, Training loss: 0.5006498098373413
INFO:agents.father_agent:Step: 20, Training loss: 1.0337011814117432
INFO:agents.father_agent:Step: 25, Training loss: 0.6768626570701599
INFO:agents.father_agent:Step: 30, Training loss: 0.49160921573638916
INFO:agents.father_agent:Step: 35, Training loss: 1.0131884813308716
INFO:agents.father_agent:Step: 40, Training loss: 1.2320114374160767
INFO:agents.father_agent:Step: 45, Training loss: 0.6175118684768677
INFO:agents.father_agent:Step: 50, Training loss: 1.241675853729248
INFO:agents.father_agent:Step: 55, Training loss: 0.8679813146591187
INFO:agents.father_agent:Step: 60, Training loss: 0.6197696924209595
INFO:agents.father_agent:Step: 65, Training loss: 0.7700261473655701
INFO:agents.father_agent:Step: 70, Training loss: 0.8634192943572998
INFO:agents.father_agent:Step: 75, Training loss: 1.1275973320007324
INFO:agents.father_agent:Step: 80, Training loss: 1.3425754308700562
INFO:agents.father_agent:Step: 85, Training loss: 0.8996008634567261
INFO:agents.father_agent:Step: 90, Training loss: 1.0311411619186401
INFO:agents.father_agent:Step: 95, Training loss: 1.0256887674331665
INFO:agents.father_agent:Step: 100, Training loss: 1.035688877105713
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -23.92400360107422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.07599639892578
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.34345054626465
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 677.6021728515625
INFO:tools.evaluation_results_class:Current Best Return = -23.299257278442383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.52791572610986
INFO:tools.evaluation_results_class:Counted Episodes = 6645
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 1.1678965091705322
INFO:agents.father_agent:Step: 110, Training loss: 1.0833617448806763
INFO:agents.father_agent:Step: 115, Training loss: 0.9810499548912048
INFO:agents.father_agent:Step: 120, Training loss: 1.0723971128463745
INFO:agents.father_agent:Step: 125, Training loss: 0.9046056270599365
INFO:agents.father_agent:Step: 130, Training loss: 0.8081060647964478
INFO:agents.father_agent:Step: 135, Training loss: 0.9142946600914001
INFO:agents.father_agent:Step: 140, Training loss: 1.1082266569137573
INFO:agents.father_agent:Step: 145, Training loss: 0.5155843496322632
INFO:agents.father_agent:Step: 150, Training loss: 0.2418418824672699
INFO:agents.father_agent:Step: 155, Training loss: 0.7488009929656982
INFO:agents.father_agent:Step: 160, Training loss: 1.2384079694747925
INFO:agents.father_agent:Step: 165, Training loss: 0.8719473481178284
INFO:agents.father_agent:Step: 170, Training loss: 0.6995364427566528
INFO:agents.father_agent:Step: 175, Training loss: 0.9330131411552429
INFO:agents.father_agent:Step: 180, Training loss: 0.39401060342788696
INFO:agents.father_agent:Step: 185, Training loss: 0.7549097537994385
INFO:agents.father_agent:Step: 190, Training loss: 0.8467925786972046
INFO:agents.father_agent:Step: 195, Training loss: 1.1819243431091309
INFO:agents.father_agent:Step: 200, Training loss: 1.008623480796814
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -23.758153915405273
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.24184799194336
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.33767318725586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 724.0811157226562
INFO:tools.evaluation_results_class:Current Best Return = -23.299257278442383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.568578916715204
INFO:tools.evaluation_results_class:Counted Episodes = 6868
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 0.8074364066123962
INFO:agents.father_agent:Step: 210, Training loss: 0.5835699439048767
INFO:agents.father_agent:Step: 215, Training loss: 1.116683006286621
INFO:agents.father_agent:Step: 220, Training loss: 0.5303449034690857
INFO:agents.father_agent:Step: 225, Training loss: 0.9351117610931396
INFO:agents.father_agent:Step: 230, Training loss: 0.8765801787376404
INFO:agents.father_agent:Step: 235, Training loss: 0.9609589576721191
INFO:agents.father_agent:Step: 240, Training loss: 1.3208160400390625
INFO:agents.father_agent:Step: 245, Training loss: 1.5998022556304932
INFO:agents.father_agent:Step: 250, Training loss: 1.412240982055664
INFO:agents.father_agent:Step: 255, Training loss: 1.432714581489563
INFO:agents.father_agent:Step: 260, Training loss: 1.3079931735992432
INFO:agents.father_agent:Step: 265, Training loss: 0.5210921764373779
INFO:agents.father_agent:Step: 270, Training loss: 0.570036768913269
INFO:agents.father_agent:Step: 275, Training loss: 0.4561464190483093
INFO:agents.father_agent:Step: 280, Training loss: 1.1038728952407837
INFO:agents.father_agent:Step: 285, Training loss: 0.9219238758087158
INFO:agents.father_agent:Step: 290, Training loss: 0.9162325859069824
INFO:agents.father_agent:Step: 295, Training loss: 0.7513157725334167
INFO:agents.father_agent:Step: 300, Training loss: 1.5815917253494263
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -27.23635482788086
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 52.28292465209961
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.977209091186523
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9939909864797196
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1315.669189453125
INFO:tools.evaluation_results_class:Current Best Return = -23.299257278442383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.48022033049574
INFO:tools.evaluation_results_class:Counted Episodes = 3994
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.598863124847412
INFO:agents.father_agent:Step: 310, Training loss: 0.7916561365127563
INFO:agents.father_agent:Step: 315, Training loss: 0.8034625053405762
INFO:agents.father_agent:Step: 320, Training loss: 0.5581011772155762
INFO:agents.father_agent:Step: 325, Training loss: 0.9031638503074646
INFO:agents.father_agent:Step: 330, Training loss: 0.5729680061340332
INFO:agents.father_agent:Step: 335, Training loss: 0.3543500304222107
INFO:agents.father_agent:Step: 340, Training loss: 1.239853024482727
INFO:agents.father_agent:Step: 345, Training loss: 1.1505720615386963
INFO:agents.father_agent:Step: 350, Training loss: 1.1083961725234985
INFO:agents.father_agent:Step: 355, Training loss: 0.9599428176879883
INFO:agents.father_agent:Step: 360, Training loss: 0.8705167770385742
INFO:agents.father_agent:Step: 365, Training loss: 0.7203623652458191
INFO:agents.father_agent:Step: 370, Training loss: 0.8730189204216003
INFO:agents.father_agent:Step: 375, Training loss: 0.8046340942382812
INFO:agents.father_agent:Step: 380, Training loss: 0.7871213555335999
INFO:agents.father_agent:Step: 385, Training loss: 0.8024860620498657
INFO:agents.father_agent:Step: 390, Training loss: 0.6715999841690063
INFO:agents.father_agent:Step: 395, Training loss: 0.761167585849762
INFO:agents.father_agent:Step: 400, Training loss: 0.8546303510665894
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -26.153162002563477
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 53.482887268066406
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.727699279785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9954506065857885
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1108.7037353515625
INFO:tools.evaluation_results_class:Current Best Return = -23.299257278442383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.72140381282496
INFO:tools.evaluation_results_class:Counted Episodes = 4616
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.2395069599151611
INFO:agents.father_agent:Step: 410, Training loss: 1.1354769468307495
INFO:agents.father_agent:Step: 415, Training loss: 0.6395463943481445
INFO:agents.father_agent:Step: 420, Training loss: 0.9393300414085388
INFO:agents.father_agent:Step: 425, Training loss: 1.1783838272094727
INFO:agents.father_agent:Step: 430, Training loss: 1.1829382181167603
INFO:agents.father_agent:Step: 435, Training loss: 1.102807879447937
INFO:agents.father_agent:Step: 440, Training loss: 1.2554253339767456
INFO:agents.father_agent:Step: 445, Training loss: 1.0890918970108032
INFO:agents.father_agent:Step: 450, Training loss: 1.088565468788147
INFO:agents.father_agent:Step: 455, Training loss: 0.6459505558013916
INFO:agents.father_agent:Step: 460, Training loss: 0.9613896608352661
INFO:agents.father_agent:Step: 465, Training loss: 0.389210969209671
INFO:agents.father_agent:Step: 470, Training loss: 0.7300471067428589
INFO:agents.father_agent:Step: 475, Training loss: 0.869290292263031
INFO:agents.father_agent:Step: 480, Training loss: 1.1591992378234863
INFO:agents.father_agent:Step: 485, Training loss: 1.4057284593582153
INFO:agents.father_agent:Step: 490, Training loss: 1.1392015218734741
INFO:agents.father_agent:Step: 495, Training loss: 0.825799286365509
INFO:agents.father_agent:Step: 500, Training loss: 0.9348001480102539
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -23.48757553100586
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.51242446899414
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.73406982421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 717.0634155273438
INFO:tools.evaluation_results_class:Current Best Return = -23.299257278442383
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.63522103438313
INFO:tools.evaluation_results_class:Counted Episodes = 6922
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -23.18852424621582
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.81147766113281
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.05842590332031
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 714.8699340820312
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.32067816258513
INFO:tools.evaluation_results_class:Counted Episodes = 6901
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -23.850080490112305
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.09175491333008
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.38721466064453
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9992729387814454
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 787.2034912109375
INFO:tools.evaluation_results_class:Current Best Return = -23.850080490112305
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9992729387814454
INFO:tools.evaluation_results_class:Average Episode Length = 47.4368183801076
INFO:tools.evaluation_results_class:Counted Episodes = 6877
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -83.43518829345703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -3.4351866245269775
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.15460777282715
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4117.5830078125
INFO:tools.evaluation_results_class:Current Best Return = -83.43518829345703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.78364565587734
INFO:tools.evaluation_results_class:Counted Episodes = 6457
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 70.5217 achieved after 1722.53 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 73.8971 achieved after 1722.55 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 74.209 achieved after 1722.56 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 74.5745 achieved after 1722.57 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 80.5753 achieved after 1722.58 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 81.6129 achieved after 1722.64 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 86.7879 achieved after 1722.65 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 88.314 achieved after 1722.68 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 89.7042 achieved after 1722.7 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 94.1527 achieved after 1722.71 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 94.15272224787209
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:robust_rl.robust_rl_trainer:Iteration 4 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -23.667293548583984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.332706451416016
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.443340301513672
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 726.0284423828125
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.406914508896286
INFO:tools.evaluation_results_class:Counted Episodes = 6913
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.6465719938278198
INFO:agents.father_agent:Step: 5, Training loss: 1.120727300643921
INFO:agents.father_agent:Step: 10, Training loss: 0.8180440664291382
INFO:agents.father_agent:Step: 15, Training loss: 0.7036291360855103
INFO:agents.father_agent:Step: 20, Training loss: 0.559269368648529
INFO:agents.father_agent:Step: 25, Training loss: 0.4500480890274048
INFO:agents.father_agent:Step: 30, Training loss: 1.3255894184112549
INFO:agents.father_agent:Step: 35, Training loss: 0.960817813873291
INFO:agents.father_agent:Step: 40, Training loss: 0.5653374791145325
INFO:agents.father_agent:Step: 45, Training loss: 0.47757744789123535
INFO:agents.father_agent:Step: 50, Training loss: 0.7281105518341064
INFO:agents.father_agent:Step: 55, Training loss: 0.5671036243438721
INFO:agents.father_agent:Step: 60, Training loss: 0.6859616637229919
INFO:agents.father_agent:Step: 65, Training loss: 1.1464649438858032
INFO:agents.father_agent:Step: 70, Training loss: 0.9136024117469788
INFO:agents.father_agent:Step: 75, Training loss: 0.8283168077468872
INFO:agents.father_agent:Step: 80, Training loss: 1.1652339696884155
INFO:agents.father_agent:Step: 85, Training loss: 1.0558949708938599
INFO:agents.father_agent:Step: 90, Training loss: 1.088740587234497
INFO:agents.father_agent:Step: 95, Training loss: 1.2382893562316895
INFO:agents.father_agent:Step: 100, Training loss: 0.47396886348724365
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -23.95132064819336
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.04867935180664
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.526233673095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 761.75390625
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 46.05327398047099
INFO:tools.evaluation_results_class:Counted Episodes = 6964
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 0.7536150217056274
INFO:agents.father_agent:Step: 110, Training loss: 0.5341546535491943
INFO:agents.father_agent:Step: 115, Training loss: 1.0933672189712524
INFO:agents.father_agent:Step: 120, Training loss: 0.7201458215713501
INFO:agents.father_agent:Step: 125, Training loss: 0.5634926557540894
INFO:agents.father_agent:Step: 130, Training loss: 0.8219398856163025
INFO:agents.father_agent:Step: 135, Training loss: 1.2362189292907715
INFO:agents.father_agent:Step: 140, Training loss: 1.4508157968521118
INFO:agents.father_agent:Step: 145, Training loss: 0.8161950707435608
INFO:agents.father_agent:Step: 150, Training loss: 0.8143762946128845
INFO:agents.father_agent:Step: 155, Training loss: 1.1878743171691895
INFO:agents.father_agent:Step: 160, Training loss: 0.7141710519790649
INFO:agents.father_agent:Step: 165, Training loss: 0.6915185451507568
INFO:agents.father_agent:Step: 170, Training loss: 1.0188506841659546
INFO:agents.father_agent:Step: 175, Training loss: 0.5822637677192688
INFO:agents.father_agent:Step: 180, Training loss: 0.8167675137519836
INFO:agents.father_agent:Step: 185, Training loss: 1.0817453861236572
INFO:agents.father_agent:Step: 190, Training loss: 0.7198036909103394
INFO:agents.father_agent:Step: 195, Training loss: 0.5788917541503906
INFO:agents.father_agent:Step: 200, Training loss: 1.2996922731399536
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -26.158281326293945
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 53.52928924560547
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.473186492919922
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9960946473696302
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1106.8780517578125
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.964162646450724
INFO:tools.evaluation_results_class:Counted Episodes = 4353
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 0.714532196521759
INFO:agents.father_agent:Step: 210, Training loss: 1.1473084688186646
INFO:agents.father_agent:Step: 215, Training loss: 0.8071632385253906
INFO:agents.father_agent:Step: 220, Training loss: 0.6427500247955322
INFO:agents.father_agent:Step: 225, Training loss: 1.1992504596710205
INFO:agents.father_agent:Step: 230, Training loss: 0.7205471992492676
INFO:agents.father_agent:Step: 235, Training loss: 0.7938882112503052
INFO:agents.father_agent:Step: 240, Training loss: 1.0574240684509277
INFO:agents.father_agent:Step: 245, Training loss: 0.4819030165672302
INFO:agents.father_agent:Step: 250, Training loss: 0.7174651026725769
INFO:agents.father_agent:Step: 255, Training loss: 1.2867118120193481
INFO:agents.father_agent:Step: 260, Training loss: 1.0590895414352417
INFO:agents.father_agent:Step: 265, Training loss: 0.6275410056114197
INFO:agents.father_agent:Step: 270, Training loss: 0.9928041100502014
INFO:agents.father_agent:Step: 275, Training loss: 0.6903204321861267
INFO:agents.father_agent:Step: 280, Training loss: 1.17044997215271
INFO:agents.father_agent:Step: 285, Training loss: 0.6923427581787109
INFO:agents.father_agent:Step: 290, Training loss: 0.9465541243553162
INFO:agents.father_agent:Step: 295, Training loss: 0.5070623159408569
INFO:agents.father_agent:Step: 300, Training loss: 0.7689145803451538
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -32.27606201171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 47.18811798095703
INFO:tools.evaluation_results_class:Average Discounted Reward = 25.772146224975586
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9933022714036109
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1674.203857421875
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 68.86575422248107
INFO:tools.evaluation_results_class:Counted Episodes = 3434
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 0.8644077777862549
INFO:agents.father_agent:Step: 310, Training loss: 0.4983742833137512
INFO:agents.father_agent:Step: 315, Training loss: 0.7794856429100037
INFO:agents.father_agent:Step: 320, Training loss: 0.7326577305793762
INFO:agents.father_agent:Step: 325, Training loss: 1.008626937866211
INFO:agents.father_agent:Step: 330, Training loss: 0.8219413161277771
INFO:agents.father_agent:Step: 335, Training loss: 1.017004132270813
INFO:agents.father_agent:Step: 340, Training loss: 0.8817166686058044
INFO:agents.father_agent:Step: 345, Training loss: 0.5239824056625366
INFO:agents.father_agent:Step: 350, Training loss: 1.0225470066070557
INFO:agents.father_agent:Step: 355, Training loss: 0.9047946929931641
INFO:agents.father_agent:Step: 360, Training loss: 0.6764410734176636
INFO:agents.father_agent:Step: 365, Training loss: 0.4369845390319824
INFO:agents.father_agent:Step: 370, Training loss: 1.0076727867126465
INFO:agents.father_agent:Step: 375, Training loss: 0.5021458864212036
INFO:agents.father_agent:Step: 380, Training loss: 0.916817307472229
INFO:agents.father_agent:Step: 385, Training loss: 1.0800046920776367
INFO:agents.father_agent:Step: 390, Training loss: 0.5554584264755249
INFO:agents.father_agent:Step: 395, Training loss: 0.486506849527359
INFO:agents.father_agent:Step: 400, Training loss: 1.2844289541244507
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -57.35984802246094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 15.306818008422852
INFO:tools.evaluation_results_class:Average Discounted Reward = 15.718717575073242
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9083333333333333
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5431.3349609375
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 144.19242424242424
INFO:tools.evaluation_results_class:Counted Episodes = 1320
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.1243840456008911
INFO:agents.father_agent:Step: 410, Training loss: 0.6128526926040649
INFO:agents.father_agent:Step: 415, Training loss: 0.89003986120224
INFO:agents.father_agent:Step: 420, Training loss: 0.46624186635017395
INFO:agents.father_agent:Step: 425, Training loss: 0.9054962396621704
INFO:agents.father_agent:Step: 430, Training loss: 0.46546024084091187
INFO:agents.father_agent:Step: 435, Training loss: 1.0547610521316528
INFO:agents.father_agent:Step: 440, Training loss: 1.2526638507843018
INFO:agents.father_agent:Step: 445, Training loss: 0.6718364953994751
INFO:agents.father_agent:Step: 450, Training loss: 0.8717169165611267
INFO:agents.father_agent:Step: 455, Training loss: 0.6334988474845886
INFO:agents.father_agent:Step: 460, Training loss: 1.0838364362716675
INFO:agents.father_agent:Step: 465, Training loss: 0.6588353514671326
INFO:agents.father_agent:Step: 470, Training loss: 0.8632464408874512
INFO:agents.father_agent:Step: 475, Training loss: 0.4888131320476532
INFO:agents.father_agent:Step: 480, Training loss: 0.5878636837005615
INFO:agents.father_agent:Step: 485, Training loss: 1.388835072517395
INFO:agents.father_agent:Step: 490, Training loss: 0.3496951460838318
INFO:agents.father_agent:Step: 495, Training loss: 0.7568030953407288
INFO:agents.father_agent:Step: 500, Training loss: 0.8286377191543579
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -25.210142135620117
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.643924713134766
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.05341339111328
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9981758482305728
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 961.2174682617188
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.639182780007296
INFO:tools.evaluation_results_class:Counted Episodes = 5482
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -24.983877182006836
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.83821487426758
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.229721069335938
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9977761304670126
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 933.421630859375
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.06356560415122
INFO:tools.evaluation_results_class:Counted Episodes = 5396
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -28.172704696655273
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.63677215576172
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.990114212036133
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9851184648521637
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1553.860595703125
INFO:tools.evaluation_results_class:Current Best Return = -28.172704696655273
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9851184648521637
INFO:tools.evaluation_results_class:Average Episode Length = 59.57450558057568
INFO:tools.evaluation_results_class:Counted Episodes = 5107
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -87.16053009033203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -7.160531520843506
INFO:tools.evaluation_results_class:Average Discounted Reward = -25.396015167236328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3728.7158203125
INFO:tools.evaluation_results_class:Current Best Return = -87.16053009033203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 61.81821631878558
INFO:tools.evaluation_results_class:Counted Episodes = 5270
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 72.3182 achieved after 2211.78 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 78.8636 achieved after 2211.78 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 79.5758 achieved after 2211.81 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 83.3718 achieved after 2211.81 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 88.7873 achieved after 2211.83 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 93.1171 achieved after 2211.9 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 93.1658 achieved after 2211.94 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 94.6626 achieved after 2211.95 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 99.0814 achieved after 2211.97 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 99.08138628252831
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
INFO:robust_rl.robust_rl_trainer:Iteration 5 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -24.360546112060547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.53601837158203
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.586915969848633
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9987070557813077
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 894.434814453125
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 48.82619135574436
INFO:tools.evaluation_results_class:Counted Episodes = 5414
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.7005826830863953
INFO:agents.father_agent:Step: 5, Training loss: 0.5820774435997009
INFO:agents.father_agent:Step: 10, Training loss: 1.0505034923553467
INFO:agents.father_agent:Step: 15, Training loss: 0.318267822265625
INFO:agents.father_agent:Step: 20, Training loss: 0.7283228635787964
INFO:agents.father_agent:Step: 25, Training loss: 0.8791389465332031
INFO:agents.father_agent:Step: 30, Training loss: 0.7360301613807678
INFO:agents.father_agent:Step: 35, Training loss: 0.4678735136985779
INFO:agents.father_agent:Step: 40, Training loss: 0.8219345808029175
INFO:agents.father_agent:Step: 45, Training loss: 0.47149181365966797
INFO:agents.father_agent:Step: 50, Training loss: 0.8137354254722595
INFO:agents.father_agent:Step: 55, Training loss: 0.6014567613601685
INFO:agents.father_agent:Step: 60, Training loss: 1.0490881204605103
INFO:agents.father_agent:Step: 65, Training loss: 0.8493905067443848
INFO:agents.father_agent:Step: 70, Training loss: 0.6297627687454224
INFO:agents.father_agent:Step: 75, Training loss: 0.637319803237915
INFO:agents.father_agent:Step: 80, Training loss: 0.9415143728256226
INFO:agents.father_agent:Step: 85, Training loss: 0.6837412118911743
INFO:agents.father_agent:Step: 90, Training loss: 0.508483350276947
INFO:agents.father_agent:Step: 95, Training loss: 0.9208418726921082
INFO:agents.father_agent:Step: 100, Training loss: 0.6874780654907227
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -33.106712341308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 45.70912170410156
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.132057189941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.985197934595525
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1981.8702392578125
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 75.02616179001721
INFO:tools.evaluation_results_class:Counted Episodes = 2905
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 1.5342859029769897
INFO:agents.father_agent:Step: 110, Training loss: 0.39249101281166077
INFO:agents.father_agent:Step: 115, Training loss: 0.5525894165039062
INFO:agents.father_agent:Step: 120, Training loss: 0.380148708820343
INFO:agents.father_agent:Step: 125, Training loss: 0.4317968785762787
INFO:agents.father_agent:Step: 130, Training loss: 0.14820392429828644
INFO:agents.father_agent:Step: 135, Training loss: 0.40517914295196533
INFO:agents.father_agent:Step: 140, Training loss: 0.6927687525749207
INFO:agents.father_agent:Step: 145, Training loss: 0.6225085258483887
INFO:agents.father_agent:Step: 150, Training loss: 0.821397602558136
INFO:agents.father_agent:Step: 155, Training loss: 0.641179621219635
INFO:agents.father_agent:Step: 160, Training loss: 0.818541407585144
INFO:agents.father_agent:Step: 165, Training loss: 0.722947895526886
INFO:agents.father_agent:Step: 170, Training loss: 0.9072346687316895
INFO:agents.father_agent:Step: 175, Training loss: 0.38904792070388794
INFO:agents.father_agent:Step: 180, Training loss: 0.41409027576446533
INFO:agents.father_agent:Step: 185, Training loss: 1.1321165561676025
INFO:agents.father_agent:Step: 190, Training loss: 0.8144036531448364
INFO:agents.father_agent:Step: 195, Training loss: 0.6481221914291382
INFO:agents.father_agent:Step: 200, Training loss: 1.1192846298217773
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -27.08980941772461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 52.436466217041016
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.816251754760742
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9940784603997039
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1245.0875244140625
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 59.64569454724895
INFO:tools.evaluation_results_class:Counted Episodes = 4053
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 1.1663670539855957
INFO:agents.father_agent:Step: 210, Training loss: 0.33920276165008545
INFO:agents.father_agent:Step: 215, Training loss: 0.43686431646347046
INFO:agents.father_agent:Step: 220, Training loss: 0.5993301868438721
INFO:agents.father_agent:Step: 225, Training loss: 0.6813490390777588
INFO:agents.father_agent:Step: 230, Training loss: 0.2863868474960327
INFO:agents.father_agent:Step: 235, Training loss: 0.5367139577865601
INFO:agents.father_agent:Step: 240, Training loss: 0.7593909502029419
INFO:agents.father_agent:Step: 245, Training loss: 0.5656421184539795
INFO:agents.father_agent:Step: 250, Training loss: 0.8174137473106384
INFO:agents.father_agent:Step: 255, Training loss: 0.802059531211853
INFO:agents.father_agent:Step: 260, Training loss: 0.5632645487785339
INFO:agents.father_agent:Step: 265, Training loss: 0.8192456960678101
INFO:agents.father_agent:Step: 270, Training loss: 0.9047231078147888
INFO:agents.father_agent:Step: 275, Training loss: 0.5227960348129272
INFO:agents.father_agent:Step: 280, Training loss: 0.46343928575515747
INFO:agents.father_agent:Step: 285, Training loss: 1.0915629863739014
INFO:agents.father_agent:Step: 290, Training loss: 0.284322589635849
INFO:agents.father_agent:Step: 295, Training loss: 0.3909262418746948
INFO:agents.father_agent:Step: 300, Training loss: 0.6107856631278992
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -45.17332458496094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 32.9584846496582
INFO:tools.evaluation_results_class:Average Discounted Reward = 17.91579246520996
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9766476388168137
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3045.333740234375
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.3502854177478
INFO:tools.evaluation_results_class:Counted Episodes = 1927
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 0.3783048689365387
INFO:agents.father_agent:Step: 310, Training loss: 0.4105592370033264
INFO:agents.father_agent:Step: 315, Training loss: 0.9984550476074219
INFO:agents.father_agent:Step: 320, Training loss: 0.7677310705184937
INFO:agents.father_agent:Step: 325, Training loss: 0.8319313526153564
INFO:agents.father_agent:Step: 330, Training loss: 0.43258175253868103
INFO:agents.father_agent:Step: 335, Training loss: 0.4525470435619354
INFO:agents.father_agent:Step: 340, Training loss: 0.7820382714271545
INFO:agents.father_agent:Step: 345, Training loss: 0.2143520563840866
INFO:agents.father_agent:Step: 350, Training loss: 0.8059551119804382
INFO:agents.father_agent:Step: 355, Training loss: 1.1749926805496216
INFO:agents.father_agent:Step: 360, Training loss: 0.7025068998336792
INFO:agents.father_agent:Step: 365, Training loss: 0.6101481318473816
INFO:agents.father_agent:Step: 370, Training loss: 0.7616912722587585
INFO:agents.father_agent:Step: 375, Training loss: 0.4922810196876526
INFO:agents.father_agent:Step: 380, Training loss: 0.5365475416183472
INFO:agents.father_agent:Step: 385, Training loss: 0.6463811993598938
INFO:agents.father_agent:Step: 390, Training loss: 0.43602603673934937
INFO:agents.father_agent:Step: 395, Training loss: 0.5971863865852356
INFO:agents.father_agent:Step: 400, Training loss: 0.700708270072937
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.4095458984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 8.612227439880371
INFO:tools.evaluation_results_class:Average Discounted Reward = 13.402894973754883
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8877721943048577
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5788.1396484375
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 164.80653266331657
INFO:tools.evaluation_results_class:Counted Episodes = 1194
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.0952731370925903
INFO:agents.father_agent:Step: 410, Training loss: 0.31863582134246826
INFO:agents.father_agent:Step: 415, Training loss: 0.28682011365890503
INFO:agents.father_agent:Step: 420, Training loss: 0.7747572064399719
INFO:agents.father_agent:Step: 425, Training loss: 0.35390591621398926
INFO:agents.father_agent:Step: 430, Training loss: 0.3318130671977997
INFO:agents.father_agent:Step: 435, Training loss: 1.1157560348510742
INFO:agents.father_agent:Step: 440, Training loss: 0.614318311214447
INFO:agents.father_agent:Step: 445, Training loss: 0.5145055651664734
INFO:agents.father_agent:Step: 450, Training loss: 0.4212074279785156
INFO:agents.father_agent:Step: 455, Training loss: 0.8767852187156677
INFO:agents.father_agent:Step: 460, Training loss: 0.42055755853652954
INFO:agents.father_agent:Step: 465, Training loss: 0.5232343673706055
INFO:agents.father_agent:Step: 470, Training loss: 0.9129458069801331
INFO:agents.father_agent:Step: 475, Training loss: 0.7827206254005432
INFO:agents.father_agent:Step: 480, Training loss: 0.7483072876930237
INFO:agents.father_agent:Step: 485, Training loss: 0.5558714270591736
INFO:agents.father_agent:Step: 490, Training loss: 0.5277666449546814
INFO:agents.father_agent:Step: 495, Training loss: 0.5051989555358887
INFO:agents.father_agent:Step: 500, Training loss: 1.1761037111282349
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -35.338356018066406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.13843536376953
INFO:tools.evaluation_results_class:Average Discounted Reward = 24.824172973632812
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9809599365331217
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2274.01318359375
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.64894882982944
INFO:tools.evaluation_results_class:Counted Episodes = 2521
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -37.6894416809082
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.28985595703125
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.640012741088867
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9747412008281573
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2578.12060546875
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.14865424430641
INFO:tools.evaluation_results_class:Counted Episodes = 2415
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -47.23689651489258
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 27.54793357849121
INFO:tools.evaluation_results_class:Average Discounted Reward = 20.068904876708984
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9348103962505326
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4114.49658203125
INFO:tools.evaluation_results_class:Current Best Return = -47.23689651489258
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9348103962505326
INFO:tools.evaluation_results_class:Average Episode Length = 118.68129527055815
INFO:tools.evaluation_results_class:Counted Episodes = 2347
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -111.01930236816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -31.019302368164062
INFO:tools.evaluation_results_class:Average Discounted Reward = -51.46391677856445
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4922.61669921875
INFO:tools.evaluation_results_class:Current Best Return = -111.01930236816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 128.0513347022587
INFO:tools.evaluation_results_class:Counted Episodes = 2435
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 84.8663 achieved after 2722.98 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 96.0385 achieved after 2722.98 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 107.9674 achieved after 2722.99 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 109.4087 achieved after 2723.01 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 110.8444 achieved after 2723.02 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 111.9897 achieved after 2723.04 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 113.2162 achieved after 2723.06 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 114.3889 achieved after 2723.08 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 117.7045 achieved after 2723.09 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 118.7525 achieved after 2723.1 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 119.7325 achieved after 2723.12 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 119.73254856863112
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
INFO:robust_rl.robust_rl_trainer:Iteration 6 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -37.66695022583008
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 40.249359130859375
INFO:tools.evaluation_results_class:Average Discounted Reward = 23.748865127563477
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9739538855678906
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2553.67822265625
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.03202391118703
INFO:tools.evaluation_results_class:Counted Episodes = 2342
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.5735496282577515
INFO:agents.father_agent:Step: 5, Training loss: 0.8264581561088562
INFO:agents.father_agent:Step: 10, Training loss: 0.33505135774612427
INFO:agents.father_agent:Step: 15, Training loss: 0.8679193258285522
INFO:agents.father_agent:Step: 20, Training loss: 0.9997605085372925
INFO:agents.father_agent:Step: 25, Training loss: 0.634768009185791
INFO:agents.father_agent:Step: 30, Training loss: 0.7841170430183411
INFO:agents.father_agent:Step: 35, Training loss: 0.6253477334976196
INFO:agents.father_agent:Step: 40, Training loss: 0.5090810060501099
INFO:agents.father_agent:Step: 45, Training loss: 0.6186739206314087
INFO:agents.father_agent:Step: 50, Training loss: 0.8667507171630859
INFO:agents.father_agent:Step: 55, Training loss: 0.5695807933807373
INFO:agents.father_agent:Step: 60, Training loss: 0.6645211577415466
INFO:agents.father_agent:Step: 65, Training loss: 0.3656107783317566
INFO:agents.father_agent:Step: 70, Training loss: 0.7517320513725281
INFO:agents.father_agent:Step: 75, Training loss: 0.8245739340782166
INFO:agents.father_agent:Step: 80, Training loss: 0.49445176124572754
INFO:agents.father_agent:Step: 85, Training loss: 0.7354831695556641
INFO:agents.father_agent:Step: 90, Training loss: 0.6067373752593994
INFO:agents.father_agent:Step: 95, Training loss: 0.6679027080535889
INFO:agents.father_agent:Step: 100, Training loss: 0.927295982837677
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -40.01715087890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.492855072021484
INFO:tools.evaluation_results_class:Average Discounted Reward = 18.136123657226562
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9938750510412413
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2127.98828125
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 100.47366271947733
INFO:tools.evaluation_results_class:Counted Episodes = 2449
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 0.6011142134666443
INFO:agents.father_agent:Step: 110, Training loss: 0.5046910047531128
INFO:agents.father_agent:Step: 115, Training loss: 0.44401323795318604
INFO:agents.father_agent:Step: 120, Training loss: 0.6851841807365417
INFO:agents.father_agent:Step: 125, Training loss: 0.626371443271637
INFO:agents.father_agent:Step: 130, Training loss: 0.6583694219589233
INFO:agents.father_agent:Step: 135, Training loss: 0.3346351981163025
INFO:agents.father_agent:Step: 140, Training loss: 0.8534538149833679
INFO:agents.father_agent:Step: 145, Training loss: 0.48203757405281067
INFO:agents.father_agent:Step: 150, Training loss: 0.6569960117340088
INFO:agents.father_agent:Step: 155, Training loss: 0.6890914440155029
INFO:agents.father_agent:Step: 160, Training loss: 0.5561501979827881
INFO:agents.father_agent:Step: 165, Training loss: 0.7385324239730835
INFO:agents.father_agent:Step: 170, Training loss: 0.7884504199028015
INFO:agents.father_agent:Step: 175, Training loss: 0.39688926935195923
INFO:agents.father_agent:Step: 180, Training loss: 0.31227684020996094
INFO:agents.father_agent:Step: 185, Training loss: 0.7671366333961487
INFO:agents.father_agent:Step: 190, Training loss: 0.6544680595397949
INFO:agents.father_agent:Step: 195, Training loss: 0.502979576587677
INFO:agents.father_agent:Step: 200, Training loss: 0.3711656332015991
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -35.23671340942383
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 44.74030303955078
INFO:tools.evaluation_results_class:Average Discounted Reward = 18.792749404907227
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9997127262280954
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1385.03564453125
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.27635736857225
INFO:tools.evaluation_results_class:Counted Episodes = 3481
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 0.5940588116645813
INFO:agents.father_agent:Step: 210, Training loss: 0.34750235080718994
INFO:agents.father_agent:Step: 215, Training loss: 0.374562531709671
INFO:agents.father_agent:Step: 220, Training loss: 0.3618745803833008
INFO:agents.father_agent:Step: 225, Training loss: 0.5486212968826294
INFO:agents.father_agent:Step: 230, Training loss: 0.42378920316696167
INFO:agents.father_agent:Step: 235, Training loss: 0.28037160634994507
INFO:agents.father_agent:Step: 240, Training loss: 0.3678169250488281
INFO:agents.father_agent:Step: 245, Training loss: 0.2614779472351074
INFO:agents.father_agent:Step: 250, Training loss: 0.5242992639541626
INFO:agents.father_agent:Step: 255, Training loss: 0.4225626289844513
INFO:agents.father_agent:Step: 260, Training loss: 0.14425499737262726
INFO:agents.father_agent:Step: 265, Training loss: 0.6098071336746216
INFO:agents.father_agent:Step: 270, Training loss: 0.5721339583396912
INFO:agents.father_agent:Step: 275, Training loss: 0.8601009845733643
INFO:agents.father_agent:Step: 280, Training loss: 0.3991110324859619
INFO:agents.father_agent:Step: 285, Training loss: 0.6244003772735596
INFO:agents.father_agent:Step: 290, Training loss: 0.8436881303787231
INFO:agents.father_agent:Step: 295, Training loss: 0.5053443312644958
INFO:agents.father_agent:Step: 300, Training loss: 0.25324708223342896
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -28.745487213134766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 51.2364616394043
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.136627197265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9997743682310469
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1037.2606201171875
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 65.42351083032491
INFO:tools.evaluation_results_class:Counted Episodes = 4432
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 0.17818990349769592
INFO:agents.father_agent:Step: 310, Training loss: 0.26354092359542847
INFO:agents.father_agent:Step: 315, Training loss: 0.5698585510253906
INFO:agents.father_agent:Step: 320, Training loss: 0.6744924187660217
INFO:agents.father_agent:Step: 325, Training loss: 0.4288148283958435
INFO:agents.father_agent:Step: 330, Training loss: 0.590219259262085
INFO:agents.father_agent:Step: 335, Training loss: 0.5009886622428894
INFO:agents.father_agent:Step: 340, Training loss: 0.6531557440757751
INFO:agents.father_agent:Step: 345, Training loss: 0.243098646402359
INFO:agents.father_agent:Step: 350, Training loss: 0.292598694562912
INFO:agents.father_agent:Step: 355, Training loss: 1.0050091743469238
INFO:agents.father_agent:Step: 360, Training loss: 0.6601279973983765
INFO:agents.father_agent:Step: 365, Training loss: 0.7268872261047363
INFO:agents.father_agent:Step: 370, Training loss: 0.4128841757774353
INFO:agents.father_agent:Step: 375, Training loss: 0.5388475060462952
INFO:agents.father_agent:Step: 380, Training loss: 0.7046909928321838
INFO:agents.father_agent:Step: 385, Training loss: 0.518302857875824
INFO:agents.father_agent:Step: 390, Training loss: 0.7942972779273987
INFO:agents.father_agent:Step: 395, Training loss: 0.6590454578399658
INFO:agents.father_agent:Step: 400, Training loss: 0.6082282066345215
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -24.716127395629883
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.283870697021484
INFO:tools.evaluation_results_class:Average Discounted Reward = 28.145177841186523
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 638.9351806640625
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.92450009344048
INFO:tools.evaluation_results_class:Counted Episodes = 5351
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 0.3450607657432556
INFO:agents.father_agent:Step: 410, Training loss: 0.49687111377716064
INFO:agents.father_agent:Step: 415, Training loss: 0.6376422643661499
INFO:agents.father_agent:Step: 420, Training loss: 0.630325973033905
INFO:agents.father_agent:Step: 425, Training loss: 0.33524322509765625
INFO:agents.father_agent:Step: 430, Training loss: 0.5357853770256042
INFO:agents.father_agent:Step: 435, Training loss: 0.5421910881996155
INFO:agents.father_agent:Step: 440, Training loss: 0.6023988723754883
INFO:agents.father_agent:Step: 445, Training loss: 0.05477021634578705
INFO:agents.father_agent:Step: 450, Training loss: 0.5307321548461914
INFO:agents.father_agent:Step: 455, Training loss: 0.5337475538253784
INFO:agents.father_agent:Step: 460, Training loss: 0.13073889911174774
INFO:agents.father_agent:Step: 465, Training loss: 0.7331029176712036
INFO:agents.father_agent:Step: 470, Training loss: 0.6696324348449707
INFO:agents.father_agent:Step: 475, Training loss: 0.24307094514369965
INFO:agents.father_agent:Step: 480, Training loss: 0.6551499962806702
INFO:agents.father_agent:Step: 485, Training loss: 0.7246841192245483
INFO:agents.father_agent:Step: 490, Training loss: 0.3259091079235077
INFO:agents.father_agent:Step: 495, Training loss: 0.4364100694656372
INFO:agents.father_agent:Step: 500, Training loss: 0.6466401815414429
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -24.026348114013672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.97365188598633
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.11737632751465
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 610.4669799804688
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.61847733105218
INFO:tools.evaluation_results_class:Counted Episodes = 5845
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -23.412227630615234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.587772369384766
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.702119827270508
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 561.2835693359375
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.44637978142077
INFO:tools.evaluation_results_class:Counted Episodes = 5856
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -24.20112419128418
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.79887390136719
INFO:tools.evaluation_results_class:Average Discounted Reward = 28.908674240112305
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 626.6129760742188
INFO:tools.evaluation_results_class:Current Best Return = -24.20112419128418
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.401568092722
INFO:tools.evaluation_results_class:Counted Episodes = 5867
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -86.13758087158203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -6.137580394744873
INFO:tools.evaluation_results_class:Average Discounted Reward = -22.72224998474121
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5185.90087890625
INFO:tools.evaluation_results_class:Current Best Return = -86.13758087158203
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 55.420874279905114
INFO:tools.evaluation_results_class:Counted Episodes = 5902
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 65.391 achieved after 3259.43 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 67.5548 achieved after 3259.44 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 72.4715 achieved after 3259.46 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 73.8428 achieved after 3259.46 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 81.3011 achieved after 3259.48 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 82.074 achieved after 3259.49 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 84.7246 achieved after 3259.55 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 86.6761 achieved after 3259.6 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 86.7405 achieved after 3259.61 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 86.74050108034173
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
INFO:robust_rl.robust_rl_trainer:Iteration 7 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -24.19508934020996
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.80491256713867
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.12892723083496
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 635.461181640625
INFO:tools.evaluation_results_class:Current Best Return = -23.18852424621582
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.35948158253752
INFO:tools.evaluation_results_class:Counted Episodes = 5864
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.5194985270500183
INFO:agents.father_agent:Step: 5, Training loss: 0.3871798813343048
INFO:agents.father_agent:Step: 10, Training loss: 0.42103782296180725
INFO:agents.father_agent:Step: 15, Training loss: 0.6251184344291687
INFO:agents.father_agent:Step: 20, Training loss: 0.7020203471183777
INFO:agents.father_agent:Step: 25, Training loss: 0.281599223613739
INFO:agents.father_agent:Step: 30, Training loss: 0.5796084403991699
INFO:agents.father_agent:Step: 35, Training loss: 0.2526552975177765
INFO:agents.father_agent:Step: 40, Training loss: 0.2504844069480896
INFO:agents.father_agent:Step: 45, Training loss: 0.6430612802505493
INFO:agents.father_agent:Step: 50, Training loss: 0.854871928691864
INFO:agents.father_agent:Step: 55, Training loss: 0.8546642065048218
INFO:agents.father_agent:Step: 60, Training loss: 0.16170062124729156
INFO:agents.father_agent:Step: 65, Training loss: 0.42923322319984436
INFO:agents.father_agent:Step: 70, Training loss: 0.1513710469007492
INFO:agents.father_agent:Step: 75, Training loss: 0.32416364550590515
INFO:agents.father_agent:Step: 80, Training loss: 0.6202526092529297
INFO:agents.father_agent:Step: 85, Training loss: 0.3120833933353424
INFO:agents.father_agent:Step: 90, Training loss: 0.23364904522895813
INFO:agents.father_agent:Step: 95, Training loss: 0.5142187476158142
INFO:agents.father_agent:Step: 100, Training loss: 0.4107387959957123
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -17.955677032470703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 62.0443229675293
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.29066848754883
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 176.29151916503906
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 47.79973494330732
INFO:tools.evaluation_results_class:Counted Episodes = 6791
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 0.8425909280776978
INFO:agents.father_agent:Step: 110, Training loss: 0.3322014808654785
INFO:agents.father_agent:Step: 115, Training loss: 0.4613076448440552
INFO:agents.father_agent:Step: 120, Training loss: 0.5836166739463806
INFO:agents.father_agent:Step: 125, Training loss: 0.3633110225200653
INFO:agents.father_agent:Step: 130, Training loss: 0.10147588700056076
INFO:agents.father_agent:Step: 135, Training loss: 0.531454861164093
INFO:agents.father_agent:Step: 140, Training loss: 0.618617594242096
INFO:agents.father_agent:Step: 145, Training loss: 0.30732449889183044
INFO:agents.father_agent:Step: 150, Training loss: 0.49191170930862427
INFO:agents.father_agent:Step: 155, Training loss: 0.299784779548645
INFO:agents.father_agent:Step: 160, Training loss: 0.21588927507400513
INFO:agents.father_agent:Step: 165, Training loss: 0.7377691268920898
INFO:agents.father_agent:Step: 170, Training loss: 0.6058563590049744
INFO:agents.father_agent:Step: 175, Training loss: 0.29666668176651
INFO:agents.father_agent:Step: 180, Training loss: 0.2773950695991516
INFO:agents.father_agent:Step: 185, Training loss: 0.48795202374458313
INFO:agents.father_agent:Step: 190, Training loss: 0.3797140121459961
INFO:agents.father_agent:Step: 195, Training loss: 0.5960575342178345
INFO:agents.father_agent:Step: 200, Training loss: 0.38870131969451904
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -25.70743751525879
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.29256057739258
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.652502059936523
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 649.123046875
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.39742883379247
INFO:tools.evaluation_results_class:Counted Episodes = 5445
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 0.608639657497406
INFO:agents.father_agent:Step: 210, Training loss: 0.6722247004508972
INFO:agents.father_agent:Step: 215, Training loss: 0.3662383258342743
INFO:agents.father_agent:Step: 220, Training loss: 0.17753396928310394
INFO:agents.father_agent:Step: 225, Training loss: 0.6591836214065552
INFO:agents.father_agent:Step: 230, Training loss: 0.4865160882472992
INFO:agents.father_agent:Step: 235, Training loss: 0.25691550970077515
INFO:agents.father_agent:Step: 240, Training loss: 0.42394518852233887
INFO:agents.father_agent:Step: 245, Training loss: 0.4227641820907593
INFO:agents.father_agent:Step: 250, Training loss: 0.4641324579715729
INFO:agents.father_agent:Step: 255, Training loss: 0.5172598361968994
INFO:agents.father_agent:Step: 260, Training loss: 0.4256800413131714
INFO:agents.father_agent:Step: 265, Training loss: 0.9477304816246033
INFO:agents.father_agent:Step: 270, Training loss: 0.40690773725509644
INFO:agents.father_agent:Step: 275, Training loss: 0.27518230676651
INFO:agents.father_agent:Step: 280, Training loss: 0.40948915481567383
INFO:agents.father_agent:Step: 285, Training loss: 0.753642201423645
INFO:agents.father_agent:Step: 290, Training loss: 0.6238852739334106
INFO:agents.father_agent:Step: 295, Training loss: 0.47927325963974
INFO:agents.father_agent:Step: 300, Training loss: 0.32083478569984436
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -22.424955368041992
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.575042724609375
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.01957130432129
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 520.9837646484375
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.105045360496575
INFO:tools.evaluation_results_class:Counted Episodes = 6283
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 0.49902114272117615
INFO:agents.father_agent:Step: 310, Training loss: 0.14324720203876495
INFO:agents.father_agent:Step: 315, Training loss: 0.8055615425109863
INFO:agents.father_agent:Step: 320, Training loss: 0.4473288953304291
INFO:agents.father_agent:Step: 325, Training loss: 0.4700636863708496
INFO:agents.father_agent:Step: 330, Training loss: 0.4367453455924988
INFO:agents.father_agent:Step: 335, Training loss: 0.37543177604675293
INFO:agents.father_agent:Step: 340, Training loss: 0.5779762268066406
INFO:agents.father_agent:Step: 345, Training loss: 0.43413734436035156
INFO:agents.father_agent:Step: 350, Training loss: 0.46381044387817383
INFO:agents.father_agent:Step: 355, Training loss: 0.6596553325653076
INFO:agents.father_agent:Step: 360, Training loss: 0.43422648310661316
INFO:agents.father_agent:Step: 365, Training loss: 0.6576793193817139
INFO:agents.father_agent:Step: 370, Training loss: 0.5155125856399536
INFO:agents.father_agent:Step: 375, Training loss: 0.34228360652923584
INFO:agents.father_agent:Step: 380, Training loss: 0.5264445543289185
INFO:agents.father_agent:Step: 385, Training loss: 0.46878933906555176
INFO:agents.father_agent:Step: 390, Training loss: 0.3430138826370239
INFO:agents.father_agent:Step: 395, Training loss: 0.46725744009017944
INFO:agents.father_agent:Step: 400, Training loss: 0.6914396286010742
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -24.03879737854004
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.961204528808594
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.55938720703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 658.0731201171875
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.89279493269992
INFO:tools.evaluation_results_class:Counted Episodes = 6315
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 0.4401334524154663
INFO:agents.father_agent:Step: 410, Training loss: 0.3074192702770233
INFO:agents.father_agent:Step: 415, Training loss: 0.5554826855659485
INFO:agents.father_agent:Step: 420, Training loss: 0.3188134431838989
INFO:agents.father_agent:Step: 425, Training loss: 0.4261324107646942
INFO:agents.father_agent:Step: 430, Training loss: 0.5394675135612488
INFO:agents.father_agent:Step: 435, Training loss: 0.49806925654411316
INFO:agents.father_agent:Step: 440, Training loss: 0.3937535583972931
INFO:agents.father_agent:Step: 445, Training loss: 0.2588273286819458
INFO:agents.father_agent:Step: 450, Training loss: 0.4995356798171997
INFO:agents.father_agent:Step: 455, Training loss: 0.7494279146194458
INFO:agents.father_agent:Step: 460, Training loss: 0.3582499623298645
INFO:agents.father_agent:Step: 465, Training loss: 0.5394202470779419
INFO:agents.father_agent:Step: 470, Training loss: 0.40676259994506836
INFO:agents.father_agent:Step: 475, Training loss: 0.23516541719436646
INFO:agents.father_agent:Step: 480, Training loss: 0.3295222520828247
INFO:agents.father_agent:Step: 485, Training loss: 0.25354161858558655
INFO:agents.father_agent:Step: 490, Training loss: 0.27405935525894165
INFO:agents.father_agent:Step: 495, Training loss: 0.5148875117301941
INFO:agents.father_agent:Step: 500, Training loss: 0.8113702535629272
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -24.774503707885742
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.22549819946289
INFO:tools.evaluation_results_class:Average Discounted Reward = 28.21880340576172
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 674.1179809570312
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.222185430463576
INFO:tools.evaluation_results_class:Counted Episodes = 6040
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -24.73060417175293
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.2693977355957
INFO:tools.evaluation_results_class:Average Discounted Reward = 28.36284828186035
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 673.664306640625
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.95212941564873
INFO:tools.evaluation_results_class:Counted Episodes = 6058
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -24.60222053527832
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.39777755737305
INFO:tools.evaluation_results_class:Average Discounted Reward = 28.263099670410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 640.5619506835938
INFO:tools.evaluation_results_class:Current Best Return = -24.60222053527832
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.36577400391901
INFO:tools.evaluation_results_class:Counted Episodes = 6124
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -88.63408660888672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -8.634085655212402
INFO:tools.evaluation_results_class:Average Discounted Reward = -24.49835777282715
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5097.36865234375
INFO:tools.evaluation_results_class:Current Best Return = -88.63408660888672
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.27231994689678
INFO:tools.evaluation_results_class:Counted Episodes = 6026
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 68.3428 achieved after 3800.18 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 70.3811 achieved after 3800.19 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 75.2622 achieved after 3800.2 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 76.6217 achieved after 3800.21 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 83.6595 achieved after 3800.22 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 84.4914 achieved after 3800.23 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 87.4379 achieved after 3800.29 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 89.5194 achieved after 3800.34 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 90.3678 achieved after 3800.35 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 90.3678432977277
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
INFO:robust_rl.robust_rl_trainer:Iteration 8 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -24.864788055419922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.13521194458008
INFO:tools.evaluation_results_class:Average Discounted Reward = 28.004804611206055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 657.04150390625
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.552350783594534
INFO:tools.evaluation_results_class:Counted Episodes = 5998
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.6096569895744324
INFO:agents.father_agent:Step: 5, Training loss: 0.34655848145484924
INFO:agents.father_agent:Step: 10, Training loss: 0.4753281772136688
INFO:agents.father_agent:Step: 15, Training loss: 0.45486682653427124
INFO:agents.father_agent:Step: 20, Training loss: 0.390168696641922
INFO:agents.father_agent:Step: 25, Training loss: 0.27078503370285034
INFO:agents.father_agent:Step: 30, Training loss: 0.2965264916419983
INFO:agents.father_agent:Step: 35, Training loss: 0.3552124500274658
INFO:agents.father_agent:Step: 40, Training loss: 0.384770005941391
INFO:agents.father_agent:Step: 45, Training loss: 0.26655998826026917
INFO:agents.father_agent:Step: 50, Training loss: 0.4195876717567444
INFO:agents.father_agent:Step: 55, Training loss: 0.18561220169067383
INFO:agents.father_agent:Step: 60, Training loss: 0.48543447256088257
INFO:agents.father_agent:Step: 65, Training loss: 0.30866262316703796
INFO:agents.father_agent:Step: 70, Training loss: 0.28329628705978394
INFO:agents.father_agent:Step: 75, Training loss: 0.26344379782676697
INFO:agents.father_agent:Step: 80, Training loss: 0.4802173674106598
INFO:agents.father_agent:Step: 85, Training loss: 0.40554559230804443
INFO:agents.father_agent:Step: 90, Training loss: 0.5715022087097168
INFO:agents.father_agent:Step: 95, Training loss: 0.4351026713848114
INFO:agents.father_agent:Step: 100, Training loss: 0.20590633153915405
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -22.997892379760742
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.00210952758789
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.983354568481445
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 535.3233642578125
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.100697259607585
INFO:tools.evaluation_results_class:Counted Episodes = 6167
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 0.6210309267044067
INFO:agents.father_agent:Step: 110, Training loss: 0.5624222159385681
INFO:agents.father_agent:Step: 115, Training loss: 0.1175987720489502
INFO:agents.father_agent:Step: 120, Training loss: 0.30147746205329895
INFO:agents.father_agent:Step: 125, Training loss: 0.18747112154960632
INFO:agents.father_agent:Step: 130, Training loss: 0.170889750123024
INFO:agents.father_agent:Step: 135, Training loss: 0.5666530132293701
INFO:agents.father_agent:Step: 140, Training loss: 0.21499872207641602
INFO:agents.father_agent:Step: 145, Training loss: 0.35659950971603394
INFO:agents.father_agent:Step: 150, Training loss: 0.3263590633869171
INFO:agents.father_agent:Step: 155, Training loss: 0.20657578110694885
INFO:agents.father_agent:Step: 160, Training loss: 0.3552005887031555
INFO:agents.father_agent:Step: 165, Training loss: 0.20847120881080627
INFO:agents.father_agent:Step: 170, Training loss: 0.15158171951770782
INFO:agents.father_agent:Step: 175, Training loss: 0.282385915517807
INFO:agents.father_agent:Step: 180, Training loss: 0.7028045058250427
INFO:agents.father_agent:Step: 185, Training loss: 0.34484627842903137
INFO:agents.father_agent:Step: 190, Training loss: 0.4958941638469696
INFO:agents.father_agent:Step: 195, Training loss: 0.36792391538619995
INFO:agents.father_agent:Step: 200, Training loss: 0.363763689994812
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -23.516437530517578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 56.48356246948242
INFO:tools.evaluation_results_class:Average Discounted Reward = 29.300006866455078
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 560.7367553710938
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 53.15876424913266
INFO:tools.evaluation_results_class:Counted Episodes = 6053
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 0.7534412741661072
INFO:agents.father_agent:Step: 210, Training loss: 0.4807230234146118
INFO:agents.father_agent:Step: 215, Training loss: 0.24659603834152222
INFO:agents.father_agent:Step: 220, Training loss: 0.1567602902650833
INFO:agents.father_agent:Step: 225, Training loss: 0.2369154989719391
INFO:agents.father_agent:Step: 230, Training loss: 0.8698930144309998
INFO:agents.father_agent:Step: 235, Training loss: 0.7579729557037354
INFO:agents.father_agent:Step: 240, Training loss: 0.19522517919540405
INFO:agents.father_agent:Step: 245, Training loss: 0.38626623153686523
INFO:agents.father_agent:Step: 250, Training loss: 0.19250725209712982
INFO:agents.father_agent:Step: 255, Training loss: 0.7128134965896606
INFO:agents.father_agent:Step: 260, Training loss: 0.46461933851242065
INFO:agents.father_agent:Step: 265, Training loss: 0.5331552624702454
INFO:agents.father_agent:Step: 270, Training loss: 0.7527962327003479
INFO:agents.father_agent:Step: 275, Training loss: 0.38985124230384827
INFO:agents.father_agent:Step: 280, Training loss: 0.2657632827758789
INFO:agents.father_agent:Step: 285, Training loss: 0.3696092963218689
INFO:agents.father_agent:Step: 290, Training loss: 0.2447998821735382
INFO:agents.father_agent:Step: 295, Training loss: 0.5243746042251587
INFO:agents.father_agent:Step: 300, Training loss: 0.31289640069007874
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -22.834447860717773
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.16555404663086
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.374279022216797
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 542.4730834960938
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.3282394141993
INFO:tools.evaluation_results_class:Counted Episodes = 6282
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 0.7877419590950012
INFO:agents.father_agent:Step: 310, Training loss: 0.3986909091472626
INFO:agents.father_agent:Step: 315, Training loss: 0.4110330045223236
INFO:agents.father_agent:Step: 320, Training loss: 0.5059632062911987
INFO:agents.father_agent:Step: 325, Training loss: 0.2637940049171448
INFO:agents.father_agent:Step: 330, Training loss: 0.4510851800441742
INFO:agents.father_agent:Step: 335, Training loss: 0.46552345156669617
INFO:agents.father_agent:Step: 340, Training loss: 0.3794208765029907
INFO:agents.father_agent:Step: 345, Training loss: 0.33744561672210693
INFO:agents.father_agent:Step: 350, Training loss: 0.47450703382492065
INFO:agents.father_agent:Step: 355, Training loss: 0.1991000473499298
INFO:agents.father_agent:Step: 360, Training loss: 0.1902408003807068
INFO:agents.father_agent:Step: 365, Training loss: 0.28127986192703247
INFO:agents.father_agent:Step: 370, Training loss: 0.22847528755664825
INFO:agents.father_agent:Step: 375, Training loss: 0.41665950417518616
INFO:agents.father_agent:Step: 380, Training loss: 0.23990115523338318
INFO:agents.father_agent:Step: 385, Training loss: 0.17322930693626404
INFO:agents.father_agent:Step: 390, Training loss: 0.39159727096557617
INFO:agents.father_agent:Step: 395, Training loss: 0.4161544144153595
INFO:agents.father_agent:Step: 400, Training loss: 0.4281696677207947
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -21.258909225463867
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 58.741092681884766
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.74931526184082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 416.0855712890625
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.82276884263639
INFO:tools.evaluation_results_class:Counted Episodes = 6342
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 0.6434736251831055
INFO:agents.father_agent:Step: 410, Training loss: 0.46521252393722534
INFO:agents.father_agent:Step: 415, Training loss: 0.34329262375831604
INFO:agents.father_agent:Step: 420, Training loss: 0.3172435760498047
INFO:agents.father_agent:Step: 425, Training loss: 0.5360444188117981
INFO:agents.father_agent:Step: 430, Training loss: 0.171921506524086
INFO:agents.father_agent:Step: 435, Training loss: 0.5952736735343933
INFO:agents.father_agent:Step: 440, Training loss: 0.35241860151290894
INFO:agents.father_agent:Step: 445, Training loss: 0.32417792081832886
INFO:agents.father_agent:Step: 450, Training loss: 0.30680710077285767
INFO:agents.father_agent:Step: 455, Training loss: 0.5664690732955933
INFO:agents.father_agent:Step: 460, Training loss: 0.33167320489883423
INFO:agents.father_agent:Step: 465, Training loss: 0.41041597723960876
INFO:agents.father_agent:Step: 470, Training loss: 0.3133164644241333
INFO:agents.father_agent:Step: 475, Training loss: 0.47575607895851135
INFO:agents.father_agent:Step: 480, Training loss: 0.5133829116821289
INFO:agents.father_agent:Step: 485, Training loss: 0.3123949468135834
INFO:agents.father_agent:Step: 490, Training loss: 0.24632230401039124
INFO:agents.father_agent:Step: 495, Training loss: 0.7518904805183411
INFO:agents.father_agent:Step: 500, Training loss: 0.2120654135942459
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -21.494003295898438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 58.50599670410156
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.536365509033203
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 430.4737243652344
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.85385101010101
INFO:tools.evaluation_results_class:Counted Episodes = 6336
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -21.908475875854492
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 58.09152603149414
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.24922752380371
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 482.1930847167969
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.7978890989288
INFO:tools.evaluation_results_class:Counted Episodes = 6348
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -21.615949630737305
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 58.38404846191406
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.337909698486328
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 449.7605285644531
INFO:tools.evaluation_results_class:Current Best Return = -21.615949630737305
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.14167318217358
INFO:tools.evaluation_results_class:Counted Episodes = 6395
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -81.79454803466797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1.7945469617843628
INFO:tools.evaluation_results_class:Average Discounted Reward = -18.412309646606445
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5233.67236328125
INFO:tools.evaluation_results_class:Current Best Return = -81.79454803466797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 52.45805934242181
INFO:tools.evaluation_results_class:Counted Episodes = 6235
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 62.5869 achieved after 4337.68 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 69.5245 achieved after 4337.7 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 78.5359 achieved after 4337.72 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 79.7223 achieved after 4337.78 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 80.7556 achieved after 4337.79 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 84.0401 achieved after 4337.85 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=7
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 84.04011045951916
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
INFO:robust_rl.robust_rl_trainer:Iteration 9 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -27.855148315429688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 52.14485168457031
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.819866180419922
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1017.1524658203125
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.65834902699309
INFO:tools.evaluation_results_class:Counted Episodes = 6372
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.8685671091079712
INFO:agents.father_agent:Step: 5, Training loss: 0.853984534740448
INFO:agents.father_agent:Step: 10, Training loss: 1.0356968641281128
INFO:agents.father_agent:Step: 15, Training loss: 0.722372829914093
INFO:agents.father_agent:Step: 20, Training loss: 0.6697667837142944
INFO:agents.father_agent:Step: 25, Training loss: 0.8553319573402405
INFO:agents.father_agent:Step: 30, Training loss: 0.7903429269790649
INFO:agents.father_agent:Step: 35, Training loss: 0.37745600938796997
INFO:agents.father_agent:Step: 40, Training loss: 0.36070534586906433
INFO:agents.father_agent:Step: 45, Training loss: 0.7886801958084106
INFO:agents.father_agent:Step: 50, Training loss: 1.2894611358642578
INFO:agents.father_agent:Step: 55, Training loss: 0.2703196108341217
INFO:agents.father_agent:Step: 60, Training loss: 0.19928160309791565
INFO:agents.father_agent:Step: 65, Training loss: 0.9447501301765442
INFO:agents.father_agent:Step: 70, Training loss: 0.2916259169578552
INFO:agents.father_agent:Step: 75, Training loss: 0.9536700248718262
INFO:agents.father_agent:Step: 80, Training loss: 0.7141861915588379
INFO:agents.father_agent:Step: 85, Training loss: 0.39787495136260986
INFO:agents.father_agent:Step: 90, Training loss: 0.47731056809425354
INFO:agents.father_agent:Step: 95, Training loss: 0.9097017049789429
INFO:agents.father_agent:Step: 100, Training loss: 0.7628884315490723
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -31.859596252441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 48.140403747558594
INFO:tools.evaluation_results_class:Average Discounted Reward = 22.240140914916992
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1128.5274658203125
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 71.4574317968015
INFO:tools.evaluation_results_class:Counted Episodes = 4252
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 0.8377857208251953
INFO:agents.father_agent:Step: 110, Training loss: 0.4235383868217468
INFO:agents.father_agent:Step: 115, Training loss: 0.5089174509048462
INFO:agents.father_agent:Step: 120, Training loss: 0.3354003429412842
INFO:agents.father_agent:Step: 125, Training loss: 0.5931991338729858
INFO:agents.father_agent:Step: 130, Training loss: 0.3458895683288574
INFO:agents.father_agent:Step: 135, Training loss: 0.3605481684207916
INFO:agents.father_agent:Step: 140, Training loss: 0.5279893279075623
INFO:agents.father_agent:Step: 145, Training loss: 0.607090950012207
INFO:agents.father_agent:Step: 150, Training loss: 0.5368613600730896
INFO:agents.father_agent:Step: 155, Training loss: 0.32407456636428833
INFO:agents.father_agent:Step: 160, Training loss: 0.4835795760154724
INFO:agents.father_agent:Step: 165, Training loss: 0.6242630481719971
INFO:agents.father_agent:Step: 170, Training loss: 0.5112993121147156
INFO:agents.father_agent:Step: 175, Training loss: 0.6613482236862183
INFO:agents.father_agent:Step: 180, Training loss: 0.8534576296806335
INFO:agents.father_agent:Step: 185, Training loss: 0.7426025867462158
INFO:agents.father_agent:Step: 190, Training loss: 0.34047752618789673
INFO:agents.father_agent:Step: 195, Training loss: 0.4636762738227844
INFO:agents.father_agent:Step: 200, Training loss: 0.5666069984436035
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -49.448665618896484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 30.06376075744629
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.987688064575195
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.993905297702766
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2208.082275390625
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 124.60853258321613
INFO:tools.evaluation_results_class:Counted Episodes = 2133
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 0.6369727253913879
INFO:agents.father_agent:Step: 210, Training loss: 0.2844823896884918
INFO:agents.father_agent:Step: 215, Training loss: 0.5409882068634033
INFO:agents.father_agent:Step: 220, Training loss: 0.6188907027244568
INFO:agents.father_agent:Step: 225, Training loss: 0.5564525127410889
INFO:agents.father_agent:Step: 230, Training loss: 0.4655028283596039
INFO:agents.father_agent:Step: 235, Training loss: 0.5657761096954346
INFO:agents.father_agent:Step: 240, Training loss: 0.9552330374717712
INFO:agents.father_agent:Step: 245, Training loss: 0.5898420810699463
INFO:agents.father_agent:Step: 250, Training loss: 0.7686903476715088
INFO:agents.father_agent:Step: 255, Training loss: 0.5484464168548584
INFO:agents.father_agent:Step: 260, Training loss: 0.5746194124221802
INFO:agents.father_agent:Step: 265, Training loss: 0.6271452307701111
INFO:agents.father_agent:Step: 270, Training loss: 0.5237231850624084
INFO:agents.father_agent:Step: 275, Training loss: 0.8282596468925476
INFO:agents.father_agent:Step: 280, Training loss: 0.4083852767944336
INFO:agents.father_agent:Step: 285, Training loss: 0.7039956450462341
INFO:agents.father_agent:Step: 290, Training loss: 0.3334757685661316
INFO:agents.father_agent:Step: 295, Training loss: 0.5217507481575012
INFO:agents.father_agent:Step: 300, Training loss: 0.5605313777923584
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.34797286987305
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 23.570945739746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.645637512207031
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9864864864864865
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2851.830322265625
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 142.13400900900902
INFO:tools.evaluation_results_class:Counted Episodes = 1776
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 0.9059865474700928
INFO:agents.father_agent:Step: 310, Training loss: 0.3931886851787567
INFO:agents.father_agent:Step: 315, Training loss: 0.5756059288978577
INFO:agents.father_agent:Step: 320, Training loss: 0.7197128534317017
INFO:agents.father_agent:Step: 325, Training loss: 0.9085149765014648
INFO:agents.father_agent:Step: 330, Training loss: 0.6632205843925476
INFO:agents.father_agent:Step: 335, Training loss: 0.42744630575180054
INFO:agents.father_agent:Step: 340, Training loss: 0.6104103922843933
INFO:agents.father_agent:Step: 345, Training loss: 0.6467171311378479
INFO:agents.father_agent:Step: 350, Training loss: 0.5229747891426086
INFO:agents.father_agent:Step: 355, Training loss: 0.4622815251350403
INFO:agents.father_agent:Step: 360, Training loss: 0.36089837551116943
INFO:agents.father_agent:Step: 365, Training loss: 0.5694745182991028
INFO:agents.father_agent:Step: 370, Training loss: 0.9824879765510559
INFO:agents.father_agent:Step: 375, Training loss: 0.2582134008407593
INFO:agents.father_agent:Step: 380, Training loss: 0.4770638942718506
INFO:agents.father_agent:Step: 385, Training loss: 0.5647194981575012
INFO:agents.father_agent:Step: 390, Training loss: 0.44460391998291016
INFO:agents.father_agent:Step: 395, Training loss: 0.6705648303031921
INFO:agents.father_agent:Step: 400, Training loss: 0.8008029460906982
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -64.68428802490234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.687143325805664
INFO:tools.evaluation_results_class:Average Discounted Reward = 3.4581298828125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9671428571428572
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4182.1630859375
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 165.50571428571428
INFO:tools.evaluation_results_class:Counted Episodes = 1400
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.004974126815796
INFO:agents.father_agent:Step: 410, Training loss: 0.3714277148246765
INFO:agents.father_agent:Step: 415, Training loss: 0.210093691945076
INFO:agents.father_agent:Step: 420, Training loss: 0.5694802403450012
INFO:agents.father_agent:Step: 425, Training loss: 0.7113798260688782
INFO:agents.father_agent:Step: 430, Training loss: 0.48036935925483704
INFO:agents.father_agent:Step: 435, Training loss: 0.3940610885620117
INFO:agents.father_agent:Step: 440, Training loss: 0.3340340852737427
INFO:agents.father_agent:Step: 445, Training loss: 0.5449065566062927
INFO:agents.father_agent:Step: 450, Training loss: 0.5079700946807861
INFO:agents.father_agent:Step: 455, Training loss: 0.36783868074417114
INFO:agents.father_agent:Step: 460, Training loss: 0.49519798159599304
INFO:agents.father_agent:Step: 465, Training loss: 0.39773693680763245
INFO:agents.father_agent:Step: 470, Training loss: 0.4526861906051636
INFO:agents.father_agent:Step: 475, Training loss: 0.5341922640800476
INFO:agents.father_agent:Step: 480, Training loss: 0.7232692837715149
INFO:agents.father_agent:Step: 485, Training loss: 0.21755382418632507
INFO:agents.father_agent:Step: 490, Training loss: 0.7927073836326599
INFO:agents.father_agent:Step: 495, Training loss: 0.3833763301372528
INFO:agents.father_agent:Step: 500, Training loss: 0.7897626161575317
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.06705856323242
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 19.21419334411621
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.09846830368042
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.978515625
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3642.798095703125
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 151.67057291666666
INFO:tools.evaluation_results_class:Counted Episodes = 1536
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -61.6338996887207
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 15.003389358520508
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.02189302444458
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9579661016949153
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4093.19384765625
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 160.81694915254238
INFO:tools.evaluation_results_class:Counted Episodes = 1475
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.04368591308594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 2.49556303024292
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.9023160934448242
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.931740614334471
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4844.90869140625
INFO:tools.evaluation_results_class:Current Best Return = -72.04368591308594
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.931740614334471
INFO:tools.evaluation_results_class:Average Episode Length = 190.65597269624573
INFO:tools.evaluation_results_class:Counted Episodes = 1465
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -131.13095092773438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -51.183860778808594
INFO:tools.evaluation_results_class:Average Discounted Reward = -62.504207611083984
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9993386243386243
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5558.48388671875
INFO:tools.evaluation_results_class:Current Best Return = -131.13095092773438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9993386243386243
INFO:tools.evaluation_results_class:Average Episode Length = 195.00198412698413
INFO:tools.evaluation_results_class:Counted Episodes = 1512
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 110.5193 achieved after 4876.56 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 121.6176 achieved after 4876.57 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 137.9063 achieved after 4876.58 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 138.5994 achieved after 4876.59 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 139.3122 achieved after 4876.61 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 141.2336 achieved after 4876.63 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 141.797 achieved after 4876.64 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 142.3455 achieved after 4876.66 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 146.1554 achieved after 4876.67 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 146.6089 achieved after 4876.69 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 147.0438 achieved after 4876.7 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 147.04380916088283
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
INFO:robust_rl.robust_rl_trainer:Iteration 10 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -59.06095886230469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 18.760051727294922
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.266678333282471
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9727626459143969
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3748.228271484375
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 152.92088197146563
INFO:tools.evaluation_results_class:Counted Episodes = 1542
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.49124833941459656
INFO:agents.father_agent:Step: 5, Training loss: 0.4138186275959015
INFO:agents.father_agent:Step: 10, Training loss: 0.4705292880535126
INFO:agents.father_agent:Step: 15, Training loss: 0.6239826083183289
INFO:agents.father_agent:Step: 20, Training loss: 0.6524068117141724
INFO:agents.father_agent:Step: 25, Training loss: 0.2546350955963135
INFO:agents.father_agent:Step: 30, Training loss: 0.422323614358902
INFO:agents.father_agent:Step: 35, Training loss: 0.4080699682235718
INFO:agents.father_agent:Step: 40, Training loss: 0.49559152126312256
INFO:agents.father_agent:Step: 45, Training loss: 0.3681280016899109
INFO:agents.father_agent:Step: 50, Training loss: 0.6201196312904358
INFO:agents.father_agent:Step: 55, Training loss: 0.43765443563461304
INFO:agents.father_agent:Step: 60, Training loss: 0.516071617603302
INFO:agents.father_agent:Step: 65, Training loss: 0.5089523792266846
INFO:agents.father_agent:Step: 70, Training loss: 0.5466882586479187
INFO:agents.father_agent:Step: 75, Training loss: 0.6272088885307312
INFO:agents.father_agent:Step: 80, Training loss: 0.48994630575180054
INFO:agents.father_agent:Step: 85, Training loss: 0.3610404133796692
INFO:agents.father_agent:Step: 90, Training loss: 0.4707512855529785
INFO:agents.father_agent:Step: 95, Training loss: 0.5712419152259827
INFO:agents.father_agent:Step: 100, Training loss: 0.5885764956474304
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.50034713745117
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.03838062286377
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.289210796356201
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9567341242149338
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4220.88916015625
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 162.1116538729937
INFO:tools.evaluation_results_class:Counted Episodes = 1433
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 0.978508710861206
INFO:agents.father_agent:Step: 110, Training loss: 0.5710152387619019
INFO:agents.father_agent:Step: 115, Training loss: 0.22537443041801453
INFO:agents.father_agent:Step: 120, Training loss: 0.9169811606407166
INFO:agents.father_agent:Step: 125, Training loss: 0.4064312279224396
INFO:agents.father_agent:Step: 130, Training loss: 0.1828550547361374
INFO:agents.father_agent:Step: 135, Training loss: 0.3020442724227905
INFO:agents.father_agent:Step: 140, Training loss: 0.2979385256767273
INFO:agents.father_agent:Step: 145, Training loss: 0.5946363806724548
INFO:agents.father_agent:Step: 150, Training loss: 0.646221935749054
INFO:agents.father_agent:Step: 155, Training loss: 0.3856329321861267
INFO:agents.father_agent:Step: 160, Training loss: 0.34396812319755554
INFO:agents.father_agent:Step: 165, Training loss: 0.3667205572128296
INFO:agents.father_agent:Step: 170, Training loss: 0.39294034242630005
INFO:agents.father_agent:Step: 175, Training loss: 0.18890711665153503
INFO:agents.father_agent:Step: 180, Training loss: 0.745223343372345
INFO:agents.father_agent:Step: 185, Training loss: 0.24677401781082153
INFO:agents.father_agent:Step: 190, Training loss: 0.26636308431625366
INFO:agents.father_agent:Step: 195, Training loss: 0.15310657024383545
INFO:agents.father_agent:Step: 200, Training loss: 0.330854207277298
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -41.06352615356445
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.35158920288086
INFO:tools.evaluation_results_class:Average Discounted Reward = 21.805391311645508
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9676889375684556
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2748.201904296875
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.84939759036145
INFO:tools.evaluation_results_class:Counted Episodes = 1826
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 0.6527538299560547
INFO:agents.father_agent:Step: 210, Training loss: 0.14516133069992065
INFO:agents.father_agent:Step: 215, Training loss: 0.3706185817718506
INFO:agents.father_agent:Step: 220, Training loss: 0.42279449105262756
INFO:agents.father_agent:Step: 225, Training loss: 0.2762296199798584
INFO:agents.father_agent:Step: 230, Training loss: 0.20431993901729584
INFO:agents.father_agent:Step: 235, Training loss: 0.370932012796402
INFO:agents.father_agent:Step: 240, Training loss: 0.5627450942993164
INFO:agents.father_agent:Step: 245, Training loss: 0.553859531879425
INFO:agents.father_agent:Step: 250, Training loss: 0.2685723900794983
INFO:agents.father_agent:Step: 255, Training loss: 0.267530232667923
INFO:agents.father_agent:Step: 260, Training loss: 0.5561544895172119
INFO:agents.father_agent:Step: 265, Training loss: 0.42956775426864624
INFO:agents.father_agent:Step: 270, Training loss: 0.602330207824707
INFO:agents.father_agent:Step: 275, Training loss: 0.635440468788147
INFO:agents.father_agent:Step: 280, Training loss: 0.5265429615974426
INFO:agents.father_agent:Step: 285, Training loss: 0.5122398734092712
INFO:agents.father_agent:Step: 290, Training loss: 0.5125501155853271
INFO:agents.father_agent:Step: 295, Training loss: 0.46380943059921265
INFO:agents.father_agent:Step: 300, Training loss: 0.9271906018257141
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.33364868164062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -4.927631378173828
INFO:tools.evaluation_results_class:Average Discounted Reward = 1.6054129600524902
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9050751879699248
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6147.39306640625
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 204.1109022556391
INFO:tools.evaluation_results_class:Counted Episodes = 1064
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 0.6103899478912354
INFO:agents.father_agent:Step: 310, Training loss: 0.7349960207939148
INFO:agents.father_agent:Step: 315, Training loss: 0.11119259148836136
INFO:agents.father_agent:Step: 320, Training loss: 0.7148399949073792
INFO:agents.father_agent:Step: 325, Training loss: 0.5767900347709656
INFO:agents.father_agent:Step: 330, Training loss: 0.36242011189460754
INFO:agents.father_agent:Step: 335, Training loss: 0.16909034550189972
INFO:agents.father_agent:Step: 340, Training loss: 0.30396726727485657
INFO:agents.father_agent:Step: 345, Training loss: 0.5921100974082947
INFO:agents.father_agent:Step: 350, Training loss: 0.6334049105644226
INFO:agents.father_agent:Step: 355, Training loss: 0.5722838044166565
INFO:agents.father_agent:Step: 360, Training loss: 0.6259894967079163
INFO:agents.father_agent:Step: 365, Training loss: 0.49273788928985596
INFO:agents.father_agent:Step: 370, Training loss: 0.3052186071872711
INFO:agents.father_agent:Step: 375, Training loss: 0.156647190451622
INFO:agents.father_agent:Step: 380, Training loss: 0.34577253460884094
INFO:agents.father_agent:Step: 385, Training loss: 0.6747531294822693
INFO:agents.father_agent:Step: 390, Training loss: 0.5315317511558533
INFO:agents.father_agent:Step: 395, Training loss: 0.39642399549484253
INFO:agents.father_agent:Step: 400, Training loss: 0.5116114020347595
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.4417724609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 21.672151565551758
INFO:tools.evaluation_results_class:Average Discounted Reward = 10.25296401977539
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9639240506329114
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3885.213623046875
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 142.63227848101266
INFO:tools.evaluation_results_class:Counted Episodes = 1580
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 0.750568687915802
INFO:agents.father_agent:Step: 410, Training loss: 0.5009106993675232
INFO:agents.father_agent:Step: 415, Training loss: 0.5653281807899475
INFO:agents.father_agent:Step: 420, Training loss: 0.486512154340744
INFO:agents.father_agent:Step: 425, Training loss: 0.41157257556915283
INFO:agents.father_agent:Step: 430, Training loss: 0.5187481641769409
INFO:agents.father_agent:Step: 435, Training loss: 0.6586002707481384
INFO:agents.father_agent:Step: 440, Training loss: 0.5100909471511841
INFO:agents.father_agent:Step: 445, Training loss: 0.4163811504840851
INFO:agents.father_agent:Step: 450, Training loss: 0.7753787040710449
INFO:agents.father_agent:Step: 455, Training loss: 0.9315215349197388
INFO:agents.father_agent:Step: 460, Training loss: 0.21730390191078186
INFO:agents.father_agent:Step: 465, Training loss: 0.631260871887207
INFO:agents.father_agent:Step: 470, Training loss: 0.3082231283187866
INFO:agents.father_agent:Step: 475, Training loss: 0.33589109778404236
INFO:agents.father_agent:Step: 480, Training loss: 0.6065300107002258
INFO:agents.father_agent:Step: 485, Training loss: 0.578214168548584
INFO:agents.father_agent:Step: 490, Training loss: 0.2291945219039917
INFO:agents.father_agent:Step: 495, Training loss: 0.3404773771762848
INFO:agents.father_agent:Step: 500, Training loss: 0.4835985600948334
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -77.59667205810547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -7.217391490936279
INFO:tools.evaluation_results_class:Average Discounted Reward = 1.9838132858276367
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.879740980573543
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5783.26904296875
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 211.45513413506012
INFO:tools.evaluation_results_class:Counted Episodes = 1081
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -78.79759979248047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -8.48336410522461
INFO:tools.evaluation_results_class:Average Discounted Reward = 1.8829758167266846
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8789279112754159
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6195.7412109375
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 213.41404805914974
INFO:tools.evaluation_results_class:Counted Episodes = 1082
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -92.06938934326172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -27.126426696777344
INFO:tools.evaluation_results_class:Average Discounted Reward = -1.4326567649841309
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.811787072243346
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7274.44091796875
INFO:tools.evaluation_results_class:Current Best Return = -92.06938934326172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.811787072243346
INFO:tools.evaluation_results_class:Average Episode Length = 252.9096958174905
INFO:tools.evaluation_results_class:Counted Episodes = 1052
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -161.7573699951172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -83.50808715820312
INFO:tools.evaluation_results_class:Average Discounted Reward = -73.76199340820312
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.978116079923882
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7708.71630859375
INFO:tools.evaluation_results_class:Current Best Return = -161.7573699951172
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.978116079923882
INFO:tools.evaluation_results_class:Average Episode Length = 275.10656517602285
INFO:tools.evaluation_results_class:Counted Episodes = 1051
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 140.3565 achieved after 5417.41 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 151.1096 achieved after 5417.41 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 169.1481 achieved after 5417.42 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 169.5559 achieved after 5417.43 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 170.0089 achieved after 5417.45 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 171.8969 achieved after 5417.47 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 172.215 achieved after 5417.48 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 172.5472 achieved after 5417.5 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 176.2417 achieved after 5417.52 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 176.4829 achieved after 5417.54 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 176.7372 achieved after 5417.55 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 176.73722339762782
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
INFO:robust_rl.robust_rl_trainer:Iteration 11 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -76.5086441040039
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -4.515923500061035
INFO:tools.evaluation_results_class:Average Discounted Reward = 2.6517107486724854
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.899909008189263
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5933.33837890625
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 205.77616014558689
INFO:tools.evaluation_results_class:Counted Episodes = 1099
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.016289234161377
INFO:agents.father_agent:Step: 5, Training loss: 0.45302921533584595
INFO:agents.father_agent:Step: 10, Training loss: 0.30268943309783936
INFO:agents.father_agent:Step: 15, Training loss: 0.2793746292591095
INFO:agents.father_agent:Step: 20, Training loss: 0.4467650055885315
INFO:agents.father_agent:Step: 25, Training loss: 0.4877411723136902
INFO:agents.father_agent:Step: 30, Training loss: 0.5781570672988892
INFO:agents.father_agent:Step: 35, Training loss: 0.6070696711540222
INFO:agents.father_agent:Step: 40, Training loss: 0.15515969693660736
INFO:agents.father_agent:Step: 45, Training loss: 0.5491136312484741
INFO:agents.father_agent:Step: 50, Training loss: 0.42223912477493286
INFO:agents.father_agent:Step: 55, Training loss: 0.3188190758228302
INFO:agents.father_agent:Step: 60, Training loss: 0.5279055237770081
INFO:agents.father_agent:Step: 65, Training loss: 0.06307508051395416
INFO:agents.father_agent:Step: 70, Training loss: 0.30362993478775024
INFO:agents.father_agent:Step: 75, Training loss: 0.25452694296836853
INFO:agents.father_agent:Step: 80, Training loss: 0.4006323218345642
INFO:agents.father_agent:Step: 85, Training loss: 0.5682701468467712
INFO:agents.father_agent:Step: 90, Training loss: 0.4112493097782135
INFO:agents.father_agent:Step: 95, Training loss: 0.39992424845695496
INFO:agents.father_agent:Step: 100, Training loss: 0.3101545572280884
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -70.37511444091797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -0.25259190797805786
INFO:tools.evaluation_results_class:Average Discounted Reward = 9.462736129760742
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8765315739868049
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6162.19091796875
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 196.19415645617343
INFO:tools.evaluation_results_class:Counted Episodes = 1061
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 0.9851059317588806
INFO:agents.father_agent:Step: 110, Training loss: 0.20754212141036987
INFO:agents.father_agent:Step: 115, Training loss: 0.4514297842979431
INFO:agents.father_agent:Step: 120, Training loss: 0.672254204750061
INFO:agents.father_agent:Step: 125, Training loss: 0.31226781010627747
INFO:agents.father_agent:Step: 130, Training loss: 0.4365755319595337
INFO:agents.father_agent:Step: 135, Training loss: 0.472421258687973
INFO:agents.father_agent:Step: 140, Training loss: 0.1949264109134674
INFO:agents.father_agent:Step: 145, Training loss: 0.40777307748794556
INFO:agents.father_agent:Step: 150, Training loss: 0.4310775399208069
INFO:agents.father_agent:Step: 155, Training loss: 0.2320001870393753
INFO:agents.father_agent:Step: 160, Training loss: 0.07574372738599777
INFO:agents.father_agent:Step: 165, Training loss: 0.38049057126045227
INFO:agents.father_agent:Step: 170, Training loss: 0.3227653503417969
INFO:agents.father_agent:Step: 175, Training loss: 0.2842187285423279
INFO:agents.father_agent:Step: 180, Training loss: 0.3354175388813019
INFO:agents.father_agent:Step: 185, Training loss: 0.36048877239227295
INFO:agents.father_agent:Step: 190, Training loss: 0.3809216022491455
INFO:agents.father_agent:Step: 195, Training loss: 0.6498070359230042
INFO:agents.father_agent:Step: 200, Training loss: 0.5627548098564148
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -18.394737243652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 61.592655181884766
INFO:tools.evaluation_results_class:Average Discounted Reward = 35.574806213378906
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9998424204223133
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 273.5600280761719
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.085408131106206
INFO:tools.evaluation_results_class:Counted Episodes = 6346
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 0.35900503396987915
INFO:agents.father_agent:Step: 210, Training loss: 0.3076908588409424
INFO:agents.father_agent:Step: 215, Training loss: 0.47868233919143677
INFO:agents.father_agent:Step: 220, Training loss: 0.38582301139831543
INFO:agents.father_agent:Step: 225, Training loss: 0.3760984539985657
INFO:agents.father_agent:Step: 230, Training loss: 0.6242892742156982
INFO:agents.father_agent:Step: 235, Training loss: 0.5895354151725769
INFO:agents.father_agent:Step: 240, Training loss: 0.5936279892921448
INFO:agents.father_agent:Step: 245, Training loss: 0.3178994655609131
INFO:agents.father_agent:Step: 250, Training loss: 0.3499578833580017
INFO:agents.father_agent:Step: 255, Training loss: 0.37059664726257324
INFO:agents.father_agent:Step: 260, Training loss: 0.5185961723327637
INFO:agents.father_agent:Step: 265, Training loss: 0.35127589106559753
INFO:agents.father_agent:Step: 270, Training loss: 0.44419920444488525
INFO:agents.father_agent:Step: 275, Training loss: 0.28875523805618286
INFO:agents.father_agent:Step: 280, Training loss: 0.6678634285926819
INFO:agents.father_agent:Step: 285, Training loss: 0.29614073038101196
INFO:agents.father_agent:Step: 290, Training loss: 0.30903804302215576
INFO:agents.father_agent:Step: 295, Training loss: 0.32114312052726746
INFO:agents.father_agent:Step: 300, Training loss: 0.4559297263622284
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.31595993041992
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.13053512573242
INFO:tools.evaluation_results_class:Average Discounted Reward = 14.283509254455566
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9930811808118081
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1960.849853515625
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 121.10793357933579
INFO:tools.evaluation_results_class:Counted Episodes = 2168
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 0.9282492995262146
INFO:agents.father_agent:Step: 310, Training loss: 0.23958373069763184
INFO:agents.father_agent:Step: 315, Training loss: 0.5964336395263672
INFO:agents.father_agent:Step: 320, Training loss: 0.4130253493785858
INFO:agents.father_agent:Step: 325, Training loss: 0.2054300308227539
INFO:agents.father_agent:Step: 330, Training loss: 0.36618658900260925
INFO:agents.father_agent:Step: 335, Training loss: 0.3461274206638336
INFO:agents.father_agent:Step: 340, Training loss: 0.22554366290569305
INFO:agents.father_agent:Step: 345, Training loss: 0.28870126605033875
INFO:agents.father_agent:Step: 350, Training loss: 0.19234015047550201
INFO:agents.father_agent:Step: 355, Training loss: 0.3134906589984894
INFO:agents.father_agent:Step: 360, Training loss: 0.43215370178222656
INFO:agents.father_agent:Step: 365, Training loss: 0.13954152166843414
INFO:agents.father_agent:Step: 370, Training loss: 0.48852023482322693
INFO:agents.father_agent:Step: 375, Training loss: 0.27208462357521057
INFO:agents.father_agent:Step: 380, Training loss: 0.24269914627075195
INFO:agents.father_agent:Step: 385, Training loss: 0.5401178002357483
INFO:agents.father_agent:Step: 390, Training loss: 0.33038368821144104
INFO:agents.father_agent:Step: 395, Training loss: 0.366160124540329
INFO:agents.father_agent:Step: 400, Training loss: 0.43456828594207764
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -58.89713668823242
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 15.286930084228516
INFO:tools.evaluation_results_class:Average Discounted Reward = 12.240965843200684
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9273008507347255
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4451.341796875
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 167.2474864655839
INFO:tools.evaluation_results_class:Counted Episodes = 1293
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 0.3563568890094757
INFO:agents.father_agent:Step: 410, Training loss: 0.3583585321903229
INFO:agents.father_agent:Step: 415, Training loss: 0.16671961545944214
INFO:agents.father_agent:Step: 420, Training loss: 0.4040392339229584
INFO:agents.father_agent:Step: 425, Training loss: 0.09168978035449982
INFO:agents.father_agent:Step: 430, Training loss: 0.3070811927318573
INFO:agents.father_agent:Step: 435, Training loss: 0.2968336343765259
INFO:agents.father_agent:Step: 440, Training loss: 0.41037580370903015
INFO:agents.father_agent:Step: 445, Training loss: 0.11642996221780777
INFO:agents.father_agent:Step: 450, Training loss: 0.3653903603553772
INFO:agents.father_agent:Step: 455, Training loss: 0.23808646202087402
INFO:agents.father_agent:Step: 460, Training loss: 0.4259876012802124
INFO:agents.father_agent:Step: 465, Training loss: 0.26932695508003235
INFO:agents.father_agent:Step: 470, Training loss: 0.1864374727010727
INFO:agents.father_agent:Step: 475, Training loss: 0.4616714417934418
INFO:agents.father_agent:Step: 480, Training loss: 0.46815401315689087
INFO:agents.father_agent:Step: 485, Training loss: 0.18464885652065277
INFO:agents.father_agent:Step: 490, Training loss: 0.7024409174919128
INFO:agents.father_agent:Step: 495, Training loss: 0.8771246671676636
INFO:agents.father_agent:Step: 500, Training loss: 0.27063626050949097
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -65.11052703857422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 10.799247741699219
INFO:tools.evaluation_results_class:Average Discounted Reward = 4.724502086639404
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9488721804511279
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4721.13427734375
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 174.3736842105263
INFO:tools.evaluation_results_class:Counted Episodes = 1330
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -65.68081665039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 10.231132507324219
INFO:tools.evaluation_results_class:Average Discounted Reward = 4.61916446685791
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9488993710691824
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4386.19677734375
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 176.35141509433961
INFO:tools.evaluation_results_class:Counted Episodes = 1272
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -75.197509765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -2.3957571983337402
INFO:tools.evaluation_results_class:Average Discounted Reward = 1.5538413524627686
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9100219458668617
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5465.873046875
INFO:tools.evaluation_results_class:Current Best Return = -75.197509765625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9100219458668617
INFO:tools.evaluation_results_class:Average Episode Length = 205.18288222384786
INFO:tools.evaluation_results_class:Counted Episodes = 1367
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -140.30038452148438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -60.66263961791992
INFO:tools.evaluation_results_class:Average Discounted Reward = -64.9561538696289
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9954716981132076
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7495.4375
INFO:tools.evaluation_results_class:Current Best Return = -140.30038452148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9954716981132076
INFO:tools.evaluation_results_class:Average Episode Length = 220.69358490566037
INFO:tools.evaluation_results_class:Counted Episodes = 1325
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 116.1603 achieved after 5967.13 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 126.6941 achieved after 5967.13 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 141.0491 achieved after 5967.14 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 141.687 achieved after 5967.16 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 142.3652 achieved after 5967.19 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 143.9299 achieved after 5967.21 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 144.4418 achieved after 5967.23 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 144.961 achieved after 5967.26 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 149.0443 achieved after 5967.27 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 149.4537 achieved after 5967.29 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 149.8678 achieved after 5967.3 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 149.86780192431883
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
INFO:robust_rl.robust_rl_trainer:Iteration 12 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.53333282470703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.631461143493652
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.0978264808654785
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9520599250936329
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4132.60693359375
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 170.53408239700374
INFO:tools.evaluation_results_class:Counted Episodes = 1335
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.25406068563461304
INFO:agents.father_agent:Step: 5, Training loss: 0.41691020131111145
INFO:agents.father_agent:Step: 10, Training loss: 0.4056929349899292
INFO:agents.father_agent:Step: 15, Training loss: 0.4769570529460907
INFO:agents.father_agent:Step: 20, Training loss: 0.18089571595191956
INFO:agents.father_agent:Step: 25, Training loss: 0.11204899847507477
INFO:agents.father_agent:Step: 30, Training loss: 0.16051146388053894
INFO:agents.father_agent:Step: 35, Training loss: 0.2166140079498291
INFO:agents.father_agent:Step: 40, Training loss: 0.5734995007514954
INFO:agents.father_agent:Step: 45, Training loss: 0.2951965928077698
INFO:agents.father_agent:Step: 50, Training loss: 0.2698202133178711
INFO:agents.father_agent:Step: 55, Training loss: 0.5400598645210266
INFO:agents.father_agent:Step: 60, Training loss: 0.2996687889099121
INFO:agents.father_agent:Step: 65, Training loss: 0.3202284574508667
INFO:agents.father_agent:Step: 70, Training loss: 0.27544909715652466
INFO:agents.father_agent:Step: 75, Training loss: 0.35437870025634766
INFO:agents.father_agent:Step: 80, Training loss: 0.3439573645591736
INFO:agents.father_agent:Step: 85, Training loss: 0.9019111394882202
INFO:agents.father_agent:Step: 90, Training loss: 0.1741589605808258
INFO:agents.father_agent:Step: 95, Training loss: 0.39558619260787964
INFO:agents.father_agent:Step: 100, Training loss: 0.08401570469141006
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -56.24882125854492
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 21.755226135253906
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.830532550811768
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9750505731625084
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3153.14697265625
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 160.54821308159137
INFO:tools.evaluation_results_class:Counted Episodes = 1483
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 0.4145243763923645
INFO:agents.father_agent:Step: 110, Training loss: 0.089744433760643
INFO:agents.father_agent:Step: 115, Training loss: 0.30193373560905457
INFO:agents.father_agent:Step: 120, Training loss: 0.56990647315979
INFO:agents.father_agent:Step: 125, Training loss: 0.27509427070617676
INFO:agents.father_agent:Step: 130, Training loss: 0.24369078874588013
INFO:agents.father_agent:Step: 135, Training loss: 0.2728383541107178
INFO:agents.father_agent:Step: 140, Training loss: 0.2648038864135742
INFO:agents.father_agent:Step: 145, Training loss: 0.15819916129112244
INFO:agents.father_agent:Step: 150, Training loss: 0.16658304631710052
INFO:agents.father_agent:Step: 155, Training loss: 0.4699200987815857
INFO:agents.father_agent:Step: 160, Training loss: 0.18832215666770935
INFO:agents.father_agent:Step: 165, Training loss: 0.43079668283462524
INFO:agents.father_agent:Step: 170, Training loss: 0.333462655544281
INFO:agents.father_agent:Step: 175, Training loss: 0.2269861102104187
INFO:agents.father_agent:Step: 180, Training loss: 0.3300793170928955
INFO:agents.father_agent:Step: 185, Training loss: 0.6769376397132874
INFO:agents.father_agent:Step: 190, Training loss: 0.18602512776851654
INFO:agents.father_agent:Step: 195, Training loss: 0.2714187502861023
INFO:agents.father_agent:Step: 200, Training loss: 0.12484877556562424
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -50.0839958190918
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 26.482526779174805
INFO:tools.evaluation_results_class:Average Discounted Reward = 15.708271026611328
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9570815450643777
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3884.933837890625
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 132.9785407725322
INFO:tools.evaluation_results_class:Counted Episodes = 1631
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 0.27104389667510986
INFO:agents.father_agent:Step: 210, Training loss: 0.2414717674255371
INFO:agents.father_agent:Step: 215, Training loss: 0.08841559290885925
INFO:agents.father_agent:Step: 220, Training loss: 0.40484195947647095
INFO:agents.father_agent:Step: 225, Training loss: 0.017967984080314636
INFO:agents.father_agent:Step: 230, Training loss: 0.16009259223937988
INFO:agents.father_agent:Step: 235, Training loss: 0.15436190366744995
INFO:agents.father_agent:Step: 240, Training loss: 0.20608070492744446
INFO:agents.father_agent:Step: 245, Training loss: 0.04715704917907715
INFO:agents.father_agent:Step: 250, Training loss: 0.4672325551509857
INFO:agents.father_agent:Step: 255, Training loss: 0.25975099205970764
INFO:agents.father_agent:Step: 260, Training loss: 0.15352407097816467
INFO:agents.father_agent:Step: 265, Training loss: 0.3250453472137451
INFO:agents.father_agent:Step: 270, Training loss: 0.22412243485450745
INFO:agents.father_agent:Step: 275, Training loss: 0.11959879100322723
INFO:agents.father_agent:Step: 280, Training loss: 0.4045226573944092
INFO:agents.father_agent:Step: 285, Training loss: 0.4427178204059601
INFO:agents.father_agent:Step: 290, Training loss: 0.22644953429698944
INFO:agents.father_agent:Step: 295, Training loss: 0.4529752731323242
INFO:agents.father_agent:Step: 300, Training loss: 0.5462599396705627
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -35.32785415649414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 43.71022033691406
INFO:tools.evaluation_results_class:Average Discounted Reward = 24.207250595092773
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9879759519038076
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2112.450439453125
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 94.56993987975952
INFO:tools.evaluation_results_class:Counted Episodes = 2495
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 0.6385255455970764
INFO:agents.father_agent:Step: 310, Training loss: 0.19439265131950378
INFO:agents.father_agent:Step: 315, Training loss: 0.09255602955818176
INFO:agents.father_agent:Step: 320, Training loss: 0.34640178084373474
INFO:agents.father_agent:Step: 325, Training loss: 0.3417297899723053
INFO:agents.father_agent:Step: 330, Training loss: 0.20719656348228455
INFO:agents.father_agent:Step: 335, Training loss: 0.12533774971961975
INFO:agents.father_agent:Step: 340, Training loss: 0.39602017402648926
INFO:agents.father_agent:Step: 345, Training loss: 0.35724180936813354
INFO:agents.father_agent:Step: 350, Training loss: 0.5432264804840088
INFO:agents.father_agent:Step: 355, Training loss: 0.3276665508747101
INFO:agents.father_agent:Step: 360, Training loss: 0.02790336310863495
INFO:agents.father_agent:Step: 365, Training loss: 0.10016902536153793
INFO:agents.father_agent:Step: 370, Training loss: 0.5095372796058655
INFO:agents.father_agent:Step: 375, Training loss: 0.1340983808040619
INFO:agents.father_agent:Step: 380, Training loss: 0.19377660751342773
INFO:agents.father_agent:Step: 385, Training loss: 0.19073539972305298
INFO:agents.father_agent:Step: 390, Training loss: 0.3463900089263916
INFO:agents.father_agent:Step: 395, Training loss: 0.40597233176231384
INFO:agents.father_agent:Step: 400, Training loss: 0.10559093952178955
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -25.268566131591797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.316925048828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.38654327392578
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9948186528497409
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1140.3857421875
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 69.30944156591825
INFO:tools.evaluation_results_class:Counted Episodes = 3474
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 0.3565664291381836
INFO:agents.father_agent:Step: 410, Training loss: 0.23024585843086243
INFO:agents.father_agent:Step: 415, Training loss: 0.10953424870967865
INFO:agents.father_agent:Step: 420, Training loss: 0.1457536220550537
INFO:agents.father_agent:Step: 425, Training loss: 0.22902414202690125
INFO:agents.father_agent:Step: 430, Training loss: 0.20820605754852295
INFO:agents.father_agent:Step: 435, Training loss: 0.35759398341178894
INFO:agents.father_agent:Step: 440, Training loss: 0.31785717606544495
INFO:agents.father_agent:Step: 445, Training loss: 0.8093997836112976
INFO:agents.father_agent:Step: 450, Training loss: 0.37918585538864136
INFO:agents.father_agent:Step: 455, Training loss: 0.5320019721984863
INFO:agents.father_agent:Step: 460, Training loss: 0.09959297627210617
INFO:agents.father_agent:Step: 465, Training loss: 0.3632877469062805
INFO:agents.father_agent:Step: 470, Training loss: 0.32456207275390625
INFO:agents.father_agent:Step: 475, Training loss: 0.05036859214305878
INFO:agents.father_agent:Step: 480, Training loss: 0.3386675715446472
INFO:agents.father_agent:Step: 485, Training loss: 0.4067709743976593
INFO:agents.father_agent:Step: 490, Training loss: 0.2560296356678009
INFO:agents.father_agent:Step: 495, Training loss: 0.1850982904434204
INFO:agents.father_agent:Step: 500, Training loss: 0.34218698740005493
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -27.991390228271484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 51.84233856201172
INFO:tools.evaluation_results_class:Average Discounted Reward = 27.500221252441406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9979216152019003
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1044.9930419921875
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 80.68646080760095
INFO:tools.evaluation_results_class:Counted Episodes = 3368
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -29.68834114074707
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 50.00416564941406
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.111221313476562
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9961563100576554
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1152.2337646484375
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.69474695707879
INFO:tools.evaluation_results_class:Counted Episodes = 3122
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -32.75495529174805
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 46.3390998840332
INFO:tools.evaluation_results_class:Average Discounted Reward = 24.768800735473633
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9886756841774142
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1617.2725830078125
INFO:tools.evaluation_results_class:Current Best Return = -32.75495529174805
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9886756841774142
INFO:tools.evaluation_results_class:Average Episode Length = 94.45894935514313
INFO:tools.evaluation_results_class:Counted Episodes = 3179
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -88.1971664428711
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -8.19717025756836
INFO:tools.evaluation_results_class:Average Discounted Reward = -30.565235137939453
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5410.8818359375
INFO:tools.evaluation_results_class:Current Best Return = -88.1971664428711
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.17138364779875
INFO:tools.evaluation_results_class:Counted Episodes = 3180
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 67.4454 achieved after 6509.7 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 74.319 achieved after 6509.7 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 77.3413 achieved after 6509.71 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 79.0259 achieved after 6509.73 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 80.803 achieved after 6509.76 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 81.9913 achieved after 6509.81 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 83.3885 achieved after 6509.83 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 84.3813 achieved after 6509.84 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 88.6518 achieved after 6509.85 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 88.8325 achieved after 6509.88 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 89.0312 achieved after 6509.88 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 90.183 achieved after 6509.91 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 90.18298368592384
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
INFO:robust_rl.robust_rl_trainer:Iteration 13 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -28.149112701416016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 51.67942428588867
INFO:tools.evaluation_results_class:Average Discounted Reward = 27.071746826171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9978567054500919
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 976.7821655273438
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 81.91488058787507
INFO:tools.evaluation_results_class:Counted Episodes = 3266
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.08742401003837585
INFO:agents.father_agent:Step: 5, Training loss: 0.21601882576942444
INFO:agents.father_agent:Step: 10, Training loss: 0.07750227302312851
INFO:agents.father_agent:Step: 15, Training loss: 0.4580036997795105
INFO:agents.father_agent:Step: 20, Training loss: 0.3768148422241211
INFO:agents.father_agent:Step: 25, Training loss: 0.3661530315876007
INFO:agents.father_agent:Step: 30, Training loss: 0.06883223354816437
INFO:agents.father_agent:Step: 35, Training loss: 0.18613217771053314
INFO:agents.father_agent:Step: 40, Training loss: 0.3807082176208496
INFO:agents.father_agent:Step: 45, Training loss: 0.2458268105983734
INFO:agents.father_agent:Step: 50, Training loss: 0.2006371021270752
INFO:agents.father_agent:Step: 55, Training loss: 0.20368117094039917
INFO:agents.father_agent:Step: 60, Training loss: 0.14224356412887573
INFO:agents.father_agent:Step: 65, Training loss: 0.15237963199615479
INFO:agents.father_agent:Step: 70, Training loss: 0.08254563808441162
INFO:agents.father_agent:Step: 75, Training loss: 0.27395254373550415
INFO:agents.father_agent:Step: 80, Training loss: 0.19797757267951965
INFO:agents.father_agent:Step: 85, Training loss: 0.16913564503192902
INFO:agents.father_agent:Step: 90, Training loss: 0.22027277946472168
INFO:agents.father_agent:Step: 95, Training loss: 0.5945037603378296
INFO:agents.father_agent:Step: 100, Training loss: 0.30153098702430725
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -20.74483871459961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 59.25516128540039
INFO:tools.evaluation_results_class:Average Discounted Reward = 32.54618453979492
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 398.153076171875
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 50.02669563867764
INFO:tools.evaluation_results_class:Counted Episodes = 6443
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 0.22455498576164246
INFO:agents.father_agent:Step: 110, Training loss: 0.16672207415103912
INFO:agents.father_agent:Step: 115, Training loss: 0.38861531019210815
INFO:agents.father_agent:Step: 120, Training loss: 0.4140607714653015
INFO:agents.father_agent:Step: 125, Training loss: 0.500623345375061
INFO:agents.father_agent:Step: 130, Training loss: 0.2146543562412262
INFO:agents.father_agent:Step: 135, Training loss: 0.1598244160413742
INFO:agents.father_agent:Step: 140, Training loss: 0.3270717263221741
INFO:agents.father_agent:Step: 145, Training loss: 0.23941068351268768
INFO:agents.father_agent:Step: 150, Training loss: 0.20337893068790436
INFO:agents.father_agent:Step: 155, Training loss: 0.297587513923645
INFO:agents.father_agent:Step: 160, Training loss: 0.295918345451355
INFO:agents.father_agent:Step: 165, Training loss: 0.4779704511165619
INFO:agents.father_agent:Step: 170, Training loss: 0.17736226320266724
INFO:agents.father_agent:Step: 175, Training loss: 0.3771643340587616
INFO:agents.father_agent:Step: 180, Training loss: 0.4662937819957733
INFO:agents.father_agent:Step: 185, Training loss: 0.16312171518802643
INFO:agents.father_agent:Step: 190, Training loss: 0.40407872200012207
INFO:agents.father_agent:Step: 195, Training loss: 0.2807385325431824
INFO:agents.father_agent:Step: 200, Training loss: 0.3199310302734375
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -24.865583419799805
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.13441467285156
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.69507598876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 554.3411865234375
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.93426883308715
INFO:tools.evaluation_results_class:Counted Episodes = 5416
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 0.44319459795951843
INFO:agents.father_agent:Step: 210, Training loss: 0.21864894032478333
INFO:agents.father_agent:Step: 215, Training loss: 0.15130390226840973
INFO:agents.father_agent:Step: 220, Training loss: 0.2958167493343353
INFO:agents.father_agent:Step: 225, Training loss: 0.2918700575828552
INFO:agents.father_agent:Step: 230, Training loss: 0.1757001280784607
INFO:agents.father_agent:Step: 235, Training loss: 0.3543436527252197
INFO:agents.father_agent:Step: 240, Training loss: 0.1710510551929474
INFO:agents.father_agent:Step: 245, Training loss: 0.27064648270606995
INFO:agents.father_agent:Step: 250, Training loss: 0.32682058215141296
INFO:agents.father_agent:Step: 255, Training loss: 0.24391672015190125
INFO:agents.father_agent:Step: 260, Training loss: 0.18343578279018402
INFO:agents.father_agent:Step: 265, Training loss: 0.37956148386001587
INFO:agents.father_agent:Step: 270, Training loss: 0.34080401062965393
INFO:agents.father_agent:Step: 275, Training loss: 0.3005216121673584
INFO:agents.father_agent:Step: 280, Training loss: 0.264693945646286
INFO:agents.father_agent:Step: 285, Training loss: 0.24575074017047882
INFO:agents.father_agent:Step: 290, Training loss: 0.4222494661808014
INFO:agents.father_agent:Step: 295, Training loss: 0.30427882075309753
INFO:agents.father_agent:Step: 300, Training loss: 0.1666589081287384
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -21.631696701049805
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 58.36830139160156
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.10721778869629
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 421.4117126464844
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.93821399839099
INFO:tools.evaluation_results_class:Counted Episodes = 6215
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 0.24139729142189026
INFO:agents.father_agent:Step: 310, Training loss: 0.18897700309753418
INFO:agents.father_agent:Step: 315, Training loss: 0.3945583701133728
INFO:agents.father_agent:Step: 320, Training loss: 0.2618658244609833
INFO:agents.father_agent:Step: 325, Training loss: 0.15158537030220032
INFO:agents.father_agent:Step: 330, Training loss: 0.27625417709350586
INFO:agents.father_agent:Step: 335, Training loss: 0.4410367012023926
INFO:agents.father_agent:Step: 340, Training loss: 0.12613186240196228
INFO:agents.father_agent:Step: 345, Training loss: 0.21801526844501495
INFO:agents.father_agent:Step: 350, Training loss: 0.20460397005081177
INFO:agents.father_agent:Step: 355, Training loss: 0.39214688539505005
INFO:agents.father_agent:Step: 360, Training loss: 0.21405474841594696
INFO:agents.father_agent:Step: 365, Training loss: 0.5623673796653748
INFO:agents.father_agent:Step: 370, Training loss: 0.3180136978626251
INFO:agents.father_agent:Step: 375, Training loss: 0.3505070209503174
INFO:agents.father_agent:Step: 380, Training loss: 0.12853237986564636
INFO:agents.father_agent:Step: 385, Training loss: 0.3667486310005188
INFO:agents.father_agent:Step: 390, Training loss: 0.30849647521972656
INFO:agents.father_agent:Step: 395, Training loss: 0.3540806770324707
INFO:agents.father_agent:Step: 400, Training loss: 0.38448116183280945
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -22.058935165405273
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.941062927246094
INFO:tools.evaluation_results_class:Average Discounted Reward = 31.081106185913086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 480.135498046875
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 51.08435266084194
INFO:tools.evaluation_results_class:Counted Episodes = 6295
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 0.42127126455307007
INFO:agents.father_agent:Step: 410, Training loss: 0.32741618156433105
INFO:agents.father_agent:Step: 415, Training loss: 0.4154708981513977
INFO:agents.father_agent:Step: 420, Training loss: 0.43878307938575745
INFO:agents.father_agent:Step: 425, Training loss: 0.2907680869102478
INFO:agents.father_agent:Step: 430, Training loss: 0.20816923677921295
INFO:agents.father_agent:Step: 435, Training loss: 0.5356432199478149
INFO:agents.father_agent:Step: 440, Training loss: 0.4226953685283661
INFO:agents.father_agent:Step: 445, Training loss: 0.29303500056266785
INFO:agents.father_agent:Step: 450, Training loss: 0.08326130360364914
INFO:agents.father_agent:Step: 455, Training loss: 0.33521437644958496
INFO:agents.father_agent:Step: 460, Training loss: 0.17015224695205688
INFO:agents.father_agent:Step: 465, Training loss: 0.21479524672031403
INFO:agents.father_agent:Step: 470, Training loss: 0.2736048102378845
INFO:agents.father_agent:Step: 475, Training loss: 0.1556100845336914
INFO:agents.father_agent:Step: 480, Training loss: 0.35997891426086426
INFO:agents.father_agent:Step: 485, Training loss: 0.2536797821521759
INFO:agents.father_agent:Step: 490, Training loss: 0.1359095275402069
INFO:agents.father_agent:Step: 495, Training loss: 0.3126305341720581
INFO:agents.father_agent:Step: 500, Training loss: 0.23424461483955383
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -25.18490982055664
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.81509017944336
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.56181526184082
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 593.9874877929688
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.152545454545454
INFO:tools.evaluation_results_class:Counted Episodes = 5500
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -24.91020393371582
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 55.08979415893555
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.773372650146484
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 559.9508056640625
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.329736649597656
INFO:tools.evaluation_results_class:Counted Episodes = 5468
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -26.38595199584961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 53.61404800415039
INFO:tools.evaluation_results_class:Average Discounted Reward = 25.488117218017578
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 686.5606079101562
INFO:tools.evaluation_results_class:Current Best Return = -26.38595199584961
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.940622737146995
INFO:tools.evaluation_results_class:Counted Episodes = 5524
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -89.5328369140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -9.532837867736816
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.60930061340332
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5234.681640625
INFO:tools.evaluation_results_class:Current Best Return = -89.5328369140625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 58.988933236574745
INFO:tools.evaluation_results_class:Counted Episodes = 5512
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 81
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 67.5599 achieved after 7057.88 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 72.1744 achieved after 7057.89 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 74.7085 achieved after 7057.91 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 78.4525 achieved after 7057.91 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 83.3534 achieved after 7057.93 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 86.3842 achieved after 7057.94 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 88.0473 achieved after 7058.0 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 88.9424 achieved after 7058.06 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 91.4485 achieved after 7058.06 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=3, o1y=5, o2x=6, o2y=6
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 91.44851266272805
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
INFO:robust_rl.robust_rl_trainer:Iteration 14 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -25.312885284423828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 54.68711471557617
INFO:tools.evaluation_results_class:Average Discounted Reward = 26.488786697387695
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 616.4454956054688
INFO:tools.evaluation_results_class:Current Best Return = -17.955677032470703
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 57.96733212341198
INFO:tools.evaluation_results_class:Counted Episodes = 5510
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 0.3375914394855499
INFO:agents.father_agent:Step: 5, Training loss: 0.37175026535987854
INFO:agents.father_agent:Step: 10, Training loss: 0.21521756052970886
INFO:agents.father_agent:Step: 15, Training loss: 0.1592848002910614
INFO:agents.father_agent:Step: 20, Training loss: 0.2761290967464447
INFO:agents.father_agent:Step: 25, Training loss: 0.2479662448167801
INFO:agents.father_agent:Step: 30, Training loss: 0.6478481292724609
INFO:agents.father_agent:Step: 35, Training loss: 0.36154675483703613
INFO:agents.father_agent:Step: 40, Training loss: 0.47174882888793945
INFO:agents.father_agent:Step: 45, Training loss: 0.2247072160243988
INFO:agents.father_agent:Step: 50, Training loss: 0.23187020421028137
INFO:agents.father_agent:Step: 55, Training loss: 0.21728260815143585
INFO:agents.father_agent:Step: 60, Training loss: 0.2754516303539276
INFO:agents.father_agent:Step: 65, Training loss: 0.08390896767377853
INFO:agents.father_agent:Step: 70, Training loss: 0.1129654198884964
INFO:agents.father_agent:Step: 75, Training loss: 0.2840101718902588
INFO:agents.father_agent:Step: 80, Training loss: 0.2986670732498169
INFO:agents.father_agent:Step: 85, Training loss: 0.12231671065092087
INFO:agents.father_agent:Step: 90, Training loss: 0.27692317962646484
INFO:agents.father_agent:Step: 95, Training loss: 0.15871861577033997
INFO:agents.father_agent:Step: 100, Training loss: 0.20452606678009033
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -17.913331985473633
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 62.086666107177734
INFO:tools.evaluation_results_class:Average Discounted Reward = 34.709259033203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 119.29991912841797
INFO:tools.evaluation_results_class:Current Best Return = -17.913331985473633
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 49.83377104897266
INFO:tools.evaluation_results_class:Counted Episodes = 6473
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 0.3211766481399536
INFO:agents.father_agent:Step: 110, Training loss: 0.3743092715740204
INFO:agents.father_agent:Step: 115, Training loss: 0.11733052134513855
INFO:agents.father_agent:Step: 120, Training loss: 0.2965834140777588
INFO:agents.father_agent:Step: 125, Training loss: 0.3554607033729553
INFO:agents.father_agent:Step: 130, Training loss: 0.48003119230270386
INFO:agents.father_agent:Step: 135, Training loss: 0.5151112675666809
INFO:agents.father_agent:Step: 140, Training loss: 0.35064488649368286
INFO:agents.father_agent:Step: 145, Training loss: 0.19023814797401428
INFO:agents.father_agent:Step: 150, Training loss: 0.309037446975708
INFO:agents.father_agent:Step: 155, Training loss: 0.3187396824359894
INFO:agents.father_agent:Step: 160, Training loss: 0.16597144305706024
INFO:agents.father_agent:Step: 165, Training loss: 0.45222437381744385
INFO:agents.father_agent:Step: 170, Training loss: 0.19655928015708923
INFO:agents.father_agent:Step: 175, Training loss: 0.24875058233737946
INFO:agents.father_agent:Step: 180, Training loss: 0.21570374071598053
INFO:agents.father_agent:Step: 185, Training loss: 0.3704656958580017
INFO:agents.father_agent:Step: 190, Training loss: 0.498421311378479
INFO:agents.father_agent:Step: 195, Training loss: 0.28843414783477783
INFO:agents.father_agent:Step: 200, Training loss: 0.22596792876720428
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -22.06085777282715
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 57.939144134521484
INFO:tools.evaluation_results_class:Average Discounted Reward = 30.056917190551758
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 394.9240417480469
INFO:tools.evaluation_results_class:Current Best Return = -17.913331985473633
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 54.36514663502289
INFO:tools.evaluation_results_class:Counted Episodes = 5899
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 0.48658204078674316
INFO:agents.father_agent:Step: 210, Training loss: 0.17979197204113007
INFO:agents.father_agent:Step: 215, Training loss: 0.5581255555152893
INFO:agents.father_agent:Step: 220, Training loss: 0.7380433082580566
INFO:agents.father_agent:Step: 225, Training loss: 0.16070950031280518

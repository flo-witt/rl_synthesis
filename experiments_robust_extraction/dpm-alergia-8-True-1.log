2025-08-03 21:20:04,463 - sketch.py:80 - loading sketch from /home/ihudak/synthesis/models_robust_subset/dpm/sketch.templ ...
2025-08-03 21:20:04,463 - sketch.py:84 - assuming sketch in PRISM format...
2025-08-03 21:20:04,467 - prism_parser.py:31 - PRISM model type: POMDP
2025-08-03 21:20:04,467 - prism_parser.py:40 - processing hole definitions...
2025-08-03 21:20:04,468 - prism_parser.py:220 - loading properties from /home/ihudak/synthesis/models_robust_subset/dpm/sketch.props ...
2025-08-03 21:20:04,468 - prism_parser.py:236 - found the following specification: optimality: R{"rew"}max=? [F (bat = 0)] 
2025-08-03 21:20:04,468 - jani.py:41 - constructing JANI program...
2025-08-03 21:20:04,476 - jani.py:61 - constructing the quotient...
2025-08-03 21:20:04,540 - jani.py:66 - associating choices of the quotient with hole assignments...
2025-08-03 21:20:04,731 - sketch.py:135 - sketch parsing OK
2025-08-03 21:20:04,732 - sketch.py:144 - WARNING: choice labeling for the quotient is not canonic
2025-08-03 21:20:04,733 - sketch.py:148 - constructed explicit quotient having 737 states and 10594 choices
2025-08-03 21:20:04,733 - sketch.py:154 - found the following specification optimality: R{"rew"}max=? [F (bat = 0)] 
2025-08-03 21:20:04,745 - vectorized_sim_initializer.py:61 - Compiling model dpm...
Hello <stormpy.storage.storage.StateValuation object at 0x764f12818db0>
2025-08-03 21:20:04,805 - storm_vec_env.py:70 - Computing row map
2025-08-03 21:20:04,821 - storm_vec_env.py:97 - Computing transitions
2025-08-03 21:20:04,855 - storm_vec_env.py:100 - Computing allowed actions
2025-08-03 21:20:04,855 - storm_vec_env.py:114 - Computing sinks
2025-08-03 21:20:04,855 - storm_vec_env.py:119 - Computing raw rewards
2025-08-03 21:20:04,864 - storm_vec_env.py:143 - Computing labels
2025-08-03 21:20:04,864 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-03 21:20:04,864 - storm_vec_env.py:161 - Computing metalabels
2025-08-03 21:20:04,864 - storm_vec_env.py:175 - Computing observations
2025-08-03 21:20:04,944 - environment_wrapper_vec.py:146 - Grid-like renderer not possible to initialize.
2025-08-03 21:20:05,053 - environment_wrapper_vec.py:153 - Vectorized simulator initialized with 256 environments.
2025-08-03 21:20:05,276 - recurrent_ppo_agent.py:98 - Agent initialized
2025-08-03 21:20:05,298 - recurrent_ppo_agent.py:100 - Replay buffer initialized
Using unmasked training with policy wrapper.
2025-08-03 21:20:05,307 - recurrent_ppo_agent.py:112 - Collector driver initialized
2025-08-03 21:20:05,310 - robust_rl_trainer.py:432 - Iteration 1 of pure RL loop
2025-08-03 21:20:05,349 - storm_vec_env.py:70 - Computing row map
2025-08-03 21:20:05,364 - storm_vec_env.py:97 - Computing transitions
2025-08-03 21:20:05,398 - storm_vec_env.py:100 - Computing allowed actions
2025-08-03 21:20:05,398 - storm_vec_env.py:114 - Computing sinks
2025-08-03 21:20:05,398 - storm_vec_env.py:119 - Computing raw rewards
2025-08-03 21:20:05,407 - storm_vec_env.py:143 - Computing labels
2025-08-03 21:20:05,407 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-03 21:20:05,407 - storm_vec_env.py:161 - Computing metalabels
2025-08-03 21:20:05,407 - storm_vec_env.py:175 - Computing observations
2025-08-03 21:20:05,491 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-03 21:20:05,492 - father_agent.py:540 - Before training evaluation.
2025-08-03 21:20:05,612 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:20:05,621 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:20:13,079 - evaluation_results_class.py:131 - Average Return = 226.02711486816406
2025-08-03 21:20:13,079 - evaluation_results_class.py:133 - Average Virtual Goal Value = 23.59123992919922
2025-08-03 21:20:13,079 - evaluation_results_class.py:135 - Average Discounted Reward = 7.835872173309326
2025-08-03 21:20:13,079 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9942648592283628
2025-08-03 21:20:13,079 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:20:13,080 - evaluation_results_class.py:141 - Variance of Return = 79507.7890625
2025-08-03 21:20:13,080 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:20:13,080 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9942648592283628
2025-08-03 21:20:13,080 - evaluation_results_class.py:147 - Average Episode Length = 133.8806047966632
2025-08-03 21:20:13,080 - evaluation_results_class.py:149 - Counted Episodes = 1918
2025-08-03 21:20:13,216 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:20:13,227 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:20:13,326 - father_agent.py:436 - Training agent on-policy
2025-08-03 21:20:27,940 - father_agent.py:386 - Step: 0, Training loss: 37.96004867553711
2025-08-03 21:20:29,521 - father_agent.py:386 - Step: 5, Training loss: 11.251276969909668
2025-08-03 21:20:31,106 - father_agent.py:386 - Step: 10, Training loss: 12.372895240783691
2025-08-03 21:20:32,696 - father_agent.py:386 - Step: 15, Training loss: 11.745393753051758
2025-08-03 21:20:34,281 - father_agent.py:386 - Step: 20, Training loss: 12.655396461486816
2025-08-03 21:20:35,855 - father_agent.py:386 - Step: 25, Training loss: 11.637170791625977
2025-08-03 21:20:37,443 - father_agent.py:386 - Step: 30, Training loss: 9.55413818359375
2025-08-03 21:20:39,043 - father_agent.py:386 - Step: 35, Training loss: 7.228248119354248
2025-08-03 21:20:40,627 - father_agent.py:386 - Step: 40, Training loss: 6.126197814941406
2025-08-03 21:20:42,193 - father_agent.py:386 - Step: 45, Training loss: 7.941475868225098
2025-08-03 21:20:43,776 - father_agent.py:386 - Step: 50, Training loss: 7.6086106300354
2025-08-03 21:20:45,345 - father_agent.py:386 - Step: 55, Training loss: 7.503852844238281
2025-08-03 21:20:46,942 - father_agent.py:386 - Step: 60, Training loss: 9.550846099853516
2025-08-03 21:20:48,506 - father_agent.py:386 - Step: 65, Training loss: 8.301970481872559
2025-08-03 21:20:50,063 - father_agent.py:386 - Step: 70, Training loss: 6.003207206726074
2025-08-03 21:20:51,618 - father_agent.py:386 - Step: 75, Training loss: 9.675264358520508
2025-08-03 21:20:53,176 - father_agent.py:386 - Step: 80, Training loss: 8.48681640625
2025-08-03 21:20:54,730 - father_agent.py:386 - Step: 85, Training loss: 6.572736740112305
2025-08-03 21:20:56,286 - father_agent.py:386 - Step: 90, Training loss: 7.056653022766113
2025-08-03 21:20:57,852 - father_agent.py:386 - Step: 95, Training loss: 7.356429576873779
2025-08-03 21:20:59,431 - father_agent.py:386 - Step: 100, Training loss: 8.84921932220459
2025-08-03 21:20:59,585 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:20:59,587 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:21:06,711 - evaluation_results_class.py:131 - Average Return = 156.0608673095703
2025-08-03 21:21:06,711 - evaluation_results_class.py:133 - Average Virtual Goal Value = 16.60186004638672
2025-08-03 21:21:06,711 - evaluation_results_class.py:135 - Average Discounted Reward = 7.31040096282959
2025-08-03 21:21:06,711 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9978867286559594
2025-08-03 21:21:06,711 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:21:06,711 - evaluation_results_class.py:141 - Variance of Return = 40507.22265625
2025-08-03 21:21:06,711 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:21:06,711 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9978867286559594
2025-08-03 21:21:06,711 - evaluation_results_class.py:147 - Average Episode Length = 113.67159763313609
2025-08-03 21:21:06,711 - evaluation_results_class.py:149 - Counted Episodes = 2366
2025-08-03 21:21:06,864 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:21:06,872 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:21:09,402 - father_agent.py:386 - Step: 105, Training loss: 7.808121204376221
2025-08-03 21:21:11,118 - father_agent.py:386 - Step: 110, Training loss: 9.982125282287598
2025-08-03 21:21:12,786 - father_agent.py:386 - Step: 115, Training loss: 6.931763172149658
2025-08-03 21:21:14,461 - father_agent.py:386 - Step: 120, Training loss: 6.667110919952393
2025-08-03 21:21:16,128 - father_agent.py:386 - Step: 125, Training loss: 8.225988388061523
2025-08-03 21:21:17,723 - father_agent.py:386 - Step: 130, Training loss: 6.764737606048584
2025-08-03 21:21:19,302 - father_agent.py:386 - Step: 135, Training loss: 7.69077205657959
2025-08-03 21:21:20,859 - father_agent.py:386 - Step: 140, Training loss: 7.5574798583984375
2025-08-03 21:21:22,422 - father_agent.py:386 - Step: 145, Training loss: 7.717491149902344
2025-08-03 21:21:23,973 - father_agent.py:386 - Step: 150, Training loss: 7.941234111785889
2025-08-03 21:21:25,527 - father_agent.py:386 - Step: 155, Training loss: 8.972490310668945
2025-08-03 21:21:27,086 - father_agent.py:386 - Step: 160, Training loss: 7.817333698272705
2025-08-03 21:21:28,632 - father_agent.py:386 - Step: 165, Training loss: 5.563429832458496
2025-08-03 21:21:30,183 - father_agent.py:386 - Step: 170, Training loss: 6.372718334197998
2025-08-03 21:21:31,744 - father_agent.py:386 - Step: 175, Training loss: 7.6417975425720215
2025-08-03 21:21:33,292 - father_agent.py:386 - Step: 180, Training loss: 8.218306541442871
2025-08-03 21:21:34,860 - father_agent.py:386 - Step: 185, Training loss: 7.605515480041504
2025-08-03 21:21:36,420 - father_agent.py:386 - Step: 190, Training loss: 8.118033409118652
2025-08-03 21:21:37,981 - father_agent.py:386 - Step: 195, Training loss: 8.861083030700684
2025-08-03 21:21:39,563 - father_agent.py:386 - Step: 200, Training loss: 10.376300811767578
2025-08-03 21:21:39,719 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:21:39,722 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:21:46,820 - evaluation_results_class.py:131 - Average Return = 133.5044708251953
2025-08-03 21:21:46,820 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.3465576171875
2025-08-03 21:21:46,820 - evaluation_results_class.py:135 - Average Discounted Reward = 6.743219375610352
2025-08-03 21:21:46,820 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9980552314274601
2025-08-03 21:21:46,820 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:21:46,820 - evaluation_results_class.py:141 - Variance of Return = 26790.482421875
2025-08-03 21:21:46,820 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:21:46,820 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9980552314274601
2025-08-03 21:21:46,820 - evaluation_results_class.py:147 - Average Episode Length = 105.72578763127188
2025-08-03 21:21:46,820 - evaluation_results_class.py:149 - Counted Episodes = 2571
2025-08-03 21:21:46,971 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:21:46,980 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:21:49,428 - father_agent.py:386 - Step: 205, Training loss: 6.972217082977295
2025-08-03 21:21:51,066 - father_agent.py:386 - Step: 210, Training loss: 11.310961723327637
2025-08-03 21:21:52,771 - father_agent.py:386 - Step: 215, Training loss: 7.744687080383301
2025-08-03 21:21:54,479 - father_agent.py:386 - Step: 220, Training loss: 6.836561679840088
2025-08-03 21:21:56,099 - father_agent.py:386 - Step: 225, Training loss: 6.6306891441345215
2025-08-03 21:21:57,723 - father_agent.py:386 - Step: 230, Training loss: 8.136712074279785
2025-08-03 21:21:59,314 - father_agent.py:386 - Step: 235, Training loss: 7.013694763183594
2025-08-03 21:22:00,894 - father_agent.py:386 - Step: 240, Training loss: 9.273505210876465
2025-08-03 21:22:02,436 - father_agent.py:386 - Step: 245, Training loss: 9.8651704788208
2025-08-03 21:22:03,987 - father_agent.py:386 - Step: 250, Training loss: 8.943145751953125
2025-08-03 21:22:05,538 - father_agent.py:386 - Step: 255, Training loss: 8.097127914428711
2025-08-03 21:22:07,083 - father_agent.py:386 - Step: 260, Training loss: 8.22326374053955
2025-08-03 21:22:08,635 - father_agent.py:386 - Step: 265, Training loss: 7.550997734069824
2025-08-03 21:22:10,232 - father_agent.py:386 - Step: 270, Training loss: 7.3089494705200195
2025-08-03 21:22:11,828 - father_agent.py:386 - Step: 275, Training loss: 7.178284645080566
2025-08-03 21:22:13,449 - father_agent.py:386 - Step: 280, Training loss: 7.043642997741699
2025-08-03 21:22:15,046 - father_agent.py:386 - Step: 285, Training loss: 7.098309516906738
2025-08-03 21:22:16,662 - father_agent.py:386 - Step: 290, Training loss: 6.283463954925537
2025-08-03 21:22:18,279 - father_agent.py:386 - Step: 295, Training loss: 7.3705220222473145
2025-08-03 21:22:19,885 - father_agent.py:386 - Step: 300, Training loss: 9.318581581115723
2025-08-03 21:22:20,043 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:22:20,046 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:22:27,052 - evaluation_results_class.py:131 - Average Return = 166.56639099121094
2025-08-03 21:22:27,052 - evaluation_results_class.py:133 - Average Virtual Goal Value = 17.652341842651367
2025-08-03 21:22:27,052 - evaluation_results_class.py:135 - Average Discounted Reward = 7.7403411865234375
2025-08-03 21:22:27,052 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9978513107004727
2025-08-03 21:22:27,052 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:22:27,052 - evaluation_results_class.py:141 - Variance of Return = 39974.0703125
2025-08-03 21:22:27,052 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:22:27,052 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9980552314274601
2025-08-03 21:22:27,052 - evaluation_results_class.py:147 - Average Episode Length = 115.11688869789428
2025-08-03 21:22:27,052 - evaluation_results_class.py:149 - Counted Episodes = 2327
2025-08-03 21:22:27,205 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:22:27,213 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:22:29,652 - father_agent.py:386 - Step: 305, Training loss: 8.321351051330566
2025-08-03 21:22:31,260 - father_agent.py:386 - Step: 310, Training loss: 9.227304458618164
2025-08-03 21:22:32,894 - father_agent.py:386 - Step: 315, Training loss: 9.697712898254395
2025-08-03 21:22:34,522 - father_agent.py:386 - Step: 320, Training loss: 7.836455345153809
2025-08-03 21:22:36,127 - father_agent.py:386 - Step: 325, Training loss: 8.673489570617676
2025-08-03 21:22:37,773 - father_agent.py:386 - Step: 330, Training loss: 10.603639602661133
2025-08-03 21:22:39,391 - father_agent.py:386 - Step: 335, Training loss: 9.14111042022705
2025-08-03 21:22:41,024 - father_agent.py:386 - Step: 340, Training loss: 8.401268005371094
2025-08-03 21:22:42,655 - father_agent.py:386 - Step: 345, Training loss: 5.747515678405762
2025-08-03 21:22:44,221 - father_agent.py:386 - Step: 350, Training loss: 9.693710327148438
2025-08-03 21:22:45,776 - father_agent.py:386 - Step: 355, Training loss: 6.5973286628723145
2025-08-03 21:22:47,333 - father_agent.py:386 - Step: 360, Training loss: 9.335103988647461
2025-08-03 21:22:48,888 - father_agent.py:386 - Step: 365, Training loss: 7.383171558380127
2025-08-03 21:22:50,436 - father_agent.py:386 - Step: 370, Training loss: 6.712879180908203
2025-08-03 21:22:51,977 - father_agent.py:386 - Step: 375, Training loss: 9.18985366821289
2025-08-03 21:22:53,520 - father_agent.py:386 - Step: 380, Training loss: 8.0091552734375
2025-08-03 21:22:55,065 - father_agent.py:386 - Step: 385, Training loss: 7.129428863525391
2025-08-03 21:22:56,610 - father_agent.py:386 - Step: 390, Training loss: 7.228371620178223
2025-08-03 21:22:58,173 - father_agent.py:386 - Step: 395, Training loss: 10.206661224365234
2025-08-03 21:22:59,742 - father_agent.py:386 - Step: 400, Training loss: 6.879978179931641
2025-08-03 21:22:59,895 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:22:59,898 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:23:06,798 - evaluation_results_class.py:131 - Average Return = 151.92417907714844
2025-08-03 21:23:06,798 - evaluation_results_class.py:133 - Average Virtual Goal Value = 16.1899471282959
2025-08-03 21:23:06,798 - evaluation_results_class.py:135 - Average Discounted Reward = 7.268583297729492
2025-08-03 21:23:06,798 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9987639060568603
2025-08-03 21:23:06,798 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:23:06,798 - evaluation_results_class.py:141 - Variance of Return = 32466.13671875
2025-08-03 21:23:06,798 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:23:06,798 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9987639060568603
2025-08-03 21:23:06,798 - evaluation_results_class.py:147 - Average Episode Length = 112.55006180469715
2025-08-03 21:23:06,798 - evaluation_results_class.py:149 - Counted Episodes = 2427
2025-08-03 21:23:06,950 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:23:06,958 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:23:09,342 - father_agent.py:386 - Step: 405, Training loss: 8.399024963378906
2025-08-03 21:23:10,893 - father_agent.py:386 - Step: 410, Training loss: 8.73102855682373
2025-08-03 21:23:12,420 - father_agent.py:386 - Step: 415, Training loss: 7.140547275543213
2025-08-03 21:23:13,936 - father_agent.py:386 - Step: 420, Training loss: 8.250758171081543
2025-08-03 21:23:15,455 - father_agent.py:386 - Step: 425, Training loss: 8.23679256439209
2025-08-03 21:23:16,966 - father_agent.py:386 - Step: 430, Training loss: 7.446712970733643
2025-08-03 21:23:18,538 - father_agent.py:386 - Step: 435, Training loss: 8.39621353149414
2025-08-03 21:23:20,124 - father_agent.py:386 - Step: 440, Training loss: 9.765633583068848
2025-08-03 21:23:21,698 - father_agent.py:386 - Step: 445, Training loss: 9.716935157775879
2025-08-03 21:23:23,281 - father_agent.py:386 - Step: 450, Training loss: 7.10959529876709
2025-08-03 21:23:24,844 - father_agent.py:386 - Step: 455, Training loss: 9.267923355102539
2025-08-03 21:23:26,414 - father_agent.py:386 - Step: 460, Training loss: 7.122754096984863
2025-08-03 21:23:27,989 - father_agent.py:386 - Step: 465, Training loss: 7.046923637390137
2025-08-03 21:23:29,567 - father_agent.py:386 - Step: 470, Training loss: 6.094211101531982
2025-08-03 21:23:31,135 - father_agent.py:386 - Step: 475, Training loss: 5.958868026733398
2025-08-03 21:23:32,702 - father_agent.py:386 - Step: 480, Training loss: 7.647620677947998
2025-08-03 21:23:34,255 - father_agent.py:386 - Step: 485, Training loss: 7.645079135894775
2025-08-03 21:23:35,814 - father_agent.py:386 - Step: 490, Training loss: 5.913303375244141
2025-08-03 21:23:37,368 - father_agent.py:386 - Step: 495, Training loss: 7.530720233917236
2025-08-03 21:23:38,965 - father_agent.py:386 - Step: 500, Training loss: 7.4594879150390625
2025-08-03 21:23:39,120 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:23:39,123 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:23:46,296 - evaluation_results_class.py:131 - Average Return = 160.2368927001953
2025-08-03 21:23:46,296 - evaluation_results_class.py:133 - Average Virtual Goal Value = 17.019458770751953
2025-08-03 21:23:46,296 - evaluation_results_class.py:135 - Average Discounted Reward = 7.611466884613037
2025-08-03 21:23:46,296 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9978849407783418
2025-08-03 21:23:46,296 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:23:46,296 - evaluation_results_class.py:141 - Variance of Return = 35022.62109375
2025-08-03 21:23:46,296 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:23:46,296 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9987639060568603
2025-08-03 21:23:46,296 - evaluation_results_class.py:147 - Average Episode Length = 115.79568527918782
2025-08-03 21:23:46,296 - evaluation_results_class.py:149 - Counted Episodes = 2364
2025-08-03 21:23:46,447 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:23:46,455 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:23:48,800 - father_agent.py:386 - Step: 505, Training loss: 6.6797051429748535
2025-08-03 21:23:50,365 - father_agent.py:386 - Step: 510, Training loss: 6.767799377441406
2025-08-03 21:23:51,925 - father_agent.py:386 - Step: 515, Training loss: 6.438144683837891
2025-08-03 21:23:53,498 - father_agent.py:386 - Step: 520, Training loss: 9.755962371826172
2025-08-03 21:23:55,059 - father_agent.py:386 - Step: 525, Training loss: 7.217334747314453
2025-08-03 21:23:56,636 - father_agent.py:386 - Step: 530, Training loss: 8.899567604064941
2025-08-03 21:23:58,220 - father_agent.py:386 - Step: 535, Training loss: 7.767863750457764
2025-08-03 21:23:59,820 - father_agent.py:386 - Step: 540, Training loss: 7.476111888885498
2025-08-03 21:24:01,431 - father_agent.py:386 - Step: 545, Training loss: 8.889405250549316
2025-08-03 21:24:03,101 - father_agent.py:386 - Step: 550, Training loss: 8.79139518737793
2025-08-03 21:24:04,767 - father_agent.py:386 - Step: 555, Training loss: 6.588198184967041
2025-08-03 21:24:06,403 - father_agent.py:386 - Step: 560, Training loss: 9.721769332885742
2025-08-03 21:24:08,035 - father_agent.py:386 - Step: 565, Training loss: 8.910685539245605
2025-08-03 21:24:09,672 - father_agent.py:386 - Step: 570, Training loss: 10.485943794250488
2025-08-03 21:24:11,263 - father_agent.py:386 - Step: 575, Training loss: 9.87910270690918
2025-08-03 21:24:12,898 - father_agent.py:386 - Step: 580, Training loss: 7.503377914428711
2025-08-03 21:24:14,543 - father_agent.py:386 - Step: 585, Training loss: 9.261405944824219
2025-08-03 21:24:16,147 - father_agent.py:386 - Step: 590, Training loss: 7.146271705627441
2025-08-03 21:24:17,747 - father_agent.py:386 - Step: 595, Training loss: 8.019681930541992
2025-08-03 21:24:19,352 - father_agent.py:386 - Step: 600, Training loss: 8.3882417678833
2025-08-03 21:24:19,515 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:24:19,518 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:24:26,583 - evaluation_results_class.py:131 - Average Return = 151.98333740234375
2025-08-03 21:24:26,583 - evaluation_results_class.py:133 - Average Virtual Goal Value = 16.19333267211914
2025-08-03 21:24:26,583 - evaluation_results_class.py:135 - Average Discounted Reward = 7.375007152557373
2025-08-03 21:24:26,583 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9975
2025-08-03 21:24:26,583 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:24:26,584 - evaluation_results_class.py:141 - Variance of Return = 33249.734375
2025-08-03 21:24:26,584 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:24:26,584 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9987639060568603
2025-08-03 21:24:26,584 - evaluation_results_class.py:147 - Average Episode Length = 115.68166666666667
2025-08-03 21:24:26,584 - evaluation_results_class.py:149 - Counted Episodes = 2400
2025-08-03 21:24:26,736 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:24:26,744 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:24:29,146 - father_agent.py:386 - Step: 605, Training loss: 6.683036804199219
2025-08-03 21:24:30,793 - father_agent.py:386 - Step: 610, Training loss: 7.4821295738220215
2025-08-03 21:24:32,438 - father_agent.py:386 - Step: 615, Training loss: 7.327589988708496
2025-08-03 21:24:34,063 - father_agent.py:386 - Step: 620, Training loss: 6.453664779663086
2025-08-03 21:24:35,715 - father_agent.py:386 - Step: 625, Training loss: 6.544801235198975
2025-08-03 21:24:37,362 - father_agent.py:386 - Step: 630, Training loss: 8.064189910888672
2025-08-03 21:24:38,992 - father_agent.py:386 - Step: 635, Training loss: 7.726194858551025
2025-08-03 21:24:40,654 - father_agent.py:386 - Step: 640, Training loss: 7.668371677398682
2025-08-03 21:24:42,298 - father_agent.py:386 - Step: 645, Training loss: 7.843258857727051
2025-08-03 21:24:43,948 - father_agent.py:386 - Step: 650, Training loss: 7.541199207305908
2025-08-03 21:24:45,591 - father_agent.py:386 - Step: 655, Training loss: 7.132774353027344
2025-08-03 21:24:47,214 - father_agent.py:386 - Step: 660, Training loss: 7.287734508514404
2025-08-03 21:24:48,878 - father_agent.py:386 - Step: 665, Training loss: 7.4877119064331055
2025-08-03 21:24:50,614 - father_agent.py:386 - Step: 670, Training loss: 7.384328842163086
2025-08-03 21:24:52,301 - father_agent.py:386 - Step: 675, Training loss: 8.169261932373047
2025-08-03 21:24:53,960 - father_agent.py:386 - Step: 680, Training loss: 8.720375061035156
2025-08-03 21:24:55,604 - father_agent.py:386 - Step: 685, Training loss: 10.489691734313965
2025-08-03 21:24:57,251 - father_agent.py:386 - Step: 690, Training loss: 6.943974494934082
2025-08-03 21:24:58,881 - father_agent.py:386 - Step: 695, Training loss: 5.784489631652832
2025-08-03 21:25:00,454 - father_agent.py:386 - Step: 700, Training loss: 6.970915794372559
2025-08-03 21:25:00,610 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:25:00,612 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:25:07,444 - evaluation_results_class.py:131 - Average Return = 144.4928741455078
2025-08-03 21:25:07,444 - evaluation_results_class.py:133 - Average Virtual Goal Value = 15.444257736206055
2025-08-03 21:25:07,444 - evaluation_results_class.py:135 - Average Discounted Reward = 7.135566234588623
2025-08-03 21:25:07,444 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9974853310980721
2025-08-03 21:25:07,444 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:25:07,444 - evaluation_results_class.py:141 - Variance of Return = 29419.8828125
2025-08-03 21:25:07,444 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:25:07,444 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9987639060568603
2025-08-03 21:25:07,444 - evaluation_results_class.py:147 - Average Episode Length = 114.18189438390611
2025-08-03 21:25:07,444 - evaluation_results_class.py:149 - Counted Episodes = 2386
2025-08-03 21:25:07,594 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:25:07,602 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:25:09,940 - father_agent.py:386 - Step: 705, Training loss: 8.065814018249512
2025-08-03 21:25:11,488 - father_agent.py:386 - Step: 710, Training loss: 7.610235691070557
2025-08-03 21:25:13,037 - father_agent.py:386 - Step: 715, Training loss: 8.141788482666016
2025-08-03 21:25:14,598 - father_agent.py:386 - Step: 720, Training loss: 10.36372184753418
2025-08-03 21:25:16,154 - father_agent.py:386 - Step: 725, Training loss: 6.679856777191162
2025-08-03 21:25:17,720 - father_agent.py:386 - Step: 730, Training loss: 8.220866203308105
2025-08-03 21:25:19,298 - father_agent.py:386 - Step: 735, Training loss: 6.601057052612305
2025-08-03 21:25:20,915 - father_agent.py:386 - Step: 740, Training loss: 7.471734523773193
2025-08-03 21:25:22,573 - father_agent.py:386 - Step: 745, Training loss: 7.269270420074463
2025-08-03 21:25:24,231 - father_agent.py:386 - Step: 750, Training loss: 8.142885208129883
2025-08-03 21:25:25,853 - father_agent.py:386 - Step: 755, Training loss: 7.2031941413879395
2025-08-03 21:25:27,472 - father_agent.py:386 - Step: 760, Training loss: 7.1241536140441895
2025-08-03 21:25:29,076 - father_agent.py:386 - Step: 765, Training loss: 8.08045768737793
2025-08-03 21:25:30,665 - father_agent.py:386 - Step: 770, Training loss: 7.643753528594971
2025-08-03 21:25:32,238 - father_agent.py:386 - Step: 775, Training loss: 6.956720352172852
2025-08-03 21:25:33,824 - father_agent.py:386 - Step: 780, Training loss: 9.793785095214844
2025-08-03 21:25:35,410 - father_agent.py:386 - Step: 785, Training loss: 8.861209869384766
2025-08-03 21:25:36,984 - father_agent.py:386 - Step: 790, Training loss: 7.506916046142578
2025-08-03 21:25:38,558 - father_agent.py:386 - Step: 795, Training loss: 6.105727672576904
2025-08-03 21:25:40,132 - father_agent.py:386 - Step: 800, Training loss: 10.368816375732422
2025-08-03 21:25:40,288 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:25:40,290 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:25:47,149 - evaluation_results_class.py:131 - Average Return = 130.63763427734375
2025-08-03 21:25:47,150 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.05987548828125
2025-08-03 21:25:47,150 - evaluation_results_class.py:135 - Average Discounted Reward = 6.783777236938477
2025-08-03 21:25:47,150 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9980559875583204
2025-08-03 21:25:47,150 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:25:47,150 - evaluation_results_class.py:141 - Variance of Return = 23538.47265625
2025-08-03 21:25:47,150 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:25:47,150 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9987639060568603
2025-08-03 21:25:47,150 - evaluation_results_class.py:147 - Average Episode Length = 106.59059097978228
2025-08-03 21:25:47,150 - evaluation_results_class.py:149 - Counted Episodes = 2572
2025-08-03 21:25:47,300 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:25:47,309 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:25:49,709 - father_agent.py:386 - Step: 805, Training loss: 7.208434104919434
2025-08-03 21:25:51,287 - father_agent.py:386 - Step: 810, Training loss: 7.24616003036499
2025-08-03 21:25:52,857 - father_agent.py:386 - Step: 815, Training loss: 5.369405269622803
2025-08-03 21:25:54,424 - father_agent.py:386 - Step: 820, Training loss: 10.065905570983887
2025-08-03 21:25:55,993 - father_agent.py:386 - Step: 825, Training loss: 6.951608180999756
2025-08-03 21:25:57,569 - father_agent.py:386 - Step: 830, Training loss: 9.223204612731934
2025-08-03 21:25:59,156 - father_agent.py:386 - Step: 835, Training loss: 8.973790168762207
2025-08-03 21:26:00,735 - father_agent.py:386 - Step: 840, Training loss: 6.4890456199646
2025-08-03 21:26:02,308 - father_agent.py:386 - Step: 845, Training loss: 8.336009979248047
2025-08-03 21:26:03,898 - father_agent.py:386 - Step: 850, Training loss: 7.014404296875
2025-08-03 21:26:05,471 - father_agent.py:386 - Step: 855, Training loss: 7.875685214996338
2025-08-03 21:26:07,053 - father_agent.py:386 - Step: 860, Training loss: 7.06516170501709
2025-08-03 21:26:08,641 - father_agent.py:386 - Step: 865, Training loss: 8.274192810058594
2025-08-03 21:26:10,222 - father_agent.py:386 - Step: 870, Training loss: 6.710543632507324
2025-08-03 21:26:11,800 - father_agent.py:386 - Step: 875, Training loss: 7.117908477783203
2025-08-03 21:26:13,390 - father_agent.py:386 - Step: 880, Training loss: 7.7391791343688965
2025-08-03 21:26:14,976 - father_agent.py:386 - Step: 885, Training loss: 7.1017351150512695
2025-08-03 21:26:16,550 - father_agent.py:386 - Step: 890, Training loss: 6.787240505218506
2025-08-03 21:26:18,118 - father_agent.py:386 - Step: 895, Training loss: 7.383685111999512
2025-08-03 21:26:19,692 - father_agent.py:386 - Step: 900, Training loss: 6.44141960144043
2025-08-03 21:26:19,853 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:26:19,856 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:26:26,676 - evaluation_results_class.py:131 - Average Return = 139.38296508789062
2025-08-03 21:26:26,676 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.935006141662598
2025-08-03 21:26:26,676 - evaluation_results_class.py:135 - Average Discounted Reward = 6.927679061889648
2025-08-03 21:26:26,676 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9983545865898807
2025-08-03 21:26:26,676 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:26:26,676 - evaluation_results_class.py:141 - Variance of Return = 29597.154296875
2025-08-03 21:26:26,676 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:26:26,676 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9987639060568603
2025-08-03 21:26:26,676 - evaluation_results_class.py:147 - Average Episode Length = 109.34594816947758
2025-08-03 21:26:26,676 - evaluation_results_class.py:149 - Counted Episodes = 2431
2025-08-03 21:26:26,832 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:26:26,840 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:26:29,234 - father_agent.py:386 - Step: 905, Training loss: 8.52224349975586
2025-08-03 21:26:30,803 - father_agent.py:386 - Step: 910, Training loss: 8.057341575622559
2025-08-03 21:26:32,371 - father_agent.py:386 - Step: 915, Training loss: 6.9738993644714355
2025-08-03 21:26:33,939 - father_agent.py:386 - Step: 920, Training loss: 9.281110763549805
2025-08-03 21:26:35,510 - father_agent.py:386 - Step: 925, Training loss: 7.947800636291504
2025-08-03 21:26:37,076 - father_agent.py:386 - Step: 930, Training loss: 7.025103569030762
2025-08-03 21:26:38,646 - father_agent.py:386 - Step: 935, Training loss: 7.858216762542725
2025-08-03 21:26:40,216 - father_agent.py:386 - Step: 940, Training loss: 8.29333209991455
2025-08-03 21:26:41,784 - father_agent.py:386 - Step: 945, Training loss: 7.170848369598389
2025-08-03 21:26:43,372 - father_agent.py:386 - Step: 950, Training loss: 9.615209579467773
2025-08-03 21:26:44,947 - father_agent.py:386 - Step: 955, Training loss: 8.230948448181152
2025-08-03 21:26:46,528 - father_agent.py:386 - Step: 960, Training loss: 8.326277732849121
2025-08-03 21:26:48,093 - father_agent.py:386 - Step: 965, Training loss: 7.2116522789001465
2025-08-03 21:26:49,666 - father_agent.py:386 - Step: 970, Training loss: 7.744943141937256
2025-08-03 21:26:51,229 - father_agent.py:386 - Step: 975, Training loss: 8.12475872039795
2025-08-03 21:26:52,794 - father_agent.py:386 - Step: 980, Training loss: 9.881497383117676
2025-08-03 21:26:54,374 - father_agent.py:386 - Step: 985, Training loss: 9.312029838562012
2025-08-03 21:26:55,945 - father_agent.py:386 - Step: 990, Training loss: 7.724441051483154
2025-08-03 21:26:57,499 - father_agent.py:386 - Step: 995, Training loss: 9.844173431396484
2025-08-03 21:26:59,081 - father_agent.py:386 - Step: 1000, Training loss: 7.66605281829834
2025-08-03 21:26:59,235 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:26:59,237 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:27:06,388 - evaluation_results_class.py:131 - Average Return = 114.92786407470703
2025-08-03 21:27:06,388 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.48974895477295
2025-08-03 21:27:06,388 - evaluation_results_class.py:135 - Average Discounted Reward = 6.140918254852295
2025-08-03 21:27:06,388 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9984813971146546
2025-08-03 21:27:06,388 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:27:06,388 - evaluation_results_class.py:141 - Variance of Return = 19340.712890625
2025-08-03 21:27:06,388 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:27:06,388 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9987639060568603
2025-08-03 21:27:06,388 - evaluation_results_class.py:147 - Average Episode Length = 103.09339407744875
2025-08-03 21:27:06,388 - evaluation_results_class.py:149 - Counted Episodes = 2634
2025-08-03 21:27:06,538 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:27:06,546 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:27:08,931 - father_agent.py:386 - Step: 1005, Training loss: 9.977071762084961
2025-08-03 21:27:10,495 - father_agent.py:386 - Step: 1010, Training loss: 7.9350266456604
2025-08-03 21:27:12,060 - father_agent.py:386 - Step: 1015, Training loss: 8.150903701782227
2025-08-03 21:27:13,633 - father_agent.py:386 - Step: 1020, Training loss: 8.015682220458984
2025-08-03 21:27:15,198 - father_agent.py:386 - Step: 1025, Training loss: 8.093372344970703
2025-08-03 21:27:16,757 - father_agent.py:386 - Step: 1030, Training loss: 9.220870018005371
2025-08-03 21:27:18,312 - father_agent.py:386 - Step: 1035, Training loss: 7.635720252990723
2025-08-03 21:27:19,895 - father_agent.py:386 - Step: 1040, Training loss: 6.977456569671631
2025-08-03 21:27:21,465 - father_agent.py:386 - Step: 1045, Training loss: 7.573925018310547
2025-08-03 21:27:23,028 - father_agent.py:386 - Step: 1050, Training loss: 7.026193141937256
2025-08-03 21:27:24,601 - father_agent.py:386 - Step: 1055, Training loss: 9.55203628540039
2025-08-03 21:27:26,162 - father_agent.py:386 - Step: 1060, Training loss: 7.305907726287842
2025-08-03 21:27:27,717 - father_agent.py:386 - Step: 1065, Training loss: 7.752354145050049
2025-08-03 21:27:29,302 - father_agent.py:386 - Step: 1070, Training loss: 7.715574741363525
2025-08-03 21:27:30,856 - father_agent.py:386 - Step: 1075, Training loss: 7.158101558685303
2025-08-03 21:27:32,412 - father_agent.py:386 - Step: 1080, Training loss: 8.155915260314941
2025-08-03 21:27:33,960 - father_agent.py:386 - Step: 1085, Training loss: 7.575610160827637
2025-08-03 21:27:35,510 - father_agent.py:386 - Step: 1090, Training loss: 5.387800693511963
2025-08-03 21:27:37,068 - father_agent.py:386 - Step: 1095, Training loss: 9.720593452453613
2025-08-03 21:27:38,622 - father_agent.py:386 - Step: 1100, Training loss: 7.805112361907959
2025-08-03 21:27:38,779 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:27:38,782 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:27:45,648 - evaluation_results_class.py:131 - Average Return = 113.31204986572266
2025-08-03 21:27:45,649 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.329608917236328
2025-08-03 21:27:45,649 - evaluation_results_class.py:135 - Average Discounted Reward = 5.949303150177002
2025-08-03 21:27:45,649 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9992019154030327
2025-08-03 21:27:45,649 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:27:45,649 - evaluation_results_class.py:141 - Variance of Return = 17456.310546875
2025-08-03 21:27:45,649 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:27:45,649 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9992019154030327
2025-08-03 21:27:45,649 - evaluation_results_class.py:147 - Average Episode Length = 110.06703910614524
2025-08-03 21:27:45,649 - evaluation_results_class.py:149 - Counted Episodes = 2506
2025-08-03 21:27:45,799 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:27:45,808 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:27:48,176 - father_agent.py:386 - Step: 1105, Training loss: 7.0832695960998535
2025-08-03 21:27:49,751 - father_agent.py:386 - Step: 1110, Training loss: 9.476778984069824
2025-08-03 21:27:51,326 - father_agent.py:386 - Step: 1115, Training loss: 8.684754371643066
2025-08-03 21:27:52,891 - father_agent.py:386 - Step: 1120, Training loss: 9.722477912902832
2025-08-03 21:27:54,452 - father_agent.py:386 - Step: 1125, Training loss: 8.567627906799316
2025-08-03 21:27:56,011 - father_agent.py:386 - Step: 1130, Training loss: 7.8354082107543945
2025-08-03 21:27:57,579 - father_agent.py:386 - Step: 1135, Training loss: 9.03385066986084
2025-08-03 21:27:59,132 - father_agent.py:386 - Step: 1140, Training loss: 6.878799915313721
2025-08-03 21:28:00,683 - father_agent.py:386 - Step: 1145, Training loss: 7.62686824798584
2025-08-03 21:28:02,263 - father_agent.py:386 - Step: 1150, Training loss: 7.553801536560059
2025-08-03 21:28:03,858 - father_agent.py:386 - Step: 1155, Training loss: 7.210651874542236
2025-08-03 21:28:05,466 - father_agent.py:386 - Step: 1160, Training loss: 7.956497669219971
2025-08-03 21:28:07,075 - father_agent.py:386 - Step: 1165, Training loss: 5.961215496063232
2025-08-03 21:28:08,736 - father_agent.py:386 - Step: 1170, Training loss: 7.890841484069824
2025-08-03 21:28:10,457 - father_agent.py:386 - Step: 1175, Training loss: 8.800015449523926
2025-08-03 21:28:12,180 - father_agent.py:386 - Step: 1180, Training loss: 7.303991317749023
2025-08-03 21:28:13,890 - father_agent.py:386 - Step: 1185, Training loss: 7.706394672393799
2025-08-03 21:28:15,573 - father_agent.py:386 - Step: 1190, Training loss: 8.31718921661377
2025-08-03 21:28:17,243 - father_agent.py:386 - Step: 1195, Training loss: 7.059637546539307
2025-08-03 21:28:18,945 - father_agent.py:386 - Step: 1200, Training loss: 6.944428443908691
2025-08-03 21:28:19,104 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:28:19,107 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:28:26,451 - evaluation_results_class.py:131 - Average Return = 132.52284240722656
2025-08-03 21:28:26,451 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.249106407165527
2025-08-03 21:28:26,451 - evaluation_results_class.py:135 - Average Discounted Reward = 6.8324360847473145
2025-08-03 21:28:26,451 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9984108065156932
2025-08-03 21:28:26,451 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:28:26,451 - evaluation_results_class.py:141 - Variance of Return = 25229.591796875
2025-08-03 21:28:26,451 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:28:26,451 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9992019154030327
2025-08-03 21:28:26,451 - evaluation_results_class.py:147 - Average Episode Length = 108.12793007548669
2025-08-03 21:28:26,451 - evaluation_results_class.py:149 - Counted Episodes = 2517
2025-08-03 21:28:26,611 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:28:26,619 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:28:29,200 - father_agent.py:386 - Step: 1205, Training loss: 7.564212322235107
2025-08-03 21:28:30,865 - father_agent.py:386 - Step: 1210, Training loss: 7.89857292175293
2025-08-03 21:28:32,518 - father_agent.py:386 - Step: 1215, Training loss: 7.251805305480957
2025-08-03 21:28:34,167 - father_agent.py:386 - Step: 1220, Training loss: 8.424781799316406
2025-08-03 21:28:35,881 - father_agent.py:386 - Step: 1225, Training loss: 8.569119453430176
2025-08-03 21:28:37,607 - father_agent.py:386 - Step: 1230, Training loss: 6.4384074211120605
2025-08-03 21:28:39,362 - father_agent.py:386 - Step: 1235, Training loss: 9.72879409790039
2025-08-03 21:28:41,071 - father_agent.py:386 - Step: 1240, Training loss: 7.457220554351807
2025-08-03 21:28:42,781 - father_agent.py:386 - Step: 1245, Training loss: 8.805194854736328
2025-08-03 21:28:44,508 - father_agent.py:386 - Step: 1250, Training loss: 7.454578399658203
2025-08-03 21:28:46,173 - father_agent.py:386 - Step: 1255, Training loss: 10.796558380126953
2025-08-03 21:28:47,914 - father_agent.py:386 - Step: 1260, Training loss: 7.63673734664917
2025-08-03 21:28:49,584 - father_agent.py:386 - Step: 1265, Training loss: 6.807850360870361
2025-08-03 21:28:51,243 - father_agent.py:386 - Step: 1270, Training loss: 6.875927448272705
2025-08-03 21:28:52,964 - father_agent.py:386 - Step: 1275, Training loss: 6.108994960784912
2025-08-03 21:28:54,687 - father_agent.py:386 - Step: 1280, Training loss: 6.391255855560303
2025-08-03 21:28:56,385 - father_agent.py:386 - Step: 1285, Training loss: 6.946505546569824
2025-08-03 21:28:58,092 - father_agent.py:386 - Step: 1290, Training loss: 8.124249458312988
2025-08-03 21:28:59,788 - father_agent.py:386 - Step: 1295, Training loss: 7.568055152893066
2025-08-03 21:29:01,477 - father_agent.py:386 - Step: 1300, Training loss: 9.108463287353516
2025-08-03 21:29:01,709 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:29:01,712 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:29:08,936 - evaluation_results_class.py:131 - Average Return = 134.92037963867188
2025-08-03 21:29:08,936 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.491242408752441
2025-08-03 21:29:08,936 - evaluation_results_class.py:135 - Average Discounted Reward = 6.80042839050293
2025-08-03 21:29:08,936 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9996019108280255
2025-08-03 21:29:08,936 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:29:08,936 - evaluation_results_class.py:141 - Variance of Return = 24488.84765625
2025-08-03 21:29:08,936 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:29:08,936 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9996019108280255
2025-08-03 21:29:08,936 - evaluation_results_class.py:147 - Average Episode Length = 109.03144904458598
2025-08-03 21:29:08,936 - evaluation_results_class.py:149 - Counted Episodes = 2512
2025-08-03 21:29:09,093 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:29:09,102 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:29:11,649 - father_agent.py:386 - Step: 1305, Training loss: 7.241829872131348
2025-08-03 21:29:13,373 - father_agent.py:386 - Step: 1310, Training loss: 8.14247989654541
2025-08-03 21:29:15,116 - father_agent.py:386 - Step: 1315, Training loss: 7.830395698547363
2025-08-03 21:29:16,855 - father_agent.py:386 - Step: 1320, Training loss: 7.667534828186035
2025-08-03 21:29:18,575 - father_agent.py:386 - Step: 1325, Training loss: 7.869431018829346
2025-08-03 21:29:20,213 - father_agent.py:386 - Step: 1330, Training loss: 10.62478256225586
2025-08-03 21:29:21,833 - father_agent.py:386 - Step: 1335, Training loss: 8.459905624389648
2025-08-03 21:29:23,469 - father_agent.py:386 - Step: 1340, Training loss: 8.336135864257812
2025-08-03 21:29:25,099 - father_agent.py:386 - Step: 1345, Training loss: 7.260481357574463
2025-08-03 21:29:26,724 - father_agent.py:386 - Step: 1350, Training loss: 8.411896705627441
2025-08-03 21:29:28,369 - father_agent.py:386 - Step: 1355, Training loss: 7.961578845977783
2025-08-03 21:29:30,044 - father_agent.py:386 - Step: 1360, Training loss: 7.379025459289551
2025-08-03 21:29:31,705 - father_agent.py:386 - Step: 1365, Training loss: 7.320276737213135
2025-08-03 21:29:33,356 - father_agent.py:386 - Step: 1370, Training loss: 8.187365531921387
2025-08-03 21:29:35,015 - father_agent.py:386 - Step: 1375, Training loss: 7.7843451499938965
2025-08-03 21:29:36,691 - father_agent.py:386 - Step: 1380, Training loss: 8.83536148071289
2025-08-03 21:29:38,355 - father_agent.py:386 - Step: 1385, Training loss: 8.235529899597168
2025-08-03 21:29:40,004 - father_agent.py:386 - Step: 1390, Training loss: 9.49512004852295
2025-08-03 21:29:41,675 - father_agent.py:386 - Step: 1395, Training loss: 8.280694007873535
2025-08-03 21:29:43,346 - father_agent.py:386 - Step: 1400, Training loss: 10.183602333068848
2025-08-03 21:29:43,559 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:29:43,562 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:29:50,555 - evaluation_results_class.py:131 - Average Return = 127.32121276855469
2025-08-03 21:29:50,556 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.729697227478027
2025-08-03 21:29:50,556 - evaluation_results_class.py:135 - Average Discounted Reward = 6.586504936218262
2025-08-03 21:29:50,556 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9987878787878788
2025-08-03 21:29:50,556 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:29:50,556 - evaluation_results_class.py:141 - Variance of Return = 21916.5
2025-08-03 21:29:50,556 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:29:50,556 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9996019108280255
2025-08-03 21:29:50,556 - evaluation_results_class.py:147 - Average Episode Length = 109.38868686868688
2025-08-03 21:29:50,556 - evaluation_results_class.py:149 - Counted Episodes = 2475
2025-08-03 21:29:50,708 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:29:50,716 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:29:50,821 - father_agent.py:547 - Training finished.
2025-08-03 21:29:50,958 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:29:50,961 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:29:50,963 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-03 21:29:58,082 - evaluation_results_class.py:131 - Average Return = 127.52177429199219
2025-08-03 21:29:58,082 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.745051383972168
2025-08-03 21:29:58,082 - evaluation_results_class.py:135 - Average Discounted Reward = 6.593197345733643
2025-08-03 21:29:58,082 - evaluation_results_class.py:137 - Goal Reach Probability = 0.996437054631829
2025-08-03 21:29:58,082 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:29:58,082 - evaluation_results_class.py:141 - Variance of Return = 24009.9296875
2025-08-03 21:29:58,082 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:29:58,082 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9996019108280255
2025-08-03 21:29:58,082 - evaluation_results_class.py:147 - Average Episode Length = 108.81353919239905
2025-08-03 21:29:58,082 - evaluation_results_class.py:149 - Counted Episodes = 2526
2025-08-03 21:29:58,237 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:29:58,239 - self_interpretable_extractor.py:286 - True
2025-08-03 21:29:58,249 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:30:10,753 - evaluation_results_class.py:131 - Average Return = 156.53085327148438
2025-08-03 21:30:10,753 - evaluation_results_class.py:133 - Average Virtual Goal Value = 16.645017623901367
2025-08-03 21:30:10,753 - evaluation_results_class.py:135 - Average Discounted Reward = 7.2430100440979
2025-08-03 21:30:10,753 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9959661153691004
2025-08-03 21:30:10,753 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:30:10,753 - evaluation_results_class.py:141 - Variance of Return = 34694.21484375
2025-08-03 21:30:10,753 - evaluation_results_class.py:143 - Current Best Return = 156.53085327148438
2025-08-03 21:30:10,753 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9959661153691004
2025-08-03 21:30:10,753 - evaluation_results_class.py:147 - Average Episode Length = 122.05889471561113
2025-08-03 21:30:10,753 - evaluation_results_class.py:149 - Counted Episodes = 2479
2025-08-03 21:30:10,753 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-03 21:30:10,754 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 11373 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 554, 556, 560, 562, 566, 570, 572, 574, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 624, 628, 634, 640, 642, 644, 646, 648, 651}
Buffer 1
2025-08-03 21:31:06,988 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 22806 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 634, 636, 638, 640, 642, 644, 646, 648, 651}
Buffer 2
2025-08-03 21:32:03,864 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 34340 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 630, 632, 634, 636, 638, 640, 642, 644, 646, 648, 651}
All trajectories collected
2025-08-03 21:33:00,633 - self_interpretable_extractor.py:346 - Data sampled
2025-08-03 21:33:00,634 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 34340 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_0.dot.
Learned FSC of size 2
2025-08-03 21:33:29,143 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-03 21:33:41,081 - evaluation_results_class.py:131 - Average Return = 146.1468963623047
2025-08-03 21:33:41,082 - evaluation_results_class.py:133 - Average Virtual Goal Value = 15.606483459472656
2025-08-03 21:33:41,082 - evaluation_results_class.py:135 - Average Discounted Reward = 6.77489709854126
2025-08-03 21:33:41,082 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9958965941731637
2025-08-03 21:33:41,082 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:33:41,082 - evaluation_results_class.py:141 - Variance of Return = 30221.87890625
2025-08-03 21:33:41,082 - evaluation_results_class.py:143 - Current Best Return = 146.1468963623047
2025-08-03 21:33:41,082 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9958965941731637
2025-08-03 21:33:41,082 - evaluation_results_class.py:147 - Average Episode Length = 124.40869922035289
2025-08-03 21:33:41,082 - evaluation_results_class.py:149 - Counted Episodes = 2437
FSC Result: {'best_episode_return': 15.606483, 'best_return': 146.1469, 'goal_value': 0.0, 'returns_episodic': [15.606483], 'returns': [146.1469], 'reach_probs': [0.9958965941731637], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.9958965941731637, 'losses': [], 'best_updated': True, 'each_episode_variance': [30221.879], 'each_episode_virtual_variance': [301.24728], 'combined_variance': [36557.625], 'num_episodes': [2437], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [124.40869922035289], 'counted_episodes': [2437], 'discounted_rewards': [6.774897], 'new_pomdp_iteration_numbers': []}
[[1, 2, 3], [4], [0]]
2025-08-03 21:33:41,147 - statistic.py:67 - synthesis initiated, design space: 131220
2025-08-03 21:33:41,147 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-03 21:33:41,232 - synthesizer_ar.py:122 - value 926.7153 achieved after 816.77 seconds
2025-08-03 21:33:41,238 - synthesizer_ar.py:122 - value 926.6041 achieved after 816.78 seconds
2025-08-03 21:33:41,325 - synthesizer_ar.py:122 - value 123.1723 achieved after 816.86 seconds
2025-08-03 21:33:41,372 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-03 21:33:41,372 - synthesizer.py:193 - P1=0, P2=0, P3=0, P4=0, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 21:33:41,374 - synthesizer.py:198 - double-checking specification satisfiability:  : 123.17234150429911
--------------------
Synthesis summary:
optimality objective: R{"rew"}min=? [F (bat = 0)] 

method: AR, synthesis time: 0.22 s
number of holes: 9, family size: 131220, quotient: 2050 states / 11335 actions
explored: 100 %
MDP stats: avg MDP size: 2050, iterations: 28

optimum: 123.172342
--------------------
2025-08-03 21:33:41,374 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=0, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 21:33:41,396 - robust_rl_trainer.py:432 - Iteration 2 of pure RL loop
2025-08-03 21:33:41,437 - storm_vec_env.py:70 - Computing row map
2025-08-03 21:33:41,453 - storm_vec_env.py:97 - Computing transitions
2025-08-03 21:33:41,487 - storm_vec_env.py:100 - Computing allowed actions
2025-08-03 21:33:41,487 - storm_vec_env.py:114 - Computing sinks
2025-08-03 21:33:41,487 - storm_vec_env.py:119 - Computing raw rewards
2025-08-03 21:33:41,496 - storm_vec_env.py:143 - Computing labels
2025-08-03 21:33:41,496 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-03 21:33:41,496 - storm_vec_env.py:161 - Computing metalabels
2025-08-03 21:33:41,496 - storm_vec_env.py:175 - Computing observations
2025-08-03 21:33:41,605 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-03 21:33:41,606 - father_agent.py:540 - Before training evaluation.
2025-08-03 21:33:41,747 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:33:41,749 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:33:49,293 - evaluation_results_class.py:131 - Average Return = 109.5605239868164
2025-08-03 21:33:49,294 - evaluation_results_class.py:133 - Average Virtual Goal Value = 11.95296859741211
2025-08-03 21:33:49,294 - evaluation_results_class.py:135 - Average Discounted Reward = 5.669026851654053
2025-08-03 21:33:49,294 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9984579799537394
2025-08-03 21:33:49,294 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:33:49,294 - evaluation_results_class.py:141 - Variance of Return = 17539.74609375
2025-08-03 21:33:49,294 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:33:49,294 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9996019108280255
2025-08-03 21:33:49,294 - evaluation_results_class.py:147 - Average Episode Length = 106.97995373939861
2025-08-03 21:33:49,294 - evaluation_results_class.py:149 - Counted Episodes = 2594
2025-08-03 21:33:49,462 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:33:49,473 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:33:49,580 - father_agent.py:436 - Training agent on-policy
2025-08-03 21:33:56,012 - father_agent.py:386 - Step: 0, Training loss: 7.958368301391602
2025-08-03 21:33:57,667 - father_agent.py:386 - Step: 5, Training loss: 5.931599140167236
2025-08-03 21:33:59,306 - father_agent.py:386 - Step: 10, Training loss: 7.851141452789307
2025-08-03 21:34:00,939 - father_agent.py:386 - Step: 15, Training loss: 6.574592590332031
2025-08-03 21:34:02,601 - father_agent.py:386 - Step: 20, Training loss: 8.102743148803711
2025-08-03 21:34:04,269 - father_agent.py:386 - Step: 25, Training loss: 5.988490104675293
2025-08-03 21:34:05,918 - father_agent.py:386 - Step: 30, Training loss: 5.987133979797363
2025-08-03 21:34:07,565 - father_agent.py:386 - Step: 35, Training loss: 6.434316158294678
2025-08-03 21:34:09,250 - father_agent.py:386 - Step: 40, Training loss: 4.743882656097412
2025-08-03 21:34:10,925 - father_agent.py:386 - Step: 45, Training loss: 8.051478385925293
2025-08-03 21:34:12,573 - father_agent.py:386 - Step: 50, Training loss: 6.611484527587891
2025-08-03 21:34:14,212 - father_agent.py:386 - Step: 55, Training loss: 7.493902206420898
2025-08-03 21:34:15,865 - father_agent.py:386 - Step: 60, Training loss: 6.912318706512451
2025-08-03 21:34:17,530 - father_agent.py:386 - Step: 65, Training loss: 6.778632164001465
2025-08-03 21:34:19,184 - father_agent.py:386 - Step: 70, Training loss: 7.7073750495910645
2025-08-03 21:34:20,839 - father_agent.py:386 - Step: 75, Training loss: 7.214569568634033
2025-08-03 21:34:22,499 - father_agent.py:386 - Step: 80, Training loss: 6.717196464538574
2025-08-03 21:34:24,140 - father_agent.py:386 - Step: 85, Training loss: 6.772371292114258
2025-08-03 21:34:25,795 - father_agent.py:386 - Step: 90, Training loss: 5.946108818054199
2025-08-03 21:34:27,443 - father_agent.py:386 - Step: 95, Training loss: 6.804165840148926
2025-08-03 21:34:29,102 - father_agent.py:386 - Step: 100, Training loss: 6.628854751586914
2025-08-03 21:34:29,283 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:34:29,285 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:34:36,763 - evaluation_results_class.py:131 - Average Return = 120.71707153320312
2025-08-03 21:34:36,763 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.070927619934082
2025-08-03 21:34:36,763 - evaluation_results_class.py:135 - Average Discounted Reward = 6.358819961547852
2025-08-03 21:34:36,763 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9996102883865939
2025-08-03 21:34:36,763 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:34:36,763 - evaluation_results_class.py:141 - Variance of Return = 19280.544921875
2025-08-03 21:34:36,763 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:34:36,763 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9996102883865939
2025-08-03 21:34:36,763 - evaluation_results_class.py:147 - Average Episode Length = 106.90296180826189
2025-08-03 21:34:36,763 - evaluation_results_class.py:149 - Counted Episodes = 2566
2025-08-03 21:34:36,941 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:34:36,951 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:34:39,692 - father_agent.py:386 - Step: 105, Training loss: 8.15097713470459
2025-08-03 21:34:41,311 - father_agent.py:386 - Step: 110, Training loss: 6.067390441894531
2025-08-03 21:34:42,937 - father_agent.py:386 - Step: 115, Training loss: 6.496520042419434
2025-08-03 21:34:44,559 - father_agent.py:386 - Step: 120, Training loss: 5.8700432777404785
2025-08-03 21:34:46,206 - father_agent.py:386 - Step: 125, Training loss: 7.842804908752441
2025-08-03 21:34:47,842 - father_agent.py:386 - Step: 130, Training loss: 5.122694492340088
2025-08-03 21:34:49,485 - father_agent.py:386 - Step: 135, Training loss: 5.63112211227417
2025-08-03 21:34:51,145 - father_agent.py:386 - Step: 140, Training loss: 5.182641983032227
2025-08-03 21:34:52,789 - father_agent.py:386 - Step: 145, Training loss: 6.9067840576171875
2025-08-03 21:34:54,425 - father_agent.py:386 - Step: 150, Training loss: 6.836970806121826
2025-08-03 21:34:56,067 - father_agent.py:386 - Step: 155, Training loss: 6.146814346313477
2025-08-03 21:34:57,715 - father_agent.py:386 - Step: 160, Training loss: 5.199804782867432
2025-08-03 21:34:59,342 - father_agent.py:386 - Step: 165, Training loss: 6.116220474243164
2025-08-03 21:35:00,966 - father_agent.py:386 - Step: 170, Training loss: 5.952840805053711
2025-08-03 21:35:02,582 - father_agent.py:386 - Step: 175, Training loss: 6.337576866149902
2025-08-03 21:35:04,194 - father_agent.py:386 - Step: 180, Training loss: 6.197651386260986
2025-08-03 21:35:05,810 - father_agent.py:386 - Step: 185, Training loss: 5.934981346130371
2025-08-03 21:35:07,452 - father_agent.py:386 - Step: 190, Training loss: 7.434952259063721
2025-08-03 21:35:09,095 - father_agent.py:386 - Step: 195, Training loss: 7.69320821762085
2025-08-03 21:35:10,805 - father_agent.py:386 - Step: 200, Training loss: 6.304235458374023
2025-08-03 21:35:10,992 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:35:10,995 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:35:18,643 - evaluation_results_class.py:131 - Average Return = 134.63925170898438
2025-08-03 21:35:18,644 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.460700988769531
2025-08-03 21:35:18,644 - evaluation_results_class.py:135 - Average Discounted Reward = 6.670820713043213
2025-08-03 21:35:18,644 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9983877468762595
2025-08-03 21:35:18,644 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:35:18,644 - evaluation_results_class.py:141 - Variance of Return = 28402.541015625
2025-08-03 21:35:18,644 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:35:18,644 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9996102883865939
2025-08-03 21:35:18,644 - evaluation_results_class.py:147 - Average Episode Length = 110.53929866989117
2025-08-03 21:35:18,644 - evaluation_results_class.py:149 - Counted Episodes = 2481
2025-08-03 21:35:18,821 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:35:18,832 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:35:21,609 - father_agent.py:386 - Step: 205, Training loss: 5.868895530700684
2025-08-03 21:35:23,265 - father_agent.py:386 - Step: 210, Training loss: 5.723267078399658
2025-08-03 21:35:24,926 - father_agent.py:386 - Step: 215, Training loss: 6.2966084480285645
2025-08-03 21:35:26,582 - father_agent.py:386 - Step: 220, Training loss: 7.082163333892822
2025-08-03 21:35:28,310 - father_agent.py:386 - Step: 225, Training loss: 7.642327308654785
2025-08-03 21:35:29,947 - father_agent.py:386 - Step: 230, Training loss: 7.227066516876221
2025-08-03 21:35:31,566 - father_agent.py:386 - Step: 235, Training loss: 6.223733901977539
2025-08-03 21:35:33,191 - father_agent.py:386 - Step: 240, Training loss: 6.291687965393066
2025-08-03 21:35:34,817 - father_agent.py:386 - Step: 245, Training loss: 6.516522407531738
2025-08-03 21:35:36,449 - father_agent.py:386 - Step: 250, Training loss: 7.672936916351318
2025-08-03 21:35:38,070 - father_agent.py:386 - Step: 255, Training loss: 5.950300693511963
2025-08-03 21:35:39,708 - father_agent.py:386 - Step: 260, Training loss: 5.326938152313232
2025-08-03 21:35:41,345 - father_agent.py:386 - Step: 265, Training loss: 5.973979473114014
2025-08-03 21:35:42,986 - father_agent.py:386 - Step: 270, Training loss: 6.273478031158447
2025-08-03 21:35:44,624 - father_agent.py:386 - Step: 275, Training loss: 6.10107946395874
2025-08-03 21:35:46,258 - father_agent.py:386 - Step: 280, Training loss: 5.338582992553711
2025-08-03 21:35:47,911 - father_agent.py:386 - Step: 285, Training loss: 6.354700565338135
2025-08-03 21:35:49,549 - father_agent.py:386 - Step: 290, Training loss: 6.371481418609619
2025-08-03 21:35:51,188 - father_agent.py:386 - Step: 295, Training loss: 6.67667293548584
2025-08-03 21:35:52,827 - father_agent.py:386 - Step: 300, Training loss: 7.3131513595581055
2025-08-03 21:35:53,009 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:35:53,011 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:36:00,611 - evaluation_results_class.py:131 - Average Return = 123.70486450195312
2025-08-03 21:36:00,611 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.363422393798828
2025-08-03 21:36:00,611 - evaluation_results_class.py:135 - Average Discounted Reward = 6.376444339752197
2025-08-03 21:36:00,611 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9964678178963893
2025-08-03 21:36:00,611 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:36:00,611 - evaluation_results_class.py:141 - Variance of Return = 20114.373046875
2025-08-03 21:36:00,611 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:36:00,611 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9996102883865939
2025-08-03 21:36:00,611 - evaluation_results_class.py:147 - Average Episode Length = 107.24215070643642
2025-08-03 21:36:00,611 - evaluation_results_class.py:149 - Counted Episodes = 2548
2025-08-03 21:36:00,786 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:36:00,796 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:36:03,581 - father_agent.py:386 - Step: 305, Training loss: 7.354188919067383
2025-08-03 21:36:05,209 - father_agent.py:386 - Step: 310, Training loss: 8.023994445800781
2025-08-03 21:36:06,861 - father_agent.py:386 - Step: 315, Training loss: 5.542382717132568
2025-08-03 21:36:08,519 - father_agent.py:386 - Step: 320, Training loss: 4.7895588874816895
2025-08-03 21:36:10,162 - father_agent.py:386 - Step: 325, Training loss: 5.482573986053467
2025-08-03 21:36:11,806 - father_agent.py:386 - Step: 330, Training loss: 7.147140026092529
2025-08-03 21:36:13,451 - father_agent.py:386 - Step: 335, Training loss: 7.423325538635254
2025-08-03 21:36:15,112 - father_agent.py:386 - Step: 340, Training loss: 6.5499444007873535
2025-08-03 21:36:16,755 - father_agent.py:386 - Step: 345, Training loss: 5.829080104827881
2025-08-03 21:36:18,385 - father_agent.py:386 - Step: 350, Training loss: 7.121680736541748
2025-08-03 21:36:20,005 - father_agent.py:386 - Step: 355, Training loss: 6.074365139007568
2025-08-03 21:36:21,682 - father_agent.py:386 - Step: 360, Training loss: 7.5625386238098145
2025-08-03 21:36:23,363 - father_agent.py:386 - Step: 365, Training loss: 6.637242794036865
2025-08-03 21:36:25,079 - father_agent.py:386 - Step: 370, Training loss: 5.6516289710998535
2025-08-03 21:36:26,718 - father_agent.py:386 - Step: 375, Training loss: 7.158607006072998
2025-08-03 21:36:28,366 - father_agent.py:386 - Step: 380, Training loss: 5.915329933166504
2025-08-03 21:36:30,061 - father_agent.py:386 - Step: 385, Training loss: 4.959895133972168
2025-08-03 21:36:31,816 - father_agent.py:386 - Step: 390, Training loss: 6.76713752746582
2025-08-03 21:36:33,558 - father_agent.py:386 - Step: 395, Training loss: 7.018497943878174
2025-08-03 21:36:35,233 - father_agent.py:386 - Step: 400, Training loss: 6.836668014526367
2025-08-03 21:36:35,416 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:36:35,419 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:36:43,240 - evaluation_results_class.py:131 - Average Return = 115.8863525390625
2025-08-03 21:36:43,240 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.585433959960938
2025-08-03 21:36:43,240 - evaluation_results_class.py:135 - Average Discounted Reward = 6.056753635406494
2025-08-03 21:36:43,240 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9983993597438976
2025-08-03 21:36:43,240 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:36:43,240 - evaluation_results_class.py:141 - Variance of Return = 18301.4453125
2025-08-03 21:36:43,240 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:36:43,240 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9996102883865939
2025-08-03 21:36:43,240 - evaluation_results_class.py:147 - Average Episode Length = 107.02120848339335
2025-08-03 21:36:43,240 - evaluation_results_class.py:149 - Counted Episodes = 2499
2025-08-03 21:36:43,458 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:36:43,471 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:36:46,232 - father_agent.py:386 - Step: 405, Training loss: 6.178404331207275
2025-08-03 21:36:47,849 - father_agent.py:386 - Step: 410, Training loss: 6.815321445465088
2025-08-03 21:36:49,500 - father_agent.py:386 - Step: 415, Training loss: 6.201672554016113
2025-08-03 21:36:51,146 - father_agent.py:386 - Step: 420, Training loss: 6.256752967834473
2025-08-03 21:36:52,794 - father_agent.py:386 - Step: 425, Training loss: 7.423864364624023
2025-08-03 21:36:54,453 - father_agent.py:386 - Step: 430, Training loss: 6.566036701202393
2025-08-03 21:36:56,101 - father_agent.py:386 - Step: 435, Training loss: 6.349095821380615
2025-08-03 21:36:57,742 - father_agent.py:386 - Step: 440, Training loss: 5.6719465255737305
2025-08-03 21:36:59,390 - father_agent.py:386 - Step: 445, Training loss: 6.1530680656433105
2025-08-03 21:37:01,058 - father_agent.py:386 - Step: 450, Training loss: 7.789102554321289
2025-08-03 21:37:02,692 - father_agent.py:386 - Step: 455, Training loss: 6.750239849090576
2025-08-03 21:37:04,325 - father_agent.py:386 - Step: 460, Training loss: 6.069344997406006
2025-08-03 21:37:05,957 - father_agent.py:386 - Step: 465, Training loss: 5.136868953704834
2025-08-03 21:37:07,585 - father_agent.py:386 - Step: 470, Training loss: 6.925612926483154
2025-08-03 21:37:09,223 - father_agent.py:386 - Step: 475, Training loss: 6.499780178070068
2025-08-03 21:37:10,840 - father_agent.py:386 - Step: 480, Training loss: 6.358680725097656
2025-08-03 21:37:12,464 - father_agent.py:386 - Step: 485, Training loss: 6.5099687576293945
2025-08-03 21:37:14,091 - father_agent.py:386 - Step: 490, Training loss: 7.870728969573975
2025-08-03 21:37:15,733 - father_agent.py:386 - Step: 495, Training loss: 6.451694011688232
2025-08-03 21:37:17,384 - father_agent.py:386 - Step: 500, Training loss: 7.830277919769287
2025-08-03 21:37:17,565 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:37:17,567 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:37:25,306 - evaluation_results_class.py:131 - Average Return = 130.35116577148438
2025-08-03 21:37:25,306 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.029264450073242
2025-08-03 21:37:25,306 - evaluation_results_class.py:135 - Average Discounted Reward = 6.521101474761963
2025-08-03 21:37:25,306 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9970735785953178
2025-08-03 21:37:25,306 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:37:25,306 - evaluation_results_class.py:141 - Variance of Return = 22494.19140625
2025-08-03 21:37:25,306 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:37:25,306 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9996102883865939
2025-08-03 21:37:25,306 - evaluation_results_class.py:147 - Average Episode Length = 114.97951505016722
2025-08-03 21:37:25,306 - evaluation_results_class.py:149 - Counted Episodes = 2392
2025-08-03 21:37:25,484 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:37:25,494 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:37:25,603 - father_agent.py:547 - Training finished.
2025-08-03 21:37:25,747 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:37:25,749 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:37:25,751 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-03 21:37:33,410 - evaluation_results_class.py:131 - Average Return = 124.27580261230469
2025-08-03 21:37:33,410 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.425948143005371
2025-08-03 21:37:33,410 - evaluation_results_class.py:135 - Average Discounted Reward = 6.332101821899414
2025-08-03 21:37:33,410 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9991840065279478
2025-08-03 21:37:33,411 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:37:33,411 - evaluation_results_class.py:141 - Variance of Return = 20102.158203125
2025-08-03 21:37:33,411 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:37:33,411 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9996102883865939
2025-08-03 21:37:33,411 - evaluation_results_class.py:147 - Average Episode Length = 110.83476132190943
2025-08-03 21:37:33,411 - evaluation_results_class.py:149 - Counted Episodes = 2451
2025-08-03 21:37:33,584 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:37:33,586 - self_interpretable_extractor.py:286 - True
2025-08-03 21:37:33,598 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:37:46,854 - evaluation_results_class.py:131 - Average Return = 154.50457763671875
2025-08-03 21:37:46,854 - evaluation_results_class.py:133 - Average Virtual Goal Value = 16.43463706970215
2025-08-03 21:37:46,854 - evaluation_results_class.py:135 - Average Discounted Reward = 6.999415397644043
2025-08-03 21:37:46,854 - evaluation_results_class.py:137 - Goal Reach Probability = 0.992089925062448
2025-08-03 21:37:46,854 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:37:46,854 - evaluation_results_class.py:141 - Variance of Return = 33764.3046875
2025-08-03 21:37:46,854 - evaluation_results_class.py:143 - Current Best Return = 154.50457763671875
2025-08-03 21:37:46,854 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.992089925062448
2025-08-03 21:37:46,854 - evaluation_results_class.py:147 - Average Episode Length = 126.39175686927561
2025-08-03 21:37:46,854 - evaluation_results_class.py:149 - Counted Episodes = 2402
2025-08-03 21:37:46,854 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-03 21:37:46,854 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 11567 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 512, 514, 516, 518, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 606, 608, 610, 612, 614, 620, 622, 624, 628, 630, 634, 636, 638, 640, 642, 644, 648, 650, 651}
Buffer 1
2025-08-03 21:38:46,427 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 23221 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 630, 632, 634, 636, 638, 640, 642, 644, 646, 648, 650, 651}
Buffer 2
2025-08-03 21:39:45,573 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 34318 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 630, 632, 634, 636, 638, 640, 642, 644, 646, 648, 650, 651}
All trajectories collected
2025-08-03 21:40:45,579 - self_interpretable_extractor.py:346 - Data sampled
2025-08-03 21:40:45,579 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 34318 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_1.dot.
Learned FSC of size 2
2025-08-03 21:41:15,316 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-03 21:41:27,842 - evaluation_results_class.py:131 - Average Return = 145.337890625
2025-08-03 21:41:27,842 - evaluation_results_class.py:133 - Average Virtual Goal Value = 15.519247055053711
2025-08-03 21:41:27,842 - evaluation_results_class.py:135 - Average Discounted Reward = 6.652951240539551
2025-08-03 21:41:27,842 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9927288280581694
2025-08-03 21:41:27,842 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:41:27,842 - evaluation_results_class.py:141 - Variance of Return = 27267.658203125
2025-08-03 21:41:27,843 - evaluation_results_class.py:143 - Current Best Return = 145.337890625
2025-08-03 21:41:27,843 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9927288280581694
2025-08-03 21:41:27,843 - evaluation_results_class.py:147 - Average Episode Length = 129.03806672369547
2025-08-03 21:41:27,843 - evaluation_results_class.py:149 - Counted Episodes = 2338
FSC Result: {'best_episode_return': 15.519247, 'best_return': 145.33789, 'goal_value': 0.0, 'returns_episodic': [15.519247], 'returns': [145.33789], 'reach_probs': [0.9927288280581694], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.9927288280581694, 'losses': [], 'best_updated': True, 'each_episode_variance': [27267.658], 'each_episode_virtual_variance': [271.0751], 'combined_variance': [32975.957], 'num_episodes': [2338], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [129.03806672369547], 'counted_episodes': [2338], 'discounted_rewards': [6.6529512], 'new_pomdp_iteration_numbers': []}
[[1, 2, 3], [4], [0]]
2025-08-03 21:41:27,914 - statistic.py:67 - synthesis initiated, design space: 131220
2025-08-03 21:41:27,915 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-03 21:41:27,993 - synthesizer_ar.py:122 - value 526.9639 achieved after 1283.53 seconds
2025-08-03 21:41:27,999 - synthesizer_ar.py:122 - value 526.95 achieved after 1283.54 seconds
2025-08-03 21:41:28,045 - synthesizer_ar.py:122 - value 141.6918 achieved after 1283.58 seconds
2025-08-03 21:41:28,101 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-03 21:41:28,101 - synthesizer.py:193 - P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 21:41:28,103 - synthesizer.py:198 - double-checking specification satisfiability:  : 141.69178788034793
--------------------
Synthesis summary:
optimality objective: R{"rew"}min=? [F (bat = 0)] 

method: AR, synthesis time: 0.19 s
number of holes: 9, family size: 131220, quotient: 2050 states / 11335 actions
explored: 100 %
MDP stats: avg MDP size: 2050, iterations: 28

optimum: 141.691788
--------------------
2025-08-03 21:41:28,103 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 21:41:28,126 - robust_rl_trainer.py:432 - Iteration 3 of pure RL loop
2025-08-03 21:41:28,167 - storm_vec_env.py:70 - Computing row map
2025-08-03 21:41:28,183 - storm_vec_env.py:97 - Computing transitions
2025-08-03 21:41:28,218 - storm_vec_env.py:100 - Computing allowed actions
2025-08-03 21:41:28,218 - storm_vec_env.py:114 - Computing sinks
2025-08-03 21:41:28,218 - storm_vec_env.py:119 - Computing raw rewards
2025-08-03 21:41:28,226 - storm_vec_env.py:143 - Computing labels
2025-08-03 21:41:28,227 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-03 21:41:28,227 - storm_vec_env.py:161 - Computing metalabels
2025-08-03 21:41:28,227 - storm_vec_env.py:175 - Computing observations
2025-08-03 21:41:28,348 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-03 21:41:28,348 - father_agent.py:540 - Before training evaluation.
2025-08-03 21:41:28,511 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:41:28,514 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:41:36,864 - evaluation_results_class.py:131 - Average Return = 115.37750244140625
2025-08-03 21:41:36,864 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.535439491271973
2025-08-03 21:41:36,864 - evaluation_results_class.py:135 - Average Discounted Reward = 6.1737380027771
2025-08-03 21:41:36,865 - evaluation_results_class.py:137 - Goal Reach Probability = 0.99884437596302
2025-08-03 21:41:36,865 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:41:36,865 - evaluation_results_class.py:141 - Variance of Return = 16329.94140625
2025-08-03 21:41:36,865 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:41:36,865 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9996102883865939
2025-08-03 21:41:36,865 - evaluation_results_class.py:147 - Average Episode Length = 106.47881355932203
2025-08-03 21:41:36,865 - evaluation_results_class.py:149 - Counted Episodes = 2596
2025-08-03 21:41:37,101 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:41:37,117 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:41:37,242 - father_agent.py:436 - Training agent on-policy
2025-08-03 21:41:44,170 - father_agent.py:386 - Step: 0, Training loss: 6.687198638916016
2025-08-03 21:41:46,011 - father_agent.py:386 - Step: 5, Training loss: 6.39762544631958
2025-08-03 21:41:47,832 - father_agent.py:386 - Step: 10, Training loss: 6.243222713470459
2025-08-03 21:41:49,689 - father_agent.py:386 - Step: 15, Training loss: 6.785177230834961
2025-08-03 21:41:51,535 - father_agent.py:386 - Step: 20, Training loss: 4.986632823944092
2025-08-03 21:41:53,308 - father_agent.py:386 - Step: 25, Training loss: 5.736464023590088
2025-08-03 21:41:55,087 - father_agent.py:386 - Step: 30, Training loss: 5.78009557723999
2025-08-03 21:41:56,865 - father_agent.py:386 - Step: 35, Training loss: 5.512851715087891
2025-08-03 21:41:58,650 - father_agent.py:386 - Step: 40, Training loss: 7.067266464233398
2025-08-03 21:42:00,483 - father_agent.py:386 - Step: 45, Training loss: 6.116495609283447
2025-08-03 21:42:02,297 - father_agent.py:386 - Step: 50, Training loss: 5.919405460357666
2025-08-03 21:42:04,099 - father_agent.py:386 - Step: 55, Training loss: 7.2298359870910645
2025-08-03 21:42:05,918 - father_agent.py:386 - Step: 60, Training loss: 7.219148635864258
2025-08-03 21:42:07,629 - father_agent.py:386 - Step: 65, Training loss: 5.461358547210693
2025-08-03 21:42:09,327 - father_agent.py:386 - Step: 70, Training loss: 5.808218002319336
2025-08-03 21:42:11,061 - father_agent.py:386 - Step: 75, Training loss: 6.349556922912598
2025-08-03 21:42:12,777 - father_agent.py:386 - Step: 80, Training loss: 6.4167799949646
2025-08-03 21:42:14,486 - father_agent.py:386 - Step: 85, Training loss: 5.765345096588135
2025-08-03 21:42:16,202 - father_agent.py:386 - Step: 90, Training loss: 6.171895503997803
2025-08-03 21:42:17,910 - father_agent.py:386 - Step: 95, Training loss: 6.144398212432861
2025-08-03 21:42:19,629 - father_agent.py:386 - Step: 100, Training loss: 6.611824035644531
2025-08-03 21:42:19,823 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:42:19,826 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:42:28,123 - evaluation_results_class.py:131 - Average Return = 113.15973663330078
2025-08-03 21:42:28,123 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.312087059020996
2025-08-03 21:42:28,123 - evaluation_results_class.py:135 - Average Discounted Reward = 5.982292652130127
2025-08-03 21:42:28,123 - evaluation_results_class.py:137 - Goal Reach Probability = 0.998056743101438
2025-08-03 21:42:28,123 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:42:28,123 - evaluation_results_class.py:141 - Variance of Return = 18237.861328125
2025-08-03 21:42:28,123 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:42:28,123 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9996102883865939
2025-08-03 21:42:28,124 - evaluation_results_class.py:147 - Average Episode Length = 103.99339292654489
2025-08-03 21:42:28,124 - evaluation_results_class.py:149 - Counted Episodes = 2573
2025-08-03 21:42:28,314 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:42:28,324 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:42:31,433 - father_agent.py:386 - Step: 105, Training loss: 6.7089033126831055
2025-08-03 21:42:33,141 - father_agent.py:386 - Step: 110, Training loss: 6.998299598693848
2025-08-03 21:42:34,854 - father_agent.py:386 - Step: 115, Training loss: 5.6015095710754395
2025-08-03 21:42:36,568 - father_agent.py:386 - Step: 120, Training loss: 8.803396224975586
2025-08-03 21:42:38,279 - father_agent.py:386 - Step: 125, Training loss: 5.053330421447754
2025-08-03 21:42:40,090 - father_agent.py:386 - Step: 130, Training loss: 6.033458709716797
2025-08-03 21:42:41,847 - father_agent.py:386 - Step: 135, Training loss: 4.347520351409912
2025-08-03 21:42:43,618 - father_agent.py:386 - Step: 140, Training loss: 7.128139019012451
2025-08-03 21:42:45,371 - father_agent.py:386 - Step: 145, Training loss: 6.7676191329956055
2025-08-03 21:42:47,103 - father_agent.py:386 - Step: 150, Training loss: 4.988051891326904
2025-08-03 21:42:48,846 - father_agent.py:386 - Step: 155, Training loss: 6.482122898101807
2025-08-03 21:42:50,635 - father_agent.py:386 - Step: 160, Training loss: 7.1203227043151855
2025-08-03 21:42:52,455 - father_agent.py:386 - Step: 165, Training loss: 5.55412483215332
2025-08-03 21:42:54,303 - father_agent.py:386 - Step: 170, Training loss: 6.2996673583984375
2025-08-03 21:42:56,098 - father_agent.py:386 - Step: 175, Training loss: 7.2766218185424805
2025-08-03 21:42:57,896 - father_agent.py:386 - Step: 180, Training loss: 5.702676296234131
2025-08-03 21:42:59,698 - father_agent.py:386 - Step: 185, Training loss: 5.742833137512207
2025-08-03 21:43:01,486 - father_agent.py:386 - Step: 190, Training loss: 6.9815545082092285
2025-08-03 21:43:03,233 - father_agent.py:386 - Step: 195, Training loss: 5.737637042999268
2025-08-03 21:43:04,966 - father_agent.py:386 - Step: 200, Training loss: 6.178927898406982
2025-08-03 21:43:05,160 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:43:05,163 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:43:13,660 - evaluation_results_class.py:131 - Average Return = 119.52922058105469
2025-08-03 21:43:13,661 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.948863983154297
2025-08-03 21:43:13,661 - evaluation_results_class.py:135 - Average Discounted Reward = 6.301487445831299
2025-08-03 21:43:13,661 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9979707792207793
2025-08-03 21:43:13,661 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:43:13,661 - evaluation_results_class.py:141 - Variance of Return = 16412.11328125
2025-08-03 21:43:13,661 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:43:13,661 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9996102883865939
2025-08-03 21:43:13,661 - evaluation_results_class.py:147 - Average Episode Length = 110.60430194805195
2025-08-03 21:43:13,661 - evaluation_results_class.py:149 - Counted Episodes = 2464
2025-08-03 21:43:13,854 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:43:13,864 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:43:17,091 - father_agent.py:386 - Step: 205, Training loss: 5.984882831573486
2025-08-03 21:43:18,821 - father_agent.py:386 - Step: 210, Training loss: 7.111672878265381
2025-08-03 21:43:20,517 - father_agent.py:386 - Step: 215, Training loss: 7.872490882873535
2025-08-03 21:43:22,203 - father_agent.py:386 - Step: 220, Training loss: 5.856651782989502
2025-08-03 21:43:23,883 - father_agent.py:386 - Step: 225, Training loss: 8.14754867553711
2025-08-03 21:43:25,627 - father_agent.py:386 - Step: 230, Training loss: 6.721469879150391
2025-08-03 21:43:27,343 - father_agent.py:386 - Step: 235, Training loss: 6.065792560577393
2025-08-03 21:43:29,080 - father_agent.py:386 - Step: 240, Training loss: 5.23338508605957
2025-08-03 21:43:30,914 - father_agent.py:386 - Step: 245, Training loss: 5.825076103210449
2025-08-03 21:43:32,692 - father_agent.py:386 - Step: 250, Training loss: 6.251403331756592
2025-08-03 21:43:34,471 - father_agent.py:386 - Step: 255, Training loss: 6.906414985656738
2025-08-03 21:43:36,219 - father_agent.py:386 - Step: 260, Training loss: 7.524272918701172
2025-08-03 21:43:37,994 - father_agent.py:386 - Step: 265, Training loss: 6.496230125427246
2025-08-03 21:43:39,750 - father_agent.py:386 - Step: 270, Training loss: 6.169796466827393
2025-08-03 21:43:41,477 - father_agent.py:386 - Step: 275, Training loss: 6.007984161376953
2025-08-03 21:43:43,215 - father_agent.py:386 - Step: 280, Training loss: 8.285317420959473
2025-08-03 21:43:44,916 - father_agent.py:386 - Step: 285, Training loss: 7.094721794128418
2025-08-03 21:43:46,604 - father_agent.py:386 - Step: 290, Training loss: 6.592398643493652
2025-08-03 21:43:48,304 - father_agent.py:386 - Step: 295, Training loss: 5.234223365783691
2025-08-03 21:43:50,007 - father_agent.py:386 - Step: 300, Training loss: 7.2300262451171875
2025-08-03 21:43:50,209 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:43:50,212 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:43:58,447 - evaluation_results_class.py:131 - Average Return = 118.62238311767578
2025-08-03 21:43:58,447 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.86059856414795
2025-08-03 21:43:58,448 - evaluation_results_class.py:135 - Average Discounted Reward = 6.2265472412109375
2025-08-03 21:43:58,448 - evaluation_results_class.py:137 - Goal Reach Probability = 0.999179991799918
2025-08-03 21:43:58,448 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:43:58,448 - evaluation_results_class.py:141 - Variance of Return = 16222.45703125
2025-08-03 21:43:58,448 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:43:58,448 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9996102883865939
2025-08-03 21:43:58,448 - evaluation_results_class.py:147 - Average Episode Length = 110.52562525625257
2025-08-03 21:43:58,448 - evaluation_results_class.py:149 - Counted Episodes = 2439
2025-08-03 21:43:58,635 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:43:58,645 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:44:01,808 - father_agent.py:386 - Step: 305, Training loss: 6.051472187042236
2025-08-03 21:44:03,532 - father_agent.py:386 - Step: 310, Training loss: 5.4376654624938965
2025-08-03 21:44:05,264 - father_agent.py:386 - Step: 315, Training loss: 6.718698024749756
2025-08-03 21:44:06,970 - father_agent.py:386 - Step: 320, Training loss: 5.206575870513916
2025-08-03 21:44:08,680 - father_agent.py:386 - Step: 325, Training loss: 5.781589984893799
2025-08-03 21:44:10,360 - father_agent.py:386 - Step: 330, Training loss: 7.117973327636719
2025-08-03 21:44:12,032 - father_agent.py:386 - Step: 335, Training loss: 7.026289463043213
2025-08-03 21:44:13,710 - father_agent.py:386 - Step: 340, Training loss: 6.516311168670654
2025-08-03 21:44:15,384 - father_agent.py:386 - Step: 345, Training loss: 6.217396259307861
2025-08-03 21:44:17,075 - father_agent.py:386 - Step: 350, Training loss: 6.166128635406494
2025-08-03 21:44:18,749 - father_agent.py:386 - Step: 355, Training loss: 6.9233880043029785
2025-08-03 21:44:20,452 - father_agent.py:386 - Step: 360, Training loss: 6.802956581115723
2025-08-03 21:44:22,147 - father_agent.py:386 - Step: 365, Training loss: 5.9642014503479
2025-08-03 21:44:23,834 - father_agent.py:386 - Step: 370, Training loss: 5.785679817199707
2025-08-03 21:44:25,509 - father_agent.py:386 - Step: 375, Training loss: 7.897881031036377
2025-08-03 21:44:27,198 - father_agent.py:386 - Step: 380, Training loss: 6.211029529571533
2025-08-03 21:44:28,905 - father_agent.py:386 - Step: 385, Training loss: 4.795182704925537
2025-08-03 21:44:30,617 - father_agent.py:386 - Step: 390, Training loss: 5.76318359375
2025-08-03 21:44:32,324 - father_agent.py:386 - Step: 395, Training loss: 6.115501880645752
2025-08-03 21:44:34,017 - father_agent.py:386 - Step: 400, Training loss: 6.368417739868164
2025-08-03 21:44:34,213 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:44:34,216 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:44:42,534 - evaluation_results_class.py:131 - Average Return = 124.00662231445312
2025-08-03 21:44:42,534 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.396523475646973
2025-08-03 21:44:42,534 - evaluation_results_class.py:135 - Average Discounted Reward = 6.3492751121521
2025-08-03 21:44:42,534 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9979304635761589
2025-08-03 21:44:42,534 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:44:42,534 - evaluation_results_class.py:141 - Variance of Return = 19068.71484375
2025-08-03 21:44:42,534 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:44:42,535 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9996102883865939
2025-08-03 21:44:42,535 - evaluation_results_class.py:147 - Average Episode Length = 111.27773178807946
2025-08-03 21:44:42,535 - evaluation_results_class.py:149 - Counted Episodes = 2416
2025-08-03 21:44:42,724 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:44:42,735 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:44:45,849 - father_agent.py:386 - Step: 405, Training loss: 5.884836673736572
2025-08-03 21:44:47,553 - father_agent.py:386 - Step: 410, Training loss: 7.013059139251709
2025-08-03 21:44:49,263 - father_agent.py:386 - Step: 415, Training loss: 6.8436994552612305
2025-08-03 21:44:50,954 - father_agent.py:386 - Step: 420, Training loss: 7.089013576507568
2025-08-03 21:44:52,652 - father_agent.py:386 - Step: 425, Training loss: 6.4420294761657715
2025-08-03 21:44:54,346 - father_agent.py:386 - Step: 430, Training loss: 5.510761737823486
2025-08-03 21:44:56,029 - father_agent.py:386 - Step: 435, Training loss: 6.238142490386963
2025-08-03 21:44:57,712 - father_agent.py:386 - Step: 440, Training loss: 6.199412822723389
2025-08-03 21:44:59,404 - father_agent.py:386 - Step: 445, Training loss: 6.31155252456665
2025-08-03 21:45:01,103 - father_agent.py:386 - Step: 450, Training loss: 6.527564525604248
2025-08-03 21:45:02,801 - father_agent.py:386 - Step: 455, Training loss: 7.554426670074463
2025-08-03 21:45:04,487 - father_agent.py:386 - Step: 460, Training loss: 7.161085605621338
2025-08-03 21:45:06,161 - father_agent.py:386 - Step: 465, Training loss: 7.485601425170898
2025-08-03 21:45:07,856 - father_agent.py:386 - Step: 470, Training loss: 6.507757663726807
2025-08-03 21:45:09,557 - father_agent.py:386 - Step: 475, Training loss: 6.852051734924316
2025-08-03 21:45:11,236 - father_agent.py:386 - Step: 480, Training loss: 6.233367919921875
2025-08-03 21:45:12,916 - father_agent.py:386 - Step: 485, Training loss: 6.19041633605957
2025-08-03 21:45:14,622 - father_agent.py:386 - Step: 490, Training loss: 5.366111755371094
2025-08-03 21:45:16,305 - father_agent.py:386 - Step: 495, Training loss: 6.718003749847412
2025-08-03 21:45:17,998 - father_agent.py:386 - Step: 500, Training loss: 5.720348834991455
2025-08-03 21:45:18,197 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:45:18,200 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:45:32,589 - evaluation_results_class.py:131 - Average Return = 110.6347427368164
2025-08-03 21:45:32,589 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.062713623046875
2025-08-03 21:45:32,590 - evaluation_results_class.py:135 - Average Discounted Reward = 6.023303031921387
2025-08-03 21:45:32,590 - evaluation_results_class.py:137 - Goal Reach Probability = 0.999619916381604
2025-08-03 21:45:32,590 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:45:32,590 - evaluation_results_class.py:141 - Variance of Return = 15208.0732421875
2025-08-03 21:45:32,590 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:45:32,590 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.999619916381604
2025-08-03 21:45:32,590 - evaluation_results_class.py:147 - Average Episode Length = 104.28240212846826
2025-08-03 21:45:32,590 - evaluation_results_class.py:149 - Counted Episodes = 2631
2025-08-03 21:45:32,850 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:45:32,861 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:45:32,969 - father_agent.py:547 - Training finished.
2025-08-03 21:45:33,109 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:45:33,111 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:45:33,114 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-03 21:45:41,404 - evaluation_results_class.py:131 - Average Return = 117.0102310180664
2025-08-03 21:45:41,405 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.69630241394043
2025-08-03 21:45:41,405 - evaluation_results_class.py:135 - Average Discounted Reward = 6.1193156242370605
2025-08-03 21:45:41,405 - evaluation_results_class.py:137 - Goal Reach Probability = 0.997639653815893
2025-08-03 21:45:41,405 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:45:41,405 - evaluation_results_class.py:141 - Variance of Return = 15948.2607421875
2025-08-03 21:45:41,405 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:45:41,405 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.999619916381604
2025-08-03 21:45:41,405 - evaluation_results_class.py:147 - Average Episode Length = 108.37136113296617
2025-08-03 21:45:41,405 - evaluation_results_class.py:149 - Counted Episodes = 2542
2025-08-03 21:45:41,581 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:45:41,583 - self_interpretable_extractor.py:286 - True
2025-08-03 21:45:41,593 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:45:55,856 - evaluation_results_class.py:131 - Average Return = 140.46817016601562
2025-08-03 21:45:55,856 - evaluation_results_class.py:133 - Average Virtual Goal Value = 15.039424896240234
2025-08-03 21:45:55,856 - evaluation_results_class.py:135 - Average Discounted Reward = 6.644940376281738
2025-08-03 21:45:55,856 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9963039014373717
2025-08-03 21:45:55,856 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:45:55,856 - evaluation_results_class.py:141 - Variance of Return = 25985.326171875
2025-08-03 21:45:55,856 - evaluation_results_class.py:143 - Current Best Return = 140.46817016601562
2025-08-03 21:45:55,857 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9963039014373717
2025-08-03 21:45:55,857 - evaluation_results_class.py:147 - Average Episode Length = 122.92689938398357
2025-08-03 21:45:55,857 - evaluation_results_class.py:149 - Counted Episodes = 2435
2025-08-03 21:45:55,857 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-03 21:45:55,857 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 11598 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 498, 500, 502, 504, 506, 508, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 574, 576, 578, 580, 582, 584, 588, 590, 592, 594, 596, 598, 600, 602, 604, 608, 610, 612, 614, 616, 618, 620, 622, 626, 628, 636, 638, 640, 644, 650, 651}
Buffer 1
2025-08-03 21:46:58,111 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 23160 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 634, 636, 638, 640, 644, 646, 648, 650, 651}
Buffer 2
2025-08-03 21:48:00,500 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 34458 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 632, 634, 636, 638, 640, 642, 644, 646, 648, 650, 651}
All trajectories collected
2025-08-03 21:49:02,890 - self_interpretable_extractor.py:346 - Data sampled
2025-08-03 21:49:02,891 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 34458 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_2.dot.
Learned FSC of size 2
2025-08-03 21:49:30,986 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-03 21:49:44,406 - evaluation_results_class.py:131 - Average Return = 130.38107299804688
2025-08-03 21:49:44,406 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.029281616210938
2025-08-03 21:49:44,406 - evaluation_results_class.py:135 - Average Discounted Reward = 6.220753192901611
2025-08-03 21:49:44,406 - evaluation_results_class.py:137 - Goal Reach Probability = 0.99558764540714
2025-08-03 21:49:44,406 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:49:44,406 - evaluation_results_class.py:141 - Variance of Return = 22002.78125
2025-08-03 21:49:44,407 - evaluation_results_class.py:143 - Current Best Return = 130.38107299804688
2025-08-03 21:49:44,407 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.99558764540714
2025-08-03 21:49:44,407 - evaluation_results_class.py:147 - Average Episode Length = 121.98596068993182
2025-08-03 21:49:44,407 - evaluation_results_class.py:149 - Counted Episodes = 2493
FSC Result: {'best_episode_return': 14.029282, 'best_return': 130.38107, 'goal_value': 0.0, 'returns_episodic': [14.029282], 'returns': [130.38107], 'reach_probs': [0.99558764540714], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.99558764540714, 'losses': [], 'best_updated': True, 'each_episode_variance': [22002.781], 'each_episode_virtual_variance': [219.00476], 'combined_variance': [26611.94], 'num_episodes': [2493], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [121.98596068993182], 'counted_episodes': [2493], 'discounted_rewards': [6.220753], 'new_pomdp_iteration_numbers': []}
[[1, 2, 3], [4], [0]]
2025-08-03 21:49:44,485 - statistic.py:67 - synthesis initiated, design space: 131220
2025-08-03 21:49:44,485 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-03 21:49:44,591 - synthesizer_ar.py:122 - value 1029.7583 achieved after 1780.13 seconds
2025-08-03 21:49:44,597 - synthesizer_ar.py:122 - value 1028.83 achieved after 1780.13 seconds
2025-08-03 21:49:44,804 - synthesizer_ar.py:122 - value 986.0959 achieved after 1780.34 seconds
2025-08-03 21:49:44,889 - synthesizer_ar.py:122 - value 685.1649 achieved after 1780.43 seconds
2025-08-03 21:49:44,895 - synthesizer_ar.py:122 - value 685.1493 achieved after 1780.43 seconds
2025-08-03 21:49:44,971 - synthesizer_ar.py:122 - value 500.6874 achieved after 1780.51 seconds
2025-08-03 21:49:44,977 - synthesizer_ar.py:122 - value 500.6851 achieved after 1780.51 seconds
2025-08-03 21:49:45,188 - synthesizer_ar.py:122 - value 460.0304 achieved after 1780.73 seconds
2025-08-03 21:49:45,240 - synthesizer_ar.py:122 - value 392.6267 achieved after 1780.78 seconds
2025-08-03 21:49:45,245 - synthesizer_ar.py:122 - value 392.6195 achieved after 1780.78 seconds
2025-08-03 21:49:45,364 - synthesizer_ar.py:122 - value 138.908 achieved after 1780.9 seconds
2025-08-03 21:49:45,412 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-03 21:49:45,412 - synthesizer.py:193 - P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 21:49:45,414 - synthesizer.py:198 - double-checking specification satisfiability:  : 138.90796767757942
--------------------
Synthesis summary:
optimality objective: R{"rew"}min=? [F (bat = 0)] 

method: AR, synthesis time: 0.93 s
number of holes: 9, family size: 131220, quotient: 2050 states / 11335 actions
explored: 100 %
MDP stats: avg MDP size: 2050, iterations: 133

optimum: 138.907968
--------------------
2025-08-03 21:49:45,414 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 21:49:45,437 - robust_rl_trainer.py:432 - Iteration 4 of pure RL loop
2025-08-03 21:49:45,479 - storm_vec_env.py:70 - Computing row map
2025-08-03 21:49:45,495 - storm_vec_env.py:97 - Computing transitions
2025-08-03 21:49:45,529 - storm_vec_env.py:100 - Computing allowed actions
2025-08-03 21:49:45,529 - storm_vec_env.py:114 - Computing sinks
2025-08-03 21:49:45,529 - storm_vec_env.py:119 - Computing raw rewards
2025-08-03 21:49:45,538 - storm_vec_env.py:143 - Computing labels
2025-08-03 21:49:45,538 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-03 21:49:45,538 - storm_vec_env.py:161 - Computing metalabels
2025-08-03 21:49:45,538 - storm_vec_env.py:175 - Computing observations
2025-08-03 21:49:45,666 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-03 21:49:45,667 - father_agent.py:540 - Before training evaluation.
2025-08-03 21:49:45,810 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:49:45,813 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:49:54,658 - evaluation_results_class.py:131 - Average Return = 110.80632019042969
2025-08-03 21:49:54,658 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.07896900177002
2025-08-03 21:49:54,659 - evaluation_results_class.py:135 - Average Discounted Reward = 5.7710466384887695
2025-08-03 21:49:54,659 - evaluation_results_class.py:137 - Goal Reach Probability = 0.999168744804655
2025-08-03 21:49:54,659 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:49:54,659 - evaluation_results_class.py:141 - Variance of Return = 15447.9765625
2025-08-03 21:49:54,659 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:49:54,659 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.999619916381604
2025-08-03 21:49:54,659 - evaluation_results_class.py:147 - Average Episode Length = 110.61263507896925
2025-08-03 21:49:54,659 - evaluation_results_class.py:149 - Counted Episodes = 2406
2025-08-03 21:49:54,856 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:49:54,867 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:49:54,975 - father_agent.py:436 - Training agent on-policy
2025-08-03 21:50:02,287 - father_agent.py:386 - Step: 0, Training loss: 7.869278907775879
2025-08-03 21:50:04,067 - father_agent.py:386 - Step: 5, Training loss: 4.876047611236572
2025-08-03 21:50:05,823 - father_agent.py:386 - Step: 10, Training loss: 5.085029125213623
2025-08-03 21:50:07,576 - father_agent.py:386 - Step: 15, Training loss: 7.50900936126709
2025-08-03 21:50:09,361 - father_agent.py:386 - Step: 20, Training loss: 5.635898590087891
2025-08-03 21:50:11,111 - father_agent.py:386 - Step: 25, Training loss: 5.953700542449951
2025-08-03 21:50:12,868 - father_agent.py:386 - Step: 30, Training loss: 5.9442315101623535
2025-08-03 21:50:14,642 - father_agent.py:386 - Step: 35, Training loss: 6.04326057434082
2025-08-03 21:50:16,429 - father_agent.py:386 - Step: 40, Training loss: 5.5567545890808105
2025-08-03 21:50:18,184 - father_agent.py:386 - Step: 45, Training loss: 5.5578718185424805
2025-08-03 21:50:19,926 - father_agent.py:386 - Step: 50, Training loss: 6.153829574584961
2025-08-03 21:50:21,688 - father_agent.py:386 - Step: 55, Training loss: 4.887963771820068
2025-08-03 21:50:23,442 - father_agent.py:386 - Step: 60, Training loss: 6.548286437988281
2025-08-03 21:50:25,223 - father_agent.py:386 - Step: 65, Training loss: 5.281702995300293
2025-08-03 21:50:27,017 - father_agent.py:386 - Step: 70, Training loss: 5.306106090545654
2025-08-03 21:50:28,797 - father_agent.py:386 - Step: 75, Training loss: 6.14762544631958
2025-08-03 21:50:30,572 - father_agent.py:386 - Step: 80, Training loss: 7.324559688568115
2025-08-03 21:50:32,359 - father_agent.py:386 - Step: 85, Training loss: 4.937005519866943
2025-08-03 21:50:34,119 - father_agent.py:386 - Step: 90, Training loss: 5.456810474395752
2025-08-03 21:50:35,875 - father_agent.py:386 - Step: 95, Training loss: 6.208792209625244
2025-08-03 21:50:37,638 - father_agent.py:386 - Step: 100, Training loss: 6.324735164642334
2025-08-03 21:50:37,843 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:50:37,846 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:50:46,632 - evaluation_results_class.py:131 - Average Return = 118.39481353759766
2025-08-03 21:50:46,632 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.837860107421875
2025-08-03 21:50:46,632 - evaluation_results_class.py:135 - Average Discounted Reward = 6.186552047729492
2025-08-03 21:50:46,632 - evaluation_results_class.py:137 - Goal Reach Probability = 0.999189298743413
2025-08-03 21:50:46,632 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:50:46,632 - evaluation_results_class.py:141 - Variance of Return = 16393.6953125
2025-08-03 21:50:46,632 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:50:46,632 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.999619916381604
2025-08-03 21:50:46,632 - evaluation_results_class.py:147 - Average Episode Length = 110.92501013376571
2025-08-03 21:50:46,632 - evaluation_results_class.py:149 - Counted Episodes = 2467
2025-08-03 21:50:46,832 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:50:46,842 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:50:50,286 - father_agent.py:386 - Step: 105, Training loss: 6.565121650695801
2025-08-03 21:50:52,026 - father_agent.py:386 - Step: 110, Training loss: 5.790286540985107
2025-08-03 21:50:53,789 - father_agent.py:386 - Step: 115, Training loss: 6.267333030700684
2025-08-03 21:50:55,542 - father_agent.py:386 - Step: 120, Training loss: 6.5969672203063965
2025-08-03 21:50:57,315 - father_agent.py:386 - Step: 125, Training loss: 5.989656925201416
2025-08-03 21:50:59,076 - father_agent.py:386 - Step: 130, Training loss: 5.300654411315918
2025-08-03 21:51:00,826 - father_agent.py:386 - Step: 135, Training loss: 5.767245769500732
2025-08-03 21:51:02,571 - father_agent.py:386 - Step: 140, Training loss: 5.657571315765381
2025-08-03 21:51:04,325 - father_agent.py:386 - Step: 145, Training loss: 5.901639938354492
2025-08-03 21:51:06,104 - father_agent.py:386 - Step: 150, Training loss: 6.098576068878174
2025-08-03 21:51:07,860 - father_agent.py:386 - Step: 155, Training loss: 5.047488689422607
2025-08-03 21:51:09,652 - father_agent.py:386 - Step: 160, Training loss: 5.840578079223633
2025-08-03 21:51:11,412 - father_agent.py:386 - Step: 165, Training loss: 6.693653583526611
2025-08-03 21:51:13,171 - father_agent.py:386 - Step: 170, Training loss: 6.002971649169922
2025-08-03 21:51:14,912 - father_agent.py:386 - Step: 175, Training loss: 5.204145431518555
2025-08-03 21:51:16,674 - father_agent.py:386 - Step: 180, Training loss: 6.161799907684326
2025-08-03 21:51:18,423 - father_agent.py:386 - Step: 185, Training loss: 5.568461894989014
2025-08-03 21:51:20,265 - father_agent.py:386 - Step: 190, Training loss: 6.134986400604248
2025-08-03 21:51:22,049 - father_agent.py:386 - Step: 195, Training loss: 6.916041374206543
2025-08-03 21:51:23,820 - father_agent.py:386 - Step: 200, Training loss: 6.2524237632751465
2025-08-03 21:51:24,032 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:51:24,034 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:51:32,904 - evaluation_results_class.py:131 - Average Return = 112.52782440185547
2025-08-03 21:51:32,905 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.252781867980957
2025-08-03 21:51:32,905 - evaluation_results_class.py:135 - Average Discounted Reward = 5.8898091316223145
2025-08-03 21:51:32,905 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-03 21:51:32,905 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:51:32,905 - evaluation_results_class.py:141 - Variance of Return = 16151.478515625
2025-08-03 21:51:32,905 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:51:32,905 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 21:51:32,905 - evaluation_results_class.py:147 - Average Episode Length = 108.09379968203497
2025-08-03 21:51:32,905 - evaluation_results_class.py:149 - Counted Episodes = 2516
2025-08-03 21:51:33,109 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:51:33,119 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:51:36,553 - father_agent.py:386 - Step: 205, Training loss: 6.65939474105835
2025-08-03 21:51:38,307 - father_agent.py:386 - Step: 210, Training loss: 5.742068290710449
2025-08-03 21:51:40,099 - father_agent.py:386 - Step: 215, Training loss: 5.679098129272461
2025-08-03 21:51:41,875 - father_agent.py:386 - Step: 220, Training loss: 5.596659183502197
2025-08-03 21:51:43,666 - father_agent.py:386 - Step: 225, Training loss: 6.777983665466309
2025-08-03 21:51:45,465 - father_agent.py:386 - Step: 230, Training loss: 4.7865753173828125
2025-08-03 21:51:47,263 - father_agent.py:386 - Step: 235, Training loss: 5.062992572784424
2025-08-03 21:51:49,027 - father_agent.py:386 - Step: 240, Training loss: 5.849362373352051
2025-08-03 21:51:50,812 - father_agent.py:386 - Step: 245, Training loss: 5.967111587524414
2025-08-03 21:51:52,556 - father_agent.py:386 - Step: 250, Training loss: 6.716022968292236
2025-08-03 21:51:54,319 - father_agent.py:386 - Step: 255, Training loss: 7.160442352294922
2025-08-03 21:51:56,074 - father_agent.py:386 - Step: 260, Training loss: 6.231734275817871
2025-08-03 21:51:57,832 - father_agent.py:386 - Step: 265, Training loss: 5.490668773651123
2025-08-03 21:51:59,669 - father_agent.py:386 - Step: 270, Training loss: 5.544924736022949
2025-08-03 21:52:01,519 - father_agent.py:386 - Step: 275, Training loss: 6.8782057762146
2025-08-03 21:52:03,354 - father_agent.py:386 - Step: 280, Training loss: 6.909847259521484
2025-08-03 21:52:05,163 - father_agent.py:386 - Step: 285, Training loss: 5.544562339782715
2025-08-03 21:52:06,944 - father_agent.py:386 - Step: 290, Training loss: 6.580197811126709
2025-08-03 21:52:08,718 - father_agent.py:386 - Step: 295, Training loss: 6.045860290527344
2025-08-03 21:52:10,496 - father_agent.py:386 - Step: 300, Training loss: 5.153740882873535
2025-08-03 21:52:10,712 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:52:10,714 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:52:19,530 - evaluation_results_class.py:131 - Average Return = 118.49325561523438
2025-08-03 21:52:19,530 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.84615421295166
2025-08-03 21:52:19,530 - evaluation_results_class.py:135 - Average Discounted Reward = 6.168502330780029
2025-08-03 21:52:19,530 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9984139571768438
2025-08-03 21:52:19,530 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:52:19,530 - evaluation_results_class.py:141 - Variance of Return = 17203.755859375
2025-08-03 21:52:19,530 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:52:19,530 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 21:52:19,530 - evaluation_results_class.py:147 - Average Episode Length = 110.53053132434576
2025-08-03 21:52:19,530 - evaluation_results_class.py:149 - Counted Episodes = 2522
2025-08-03 21:52:19,731 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:52:19,743 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:52:23,234 - father_agent.py:386 - Step: 305, Training loss: 5.892239570617676
2025-08-03 21:52:25,015 - father_agent.py:386 - Step: 310, Training loss: 6.266928195953369
2025-08-03 21:52:26,773 - father_agent.py:386 - Step: 315, Training loss: 5.625853061676025
2025-08-03 21:52:28,534 - father_agent.py:386 - Step: 320, Training loss: 6.903199672698975
2025-08-03 21:52:30,305 - father_agent.py:386 - Step: 325, Training loss: 6.478295803070068
2025-08-03 21:52:32,067 - father_agent.py:386 - Step: 330, Training loss: 7.5128021240234375
2025-08-03 21:52:33,812 - father_agent.py:386 - Step: 335, Training loss: 6.329571723937988
2025-08-03 21:52:35,591 - father_agent.py:386 - Step: 340, Training loss: 6.990399360656738
2025-08-03 21:52:37,358 - father_agent.py:386 - Step: 345, Training loss: 5.647259712219238
2025-08-03 21:52:39,134 - father_agent.py:386 - Step: 350, Training loss: 5.902486801147461
2025-08-03 21:52:40,919 - father_agent.py:386 - Step: 355, Training loss: 5.696816444396973
2025-08-03 21:52:42,685 - father_agent.py:386 - Step: 360, Training loss: 4.910031318664551
2025-08-03 21:52:44,448 - father_agent.py:386 - Step: 365, Training loss: 5.725644588470459
2025-08-03 21:52:46,206 - father_agent.py:386 - Step: 370, Training loss: 7.950364112854004
2025-08-03 21:52:47,984 - father_agent.py:386 - Step: 375, Training loss: 7.176943778991699
2025-08-03 21:52:49,766 - father_agent.py:386 - Step: 380, Training loss: 6.901458263397217
2025-08-03 21:52:51,515 - father_agent.py:386 - Step: 385, Training loss: 6.929762363433838
2025-08-03 21:52:53,268 - father_agent.py:386 - Step: 390, Training loss: 6.539932727813721
2025-08-03 21:52:55,022 - father_agent.py:386 - Step: 395, Training loss: 5.873274326324463
2025-08-03 21:52:56,783 - father_agent.py:386 - Step: 400, Training loss: 6.570406436920166
2025-08-03 21:52:56,994 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:52:56,996 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:53:05,912 - evaluation_results_class.py:131 - Average Return = 121.24494934082031
2025-08-03 21:53:05,913 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.119644165039062
2025-08-03 21:53:05,913 - evaluation_results_class.py:135 - Average Discounted Reward = 6.306064128875732
2025-08-03 21:53:05,913 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9975747776879548
2025-08-03 21:53:05,913 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:53:05,913 - evaluation_results_class.py:141 - Variance of Return = 17027.392578125
2025-08-03 21:53:05,913 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:53:05,913 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 21:53:05,913 - evaluation_results_class.py:147 - Average Episode Length = 111.26434923201293
2025-08-03 21:53:05,913 - evaluation_results_class.py:149 - Counted Episodes = 2474
2025-08-03 21:53:06,111 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:53:06,122 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:53:09,613 - father_agent.py:386 - Step: 405, Training loss: 5.918257713317871
2025-08-03 21:53:11,368 - father_agent.py:386 - Step: 410, Training loss: 6.176742076873779
2025-08-03 21:53:13,140 - father_agent.py:386 - Step: 415, Training loss: 6.00801944732666
2025-08-03 21:53:14,906 - father_agent.py:386 - Step: 420, Training loss: 5.708193302154541
2025-08-03 21:53:16,646 - father_agent.py:386 - Step: 425, Training loss: 5.90181827545166
2025-08-03 21:53:18,412 - father_agent.py:386 - Step: 430, Training loss: 7.232291221618652
2025-08-03 21:53:20,162 - father_agent.py:386 - Step: 435, Training loss: 7.510800838470459
2025-08-03 21:53:21,947 - father_agent.py:386 - Step: 440, Training loss: 6.609460830688477
2025-08-03 21:53:23,699 - father_agent.py:386 - Step: 445, Training loss: 6.411482334136963
2025-08-03 21:53:25,458 - father_agent.py:386 - Step: 450, Training loss: 7.70393180847168
2025-08-03 21:53:27,214 - father_agent.py:386 - Step: 455, Training loss: 6.833419322967529
2025-08-03 21:53:28,983 - father_agent.py:386 - Step: 460, Training loss: 5.853153228759766
2025-08-03 21:53:30,775 - father_agent.py:386 - Step: 465, Training loss: 7.471596717834473
2025-08-03 21:53:32,579 - father_agent.py:386 - Step: 470, Training loss: 4.985080718994141
2025-08-03 21:53:34,365 - father_agent.py:386 - Step: 475, Training loss: 5.888963222503662
2025-08-03 21:53:36,168 - father_agent.py:386 - Step: 480, Training loss: 6.08681058883667
2025-08-03 21:53:37,986 - father_agent.py:386 - Step: 485, Training loss: 5.9044928550720215
2025-08-03 21:53:39,748 - father_agent.py:386 - Step: 490, Training loss: 6.214938640594482
2025-08-03 21:53:41,505 - father_agent.py:386 - Step: 495, Training loss: 5.074743270874023
2025-08-03 21:53:43,260 - father_agent.py:386 - Step: 500, Training loss: 4.469131946563721
2025-08-03 21:53:43,466 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:53:43,469 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:53:52,281 - evaluation_results_class.py:131 - Average Return = 114.61869049072266
2025-08-03 21:53:52,281 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.457958221435547
2025-08-03 21:53:52,281 - evaluation_results_class.py:135 - Average Discounted Reward = 6.005174160003662
2025-08-03 21:53:52,281 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9980445834962847
2025-08-03 21:53:52,281 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:53:52,281 - evaluation_results_class.py:141 - Variance of Return = 17635.4921875
2025-08-03 21:53:52,282 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:53:52,282 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 21:53:52,282 - evaluation_results_class.py:147 - Average Episode Length = 106.93273367227219
2025-08-03 21:53:52,282 - evaluation_results_class.py:149 - Counted Episodes = 2557
2025-08-03 21:53:52,484 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:53:52,494 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:53:52,605 - father_agent.py:547 - Training finished.
2025-08-03 21:53:52,752 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:53:52,754 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:53:52,756 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-03 21:54:01,672 - evaluation_results_class.py:131 - Average Return = 109.88272094726562
2025-08-03 21:54:01,672 - evaluation_results_class.py:133 - Average Virtual Goal Value = 11.986708641052246
2025-08-03 21:54:01,672 - evaluation_results_class.py:135 - Average Discounted Reward = 5.868325233459473
2025-08-03 21:54:01,672 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9992181391712275
2025-08-03 21:54:01,672 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:54:01,672 - evaluation_results_class.py:141 - Variance of Return = 14834.935546875
2025-08-03 21:54:01,672 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:54:01,672 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 21:54:01,672 - evaluation_results_class.py:147 - Average Episode Length = 107.15871774824082
2025-08-03 21:54:01,672 - evaluation_results_class.py:149 - Counted Episodes = 2558
2025-08-03 21:54:01,876 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:54:01,879 - self_interpretable_extractor.py:286 - True
2025-08-03 21:54:01,891 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:54:17,353 - evaluation_results_class.py:131 - Average Return = 137.90335083007812
2025-08-03 21:54:17,353 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.781327247619629
2025-08-03 21:54:17,353 - evaluation_results_class.py:135 - Average Discounted Reward = 6.632019519805908
2025-08-03 21:54:17,353 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9954954954954955
2025-08-03 21:54:17,354 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:54:17,354 - evaluation_results_class.py:141 - Variance of Return = 24361.53515625
2025-08-03 21:54:17,354 - evaluation_results_class.py:143 - Current Best Return = 137.90335083007812
2025-08-03 21:54:17,354 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9954954954954955
2025-08-03 21:54:17,354 - evaluation_results_class.py:147 - Average Episode Length = 123.38370188370189
2025-08-03 21:54:17,354 - evaluation_results_class.py:149 - Counted Episodes = 2442
2025-08-03 21:54:17,354 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-03 21:54:17,354 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 11666 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 496, 498, 500, 502, 506, 508, 510, 514, 516, 518, 520, 522, 524, 526, 528, 530, 534, 538, 540, 544, 546, 548, 550, 552, 554, 558, 562, 564, 566, 570, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 614, 616, 618, 620, 622, 626, 632, 636, 638, 640, 642, 644, 646, 648, 650, 651}
Buffer 1
2025-08-03 21:55:23,550 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 23097 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 626, 630, 632, 634, 636, 638, 640, 642, 644, 646, 648, 650, 651}
Buffer 2
2025-08-03 21:56:29,166 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 34588 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 630, 632, 634, 636, 638, 640, 642, 644, 646, 648, 650, 651}
All trajectories collected
2025-08-03 21:57:34,916 - self_interpretable_extractor.py:346 - Data sampled
2025-08-03 21:57:34,916 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 34588 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_3.dot.
Learned FSC of size 2
2025-08-03 21:58:07,678 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-03 21:58:21,777 - evaluation_results_class.py:131 - Average Return = 128.95404052734375
2025-08-03 21:58:21,777 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.882725715637207
2025-08-03 21:58:21,777 - evaluation_results_class.py:135 - Average Discounted Reward = 6.171308517456055
2025-08-03 21:58:21,777 - evaluation_results_class.py:137 - Goal Reach Probability = 0.993660855784469
2025-08-03 21:58:21,777 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:58:21,777 - evaluation_results_class.py:141 - Variance of Return = 21704.29296875
2025-08-03 21:58:21,777 - evaluation_results_class.py:143 - Current Best Return = 128.95404052734375
2025-08-03 21:58:21,777 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.993660855784469
2025-08-03 21:58:21,777 - evaluation_results_class.py:147 - Average Episode Length = 120.5364500792393
2025-08-03 21:58:21,777 - evaluation_results_class.py:149 - Counted Episodes = 2524
FSC Result: {'best_episode_return': 13.882726, 'best_return': 128.95404, 'goal_value': 0.0, 'returns_episodic': [13.882726], 'returns': [128.95404], 'reach_probs': [0.993660855784469], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.993660855784469, 'losses': [], 'best_updated': True, 'each_episode_variance': [21704.293], 'each_episode_virtual_variance': [215.75961], 'combined_variance': [26247.826], 'num_episodes': [2524], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [120.5364500792393], 'counted_episodes': [2524], 'discounted_rewards': [6.1713085], 'new_pomdp_iteration_numbers': []}
[[1, 2, 3], [4], [0]]
2025-08-03 21:58:21,847 - statistic.py:67 - synthesis initiated, design space: 131220
2025-08-03 21:58:21,848 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-03 21:58:21,929 - synthesizer_ar.py:122 - value 528.099 achieved after 2297.47 seconds
2025-08-03 21:58:21,935 - synthesizer_ar.py:122 - value 528.0479 achieved after 2297.47 seconds
2025-08-03 21:58:21,940 - synthesizer_ar.py:122 - value 528.0341 achieved after 2297.48 seconds
2025-08-03 21:58:22,010 - synthesizer_ar.py:122 - value 141.9851 achieved after 2297.55 seconds
2025-08-03 21:58:22,068 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-03 21:58:22,068 - synthesizer.py:193 - P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 21:58:22,070 - synthesizer.py:198 - double-checking specification satisfiability:  : 141.98505654299393
--------------------
Synthesis summary:
optimality objective: R{"rew"}min=? [F (bat = 0)] 

method: AR, synthesis time: 0.22 s
number of holes: 9, family size: 131220, quotient: 2050 states / 11335 actions
explored: 100 %
MDP stats: avg MDP size: 2050, iterations: 31

optimum: 141.985057
--------------------
2025-08-03 21:58:22,070 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 21:58:22,093 - robust_rl_trainer.py:432 - Iteration 5 of pure RL loop
2025-08-03 21:58:22,133 - storm_vec_env.py:70 - Computing row map
2025-08-03 21:58:22,149 - storm_vec_env.py:97 - Computing transitions
2025-08-03 21:58:22,184 - storm_vec_env.py:100 - Computing allowed actions
2025-08-03 21:58:22,184 - storm_vec_env.py:114 - Computing sinks
2025-08-03 21:58:22,184 - storm_vec_env.py:119 - Computing raw rewards
2025-08-03 21:58:22,192 - storm_vec_env.py:143 - Computing labels
2025-08-03 21:58:22,193 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-03 21:58:22,193 - storm_vec_env.py:161 - Computing metalabels
2025-08-03 21:58:22,193 - storm_vec_env.py:175 - Computing observations
2025-08-03 21:58:22,333 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-03 21:58:22,334 - father_agent.py:540 - Before training evaluation.
2025-08-03 21:58:22,488 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:58:22,491 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:58:31,809 - evaluation_results_class.py:131 - Average Return = 115.46900177001953
2025-08-03 21:58:31,809 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.545310020446777
2025-08-03 21:58:31,809 - evaluation_results_class.py:135 - Average Discounted Reward = 6.054412841796875
2025-08-03 21:58:31,809 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9992050874403816
2025-08-03 21:58:31,809 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:58:31,809 - evaluation_results_class.py:141 - Variance of Return = 17239.248046875
2025-08-03 21:58:31,809 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:58:31,809 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 21:58:31,809 - evaluation_results_class.py:147 - Average Episode Length = 107.44992050874404
2025-08-03 21:58:31,809 - evaluation_results_class.py:149 - Counted Episodes = 2516
2025-08-03 21:58:32,023 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:58:32,034 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:58:32,146 - father_agent.py:436 - Training agent on-policy
2025-08-03 21:58:39,880 - father_agent.py:386 - Step: 0, Training loss: 5.077358245849609
2025-08-03 21:58:41,750 - father_agent.py:386 - Step: 5, Training loss: 6.194985866546631
2025-08-03 21:58:43,596 - father_agent.py:386 - Step: 10, Training loss: 5.951415061950684
2025-08-03 21:58:45,444 - father_agent.py:386 - Step: 15, Training loss: 5.147939205169678
2025-08-03 21:58:47,323 - father_agent.py:386 - Step: 20, Training loss: 5.766426086425781
2025-08-03 21:58:49,172 - father_agent.py:386 - Step: 25, Training loss: 5.922513961791992
2025-08-03 21:58:51,010 - father_agent.py:386 - Step: 30, Training loss: 5.404325008392334
2025-08-03 21:58:52,827 - father_agent.py:386 - Step: 35, Training loss: 5.831610679626465
2025-08-03 21:58:54,644 - father_agent.py:386 - Step: 40, Training loss: 5.476568222045898
2025-08-03 21:58:56,464 - father_agent.py:386 - Step: 45, Training loss: 5.891195774078369
2025-08-03 21:58:58,358 - father_agent.py:386 - Step: 50, Training loss: 6.168359756469727
2025-08-03 21:59:00,250 - father_agent.py:386 - Step: 55, Training loss: 6.699195384979248
2025-08-03 21:59:02,145 - father_agent.py:386 - Step: 60, Training loss: 6.0500898361206055
2025-08-03 21:59:04,117 - father_agent.py:386 - Step: 65, Training loss: 5.59852933883667
2025-08-03 21:59:06,092 - father_agent.py:386 - Step: 70, Training loss: 6.034692764282227
2025-08-03 21:59:07,950 - father_agent.py:386 - Step: 75, Training loss: 7.338404655456543
2025-08-03 21:59:09,809 - father_agent.py:386 - Step: 80, Training loss: 7.180629730224609
2025-08-03 21:59:11,715 - father_agent.py:386 - Step: 85, Training loss: 6.315247535705566
2025-08-03 21:59:13,637 - father_agent.py:386 - Step: 90, Training loss: 5.806336879730225
2025-08-03 21:59:15,522 - father_agent.py:386 - Step: 95, Training loss: 5.799797534942627
2025-08-03 21:59:17,431 - father_agent.py:386 - Step: 100, Training loss: 5.7159504890441895
2025-08-03 21:59:17,658 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:59:17,661 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:59:27,283 - evaluation_results_class.py:131 - Average Return = 105.54048919677734
2025-08-03 21:59:27,283 - evaluation_results_class.py:133 - Average Virtual Goal Value = 11.553274154663086
2025-08-03 21:59:27,283 - evaluation_results_class.py:135 - Average Discounted Reward = 5.68569803237915
2025-08-03 21:59:27,283 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9996125532739248
2025-08-03 21:59:27,283 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 21:59:27,283 - evaluation_results_class.py:141 - Variance of Return = 13899.5615234375
2025-08-03 21:59:27,283 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 21:59:27,283 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 21:59:27,283 - evaluation_results_class.py:147 - Average Episode Length = 106.75590856257264
2025-08-03 21:59:27,283 - evaluation_results_class.py:149 - Counted Episodes = 2581
2025-08-03 21:59:27,498 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:59:27,509 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 21:59:31,410 - father_agent.py:386 - Step: 105, Training loss: 5.811309814453125
2025-08-03 21:59:33,393 - father_agent.py:386 - Step: 110, Training loss: 6.108206748962402
2025-08-03 21:59:35,331 - father_agent.py:386 - Step: 115, Training loss: 5.957052230834961
2025-08-03 21:59:37,267 - father_agent.py:386 - Step: 120, Training loss: 6.67857027053833
2025-08-03 21:59:39,297 - father_agent.py:386 - Step: 125, Training loss: 7.119571685791016
2025-08-03 21:59:41,220 - father_agent.py:386 - Step: 130, Training loss: 6.28584098815918
2025-08-03 21:59:43,125 - father_agent.py:386 - Step: 135, Training loss: 5.41318941116333
2025-08-03 21:59:45,039 - father_agent.py:386 - Step: 140, Training loss: 4.562588691711426
2025-08-03 21:59:46,971 - father_agent.py:386 - Step: 145, Training loss: 6.258906841278076
2025-08-03 21:59:48,872 - father_agent.py:386 - Step: 150, Training loss: 5.768538475036621
2025-08-03 21:59:50,766 - father_agent.py:386 - Step: 155, Training loss: 6.082139492034912
2025-08-03 21:59:52,717 - father_agent.py:386 - Step: 160, Training loss: 6.294673442840576
2025-08-03 21:59:54,676 - father_agent.py:386 - Step: 165, Training loss: 6.409134387969971
2025-08-03 21:59:56,703 - father_agent.py:386 - Step: 170, Training loss: 6.217860221862793
2025-08-03 21:59:58,719 - father_agent.py:386 - Step: 175, Training loss: 6.417717456817627
2025-08-03 22:00:00,643 - father_agent.py:386 - Step: 180, Training loss: 5.775248050689697
2025-08-03 22:00:02,538 - father_agent.py:386 - Step: 185, Training loss: 8.286101341247559
2025-08-03 22:00:04,471 - father_agent.py:386 - Step: 190, Training loss: 6.331557750701904
2025-08-03 22:00:06,362 - father_agent.py:386 - Step: 195, Training loss: 6.124590873718262
2025-08-03 22:00:08,274 - father_agent.py:386 - Step: 200, Training loss: 6.457214832305908
2025-08-03 22:00:08,511 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:00:08,513 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:00:17,969 - evaluation_results_class.py:131 - Average Return = 114.1976318359375
2025-08-03 22:00:17,969 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.41264820098877
2025-08-03 22:00:17,969 - evaluation_results_class.py:135 - Average Discounted Reward = 5.959761619567871
2025-08-03 22:00:17,969 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9964426877470356
2025-08-03 22:00:17,969 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:00:17,969 - evaluation_results_class.py:141 - Variance of Return = 16522.220703125
2025-08-03 22:00:17,969 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:00:17,969 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:00:17,969 - evaluation_results_class.py:147 - Average Episode Length = 107.9395256916996
2025-08-03 22:00:17,969 - evaluation_results_class.py:149 - Counted Episodes = 2530
2025-08-03 22:00:18,191 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:00:18,203 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:00:22,033 - father_agent.py:386 - Step: 205, Training loss: 7.055053234100342
2025-08-03 22:00:23,858 - father_agent.py:386 - Step: 210, Training loss: 6.579420566558838
2025-08-03 22:00:25,694 - father_agent.py:386 - Step: 215, Training loss: 6.699692726135254
2025-08-03 22:00:27,533 - father_agent.py:386 - Step: 220, Training loss: 5.739720821380615
2025-08-03 22:00:29,374 - father_agent.py:386 - Step: 225, Training loss: 5.249958038330078
2025-08-03 22:00:31,274 - father_agent.py:386 - Step: 230, Training loss: 8.036601066589355
2025-08-03 22:00:33,121 - father_agent.py:386 - Step: 235, Training loss: 6.128241062164307
2025-08-03 22:00:34,983 - father_agent.py:386 - Step: 240, Training loss: 6.812550067901611
2025-08-03 22:00:36,830 - father_agent.py:386 - Step: 245, Training loss: 5.625795841217041
2025-08-03 22:00:38,700 - father_agent.py:386 - Step: 250, Training loss: 6.139548301696777
2025-08-03 22:00:40,541 - father_agent.py:386 - Step: 255, Training loss: 6.723611831665039
2025-08-03 22:00:42,403 - father_agent.py:386 - Step: 260, Training loss: 6.779973030090332
2025-08-03 22:00:44,267 - father_agent.py:386 - Step: 265, Training loss: 6.163397312164307
2025-08-03 22:00:46,112 - father_agent.py:386 - Step: 270, Training loss: 6.072132110595703
2025-08-03 22:00:47,964 - father_agent.py:386 - Step: 275, Training loss: 5.510005474090576
2025-08-03 22:00:49,816 - father_agent.py:386 - Step: 280, Training loss: 5.820909023284912
2025-08-03 22:00:51,668 - father_agent.py:386 - Step: 285, Training loss: 6.0958027839660645
2025-08-03 22:00:53,553 - father_agent.py:386 - Step: 290, Training loss: 6.49618673324585
2025-08-03 22:00:55,426 - father_agent.py:386 - Step: 295, Training loss: 7.126001834869385
2025-08-03 22:00:57,294 - father_agent.py:386 - Step: 300, Training loss: 6.6654863357543945
2025-08-03 22:00:57,531 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:00:57,534 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:01:07,003 - evaluation_results_class.py:131 - Average Return = 118.65641784667969
2025-08-03 22:01:07,003 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.864022254943848
2025-08-03 22:01:07,003 - evaluation_results_class.py:135 - Average Discounted Reward = 6.1905341148376465
2025-08-03 22:01:07,003 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9991906110886281
2025-08-03 22:01:07,003 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:01:07,004 - evaluation_results_class.py:141 - Variance of Return = 17776.90625
2025-08-03 22:01:07,004 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:01:07,004 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:01:07,004 - evaluation_results_class.py:147 - Average Episode Length = 108.4063132335087
2025-08-03 22:01:07,004 - evaluation_results_class.py:149 - Counted Episodes = 2471
2025-08-03 22:01:07,224 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:01:07,236 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:01:11,013 - father_agent.py:386 - Step: 305, Training loss: 6.160251617431641
2025-08-03 22:01:12,851 - father_agent.py:386 - Step: 310, Training loss: 6.026224136352539
2025-08-03 22:01:14,681 - father_agent.py:386 - Step: 315, Training loss: 6.661170959472656
2025-08-03 22:01:16,518 - father_agent.py:386 - Step: 320, Training loss: 6.038251876831055
2025-08-03 22:01:18,368 - father_agent.py:386 - Step: 325, Training loss: 5.564490795135498
2025-08-03 22:01:20,229 - father_agent.py:386 - Step: 330, Training loss: 6.487903594970703
2025-08-03 22:01:22,074 - father_agent.py:386 - Step: 335, Training loss: 7.216208457946777
2025-08-03 22:01:23,909 - father_agent.py:386 - Step: 340, Training loss: 7.445733547210693
2025-08-03 22:01:25,765 - father_agent.py:386 - Step: 345, Training loss: 7.336677551269531
2025-08-03 22:01:27,610 - father_agent.py:386 - Step: 350, Training loss: 5.71221399307251
2025-08-03 22:01:29,454 - father_agent.py:386 - Step: 355, Training loss: 6.172823905944824
2025-08-03 22:01:31,308 - father_agent.py:386 - Step: 360, Training loss: 5.4462690353393555
2025-08-03 22:01:33,169 - father_agent.py:386 - Step: 365, Training loss: 5.304385662078857
2025-08-03 22:01:35,037 - father_agent.py:386 - Step: 370, Training loss: 5.24939489364624
2025-08-03 22:01:36,908 - father_agent.py:386 - Step: 375, Training loss: 6.03795862197876
2025-08-03 22:01:38,740 - father_agent.py:386 - Step: 380, Training loss: 8.600322723388672
2025-08-03 22:01:40,589 - father_agent.py:386 - Step: 385, Training loss: 5.282834529876709
2025-08-03 22:01:42,450 - father_agent.py:386 - Step: 390, Training loss: 6.734582901000977
2025-08-03 22:01:44,278 - father_agent.py:386 - Step: 395, Training loss: 5.895063400268555
2025-08-03 22:01:46,142 - father_agent.py:386 - Step: 400, Training loss: 7.376665115356445
2025-08-03 22:01:46,376 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:01:46,379 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:01:56,060 - evaluation_results_class.py:131 - Average Return = 120.70318603515625
2025-08-03 22:01:56,060 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.067866325378418
2025-08-03 22:01:56,060 - evaluation_results_class.py:135 - Average Discounted Reward = 6.170750617980957
2025-08-03 22:01:56,060 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9987735077677842
2025-08-03 22:01:56,060 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:01:56,061 - evaluation_results_class.py:141 - Variance of Return = 17999.1796875
2025-08-03 22:01:56,061 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:01:56,061 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:01:56,061 - evaluation_results_class.py:147 - Average Episode Length = 110.79762878168438
2025-08-03 22:01:56,061 - evaluation_results_class.py:149 - Counted Episodes = 2446
2025-08-03 22:01:56,284 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:01:56,296 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:02:00,200 - father_agent.py:386 - Step: 405, Training loss: 5.866645812988281
2025-08-03 22:02:02,084 - father_agent.py:386 - Step: 410, Training loss: 5.985840797424316
2025-08-03 22:02:03,951 - father_agent.py:386 - Step: 415, Training loss: 5.705784320831299
2025-08-03 22:02:05,860 - father_agent.py:386 - Step: 420, Training loss: 6.902276992797852
2025-08-03 22:02:07,770 - father_agent.py:386 - Step: 425, Training loss: 6.416818618774414
2025-08-03 22:02:09,633 - father_agent.py:386 - Step: 430, Training loss: 6.690967082977295
2025-08-03 22:02:11,470 - father_agent.py:386 - Step: 435, Training loss: 5.833977222442627
2025-08-03 22:02:13,311 - father_agent.py:386 - Step: 440, Training loss: 6.094120025634766
2025-08-03 22:02:15,150 - father_agent.py:386 - Step: 445, Training loss: 6.731176853179932
2025-08-03 22:02:16,990 - father_agent.py:386 - Step: 450, Training loss: 4.8787922859191895
2025-08-03 22:02:18,866 - father_agent.py:386 - Step: 455, Training loss: 6.058764934539795
2025-08-03 22:02:20,714 - father_agent.py:386 - Step: 460, Training loss: 5.505537986755371
2025-08-03 22:02:22,558 - father_agent.py:386 - Step: 465, Training loss: 5.954837322235107
2025-08-03 22:02:24,404 - father_agent.py:386 - Step: 470, Training loss: 6.139859199523926
2025-08-03 22:02:26,267 - father_agent.py:386 - Step: 475, Training loss: 6.601536273956299
2025-08-03 22:02:28,122 - father_agent.py:386 - Step: 480, Training loss: 6.549734115600586
2025-08-03 22:02:30,021 - father_agent.py:386 - Step: 485, Training loss: 6.304845809936523
2025-08-03 22:02:31,883 - father_agent.py:386 - Step: 490, Training loss: 6.323292255401611
2025-08-03 22:02:33,704 - father_agent.py:386 - Step: 495, Training loss: 6.5635175704956055
2025-08-03 22:02:35,528 - father_agent.py:386 - Step: 500, Training loss: 5.629829406738281
2025-08-03 22:02:35,760 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:02:35,763 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:02:45,300 - evaluation_results_class.py:131 - Average Return = 119.83773040771484
2025-08-03 22:02:45,300 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.979716300964355
2025-08-03 22:02:45,300 - evaluation_results_class.py:135 - Average Discounted Reward = 6.084364891052246
2025-08-03 22:02:45,301 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9979716024340771
2025-08-03 22:02:45,301 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:02:45,301 - evaluation_results_class.py:141 - Variance of Return = 18013.44140625
2025-08-03 22:02:45,301 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:02:45,301 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:02:45,301 - evaluation_results_class.py:147 - Average Episode Length = 110.76551724137931
2025-08-03 22:02:45,301 - evaluation_results_class.py:149 - Counted Episodes = 2465
2025-08-03 22:02:45,525 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:02:45,536 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:02:45,651 - father_agent.py:547 - Training finished.
2025-08-03 22:02:45,802 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:02:45,805 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:02:45,807 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-03 22:02:55,360 - evaluation_results_class.py:131 - Average Return = 121.9532241821289
2025-08-03 22:02:55,361 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.192039489746094
2025-08-03 22:02:55,361 - evaluation_results_class.py:135 - Average Discounted Reward = 6.176410675048828
2025-08-03 22:02:55,361 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9983586376692655
2025-08-03 22:02:55,361 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:02:55,361 - evaluation_results_class.py:141 - Variance of Return = 18162.126953125
2025-08-03 22:02:55,361 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:02:55,361 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:02:55,361 - evaluation_results_class.py:147 - Average Episode Length = 110.67952400492409
2025-08-03 22:02:55,361 - evaluation_results_class.py:149 - Counted Episodes = 2437
2025-08-03 22:02:55,583 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:02:55,585 - self_interpretable_extractor.py:286 - True
2025-08-03 22:02:55,597 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:03:12,027 - evaluation_results_class.py:131 - Average Return = 141.5841522216797
2025-08-03 22:03:12,027 - evaluation_results_class.py:133 - Average Virtual Goal Value = 15.14108943939209
2025-08-03 22:03:12,027 - evaluation_results_class.py:135 - Average Discounted Reward = 6.568908214569092
2025-08-03 22:03:12,027 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9913366336633663
2025-08-03 22:03:12,027 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:03:12,027 - evaluation_results_class.py:141 - Variance of Return = 25650.294921875
2025-08-03 22:03:12,027 - evaluation_results_class.py:143 - Current Best Return = 141.5841522216797
2025-08-03 22:03:12,027 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9913366336633663
2025-08-03 22:03:12,027 - evaluation_results_class.py:147 - Average Episode Length = 125.7190594059406
2025-08-03 22:03:12,027 - evaluation_results_class.py:149 - Counted Episodes = 2424
2025-08-03 22:03:12,027 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-03 22:03:12,028 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 11597 trajectories
Learned trajectory lengths  {512, 2, 514, 4, 516, 6, 518, 8, 520, 10, 522, 12, 524, 14, 526, 16, 528, 18, 530, 20, 532, 22, 534, 24, 536, 26, 538, 28, 540, 30, 542, 32, 34, 36, 548, 38, 40, 552, 42, 44, 556, 46, 558, 48, 560, 50, 562, 52, 564, 54, 566, 56, 568, 58, 570, 60, 572, 62, 574, 64, 576, 66, 578, 68, 70, 72, 74, 586, 76, 78, 80, 592, 82, 594, 84, 596, 86, 88, 600, 90, 602, 92, 604, 94, 96, 98, 610, 100, 612, 102, 614, 104, 616, 106, 618, 108, 620, 110, 112, 624, 114, 116, 118, 630, 120, 122, 124, 636, 126, 128, 640, 130, 642, 132, 134, 136, 648, 138, 651, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510}
Buffer 1
2025-08-03 22:04:20,995 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 23316 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 630, 632, 634, 636, 638, 640, 642, 644, 646, 648, 650, 651}
Buffer 2
2025-08-03 22:05:30,088 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 34933 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 630, 632, 634, 636, 638, 640, 642, 644, 646, 648, 650, 651}
All trajectories collected
2025-08-03 22:06:39,003 - self_interpretable_extractor.py:346 - Data sampled
2025-08-03 22:06:39,003 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 34933 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_4.dot.
Learned FSC of size 2
2025-08-03 22:07:11,556 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-03 22:07:26,837 - evaluation_results_class.py:131 - Average Return = 133.88113403320312
2025-08-03 22:07:26,838 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.38093376159668
2025-08-03 22:07:26,838 - evaluation_results_class.py:135 - Average Discounted Reward = 6.418830394744873
2025-08-03 22:07:26,838 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9964100518548066
2025-08-03 22:07:26,838 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:07:26,838 - evaluation_results_class.py:141 - Variance of Return = 23579.47265625
2025-08-03 22:07:26,838 - evaluation_results_class.py:143 - Current Best Return = 133.88113403320312
2025-08-03 22:07:26,838 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9964100518548066
2025-08-03 22:07:26,838 - evaluation_results_class.py:147 - Average Episode Length = 121.41244515357
2025-08-03 22:07:26,838 - evaluation_results_class.py:149 - Counted Episodes = 2507
FSC Result: {'best_episode_return': 14.380934, 'best_return': 133.88113, 'goal_value': 0.0, 'returns_episodic': [14.380934], 'returns': [133.88113], 'reach_probs': [0.9964100518548066], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.9964100518548066, 'losses': [], 'best_updated': True, 'each_episode_variance': [23579.473], 'each_episode_virtual_variance': [234.82697], 'combined_variance': [28520.373], 'num_episodes': [2507], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [121.41244515357], 'counted_episodes': [2507], 'discounted_rewards': [6.4188304], 'new_pomdp_iteration_numbers': []}
[[1, 2, 3], [4], [0]]
2025-08-03 22:07:26,908 - statistic.py:67 - synthesis initiated, design space: 131220
2025-08-03 22:07:26,908 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-03 22:07:26,985 - synthesizer_ar.py:122 - value 541.5963 achieved after 2842.52 seconds
2025-08-03 22:07:26,991 - synthesizer_ar.py:122 - value 541.5356 achieved after 2842.53 seconds
2025-08-03 22:07:26,997 - synthesizer_ar.py:122 - value 541.5191 achieved after 2842.53 seconds
2025-08-03 22:07:27,023 - synthesizer_ar.py:122 - value 145.8602 achieved after 2842.56 seconds
2025-08-03 22:07:27,077 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-03 22:07:27,077 - synthesizer.py:193 - P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 22:07:27,079 - synthesizer.py:198 - double-checking specification satisfiability:  : 145.860238155256
--------------------
Synthesis summary:
optimality objective: R{"rew"}min=? [F (bat = 0)] 

method: AR, synthesis time: 0.17 s
number of holes: 9, family size: 131220, quotient: 2050 states / 11335 actions
explored: 100 %
MDP stats: avg MDP size: 2050, iterations: 25

optimum: 145.860238
--------------------
2025-08-03 22:07:27,079 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 22:07:27,103 - robust_rl_trainer.py:432 - Iteration 6 of pure RL loop
2025-08-03 22:07:27,146 - storm_vec_env.py:70 - Computing row map
2025-08-03 22:07:27,161 - storm_vec_env.py:97 - Computing transitions
2025-08-03 22:07:27,196 - storm_vec_env.py:100 - Computing allowed actions
2025-08-03 22:07:27,196 - storm_vec_env.py:114 - Computing sinks
2025-08-03 22:07:27,196 - storm_vec_env.py:119 - Computing raw rewards
2025-08-03 22:07:27,205 - storm_vec_env.py:143 - Computing labels
2025-08-03 22:07:27,205 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-03 22:07:27,205 - storm_vec_env.py:161 - Computing metalabels
2025-08-03 22:07:27,205 - storm_vec_env.py:175 - Computing observations
2025-08-03 22:07:27,372 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-03 22:07:27,372 - father_agent.py:540 - Before training evaluation.
2025-08-03 22:07:27,535 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:07:27,537 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:07:37,613 - evaluation_results_class.py:131 - Average Return = 114.86690521240234
2025-08-03 22:07:37,614 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.483511924743652
2025-08-03 22:07:37,614 - evaluation_results_class.py:135 - Average Discounted Reward = 5.9656171798706055
2025-08-03 22:07:37,614 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9984108065156932
2025-08-03 22:07:37,614 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:07:37,614 - evaluation_results_class.py:141 - Variance of Return = 16879.4921875
2025-08-03 22:07:37,614 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:07:37,614 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:07:37,614 - evaluation_results_class.py:147 - Average Episode Length = 108.36551450139055
2025-08-03 22:07:37,614 - evaluation_results_class.py:149 - Counted Episodes = 2517
2025-08-03 22:07:37,848 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:07:37,860 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:07:37,975 - father_agent.py:436 - Training agent on-policy
2025-08-03 22:07:46,291 - father_agent.py:386 - Step: 0, Training loss: 5.976659774780273
2025-08-03 22:07:48,226 - father_agent.py:386 - Step: 5, Training loss: 6.9290900230407715
2025-08-03 22:07:50,170 - father_agent.py:386 - Step: 10, Training loss: 6.074912071228027
2025-08-03 22:07:52,101 - father_agent.py:386 - Step: 15, Training loss: 6.105380535125732
2025-08-03 22:07:54,040 - father_agent.py:386 - Step: 20, Training loss: 5.3884477615356445
2025-08-03 22:07:55,968 - father_agent.py:386 - Step: 25, Training loss: 5.971008777618408
2025-08-03 22:07:57,930 - father_agent.py:386 - Step: 30, Training loss: 6.174515247344971
2025-08-03 22:07:59,883 - father_agent.py:386 - Step: 35, Training loss: 6.57791805267334
2025-08-03 22:08:01,873 - father_agent.py:386 - Step: 40, Training loss: 6.151031017303467
2025-08-03 22:08:03,839 - father_agent.py:386 - Step: 45, Training loss: 7.4029035568237305
2025-08-03 22:08:05,770 - father_agent.py:386 - Step: 50, Training loss: 6.348268032073975
2025-08-03 22:08:07,671 - father_agent.py:386 - Step: 55, Training loss: 6.105416297912598
2025-08-03 22:08:09,593 - father_agent.py:386 - Step: 60, Training loss: 6.182101249694824
2025-08-03 22:08:11,554 - father_agent.py:386 - Step: 65, Training loss: 5.57761812210083
2025-08-03 22:08:13,507 - father_agent.py:386 - Step: 70, Training loss: 6.455225944519043
2025-08-03 22:08:15,476 - father_agent.py:386 - Step: 75, Training loss: 6.251332759857178
2025-08-03 22:08:17,413 - father_agent.py:386 - Step: 80, Training loss: 7.165425777435303
2025-08-03 22:08:19,364 - father_agent.py:386 - Step: 85, Training loss: 5.712741374969482
2025-08-03 22:08:21,324 - father_agent.py:386 - Step: 90, Training loss: 6.261099815368652
2025-08-03 22:08:23,262 - father_agent.py:386 - Step: 95, Training loss: 5.727981090545654
2025-08-03 22:08:25,213 - father_agent.py:386 - Step: 100, Training loss: 6.102267742156982
2025-08-03 22:08:25,460 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:08:25,462 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:08:35,610 - evaluation_results_class.py:131 - Average Return = 121.70352172851562
2025-08-03 22:08:35,610 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.169532775878906
2025-08-03 22:08:35,610 - evaluation_results_class.py:135 - Average Discounted Reward = 6.11841344833374
2025-08-03 22:08:35,610 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9995904995904996
2025-08-03 22:08:35,610 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:08:35,610 - evaluation_results_class.py:141 - Variance of Return = 18379.41015625
2025-08-03 22:08:35,610 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:08:35,610 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:08:35,611 - evaluation_results_class.py:147 - Average Episode Length = 112.05364455364456
2025-08-03 22:08:35,611 - evaluation_results_class.py:149 - Counted Episodes = 2442
2025-08-03 22:08:35,861 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:08:35,873 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:08:40,273 - father_agent.py:386 - Step: 105, Training loss: 7.105896949768066
2025-08-03 22:08:42,311 - father_agent.py:386 - Step: 110, Training loss: 5.8697733879089355
2025-08-03 22:08:44,335 - father_agent.py:386 - Step: 115, Training loss: 6.631324291229248
2025-08-03 22:08:46,368 - father_agent.py:386 - Step: 120, Training loss: 6.315116882324219
2025-08-03 22:08:48,379 - father_agent.py:386 - Step: 125, Training loss: 7.289782524108887
2025-08-03 22:08:50,364 - father_agent.py:386 - Step: 130, Training loss: 5.7054057121276855
2025-08-03 22:08:52,300 - father_agent.py:386 - Step: 135, Training loss: 6.5667500495910645
2025-08-03 22:08:54,242 - father_agent.py:386 - Step: 140, Training loss: 5.3909525871276855
2025-08-03 22:08:56,150 - father_agent.py:386 - Step: 145, Training loss: 6.210834980010986
2025-08-03 22:08:58,071 - father_agent.py:386 - Step: 150, Training loss: 7.672468185424805
2025-08-03 22:09:00,039 - father_agent.py:386 - Step: 155, Training loss: 5.914921283721924
2025-08-03 22:09:02,047 - father_agent.py:386 - Step: 160, Training loss: 5.645034313201904
2025-08-03 22:09:04,095 - father_agent.py:386 - Step: 165, Training loss: 6.685416221618652
2025-08-03 22:09:06,155 - father_agent.py:386 - Step: 170, Training loss: 6.056153774261475
2025-08-03 22:09:08,217 - father_agent.py:386 - Step: 175, Training loss: 5.860410690307617
2025-08-03 22:09:10,206 - father_agent.py:386 - Step: 180, Training loss: 7.3307085037231445
2025-08-03 22:09:12,212 - father_agent.py:386 - Step: 185, Training loss: 7.4683990478515625
2025-08-03 22:09:14,175 - father_agent.py:386 - Step: 190, Training loss: 6.718390941619873
2025-08-03 22:09:16,139 - father_agent.py:386 - Step: 195, Training loss: 6.312425136566162
2025-08-03 22:09:18,112 - father_agent.py:386 - Step: 200, Training loss: 5.835049629211426
2025-08-03 22:09:18,357 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:09:18,360 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:09:28,540 - evaluation_results_class.py:131 - Average Return = 115.96923065185547
2025-08-03 22:09:28,540 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.59538459777832
2025-08-03 22:09:28,540 - evaluation_results_class.py:135 - Average Discounted Reward = 6.085744380950928
2025-08-03 22:09:28,540 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9992307692307693
2025-08-03 22:09:28,540 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:09:28,540 - evaluation_results_class.py:141 - Variance of Return = 17299.4453125
2025-08-03 22:09:28,541 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:09:28,541 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:09:28,541 - evaluation_results_class.py:147 - Average Episode Length = 105.93923076923078
2025-08-03 22:09:28,541 - evaluation_results_class.py:149 - Counted Episodes = 2600
2025-08-03 22:09:28,784 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:09:28,796 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:09:32,918 - father_agent.py:386 - Step: 205, Training loss: 6.851756572723389
2025-08-03 22:09:34,856 - father_agent.py:386 - Step: 210, Training loss: 7.00126838684082
2025-08-03 22:09:36,774 - father_agent.py:386 - Step: 215, Training loss: 5.30430269241333
2025-08-03 22:09:38,691 - father_agent.py:386 - Step: 220, Training loss: 5.421182155609131
2025-08-03 22:09:40,640 - father_agent.py:386 - Step: 225, Training loss: 7.548310279846191
2025-08-03 22:09:42,565 - father_agent.py:386 - Step: 230, Training loss: 5.062644958496094
2025-08-03 22:09:44,487 - father_agent.py:386 - Step: 235, Training loss: 5.973336696624756
2025-08-03 22:09:46,437 - father_agent.py:386 - Step: 240, Training loss: 5.294588565826416
2025-08-03 22:09:48,365 - father_agent.py:386 - Step: 245, Training loss: 6.039906024932861
2025-08-03 22:09:50,334 - father_agent.py:386 - Step: 250, Training loss: 5.9215826988220215
2025-08-03 22:09:52,299 - father_agent.py:386 - Step: 255, Training loss: 6.951162815093994
2025-08-03 22:09:54,272 - father_agent.py:386 - Step: 260, Training loss: 5.824337959289551
2025-08-03 22:09:56,221 - father_agent.py:386 - Step: 265, Training loss: 6.0020670890808105
2025-08-03 22:09:58,176 - father_agent.py:386 - Step: 270, Training loss: 6.8010334968566895
2025-08-03 22:10:00,148 - father_agent.py:386 - Step: 275, Training loss: 5.071169853210449
2025-08-03 22:10:02,232 - father_agent.py:386 - Step: 280, Training loss: 5.778422832489014
2025-08-03 22:10:04,284 - father_agent.py:386 - Step: 285, Training loss: 5.438854694366455
2025-08-03 22:10:06,365 - father_agent.py:386 - Step: 290, Training loss: 5.961601734161377
2025-08-03 22:10:08,364 - father_agent.py:386 - Step: 295, Training loss: 6.4839348793029785
2025-08-03 22:10:10,355 - father_agent.py:386 - Step: 300, Training loss: 5.985664367675781
2025-08-03 22:10:10,610 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:10:10,613 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:10:20,996 - evaluation_results_class.py:131 - Average Return = 118.41191101074219
2025-08-03 22:10:20,997 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.839536666870117
2025-08-03 22:10:20,997 - evaluation_results_class.py:135 - Average Discounted Reward = 6.036373615264893
2025-08-03 22:10:20,997 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9991728701406121
2025-08-03 22:10:20,997 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:10:20,997 - evaluation_results_class.py:141 - Variance of Return = 18652.234375
2025-08-03 22:10:20,997 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:10:20,997 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:10:20,997 - evaluation_results_class.py:147 - Average Episode Length = 110.09346567411083
2025-08-03 22:10:20,997 - evaluation_results_class.py:149 - Counted Episodes = 2418
2025-08-03 22:10:21,335 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:10:21,352 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:10:25,573 - father_agent.py:386 - Step: 305, Training loss: 5.055324554443359
2025-08-03 22:10:27,610 - father_agent.py:386 - Step: 310, Training loss: 6.43208122253418
2025-08-03 22:10:29,677 - father_agent.py:386 - Step: 315, Training loss: 6.508772373199463
2025-08-03 22:10:31,730 - father_agent.py:386 - Step: 320, Training loss: 5.16575813293457
2025-08-03 22:10:33,822 - father_agent.py:386 - Step: 325, Training loss: 5.589850902557373
2025-08-03 22:10:35,854 - father_agent.py:386 - Step: 330, Training loss: 5.773040294647217
2025-08-03 22:10:37,895 - father_agent.py:386 - Step: 335, Training loss: 5.3649210929870605
2025-08-03 22:10:39,897 - father_agent.py:386 - Step: 340, Training loss: 4.977837562561035
2025-08-03 22:10:41,936 - father_agent.py:386 - Step: 345, Training loss: 5.621018886566162
2025-08-03 22:10:44,008 - father_agent.py:386 - Step: 350, Training loss: 6.930901527404785
2025-08-03 22:10:46,088 - father_agent.py:386 - Step: 355, Training loss: 5.443011283874512
2025-08-03 22:10:48,119 - father_agent.py:386 - Step: 360, Training loss: 6.178966522216797
2025-08-03 22:10:50,132 - father_agent.py:386 - Step: 365, Training loss: 5.4770073890686035
2025-08-03 22:10:52,132 - father_agent.py:386 - Step: 370, Training loss: 5.781129837036133
2025-08-03 22:10:54,127 - father_agent.py:386 - Step: 375, Training loss: 7.1091437339782715
2025-08-03 22:10:56,160 - father_agent.py:386 - Step: 380, Training loss: 6.445742130279541
2025-08-03 22:10:58,174 - father_agent.py:386 - Step: 385, Training loss: 6.228840351104736
2025-08-03 22:11:00,154 - father_agent.py:386 - Step: 390, Training loss: 5.569300174713135
2025-08-03 22:11:02,151 - father_agent.py:386 - Step: 395, Training loss: 5.438747882843018
2025-08-03 22:11:04,176 - father_agent.py:386 - Step: 400, Training loss: 6.414996147155762
2025-08-03 22:11:04,430 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:11:04,432 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:11:14,682 - evaluation_results_class.py:131 - Average Return = 115.53045654296875
2025-08-03 22:11:14,683 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.545784950256348
2025-08-03 22:11:14,683 - evaluation_results_class.py:135 - Average Discounted Reward = 5.899240493774414
2025-08-03 22:11:14,683 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9963695038321904
2025-08-03 22:11:14,683 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:11:14,683 - evaluation_results_class.py:141 - Variance of Return = 17151.462890625
2025-08-03 22:11:14,683 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:11:14,683 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:11:14,683 - evaluation_results_class.py:147 - Average Episode Length = 112.09398951189996
2025-08-03 22:11:14,683 - evaluation_results_class.py:149 - Counted Episodes = 2479
2025-08-03 22:11:14,925 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:11:14,937 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:11:19,072 - father_agent.py:386 - Step: 405, Training loss: 6.34136438369751
2025-08-03 22:11:21,034 - father_agent.py:386 - Step: 410, Training loss: 7.005662441253662
2025-08-03 22:11:23,011 - father_agent.py:386 - Step: 415, Training loss: 6.82813835144043
2025-08-03 22:11:24,977 - father_agent.py:386 - Step: 420, Training loss: 6.445383548736572
2025-08-03 22:11:26,963 - father_agent.py:386 - Step: 425, Training loss: 5.789213180541992
2025-08-03 22:11:28,935 - father_agent.py:386 - Step: 430, Training loss: 5.611799240112305
2025-08-03 22:11:30,959 - father_agent.py:386 - Step: 435, Training loss: 5.972916603088379
2025-08-03 22:11:32,930 - father_agent.py:386 - Step: 440, Training loss: 5.183743000030518
2025-08-03 22:11:34,938 - father_agent.py:386 - Step: 445, Training loss: 5.842177391052246
2025-08-03 22:11:36,915 - father_agent.py:386 - Step: 450, Training loss: 6.767280101776123
2025-08-03 22:11:38,850 - father_agent.py:386 - Step: 455, Training loss: 5.27296781539917
2025-08-03 22:11:40,772 - father_agent.py:386 - Step: 460, Training loss: 6.209467887878418
2025-08-03 22:11:42,686 - father_agent.py:386 - Step: 465, Training loss: 7.150278091430664
2025-08-03 22:11:44,619 - father_agent.py:386 - Step: 470, Training loss: 6.338915824890137
2025-08-03 22:11:46,530 - father_agent.py:386 - Step: 475, Training loss: 6.800600051879883
2025-08-03 22:11:48,453 - father_agent.py:386 - Step: 480, Training loss: 6.718503475189209
2025-08-03 22:11:50,383 - father_agent.py:386 - Step: 485, Training loss: 5.587846279144287
2025-08-03 22:11:52,308 - father_agent.py:386 - Step: 490, Training loss: 5.165009498596191
2025-08-03 22:11:54,234 - father_agent.py:386 - Step: 495, Training loss: 6.114411354064941
2025-08-03 22:11:56,180 - father_agent.py:386 - Step: 500, Training loss: 4.585288047790527
2025-08-03 22:11:56,426 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:11:56,429 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:12:06,501 - evaluation_results_class.py:131 - Average Return = 120.64822387695312
2025-08-03 22:12:06,502 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.05928897857666
2025-08-03 22:12:06,502 - evaluation_results_class.py:135 - Average Discounted Reward = 6.088019847869873
2025-08-03 22:12:06,502 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9972332015810277
2025-08-03 22:12:06,502 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:12:06,502 - evaluation_results_class.py:141 - Variance of Return = 18607.01171875
2025-08-03 22:12:06,502 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:12:06,502 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:12:06,502 - evaluation_results_class.py:147 - Average Episode Length = 109.69841897233202
2025-08-03 22:12:06,502 - evaluation_results_class.py:149 - Counted Episodes = 2530
2025-08-03 22:12:06,741 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:12:06,753 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:12:06,870 - father_agent.py:547 - Training finished.
2025-08-03 22:12:07,025 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:12:07,028 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:12:07,030 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-03 22:12:17,152 - evaluation_results_class.py:131 - Average Return = 114.50251770019531
2025-08-03 22:12:17,152 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.447928428649902
2025-08-03 22:12:17,153 - evaluation_results_class.py:135 - Average Discounted Reward = 5.9181132316589355
2025-08-03 22:12:17,153 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9988385598141696
2025-08-03 22:12:17,153 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:12:17,153 - evaluation_results_class.py:141 - Variance of Return = 16769.46875
2025-08-03 22:12:17,153 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:12:17,153 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:12:17,153 - evaluation_results_class.py:147 - Average Episode Length = 106.65543941153697
2025-08-03 22:12:17,153 - evaluation_results_class.py:149 - Counted Episodes = 2583
2025-08-03 22:12:17,397 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:12:17,399 - self_interpretable_extractor.py:286 - True
2025-08-03 22:12:17,412 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:12:34,627 - evaluation_results_class.py:131 - Average Return = 149.48992919921875
2025-08-03 22:12:34,627 - evaluation_results_class.py:133 - Average Virtual Goal Value = 15.940420150756836
2025-08-03 22:12:34,627 - evaluation_results_class.py:135 - Average Discounted Reward = 6.861371040344238
2025-08-03 22:12:34,627 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9957136733819117
2025-08-03 22:12:34,627 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:12:34,627 - evaluation_results_class.py:141 - Variance of Return = 25647.958984375
2025-08-03 22:12:34,627 - evaluation_results_class.py:143 - Current Best Return = 149.48992919921875
2025-08-03 22:12:34,627 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9957136733819117
2025-08-03 22:12:34,627 - evaluation_results_class.py:147 - Average Episode Length = 130.2666095156451
2025-08-03 22:12:34,627 - evaluation_results_class.py:149 - Counted Episodes = 2333
2025-08-03 22:12:34,628 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-03 22:12:34,628 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 11321 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 626, 628, 630, 632, 636, 640, 650, 651}
Buffer 1
2025-08-03 22:13:47,501 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 22666 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 626, 628, 630, 632, 634, 636, 638, 640, 642, 648, 650, 651}
Buffer 2
2025-08-03 22:14:59,656 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 33927 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 626, 628, 630, 632, 634, 636, 638, 640, 642, 644, 648, 650, 651}
All trajectories collected
2025-08-03 22:16:12,172 - self_interpretable_extractor.py:346 - Data sampled
2025-08-03 22:16:12,172 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 33927 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_5.dot.
Learned FSC of size 2
2025-08-03 22:16:44,431 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-03 22:17:00,763 - evaluation_results_class.py:131 - Average Return = 135.70863342285156
2025-08-03 22:17:00,763 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.564528465270996
2025-08-03 22:17:00,763 - evaluation_results_class.py:135 - Average Discounted Reward = 6.536314487457275
2025-08-03 22:17:00,763 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9968329374505146
2025-08-03 22:17:00,764 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:17:00,764 - evaluation_results_class.py:141 - Variance of Return = 21984.751953125
2025-08-03 22:17:00,764 - evaluation_results_class.py:143 - Current Best Return = 135.70863342285156
2025-08-03 22:17:00,764 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9968329374505146
2025-08-03 22:17:00,764 - evaluation_results_class.py:147 - Average Episode Length = 121.04275534441805
2025-08-03 22:17:00,764 - evaluation_results_class.py:149 - Counted Episodes = 2526
FSC Result: {'best_episode_return': 14.564528, 'best_return': 135.70863, 'goal_value': 0.0, 'returns_episodic': [14.564528], 'returns': [135.70863], 'reach_probs': [0.9968329374505146], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.9968329374505146, 'losses': [], 'best_updated': True, 'each_episode_variance': [21984.752], 'each_episode_virtual_variance': [219.03761], 'combined_variance': [26592.514], 'num_episodes': [2526], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [121.04275534441805], 'counted_episodes': [2526], 'discounted_rewards': [6.5363145], 'new_pomdp_iteration_numbers': []}
[[1, 2, 3], [4], [0]]
2025-08-03 22:17:00,833 - statistic.py:67 - synthesis initiated, design space: 131220
2025-08-03 22:17:00,833 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-03 22:17:00,916 - synthesizer_ar.py:122 - value 1016.9827 achieved after 3416.45 seconds
2025-08-03 22:17:00,922 - synthesizer_ar.py:122 - value 1016.656 achieved after 3416.46 seconds
2025-08-03 22:17:00,928 - synthesizer_ar.py:122 - value 1016.5664 achieved after 3416.47 seconds
2025-08-03 22:17:01,060 - synthesizer_ar.py:122 - value 150.3959 achieved after 3416.6 seconds
2025-08-03 22:17:01,121 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-03 22:17:01,121 - synthesizer.py:193 - P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 22:17:01,123 - synthesizer.py:198 - double-checking specification satisfiability:  : 150.39590192510946
--------------------
Synthesis summary:
optimality objective: R{"rew"}min=? [F (bat = 0)] 

method: AR, synthesis time: 0.29 s
number of holes: 9, family size: 131220, quotient: 2050 states / 11335 actions
explored: 100 %
MDP stats: avg MDP size: 2050, iterations: 46

optimum: 150.395902
--------------------
2025-08-03 22:17:01,123 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 22:17:01,145 - robust_rl_trainer.py:432 - Iteration 7 of pure RL loop
2025-08-03 22:17:01,185 - storm_vec_env.py:70 - Computing row map
2025-08-03 22:17:01,201 - storm_vec_env.py:97 - Computing transitions
2025-08-03 22:17:01,236 - storm_vec_env.py:100 - Computing allowed actions
2025-08-03 22:17:01,236 - storm_vec_env.py:114 - Computing sinks
2025-08-03 22:17:01,236 - storm_vec_env.py:119 - Computing raw rewards
2025-08-03 22:17:01,245 - storm_vec_env.py:143 - Computing labels
2025-08-03 22:17:01,245 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-03 22:17:01,245 - storm_vec_env.py:161 - Computing metalabels
2025-08-03 22:17:01,245 - storm_vec_env.py:175 - Computing observations
2025-08-03 22:17:01,414 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-03 22:17:01,415 - father_agent.py:540 - Before training evaluation.
2025-08-03 22:17:01,574 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:17:01,576 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:17:12,294 - evaluation_results_class.py:131 - Average Return = 115.82112884521484
2025-08-03 22:17:12,294 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.578947067260742
2025-08-03 22:17:12,294 - evaluation_results_class.py:135 - Average Discounted Reward = 6.1367926597595215
2025-08-03 22:17:12,294 - evaluation_results_class.py:137 - Goal Reach Probability = 0.998417095370004
2025-08-03 22:17:12,294 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:17:12,294 - evaluation_results_class.py:141 - Variance of Return = 16381.4296875
2025-08-03 22:17:12,294 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:17:12,295 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:17:12,295 - evaluation_results_class.py:147 - Average Episode Length = 105.77522754254056
2025-08-03 22:17:12,295 - evaluation_results_class.py:149 - Counted Episodes = 2527
2025-08-03 22:17:12,545 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:17:12,558 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:17:12,672 - father_agent.py:436 - Training agent on-policy
2025-08-03 22:17:20,968 - father_agent.py:386 - Step: 0, Training loss: 6.058713912963867
2025-08-03 22:17:22,921 - father_agent.py:386 - Step: 5, Training loss: 6.260735988616943
2025-08-03 22:17:24,848 - father_agent.py:386 - Step: 10, Training loss: 7.711390018463135
2025-08-03 22:17:26,788 - father_agent.py:386 - Step: 15, Training loss: 6.90140438079834
2025-08-03 22:17:28,701 - father_agent.py:386 - Step: 20, Training loss: 6.169919013977051
2025-08-03 22:17:30,676 - father_agent.py:386 - Step: 25, Training loss: 8.528006553649902
2025-08-03 22:17:32,688 - father_agent.py:386 - Step: 30, Training loss: 7.806458473205566
2025-08-03 22:17:34,733 - father_agent.py:386 - Step: 35, Training loss: 7.588463306427002
2025-08-03 22:17:36,825 - father_agent.py:386 - Step: 40, Training loss: 6.132648468017578
2025-08-03 22:17:38,910 - father_agent.py:386 - Step: 45, Training loss: 7.9506425857543945
2025-08-03 22:17:40,928 - father_agent.py:386 - Step: 50, Training loss: 5.966076374053955
2025-08-03 22:17:42,935 - father_agent.py:386 - Step: 55, Training loss: 5.661406993865967
2025-08-03 22:17:44,947 - father_agent.py:386 - Step: 60, Training loss: 5.614034175872803
2025-08-03 22:17:46,998 - father_agent.py:386 - Step: 65, Training loss: 6.4232892990112305
2025-08-03 22:17:49,074 - father_agent.py:386 - Step: 70, Training loss: 6.618957996368408
2025-08-03 22:17:51,097 - father_agent.py:386 - Step: 75, Training loss: 6.98015832901001
2025-08-03 22:17:53,138 - father_agent.py:386 - Step: 80, Training loss: 6.617681503295898
2025-08-03 22:17:55,125 - father_agent.py:386 - Step: 85, Training loss: 5.221291542053223
2025-08-03 22:17:57,145 - father_agent.py:386 - Step: 90, Training loss: 7.429337501525879
2025-08-03 22:17:59,169 - father_agent.py:386 - Step: 95, Training loss: 6.3651041984558105
2025-08-03 22:18:01,103 - father_agent.py:386 - Step: 100, Training loss: 6.396717548370361
2025-08-03 22:18:01,366 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:18:01,368 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:18:12,136 - evaluation_results_class.py:131 - Average Return = 110.53434753417969
2025-08-03 22:18:12,136 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.049417495727539
2025-08-03 22:18:12,136 - evaluation_results_class.py:135 - Average Discounted Reward = 5.819885730743408
2025-08-03 22:18:12,136 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9979911611088791
2025-08-03 22:18:12,136 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:18:12,136 - evaluation_results_class.py:141 - Variance of Return = 15710.4833984375
2025-08-03 22:18:12,136 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:18:12,136 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:18:12,136 - evaluation_results_class.py:147 - Average Episode Length = 109.47569304941743
2025-08-03 22:18:12,136 - evaluation_results_class.py:149 - Counted Episodes = 2489
2025-08-03 22:18:12,395 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:18:12,407 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:18:16,606 - father_agent.py:386 - Step: 105, Training loss: 7.743549823760986
2025-08-03 22:18:18,577 - father_agent.py:386 - Step: 110, Training loss: 5.484925746917725
2025-08-03 22:18:20,530 - father_agent.py:386 - Step: 115, Training loss: 6.5245466232299805
2025-08-03 22:18:22,464 - father_agent.py:386 - Step: 120, Training loss: 5.087007999420166
2025-08-03 22:18:24,387 - father_agent.py:386 - Step: 125, Training loss: 7.071347713470459
2025-08-03 22:18:26,349 - father_agent.py:386 - Step: 130, Training loss: 5.899582386016846
2025-08-03 22:18:28,289 - father_agent.py:386 - Step: 135, Training loss: 6.418567180633545
2025-08-03 22:18:30,232 - father_agent.py:386 - Step: 140, Training loss: 6.049588680267334
2025-08-03 22:18:32,170 - father_agent.py:386 - Step: 145, Training loss: 7.290346622467041
2025-08-03 22:18:34,122 - father_agent.py:386 - Step: 150, Training loss: 6.3098673820495605
2025-08-03 22:18:36,052 - father_agent.py:386 - Step: 155, Training loss: 6.251559734344482
2025-08-03 22:18:37,984 - father_agent.py:386 - Step: 160, Training loss: 5.510388374328613
2025-08-03 22:18:39,942 - father_agent.py:386 - Step: 165, Training loss: 5.859035015106201
2025-08-03 22:18:41,881 - father_agent.py:386 - Step: 170, Training loss: 6.587639331817627
2025-08-03 22:18:43,817 - father_agent.py:386 - Step: 175, Training loss: 6.489724636077881
2025-08-03 22:18:45,755 - father_agent.py:386 - Step: 180, Training loss: 6.588868618011475
2025-08-03 22:18:47,688 - father_agent.py:386 - Step: 185, Training loss: 6.925520420074463
2025-08-03 22:18:49,605 - father_agent.py:386 - Step: 190, Training loss: 6.721151828765869
2025-08-03 22:18:51,533 - father_agent.py:386 - Step: 195, Training loss: 7.429965496063232
2025-08-03 22:18:53,504 - father_agent.py:386 - Step: 200, Training loss: 6.135411739349365
2025-08-03 22:18:53,764 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:18:53,766 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:19:04,575 - evaluation_results_class.py:131 - Average Return = 122.36576080322266
2025-08-03 22:19:04,575 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.234241485595703
2025-08-03 22:19:04,575 - evaluation_results_class.py:135 - Average Discounted Reward = 6.316666126251221
2025-08-03 22:19:04,575 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9988326848249027
2025-08-03 22:19:04,575 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:19:04,575 - evaluation_results_class.py:141 - Variance of Return = 18448.56640625
2025-08-03 22:19:04,576 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:19:04,576 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:19:04,576 - evaluation_results_class.py:147 - Average Episode Length = 108.58171206225681
2025-08-03 22:19:04,576 - evaluation_results_class.py:149 - Counted Episodes = 2570
2025-08-03 22:19:04,848 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:19:04,860 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:19:09,014 - father_agent.py:386 - Step: 205, Training loss: 7.592808723449707
2025-08-03 22:19:10,920 - father_agent.py:386 - Step: 210, Training loss: 7.17650842666626
2025-08-03 22:19:12,822 - father_agent.py:386 - Step: 215, Training loss: 6.9177470207214355
2025-08-03 22:19:14,727 - father_agent.py:386 - Step: 220, Training loss: 8.486549377441406
2025-08-03 22:19:16,677 - father_agent.py:386 - Step: 225, Training loss: 6.014070510864258
2025-08-03 22:19:18,610 - father_agent.py:386 - Step: 230, Training loss: 5.69135856628418
2025-08-03 22:19:20,538 - father_agent.py:386 - Step: 235, Training loss: 5.427296161651611
2025-08-03 22:19:22,451 - father_agent.py:386 - Step: 240, Training loss: 5.4213457107543945
2025-08-03 22:19:24,347 - father_agent.py:386 - Step: 245, Training loss: 6.719364166259766
2025-08-03 22:19:26,264 - father_agent.py:386 - Step: 250, Training loss: 6.482512474060059
2025-08-03 22:19:28,174 - father_agent.py:386 - Step: 255, Training loss: 5.848236560821533
2025-08-03 22:19:30,095 - father_agent.py:386 - Step: 260, Training loss: 5.66523551940918
2025-08-03 22:19:32,002 - father_agent.py:386 - Step: 265, Training loss: 6.4424357414245605
2025-08-03 22:19:33,914 - father_agent.py:386 - Step: 270, Training loss: 6.775234222412109
2025-08-03 22:19:35,823 - father_agent.py:386 - Step: 275, Training loss: 6.01603364944458
2025-08-03 22:19:37,736 - father_agent.py:386 - Step: 280, Training loss: 6.485769748687744
2025-08-03 22:19:39,663 - father_agent.py:386 - Step: 285, Training loss: 6.42323112487793
2025-08-03 22:19:41,585 - father_agent.py:386 - Step: 290, Training loss: 6.709132194519043
2025-08-03 22:19:43,537 - father_agent.py:386 - Step: 295, Training loss: 7.261381149291992
2025-08-03 22:19:45,502 - father_agent.py:386 - Step: 300, Training loss: 7.526224613189697
2025-08-03 22:19:45,764 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:19:45,767 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:19:56,770 - evaluation_results_class.py:131 - Average Return = 111.11813354492188
2025-08-03 22:19:56,770 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.107071876525879
2025-08-03 22:19:56,770 - evaluation_results_class.py:135 - Average Discounted Reward = 5.903007984161377
2025-08-03 22:19:56,771 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9976293954958514
2025-08-03 22:19:56,771 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:19:56,771 - evaluation_results_class.py:141 - Variance of Return = 15118.267578125
2025-08-03 22:19:56,771 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:19:56,771 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:19:56,771 - evaluation_results_class.py:147 - Average Episode Length = 109.73528249703675
2025-08-03 22:19:56,771 - evaluation_results_class.py:149 - Counted Episodes = 2531
2025-08-03 22:19:57,025 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:19:57,037 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:20:01,168 - father_agent.py:386 - Step: 305, Training loss: 7.022862434387207
2025-08-03 22:20:03,076 - father_agent.py:386 - Step: 310, Training loss: 6.139416217803955
2025-08-03 22:20:04,978 - father_agent.py:386 - Step: 315, Training loss: 5.902683734893799
2025-08-03 22:20:06,897 - father_agent.py:386 - Step: 320, Training loss: 5.460123062133789
2025-08-03 22:20:08,815 - father_agent.py:386 - Step: 325, Training loss: 6.22254753112793
2025-08-03 22:20:10,728 - father_agent.py:386 - Step: 330, Training loss: 6.207090377807617
2025-08-03 22:20:12,658 - father_agent.py:386 - Step: 335, Training loss: 5.928768157958984
2025-08-03 22:20:14,595 - father_agent.py:386 - Step: 340, Training loss: 6.680691719055176
2025-08-03 22:20:16,525 - father_agent.py:386 - Step: 345, Training loss: 5.709138870239258
2025-08-03 22:20:18,425 - father_agent.py:386 - Step: 350, Training loss: 6.38005256652832
2025-08-03 22:20:20,342 - father_agent.py:386 - Step: 355, Training loss: 6.340807914733887
2025-08-03 22:20:22,241 - father_agent.py:386 - Step: 360, Training loss: 6.30653715133667
2025-08-03 22:20:24,136 - father_agent.py:386 - Step: 365, Training loss: 7.001504898071289
2025-08-03 22:20:26,036 - father_agent.py:386 - Step: 370, Training loss: 5.557754039764404
2025-08-03 22:20:27,915 - father_agent.py:386 - Step: 375, Training loss: 9.194478988647461
2025-08-03 22:20:29,821 - father_agent.py:386 - Step: 380, Training loss: 5.99130916595459
2025-08-03 22:20:31,729 - father_agent.py:386 - Step: 385, Training loss: 7.207483291625977
2025-08-03 22:20:33,641 - father_agent.py:386 - Step: 390, Training loss: 5.649862289428711
2025-08-03 22:20:35,570 - father_agent.py:386 - Step: 395, Training loss: 6.150219440460205
2025-08-03 22:20:37,462 - father_agent.py:386 - Step: 400, Training loss: 6.836012840270996
2025-08-03 22:20:37,726 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:20:37,729 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:20:48,536 - evaluation_results_class.py:131 - Average Return = 113.90214538574219
2025-08-03 22:20:48,537 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.387787818908691
2025-08-03 22:20:48,537 - evaluation_results_class.py:135 - Average Discounted Reward = 6.007964611053467
2025-08-03 22:20:48,537 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9987868985038415
2025-08-03 22:20:48,537 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:20:48,537 - evaluation_results_class.py:141 - Variance of Return = 15024.03515625
2025-08-03 22:20:48,537 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:20:48,537 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:20:48,537 - evaluation_results_class.py:147 - Average Episode Length = 108.9708855640922
2025-08-03 22:20:48,537 - evaluation_results_class.py:149 - Counted Episodes = 2473
2025-08-03 22:20:48,798 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:20:48,810 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:20:52,964 - father_agent.py:386 - Step: 405, Training loss: 6.2979326248168945
2025-08-03 22:20:54,858 - father_agent.py:386 - Step: 410, Training loss: 6.394927024841309
2025-08-03 22:20:56,758 - father_agent.py:386 - Step: 415, Training loss: 6.032025337219238
2025-08-03 22:20:58,695 - father_agent.py:386 - Step: 420, Training loss: 5.616884708404541
2025-08-03 22:21:00,596 - father_agent.py:386 - Step: 425, Training loss: 7.099503993988037
2025-08-03 22:21:02,508 - father_agent.py:386 - Step: 430, Training loss: 4.767827987670898
2025-08-03 22:21:04,446 - father_agent.py:386 - Step: 435, Training loss: 7.223165512084961
2025-08-03 22:21:06,366 - father_agent.py:386 - Step: 440, Training loss: 7.407082557678223
2025-08-03 22:21:08,286 - father_agent.py:386 - Step: 445, Training loss: 6.641197204589844
2025-08-03 22:21:10,210 - father_agent.py:386 - Step: 450, Training loss: 6.323697566986084
2025-08-03 22:21:12,138 - father_agent.py:386 - Step: 455, Training loss: 5.815857410430908
2025-08-03 22:21:14,052 - father_agent.py:386 - Step: 460, Training loss: 6.138136386871338
2025-08-03 22:21:15,962 - father_agent.py:386 - Step: 465, Training loss: 6.299691200256348
2025-08-03 22:21:17,871 - father_agent.py:386 - Step: 470, Training loss: 5.666573524475098
2025-08-03 22:21:19,801 - father_agent.py:386 - Step: 475, Training loss: 5.924004077911377
2025-08-03 22:21:21,713 - father_agent.py:386 - Step: 480, Training loss: 6.411180019378662
2025-08-03 22:21:23,644 - father_agent.py:386 - Step: 485, Training loss: 6.174498081207275
2025-08-03 22:21:25,586 - father_agent.py:386 - Step: 490, Training loss: 6.333826541900635
2025-08-03 22:21:27,512 - father_agent.py:386 - Step: 495, Training loss: 5.600230693817139
2025-08-03 22:21:29,460 - father_agent.py:386 - Step: 500, Training loss: 5.9023051261901855
2025-08-03 22:21:29,722 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:21:29,725 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:21:40,431 - evaluation_results_class.py:131 - Average Return = 125.71195983886719
2025-08-03 22:21:40,431 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.568755149841309
2025-08-03 22:21:40,431 - evaluation_results_class.py:135 - Average Discounted Reward = 6.496009349822998
2025-08-03 22:21:40,431 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9987794955248169
2025-08-03 22:21:40,431 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:21:40,432 - evaluation_results_class.py:141 - Variance of Return = 17605.779296875
2025-08-03 22:21:40,432 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:21:40,432 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:21:40,432 - evaluation_results_class.py:147 - Average Episode Length = 111.53580146460537
2025-08-03 22:21:40,432 - evaluation_results_class.py:149 - Counted Episodes = 2458
2025-08-03 22:21:40,694 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:21:40,706 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:21:40,825 - father_agent.py:547 - Training finished.
2025-08-03 22:21:40,984 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:21:40,987 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:21:40,989 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-03 22:21:58,793 - evaluation_results_class.py:131 - Average Return = 116.88716125488281
2025-08-03 22:21:58,794 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.684046745300293
2025-08-03 22:21:58,794 - evaluation_results_class.py:135 - Average Discounted Reward = 6.032649040222168
2025-08-03 22:21:58,794 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9976653696498055
2025-08-03 22:21:58,794 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:21:58,794 - evaluation_results_class.py:141 - Variance of Return = 18615.05859375
2025-08-03 22:21:58,794 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:21:58,795 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:21:58,795 - evaluation_results_class.py:147 - Average Episode Length = 105.72140077821011
2025-08-03 22:21:58,795 - evaluation_results_class.py:149 - Counted Episodes = 2570
2025-08-03 22:21:59,117 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:21:59,120 - self_interpretable_extractor.py:286 - True
2025-08-03 22:21:59,131 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:22:16,263 - evaluation_results_class.py:131 - Average Return = 157.00563049316406
2025-08-03 22:22:16,263 - evaluation_results_class.py:133 - Average Virtual Goal Value = 16.685850143432617
2025-08-03 22:22:16,263 - evaluation_results_class.py:135 - Average Discounted Reward = 7.02346658706665
2025-08-03 22:22:16,263 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9926438771094764
2025-08-03 22:22:16,263 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:22:16,263 - evaluation_results_class.py:141 - Variance of Return = 31112.453125
2025-08-03 22:22:16,263 - evaluation_results_class.py:143 - Current Best Return = 157.00563049316406
2025-08-03 22:22:16,263 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9926438771094764
2025-08-03 22:22:16,263 - evaluation_results_class.py:147 - Average Episode Length = 130.5179575941151
2025-08-03 22:22:16,263 - evaluation_results_class.py:149 - Counted Episodes = 2311
2025-08-03 22:22:16,263 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-03 22:22:16,263 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 11095 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 536, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 596, 598, 600, 602, 604, 606, 608, 610, 612, 616, 618, 620, 622, 624, 626, 628, 630, 636, 638, 640, 642, 644, 646, 648, 650, 651}
Buffer 1
2025-08-03 22:23:27,862 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 22363 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 630, 634, 636, 638, 640, 642, 644, 646, 648, 650, 651}
Buffer 2
2025-08-03 22:24:39,812 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 33673 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 630, 632, 634, 636, 638, 640, 642, 644, 646, 648, 650, 651}
All trajectories collected
2025-08-03 22:25:52,365 - self_interpretable_extractor.py:346 - Data sampled
2025-08-03 22:25:52,365 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 33673 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_6.dot.
Learned FSC of size 2
2025-08-03 22:26:22,151 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-03 22:26:38,277 - evaluation_results_class.py:131 - Average Return = 139.1630096435547
2025-08-03 22:26:38,277 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.913909912109375
2025-08-03 22:26:38,277 - evaluation_results_class.py:135 - Average Discounted Reward = 6.63300085067749
2025-08-03 22:26:38,277 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9988043045037863
2025-08-03 22:26:38,277 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:26:38,277 - evaluation_results_class.py:141 - Variance of Return = 23676.779296875
2025-08-03 22:26:38,277 - evaluation_results_class.py:143 - Current Best Return = 139.1630096435547
2025-08-03 22:26:38,277 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9988043045037863
2025-08-03 22:26:38,277 - evaluation_results_class.py:147 - Average Episode Length = 120.74970107612594
2025-08-03 22:26:38,277 - evaluation_results_class.py:149 - Counted Episodes = 2509
FSC Result: {'best_episode_return': 14.91391, 'best_return': 139.16301, 'goal_value': 0.0, 'returns_episodic': [14.91391], 'returns': [139.16301], 'reach_probs': [0.9988043045037863], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.9988043045037863, 'losses': [], 'best_updated': True, 'each_episode_variance': [23676.78], 'each_episode_virtual_variance': [236.46928], 'combined_variance': [28645.574], 'num_episodes': [2509], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [120.74970107612594], 'counted_episodes': [2509], 'discounted_rewards': [6.633001], 'new_pomdp_iteration_numbers': []}
[[1, 2, 3], [4], [0]]
2025-08-03 22:26:38,359 - statistic.py:67 - synthesis initiated, design space: 131220
2025-08-03 22:26:38,360 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-03 22:26:38,441 - synthesizer_ar.py:122 - value 581.9424 achieved after 3993.98 seconds
2025-08-03 22:26:38,447 - synthesizer_ar.py:122 - value 581.9035 achieved after 3993.98 seconds
2025-08-03 22:26:38,497 - synthesizer_ar.py:122 - value 158.0575 achieved after 3994.03 seconds
2025-08-03 22:26:38,553 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-03 22:26:38,554 - synthesizer.py:193 - P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 22:26:38,555 - synthesizer.py:198 - double-checking specification satisfiability:  : 158.05745827376853
--------------------
Synthesis summary:
optimality objective: R{"rew"}min=? [F (bat = 0)] 

method: AR, synthesis time: 0.19 s
number of holes: 9, family size: 131220, quotient: 2050 states / 11335 actions
explored: 100 %
MDP stats: avg MDP size: 2050, iterations: 28

optimum: 158.057458
--------------------
2025-08-03 22:26:38,556 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 22:26:38,580 - robust_rl_trainer.py:432 - Iteration 8 of pure RL loop
2025-08-03 22:26:38,620 - storm_vec_env.py:70 - Computing row map
2025-08-03 22:26:38,636 - storm_vec_env.py:97 - Computing transitions
2025-08-03 22:26:38,671 - storm_vec_env.py:100 - Computing allowed actions
2025-08-03 22:26:38,671 - storm_vec_env.py:114 - Computing sinks
2025-08-03 22:26:38,671 - storm_vec_env.py:119 - Computing raw rewards
2025-08-03 22:26:38,680 - storm_vec_env.py:143 - Computing labels
2025-08-03 22:26:38,680 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-03 22:26:38,680 - storm_vec_env.py:161 - Computing metalabels
2025-08-03 22:26:38,680 - storm_vec_env.py:175 - Computing observations
2025-08-03 22:26:38,849 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-03 22:26:38,849 - father_agent.py:540 - Before training evaluation.
2025-08-03 22:26:39,008 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:26:39,011 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:26:49,566 - evaluation_results_class.py:131 - Average Return = 127.55646514892578
2025-08-03 22:26:49,566 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.753182411193848
2025-08-03 22:26:49,566 - evaluation_results_class.py:135 - Average Discounted Reward = 6.474099159240723
2025-08-03 22:26:49,566 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9987679671457905
2025-08-03 22:26:49,566 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:26:49,566 - evaluation_results_class.py:141 - Variance of Return = 19801.296875
2025-08-03 22:26:49,566 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:26:49,566 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:26:49,566 - evaluation_results_class.py:147 - Average Episode Length = 110.05462012320328
2025-08-03 22:26:49,566 - evaluation_results_class.py:149 - Counted Episodes = 2435
2025-08-03 22:26:49,817 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:26:49,829 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:26:49,945 - father_agent.py:436 - Training agent on-policy
2025-08-03 22:26:58,356 - father_agent.py:386 - Step: 0, Training loss: 5.930599689483643
2025-08-03 22:27:00,342 - father_agent.py:386 - Step: 5, Training loss: 5.954732418060303
2025-08-03 22:27:02,281 - father_agent.py:386 - Step: 10, Training loss: 5.588107109069824
2025-08-03 22:27:04,206 - father_agent.py:386 - Step: 15, Training loss: 5.576629638671875
2025-08-03 22:27:06,121 - father_agent.py:386 - Step: 20, Training loss: 6.475926399230957
2025-08-03 22:27:08,064 - father_agent.py:386 - Step: 25, Training loss: 7.389554023742676
2025-08-03 22:27:09,994 - father_agent.py:386 - Step: 30, Training loss: 7.4729132652282715
2025-08-03 22:27:11,921 - father_agent.py:386 - Step: 35, Training loss: 7.652044773101807
2025-08-03 22:27:13,840 - father_agent.py:386 - Step: 40, Training loss: 4.824005603790283
2025-08-03 22:27:15,775 - father_agent.py:386 - Step: 45, Training loss: 6.137105941772461
2025-08-03 22:27:17,737 - father_agent.py:386 - Step: 50, Training loss: 6.646721363067627
2025-08-03 22:27:19,682 - father_agent.py:386 - Step: 55, Training loss: 6.014156341552734
2025-08-03 22:27:21,624 - father_agent.py:386 - Step: 60, Training loss: 5.708845138549805
2025-08-03 22:27:23,554 - father_agent.py:386 - Step: 65, Training loss: 5.315739631652832
2025-08-03 22:27:25,509 - father_agent.py:386 - Step: 70, Training loss: 7.14202356338501
2025-08-03 22:27:27,458 - father_agent.py:386 - Step: 75, Training loss: 6.41470193862915
2025-08-03 22:27:29,423 - father_agent.py:386 - Step: 80, Training loss: 6.944883346557617
2025-08-03 22:27:31,337 - father_agent.py:386 - Step: 85, Training loss: 7.523043632507324
2025-08-03 22:27:33,269 - father_agent.py:386 - Step: 90, Training loss: 6.7422332763671875
2025-08-03 22:27:35,195 - father_agent.py:386 - Step: 95, Training loss: 6.3000006675720215
2025-08-03 22:27:37,107 - father_agent.py:386 - Step: 100, Training loss: 6.847054481506348
2025-08-03 22:27:37,380 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:27:37,383 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:27:47,911 - evaluation_results_class.py:131 - Average Return = 118.44613647460938
2025-08-03 22:27:47,911 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.843812942504883
2025-08-03 22:27:47,911 - evaluation_results_class.py:135 - Average Discounted Reward = 6.1663970947265625
2025-08-03 22:27:47,911 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9995995194233079
2025-08-03 22:27:47,911 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:27:47,911 - evaluation_results_class.py:141 - Variance of Return = 16685.12890625
2025-08-03 22:27:47,911 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:27:47,911 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:27:47,911 - evaluation_results_class.py:147 - Average Episode Length = 109.7537044453344
2025-08-03 22:27:47,911 - evaluation_results_class.py:149 - Counted Episodes = 2497
2025-08-03 22:27:48,157 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:27:48,168 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:27:52,390 - father_agent.py:386 - Step: 105, Training loss: 5.8883137702941895
2025-08-03 22:27:54,412 - father_agent.py:386 - Step: 110, Training loss: 6.386667728424072
2025-08-03 22:27:56,412 - father_agent.py:386 - Step: 115, Training loss: 5.4128265380859375
2025-08-03 22:27:58,368 - father_agent.py:386 - Step: 120, Training loss: 5.48704195022583
2025-08-03 22:28:00,334 - father_agent.py:386 - Step: 125, Training loss: 4.83997917175293
2025-08-03 22:28:02,214 - father_agent.py:386 - Step: 130, Training loss: 5.845348834991455
2025-08-03 22:28:04,098 - father_agent.py:386 - Step: 135, Training loss: 6.036131381988525
2025-08-03 22:28:06,035 - father_agent.py:386 - Step: 140, Training loss: 5.983754634857178
2025-08-03 22:28:07,972 - father_agent.py:386 - Step: 145, Training loss: 5.54878568649292
2025-08-03 22:28:09,926 - father_agent.py:386 - Step: 150, Training loss: 5.447357177734375
2025-08-03 22:28:11,934 - father_agent.py:386 - Step: 155, Training loss: 6.323862075805664
2025-08-03 22:28:13,888 - father_agent.py:386 - Step: 160, Training loss: 6.144177436828613
2025-08-03 22:28:15,860 - father_agent.py:386 - Step: 165, Training loss: 6.26200008392334
2025-08-03 22:28:17,859 - father_agent.py:386 - Step: 170, Training loss: 7.279195785522461
2025-08-03 22:28:19,867 - father_agent.py:386 - Step: 175, Training loss: 6.649927616119385
2025-08-03 22:28:21,882 - father_agent.py:386 - Step: 180, Training loss: 6.554861545562744
2025-08-03 22:28:23,934 - father_agent.py:386 - Step: 185, Training loss: 5.9053192138671875
2025-08-03 22:28:25,936 - father_agent.py:386 - Step: 190, Training loss: 5.070712566375732
2025-08-03 22:28:27,956 - father_agent.py:386 - Step: 195, Training loss: 6.816132068634033
2025-08-03 22:28:30,024 - father_agent.py:386 - Step: 200, Training loss: 5.826594829559326
2025-08-03 22:28:30,274 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:28:30,277 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:28:40,889 - evaluation_results_class.py:131 - Average Return = 112.10802459716797
2025-08-03 22:28:40,889 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.207801818847656
2025-08-03 22:28:40,889 - evaluation_results_class.py:135 - Average Discounted Reward = 5.903228759765625
2025-08-03 22:28:40,889 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9984996249062266
2025-08-03 22:28:40,889 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:28:40,889 - evaluation_results_class.py:141 - Variance of Return = 15781.677734375
2025-08-03 22:28:40,889 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:28:40,889 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:28:40,889 - evaluation_results_class.py:147 - Average Episode Length = 105.60765191297824
2025-08-03 22:28:40,889 - evaluation_results_class.py:149 - Counted Episodes = 2666
2025-08-03 22:28:41,137 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:28:41,149 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:28:45,261 - father_agent.py:386 - Step: 205, Training loss: 5.759581089019775
2025-08-03 22:28:47,183 - father_agent.py:386 - Step: 210, Training loss: 6.153250694274902
2025-08-03 22:28:49,087 - father_agent.py:386 - Step: 215, Training loss: 5.9987568855285645
2025-08-03 22:28:50,983 - father_agent.py:386 - Step: 220, Training loss: 6.044266700744629
2025-08-03 22:28:52,881 - father_agent.py:386 - Step: 225, Training loss: 6.653824329376221
2025-08-03 22:28:54,804 - father_agent.py:386 - Step: 230, Training loss: 7.658427715301514
2025-08-03 22:28:56,720 - father_agent.py:386 - Step: 235, Training loss: 5.87774658203125
2025-08-03 22:28:58,609 - father_agent.py:386 - Step: 240, Training loss: 6.69758939743042
2025-08-03 22:29:00,498 - father_agent.py:386 - Step: 245, Training loss: 8.365424156188965
2025-08-03 22:29:02,410 - father_agent.py:386 - Step: 250, Training loss: 6.637237548828125
2025-08-03 22:29:04,318 - father_agent.py:386 - Step: 255, Training loss: 5.967598915100098
2025-08-03 22:29:06,216 - father_agent.py:386 - Step: 260, Training loss: 6.078152656555176
2025-08-03 22:29:08,137 - father_agent.py:386 - Step: 265, Training loss: 5.704794883728027
2025-08-03 22:29:10,089 - father_agent.py:386 - Step: 270, Training loss: 7.049102783203125
2025-08-03 22:29:12,022 - father_agent.py:386 - Step: 275, Training loss: 5.942655563354492
2025-08-03 22:29:13,951 - father_agent.py:386 - Step: 280, Training loss: 5.9209747314453125
2025-08-03 22:29:15,855 - father_agent.py:386 - Step: 285, Training loss: 7.124337196350098
2025-08-03 22:29:17,765 - father_agent.py:386 - Step: 290, Training loss: 5.812422752380371
2025-08-03 22:29:19,729 - father_agent.py:386 - Step: 295, Training loss: 6.366081237792969
2025-08-03 22:29:21,653 - father_agent.py:386 - Step: 300, Training loss: 6.405941963195801
2025-08-03 22:29:21,907 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:29:21,909 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:29:32,549 - evaluation_results_class.py:131 - Average Return = 118.8268814086914
2025-08-03 22:29:32,549 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.880244255065918
2025-08-03 22:29:32,549 - evaluation_results_class.py:135 - Average Discounted Reward = 6.097870826721191
2025-08-03 22:29:32,549 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9987780040733197
2025-08-03 22:29:32,549 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:29:32,549 - evaluation_results_class.py:141 - Variance of Return = 18721.556640625
2025-08-03 22:29:32,549 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:29:32,549 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:29:32,549 - evaluation_results_class.py:147 - Average Episode Length = 109.949083503055
2025-08-03 22:29:32,549 - evaluation_results_class.py:149 - Counted Episodes = 2455
2025-08-03 22:29:32,804 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:29:32,816 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:29:36,941 - father_agent.py:386 - Step: 305, Training loss: 5.381060600280762
2025-08-03 22:29:38,873 - father_agent.py:386 - Step: 310, Training loss: 5.716531276702881
2025-08-03 22:29:40,789 - father_agent.py:386 - Step: 315, Training loss: 4.9923176765441895
2025-08-03 22:29:42,694 - father_agent.py:386 - Step: 320, Training loss: 6.836911201477051
2025-08-03 22:29:44,622 - father_agent.py:386 - Step: 325, Training loss: 5.779417037963867
2025-08-03 22:29:46,525 - father_agent.py:386 - Step: 330, Training loss: 7.089065074920654
2025-08-03 22:29:48,424 - father_agent.py:386 - Step: 335, Training loss: 8.049094200134277
2025-08-03 22:29:50,313 - father_agent.py:386 - Step: 340, Training loss: 5.878228664398193
2025-08-03 22:29:52,199 - father_agent.py:386 - Step: 345, Training loss: 5.406968593597412
2025-08-03 22:29:54,114 - father_agent.py:386 - Step: 350, Training loss: 6.076835632324219
2025-08-03 22:29:56,009 - father_agent.py:386 - Step: 355, Training loss: 6.898627758026123
2025-08-03 22:29:57,921 - father_agent.py:386 - Step: 360, Training loss: 6.181546211242676
2025-08-03 22:29:59,821 - father_agent.py:386 - Step: 365, Training loss: 5.778982639312744
2025-08-03 22:30:01,766 - father_agent.py:386 - Step: 370, Training loss: 6.898027420043945
2025-08-03 22:30:03,696 - father_agent.py:386 - Step: 375, Training loss: 7.5262227058410645
2025-08-03 22:30:05,606 - father_agent.py:386 - Step: 380, Training loss: 5.584316253662109
2025-08-03 22:30:07,509 - father_agent.py:386 - Step: 385, Training loss: 6.847114086151123
2025-08-03 22:30:09,445 - father_agent.py:386 - Step: 390, Training loss: 5.786708831787109
2025-08-03 22:30:11,348 - father_agent.py:386 - Step: 395, Training loss: 6.20557165145874
2025-08-03 22:30:13,237 - father_agent.py:386 - Step: 400, Training loss: 4.80797004699707
2025-08-03 22:30:13,488 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:30:13,491 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:30:24,042 - evaluation_results_class.py:131 - Average Return = 122.07897186279297
2025-08-03 22:30:24,042 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.20386791229248
2025-08-03 22:30:24,042 - evaluation_results_class.py:135 - Average Discounted Reward = 6.3425469398498535
2025-08-03 22:30:24,042 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9979854955680902
2025-08-03 22:30:24,042 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:30:24,042 - evaluation_results_class.py:141 - Variance of Return = 17618.23828125
2025-08-03 22:30:24,042 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:30:24,042 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:30:24,042 - evaluation_results_class.py:147 - Average Episode Length = 110.79170024174053
2025-08-03 22:30:24,042 - evaluation_results_class.py:149 - Counted Episodes = 2482
2025-08-03 22:30:24,292 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:30:24,304 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:30:28,393 - father_agent.py:386 - Step: 405, Training loss: 6.1401777267456055
2025-08-03 22:30:30,311 - father_agent.py:386 - Step: 410, Training loss: 5.870676517486572
2025-08-03 22:30:32,221 - father_agent.py:386 - Step: 415, Training loss: 6.316328525543213
2025-08-03 22:30:34,145 - father_agent.py:386 - Step: 420, Training loss: 6.837085723876953
2025-08-03 22:30:36,047 - father_agent.py:386 - Step: 425, Training loss: 6.716129779815674
2025-08-03 22:30:37,961 - father_agent.py:386 - Step: 430, Training loss: 5.174465179443359
2025-08-03 22:30:39,868 - father_agent.py:386 - Step: 435, Training loss: 5.596352577209473
2025-08-03 22:30:41,772 - father_agent.py:386 - Step: 440, Training loss: 6.37427282333374
2025-08-03 22:30:43,684 - father_agent.py:386 - Step: 445, Training loss: 6.123776435852051
2025-08-03 22:30:45,636 - father_agent.py:386 - Step: 450, Training loss: 6.29926061630249
2025-08-03 22:30:47,559 - father_agent.py:386 - Step: 455, Training loss: 6.989103317260742
2025-08-03 22:30:49,502 - father_agent.py:386 - Step: 460, Training loss: 6.915155410766602
2025-08-03 22:30:51,491 - father_agent.py:386 - Step: 465, Training loss: 6.394895076751709
2025-08-03 22:30:53,452 - father_agent.py:386 - Step: 470, Training loss: 6.853109836578369
2025-08-03 22:30:55,415 - father_agent.py:386 - Step: 475, Training loss: 5.753210544586182
2025-08-03 22:30:57,373 - father_agent.py:386 - Step: 480, Training loss: 6.371788024902344
2025-08-03 22:30:59,361 - father_agent.py:386 - Step: 485, Training loss: 6.378171920776367
2025-08-03 22:31:01,254 - father_agent.py:386 - Step: 490, Training loss: 6.27076530456543
2025-08-03 22:31:03,157 - father_agent.py:386 - Step: 495, Training loss: 5.817770004272461
2025-08-03 22:31:05,087 - father_agent.py:386 - Step: 500, Training loss: 6.354750633239746
2025-08-03 22:31:05,340 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:31:05,343 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:31:15,808 - evaluation_results_class.py:131 - Average Return = 122.58755493164062
2025-08-03 22:31:15,809 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.252986907958984
2025-08-03 22:31:15,809 - evaluation_results_class.py:135 - Average Discounted Reward = 6.151284694671631
2025-08-03 22:31:15,809 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9971157807993407
2025-08-03 22:31:15,809 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:31:15,809 - evaluation_results_class.py:141 - Variance of Return = 18281.5625
2025-08-03 22:31:15,809 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:31:15,809 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:31:15,809 - evaluation_results_class.py:147 - Average Episode Length = 113.66831479192419
2025-08-03 22:31:15,809 - evaluation_results_class.py:149 - Counted Episodes = 2427
2025-08-03 22:31:16,061 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:31:16,073 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:31:16,189 - father_agent.py:547 - Training finished.
2025-08-03 22:31:16,345 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:31:16,348 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:31:16,351 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-03 22:31:26,882 - evaluation_results_class.py:131 - Average Return = 124.67874908447266
2025-08-03 22:31:26,882 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.463756561279297
2025-08-03 22:31:26,882 - evaluation_results_class.py:135 - Average Discounted Reward = 6.182711124420166
2025-08-03 22:31:26,882 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9979406919275123
2025-08-03 22:31:26,882 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:31:26,882 - evaluation_results_class.py:141 - Variance of Return = 19775.14453125
2025-08-03 22:31:26,882 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:31:26,882 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:31:26,882 - evaluation_results_class.py:147 - Average Episode Length = 114.32413509060956
2025-08-03 22:31:26,882 - evaluation_results_class.py:149 - Counted Episodes = 2428
2025-08-03 22:31:27,138 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:31:27,140 - self_interpretable_extractor.py:286 - True
2025-08-03 22:31:27,153 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:31:44,276 - evaluation_results_class.py:131 - Average Return = 142.45962524414062
2025-08-03 22:31:44,276 - evaluation_results_class.py:133 - Average Virtual Goal Value = 15.236024856567383
2025-08-03 22:31:44,276 - evaluation_results_class.py:135 - Average Discounted Reward = 6.611499786376953
2025-08-03 22:31:44,276 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9950310559006211
2025-08-03 22:31:44,276 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:31:44,276 - evaluation_results_class.py:141 - Variance of Return = 25791.21875
2025-08-03 22:31:44,276 - evaluation_results_class.py:143 - Current Best Return = 142.45962524414062
2025-08-03 22:31:44,276 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9950310559006211
2025-08-03 22:31:44,277 - evaluation_results_class.py:147 - Average Episode Length = 124.28488612836439
2025-08-03 22:31:44,277 - evaluation_results_class.py:149 - Counted Episodes = 2415
2025-08-03 22:31:44,277 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-03 22:31:44,277 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 11225 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 538, 540, 546, 548, 550, 552, 554, 558, 560, 564, 566, 568, 570, 576, 578, 580, 584, 586, 588, 590, 592, 596, 600, 602, 604, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 630, 632, 634, 638, 642, 646, 648, 650, 651}
Buffer 1
2025-08-03 22:32:56,450 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 22511 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 630, 632, 634, 638, 640, 642, 644, 646, 648, 650, 651}
Buffer 2
2025-08-03 22:34:08,240 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 33784 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 630, 632, 634, 636, 638, 640, 642, 644, 646, 648, 650, 651}
All trajectories collected
2025-08-03 22:35:19,869 - self_interpretable_extractor.py:346 - Data sampled
2025-08-03 22:35:19,869 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 33784 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_7.dot.
Learned FSC of size 2
2025-08-03 22:35:51,664 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-03 22:36:15,033 - evaluation_results_class.py:131 - Average Return = 134.46707153320312
2025-08-03 22:36:15,033 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.440319061279297
2025-08-03 22:36:15,033 - evaluation_results_class.py:135 - Average Discounted Reward = 6.489317417144775
2025-08-03 22:36:15,033 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9968063872255489
2025-08-03 22:36:15,033 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:36:15,034 - evaluation_results_class.py:141 - Variance of Return = 22427.033203125
2025-08-03 22:36:15,034 - evaluation_results_class.py:143 - Current Best Return = 134.46707153320312
2025-08-03 22:36:15,034 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9968063872255489
2025-08-03 22:36:15,034 - evaluation_results_class.py:147 - Average Episode Length = 120.29540918163673
2025-08-03 22:36:15,034 - evaluation_results_class.py:149 - Counted Episodes = 2505
FSC Result: {'best_episode_return': 14.440319, 'best_return': 134.46707, 'goal_value': 0.0, 'returns_episodic': [14.440319], 'returns': [134.46707], 'reach_probs': [0.9968063872255489], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.9968063872255489, 'losses': [], 'best_updated': True, 'each_episode_variance': [22427.033], 'each_episode_virtual_variance': [223.63089], 'combined_variance': [27129.549], 'num_episodes': [2505], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [120.29540918163673], 'counted_episodes': [2505], 'discounted_rewards': [6.4893174], 'new_pomdp_iteration_numbers': []}
[[1, 2, 3], [4], [0]]
2025-08-03 22:36:15,152 - statistic.py:67 - synthesis initiated, design space: 131220
2025-08-03 22:36:15,153 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-03 22:36:15,290 - synthesizer_ar.py:122 - value 1063.0548 achieved after 4570.83 seconds
2025-08-03 22:36:15,298 - synthesizer_ar.py:122 - value 1062.1426 achieved after 4570.84 seconds
2025-08-03 22:36:15,306 - synthesizer_ar.py:122 - value 1061.8879 achieved after 4570.84 seconds
2025-08-03 22:36:15,480 - synthesizer_ar.py:122 - value 1034.2202 achieved after 4571.02 seconds
2025-08-03 22:36:15,488 - synthesizer_ar.py:122 - value 1033.6844 achieved after 4571.03 seconds
2025-08-03 22:36:15,522 - synthesizer_ar.py:122 - value 1031.4036 achieved after 4571.06 seconds
2025-08-03 22:36:15,531 - synthesizer_ar.py:122 - value 1030.8244 achieved after 4571.07 seconds
2025-08-03 22:36:15,756 - synthesizer_ar.py:122 - value 1020.7071 achieved after 4571.29 seconds
2025-08-03 22:36:15,763 - synthesizer_ar.py:122 - value 1020.6097 achieved after 4571.3 seconds
2025-08-03 22:36:15,958 - synthesizer_ar.py:122 - value 998.9563 achieved after 4571.5 seconds
2025-08-03 22:36:15,975 - synthesizer_ar.py:122 - value 995.9717 achieved after 4571.51 seconds
2025-08-03 22:36:15,983 - synthesizer_ar.py:122 - value 995.5538 achieved after 4571.52 seconds
2025-08-03 22:36:16,149 - synthesizer_ar.py:122 - value 726.6142 achieved after 4571.69 seconds
2025-08-03 22:36:16,157 - synthesizer_ar.py:122 - value 726.6055 achieved after 4571.69 seconds
2025-08-03 22:36:16,196 - synthesizer_ar.py:122 - value 725.962 achieved after 4571.73 seconds
2025-08-03 22:36:16,204 - synthesizer_ar.py:122 - value 725.9435 achieved after 4571.74 seconds
2025-08-03 22:36:16,430 - synthesizer_ar.py:122 - value 538.0755 achieved after 4571.97 seconds
2025-08-03 22:36:16,438 - synthesizer_ar.py:122 - value 538.0704 achieved after 4571.98 seconds
2025-08-03 22:36:16,619 - synthesizer_ar.py:122 - value 499.3874 achieved after 4572.16 seconds
2025-08-03 22:36:16,626 - synthesizer_ar.py:122 - value 499.3812 achieved after 4572.16 seconds
2025-08-03 22:36:16,683 - synthesizer_ar.py:122 - value 426.9199 achieved after 4572.22 seconds
2025-08-03 22:36:16,691 - synthesizer_ar.py:122 - value 426.9154 achieved after 4572.23 seconds
2025-08-03 22:36:16,832 - synthesizer_ar.py:122 - value 152.7136 achieved after 4572.37 seconds
2025-08-03 22:36:16,902 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-03 22:36:16,902 - synthesizer.py:193 - P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 22:36:16,904 - synthesizer.py:198 - double-checking specification satisfiability:  : 152.71357266925943
--------------------
Synthesis summary:
optimality objective: R{"rew"}min=? [F (bat = 0)] 

method: AR, synthesis time: 1.75 s
number of holes: 9, family size: 131220, quotient: 2050 states / 11335 actions
explored: 100 %
MDP stats: avg MDP size: 2050, iterations: 205

optimum: 152.713573
--------------------
2025-08-03 22:36:16,905 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 22:36:16,942 - robust_rl_trainer.py:432 - Iteration 9 of pure RL loop
2025-08-03 22:36:16,983 - storm_vec_env.py:70 - Computing row map
2025-08-03 22:36:16,999 - storm_vec_env.py:97 - Computing transitions
2025-08-03 22:36:17,034 - storm_vec_env.py:100 - Computing allowed actions
2025-08-03 22:36:17,035 - storm_vec_env.py:114 - Computing sinks
2025-08-03 22:36:17,035 - storm_vec_env.py:119 - Computing raw rewards
2025-08-03 22:36:17,043 - storm_vec_env.py:143 - Computing labels
2025-08-03 22:36:17,044 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-03 22:36:17,044 - storm_vec_env.py:161 - Computing metalabels
2025-08-03 22:36:17,044 - storm_vec_env.py:175 - Computing observations
2025-08-03 22:36:17,307 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-03 22:36:17,307 - father_agent.py:540 - Before training evaluation.
2025-08-03 22:36:17,487 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:36:17,491 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:36:28,084 - evaluation_results_class.py:131 - Average Return = 117.79859161376953
2025-08-03 22:36:28,084 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.775176048278809
2025-08-03 22:36:28,084 - evaluation_results_class.py:135 - Average Discounted Reward = 6.116440773010254
2025-08-03 22:36:28,084 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9976580796252927
2025-08-03 22:36:28,084 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:36:28,084 - evaluation_results_class.py:141 - Variance of Return = 19455.576171875
2025-08-03 22:36:28,084 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:36:28,084 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:36:28,085 - evaluation_results_class.py:147 - Average Episode Length = 105.21779859484778
2025-08-03 22:36:28,085 - evaluation_results_class.py:149 - Counted Episodes = 2562
2025-08-03 22:36:28,319 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:36:28,330 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:36:28,443 - father_agent.py:436 - Training agent on-policy
2025-08-03 22:36:36,892 - father_agent.py:386 - Step: 0, Training loss: 6.519786834716797
2025-08-03 22:36:38,812 - father_agent.py:386 - Step: 5, Training loss: 6.52091646194458
2025-08-03 22:36:40,732 - father_agent.py:386 - Step: 10, Training loss: 5.627841949462891
2025-08-03 22:36:42,639 - father_agent.py:386 - Step: 15, Training loss: 6.591888904571533
2025-08-03 22:36:44,563 - father_agent.py:386 - Step: 20, Training loss: 6.631446361541748
2025-08-03 22:36:46,523 - father_agent.py:386 - Step: 25, Training loss: 7.3277268409729
2025-08-03 22:36:48,421 - father_agent.py:386 - Step: 30, Training loss: 7.378269195556641
2025-08-03 22:36:50,357 - father_agent.py:386 - Step: 35, Training loss: 6.28031587600708
2025-08-03 22:36:52,296 - father_agent.py:386 - Step: 40, Training loss: 5.22119665145874
2025-08-03 22:36:54,234 - father_agent.py:386 - Step: 45, Training loss: 5.457557201385498
2025-08-03 22:36:56,209 - father_agent.py:386 - Step: 50, Training loss: 5.966043472290039
2025-08-03 22:36:58,181 - father_agent.py:386 - Step: 55, Training loss: 6.840553283691406
2025-08-03 22:37:00,130 - father_agent.py:386 - Step: 60, Training loss: 6.361926555633545
2025-08-03 22:37:02,126 - father_agent.py:386 - Step: 65, Training loss: 7.268738269805908
2025-08-03 22:37:04,102 - father_agent.py:386 - Step: 70, Training loss: 6.095438003540039
2025-08-03 22:37:06,088 - father_agent.py:386 - Step: 75, Training loss: 5.9321088790893555
2025-08-03 22:37:07,993 - father_agent.py:386 - Step: 80, Training loss: 7.287229061126709
2025-08-03 22:37:09,914 - father_agent.py:386 - Step: 85, Training loss: 5.709557056427002
2025-08-03 22:37:11,962 - father_agent.py:386 - Step: 90, Training loss: 7.027261734008789
2025-08-03 22:37:13,956 - father_agent.py:386 - Step: 95, Training loss: 5.761179447174072
2025-08-03 22:37:15,973 - father_agent.py:386 - Step: 100, Training loss: 6.562197208404541
2025-08-03 22:37:16,212 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:37:16,215 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:37:27,016 - evaluation_results_class.py:131 - Average Return = 125.43816375732422
2025-08-03 22:37:27,017 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.539721488952637
2025-08-03 22:37:27,017 - evaluation_results_class.py:135 - Average Discounted Reward = 6.4251627922058105
2025-08-03 22:37:27,017 - evaluation_results_class.py:137 - Goal Reach Probability = 0.997952497952498
2025-08-03 22:37:27,017 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:37:27,017 - evaluation_results_class.py:141 - Variance of Return = 18676.404296875
2025-08-03 22:37:27,017 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:37:27,017 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:37:27,017 - evaluation_results_class.py:147 - Average Episode Length = 110.34111384111384
2025-08-03 22:37:27,017 - evaluation_results_class.py:149 - Counted Episodes = 2442
2025-08-03 22:37:27,339 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:37:27,350 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:37:31,909 - father_agent.py:386 - Step: 105, Training loss: 5.685607433319092
2025-08-03 22:37:33,975 - father_agent.py:386 - Step: 110, Training loss: 7.213993549346924
2025-08-03 22:37:35,951 - father_agent.py:386 - Step: 115, Training loss: 6.597235679626465
2025-08-03 22:37:37,989 - father_agent.py:386 - Step: 120, Training loss: 8.123100280761719
2025-08-03 22:37:39,941 - father_agent.py:386 - Step: 125, Training loss: 6.110349178314209
2025-08-03 22:37:41,874 - father_agent.py:386 - Step: 130, Training loss: 7.025285720825195
2025-08-03 22:37:43,799 - father_agent.py:386 - Step: 135, Training loss: 6.665767669677734
2025-08-03 22:37:45,711 - father_agent.py:386 - Step: 140, Training loss: 5.864353179931641
2025-08-03 22:37:47,654 - father_agent.py:386 - Step: 145, Training loss: 5.635279655456543
2025-08-03 22:37:49,607 - father_agent.py:386 - Step: 150, Training loss: 6.8617777824401855
2025-08-03 22:37:51,533 - father_agent.py:386 - Step: 155, Training loss: 6.800343990325928
2025-08-03 22:37:53,496 - father_agent.py:386 - Step: 160, Training loss: 5.658390045166016
2025-08-03 22:37:55,447 - father_agent.py:386 - Step: 165, Training loss: 6.599184036254883
2025-08-03 22:37:57,387 - father_agent.py:386 - Step: 170, Training loss: 6.017125129699707
2025-08-03 22:37:59,333 - father_agent.py:386 - Step: 175, Training loss: 7.577653408050537
2025-08-03 22:38:01,269 - father_agent.py:386 - Step: 180, Training loss: 7.741282939910889
2025-08-03 22:38:03,194 - father_agent.py:386 - Step: 185, Training loss: 7.158028602600098
2025-08-03 22:38:05,118 - father_agent.py:386 - Step: 190, Training loss: 6.213776588439941
2025-08-03 22:38:07,061 - father_agent.py:386 - Step: 195, Training loss: 6.657613754272461
2025-08-03 22:38:08,995 - father_agent.py:386 - Step: 200, Training loss: 6.431698799133301
2025-08-03 22:38:09,231 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:38:09,233 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:38:19,826 - evaluation_results_class.py:131 - Average Return = 112.89236450195312
2025-08-03 22:38:19,827 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.289237022399902
2025-08-03 22:38:19,827 - evaluation_results_class.py:135 - Average Discounted Reward = 5.907145977020264
2025-08-03 22:38:19,827 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-03 22:38:19,827 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:38:19,827 - evaluation_results_class.py:141 - Variance of Return = 15751.5947265625
2025-08-03 22:38:19,827 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:38:19,827 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:38:19,827 - evaluation_results_class.py:147 - Average Episode Length = 108.24500978473581
2025-08-03 22:38:19,827 - evaluation_results_class.py:149 - Counted Episodes = 2555
2025-08-03 22:38:20,072 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:38:20,083 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:38:24,106 - father_agent.py:386 - Step: 205, Training loss: 4.669464588165283
2025-08-03 22:38:25,996 - father_agent.py:386 - Step: 210, Training loss: 6.212978839874268
2025-08-03 22:38:27,914 - father_agent.py:386 - Step: 215, Training loss: 5.854001998901367
2025-08-03 22:38:29,847 - father_agent.py:386 - Step: 220, Training loss: 6.189434051513672
2025-08-03 22:38:31,806 - father_agent.py:386 - Step: 225, Training loss: 5.696705341339111
2025-08-03 22:38:33,728 - father_agent.py:386 - Step: 230, Training loss: 7.8366875648498535
2025-08-03 22:38:35,672 - father_agent.py:386 - Step: 235, Training loss: 7.652747631072998
2025-08-03 22:38:37,616 - father_agent.py:386 - Step: 240, Training loss: 5.875597953796387
2025-08-03 22:38:39,542 - father_agent.py:386 - Step: 245, Training loss: 6.988555908203125
2025-08-03 22:38:41,486 - father_agent.py:386 - Step: 250, Training loss: 6.842817783355713
2025-08-03 22:38:43,443 - father_agent.py:386 - Step: 255, Training loss: 5.517460346221924
2025-08-03 22:38:45,359 - father_agent.py:386 - Step: 260, Training loss: 6.930553436279297
2025-08-03 22:38:47,270 - father_agent.py:386 - Step: 265, Training loss: 6.357874870300293
2025-08-03 22:38:49,200 - father_agent.py:386 - Step: 270, Training loss: 5.462675094604492
2025-08-03 22:38:51,118 - father_agent.py:386 - Step: 275, Training loss: 6.890106678009033
2025-08-03 22:38:53,030 - father_agent.py:386 - Step: 280, Training loss: 7.865045547485352
2025-08-03 22:38:54,938 - father_agent.py:386 - Step: 285, Training loss: 7.754081726074219
2025-08-03 22:38:56,836 - father_agent.py:386 - Step: 290, Training loss: 6.747724533081055
2025-08-03 22:38:58,729 - father_agent.py:386 - Step: 295, Training loss: 7.617092132568359
2025-08-03 22:39:00,692 - father_agent.py:386 - Step: 300, Training loss: 6.106090068817139
2025-08-03 22:39:00,939 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:39:00,941 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:39:11,507 - evaluation_results_class.py:131 - Average Return = 109.19731140136719
2025-08-03 22:39:11,508 - evaluation_results_class.py:133 - Average Virtual Goal Value = 11.9165678024292
2025-08-03 22:39:11,508 - evaluation_results_class.py:135 - Average Discounted Reward = 5.857134819030762
2025-08-03 22:39:11,508 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9984183471727955
2025-08-03 22:39:11,508 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:39:11,508 - evaluation_results_class.py:141 - Variance of Return = 14469.3037109375
2025-08-03 22:39:11,508 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:39:11,508 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:39:11,508 - evaluation_results_class.py:147 - Average Episode Length = 106.28786081455121
2025-08-03 22:39:11,508 - evaluation_results_class.py:149 - Counted Episodes = 2529
2025-08-03 22:39:11,752 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:39:11,763 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:39:16,172 - father_agent.py:386 - Step: 305, Training loss: 7.309399604797363
2025-08-03 22:39:18,094 - father_agent.py:386 - Step: 310, Training loss: 5.250945568084717
2025-08-03 22:39:20,001 - father_agent.py:386 - Step: 315, Training loss: 5.956218242645264
2025-08-03 22:39:21,895 - father_agent.py:386 - Step: 320, Training loss: 6.9331183433532715
2025-08-03 22:39:23,796 - father_agent.py:386 - Step: 325, Training loss: 7.046900272369385
2025-08-03 22:39:25,737 - father_agent.py:386 - Step: 330, Training loss: 6.561826229095459
2025-08-03 22:39:27,658 - father_agent.py:386 - Step: 335, Training loss: 7.313136577606201
2025-08-03 22:39:29,595 - father_agent.py:386 - Step: 340, Training loss: 5.725825309753418
2025-08-03 22:39:31,554 - father_agent.py:386 - Step: 345, Training loss: 5.632630348205566
2025-08-03 22:39:33,606 - father_agent.py:386 - Step: 350, Training loss: 5.8628740310668945
2025-08-03 22:39:35,541 - father_agent.py:386 - Step: 355, Training loss: 7.148489475250244
2025-08-03 22:39:37,553 - father_agent.py:386 - Step: 360, Training loss: 7.030120372772217
2025-08-03 22:39:39,643 - father_agent.py:386 - Step: 365, Training loss: 6.40359354019165
2025-08-03 22:39:41,722 - father_agent.py:386 - Step: 370, Training loss: 5.333794593811035
2025-08-03 22:39:43,817 - father_agent.py:386 - Step: 375, Training loss: 6.539862155914307
2025-08-03 22:39:45,845 - father_agent.py:386 - Step: 380, Training loss: 7.762920379638672
2025-08-03 22:39:47,883 - father_agent.py:386 - Step: 385, Training loss: 7.805646896362305
2025-08-03 22:39:49,931 - father_agent.py:386 - Step: 390, Training loss: 7.33590841293335
2025-08-03 22:39:51,960 - father_agent.py:386 - Step: 395, Training loss: 6.594534397125244
2025-08-03 22:39:54,013 - father_agent.py:386 - Step: 400, Training loss: 7.227494239807129
2025-08-03 22:39:54,291 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:39:54,294 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:40:05,476 - evaluation_results_class.py:131 - Average Return = 121.20063018798828
2025-08-03 22:40:05,481 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.118483543395996
2025-08-03 22:40:05,481 - evaluation_results_class.py:135 - Average Discounted Reward = 6.346732139587402
2025-08-03 22:40:05,481 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9992101105845181
2025-08-03 22:40:05,481 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:40:05,481 - evaluation_results_class.py:141 - Variance of Return = 17108.1953125
2025-08-03 22:40:05,481 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:40:05,481 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:40:05,482 - evaluation_results_class.py:147 - Average Episode Length = 109.07898894154819
2025-08-03 22:40:05,482 - evaluation_results_class.py:149 - Counted Episodes = 2532
2025-08-03 22:40:05,847 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:40:05,863 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:40:10,135 - father_agent.py:386 - Step: 405, Training loss: 6.834801197052002
2025-08-03 22:40:12,108 - father_agent.py:386 - Step: 410, Training loss: 7.433077335357666
2025-08-03 22:40:14,074 - father_agent.py:386 - Step: 415, Training loss: 6.813177108764648
2025-08-03 22:40:16,052 - father_agent.py:386 - Step: 420, Training loss: 6.381399631500244
2025-08-03 22:40:18,069 - father_agent.py:386 - Step: 425, Training loss: 6.7910542488098145
2025-08-03 22:40:20,081 - father_agent.py:386 - Step: 430, Training loss: 6.353574275970459
2025-08-03 22:40:22,039 - father_agent.py:386 - Step: 435, Training loss: 5.762293338775635
2025-08-03 22:40:24,013 - father_agent.py:386 - Step: 440, Training loss: 6.547077655792236
2025-08-03 22:40:26,017 - father_agent.py:386 - Step: 445, Training loss: 5.372221946716309
2025-08-03 22:40:27,956 - father_agent.py:386 - Step: 450, Training loss: 5.662865161895752
2025-08-03 22:40:29,900 - father_agent.py:386 - Step: 455, Training loss: 5.816431999206543
2025-08-03 22:40:31,866 - father_agent.py:386 - Step: 460, Training loss: 5.676017761230469
2025-08-03 22:40:33,810 - father_agent.py:386 - Step: 465, Training loss: 6.148895263671875
2025-08-03 22:40:35,752 - father_agent.py:386 - Step: 470, Training loss: 5.563199520111084
2025-08-03 22:40:37,688 - father_agent.py:386 - Step: 475, Training loss: 6.112864017486572
2025-08-03 22:40:39,624 - father_agent.py:386 - Step: 480, Training loss: 6.226208209991455
2025-08-03 22:40:41,579 - father_agent.py:386 - Step: 485, Training loss: 6.129925727844238
2025-08-03 22:40:43,507 - father_agent.py:386 - Step: 490, Training loss: 6.696139335632324
2025-08-03 22:40:45,445 - father_agent.py:386 - Step: 495, Training loss: 6.265610694885254
2025-08-03 22:40:47,378 - father_agent.py:386 - Step: 500, Training loss: 5.735601902008057
2025-08-03 22:40:47,632 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:40:47,635 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:40:58,273 - evaluation_results_class.py:131 - Average Return = 116.81942749023438
2025-08-03 22:40:58,274 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.679593086242676
2025-08-03 22:40:58,274 - evaluation_results_class.py:135 - Average Discounted Reward = 6.138503074645996
2025-08-03 22:40:58,274 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9988249118683902
2025-08-03 22:40:58,274 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:40:58,274 - evaluation_results_class.py:141 - Variance of Return = 17449.498046875
2025-08-03 22:40:58,274 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:40:58,274 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:40:58,274 - evaluation_results_class.py:147 - Average Episode Length = 107.82804543674109
2025-08-03 22:40:58,274 - evaluation_results_class.py:149 - Counted Episodes = 2553
2025-08-03 22:40:58,532 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:40:58,544 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:40:58,662 - father_agent.py:547 - Training finished.
2025-08-03 22:40:58,818 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:40:58,820 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:40:58,823 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-03 22:41:09,543 - evaluation_results_class.py:131 - Average Return = 123.8087387084961
2025-08-03 22:41:09,544 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.374279022216797
2025-08-03 22:41:09,544 - evaluation_results_class.py:135 - Average Discounted Reward = 6.172017574310303
2025-08-03 22:41:09,544 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9967023907666942
2025-08-03 22:41:09,544 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:41:09,544 - evaluation_results_class.py:141 - Variance of Return = 20185.98828125
2025-08-03 22:41:09,544 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:41:09,544 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:41:09,544 - evaluation_results_class.py:147 - Average Episode Length = 114.23495465787305
2025-08-03 22:41:09,544 - evaluation_results_class.py:149 - Counted Episodes = 2426
2025-08-03 22:41:09,798 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:41:09,800 - self_interpretable_extractor.py:286 - True
2025-08-03 22:41:09,813 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:41:27,055 - evaluation_results_class.py:131 - Average Return = 143.79452514648438
2025-08-03 22:41:27,056 - evaluation_results_class.py:133 - Average Virtual Goal Value = 15.370339393615723
2025-08-03 22:41:27,056 - evaluation_results_class.py:135 - Average Discounted Reward = 6.555772304534912
2025-08-03 22:41:27,056 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9954432477216238
2025-08-03 22:41:27,056 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:41:27,056 - evaluation_results_class.py:141 - Variance of Return = 25895.458984375
2025-08-03 22:41:27,056 - evaluation_results_class.py:143 - Current Best Return = 143.79452514648438
2025-08-03 22:41:27,056 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9954432477216238
2025-08-03 22:41:27,056 - evaluation_results_class.py:147 - Average Episode Length = 126.68392709196354
2025-08-03 22:41:27,056 - evaluation_results_class.py:149 - Counted Episodes = 2414
2025-08-03 22:41:27,056 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-03 22:41:27,056 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 11217 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 530, 534, 536, 538, 540, 542, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 586, 588, 592, 594, 596, 598, 600, 602, 604, 608, 610, 612, 614, 616, 618, 620, 622, 624, 632, 634, 636, 638, 642, 644, 650, 651}
Buffer 1
2025-08-03 22:42:39,602 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 22607 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 630, 632, 634, 636, 638, 640, 642, 644, 646, 650, 651}
Buffer 2
2025-08-03 22:43:52,399 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 34026 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 628, 630, 632, 634, 636, 638, 640, 642, 644, 646, 650, 651}
All trajectories collected
2025-08-03 22:45:04,620 - self_interpretable_extractor.py:346 - Data sampled
2025-08-03 22:45:04,620 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 34026 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_8.dot.
Learned FSC of size 2
2025-08-03 22:45:34,019 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-03 22:45:50,430 - evaluation_results_class.py:131 - Average Return = 130.78233337402344
2025-08-03 22:45:50,431 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.066883087158203
2025-08-03 22:45:50,431 - evaluation_results_class.py:135 - Average Discounted Reward = 6.216588973999023
2025-08-03 22:45:50,431 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9943250912038913
2025-08-03 22:45:50,431 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:45:50,431 - evaluation_results_class.py:141 - Variance of Return = 22892.740234375
2025-08-03 22:45:50,431 - evaluation_results_class.py:143 - Current Best Return = 130.78233337402344
2025-08-03 22:45:50,431 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9943250912038913
2025-08-03 22:45:50,431 - evaluation_results_class.py:147 - Average Episode Length = 121.98621807863802
2025-08-03 22:45:50,431 - evaluation_results_class.py:149 - Counted Episodes = 2467
FSC Result: {'best_episode_return': 14.066883, 'best_return': 130.78233, 'goal_value': 0.0, 'returns_episodic': [14.066883], 'returns': [130.78233], 'reach_probs': [0.9943250912038913], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.9943250912038913, 'losses': [], 'best_updated': True, 'each_episode_variance': [22892.74], 'each_episode_virtual_variance': [227.4568], 'combined_variance': [27683.814], 'num_episodes': [2467], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [121.98621807863802], 'counted_episodes': [2467], 'discounted_rewards': [6.216589], 'new_pomdp_iteration_numbers': []}
[[1, 2, 3], [4], [0]]
2025-08-03 22:45:50,521 - statistic.py:67 - synthesis initiated, design space: 131220
2025-08-03 22:45:50,521 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-03 22:45:50,622 - synthesizer_ar.py:122 - value 1047.8895 achieved after 5146.16 seconds
2025-08-03 22:45:50,762 - synthesizer_ar.py:122 - value 1038.2984 achieved after 5146.3 seconds
2025-08-03 22:45:50,768 - synthesizer_ar.py:122 - value 1037.7017 achieved after 5146.31 seconds
2025-08-03 22:45:50,787 - synthesizer_ar.py:122 - value 1035.4535 achieved after 5146.32 seconds
2025-08-03 22:45:50,799 - synthesizer_ar.py:122 - value 1034.7868 achieved after 5146.34 seconds
2025-08-03 22:45:51,012 - synthesizer_ar.py:122 - value 1004.963 achieved after 5146.55 seconds
2025-08-03 22:45:51,019 - synthesizer_ar.py:122 - value 1004.8749 achieved after 5146.56 seconds
2025-08-03 22:45:51,174 - synthesizer_ar.py:122 - value 705.4326 achieved after 5146.71 seconds
2025-08-03 22:45:51,180 - synthesizer_ar.py:122 - value 705.4253 achieved after 5146.72 seconds
2025-08-03 22:45:51,212 - synthesizer_ar.py:122 - value 705.2498 achieved after 5146.75 seconds
2025-08-03 22:45:51,341 - synthesizer_ar.py:122 - value 519.7552 achieved after 5146.88 seconds
2025-08-03 22:45:51,347 - synthesizer_ar.py:122 - value 519.751 achieved after 5146.88 seconds
2025-08-03 22:45:51,541 - synthesizer_ar.py:122 - value 480.7912 achieved after 5147.08 seconds
2025-08-03 22:45:51,581 - synthesizer_ar.py:122 - value 410.9234 achieved after 5147.12 seconds
2025-08-03 22:45:51,586 - synthesizer_ar.py:122 - value 410.9101 achieved after 5147.12 seconds
2025-08-03 22:45:51,698 - synthesizer_ar.py:122 - value 146.4377 achieved after 5147.24 seconds
2025-08-03 22:45:51,747 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-03 22:45:51,747 - synthesizer.py:193 - P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 22:45:51,749 - synthesizer.py:198 - double-checking specification satisfiability:  : 146.43771082556665
--------------------
Synthesis summary:
optimality objective: R{"rew"}min=? [F (bat = 0)] 

method: AR, synthesis time: 1.23 s
number of holes: 9, family size: 131220, quotient: 2050 states / 11335 actions
explored: 100 %
MDP stats: avg MDP size: 2050, iterations: 179

optimum: 146.437711
--------------------
2025-08-03 22:45:51,749 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 22:45:51,774 - robust_rl_trainer.py:432 - Iteration 10 of pure RL loop
2025-08-03 22:45:51,815 - storm_vec_env.py:70 - Computing row map
2025-08-03 22:45:51,831 - storm_vec_env.py:97 - Computing transitions
2025-08-03 22:45:51,866 - storm_vec_env.py:100 - Computing allowed actions
2025-08-03 22:45:51,866 - storm_vec_env.py:114 - Computing sinks
2025-08-03 22:45:51,866 - storm_vec_env.py:119 - Computing raw rewards
2025-08-03 22:45:51,875 - storm_vec_env.py:143 - Computing labels
2025-08-03 22:45:51,875 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-03 22:45:51,875 - storm_vec_env.py:161 - Computing metalabels
2025-08-03 22:45:51,875 - storm_vec_env.py:175 - Computing observations
2025-08-03 22:45:52,054 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-03 22:45:52,054 - father_agent.py:540 - Before training evaluation.
2025-08-03 22:45:52,222 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:45:52,224 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:46:02,918 - evaluation_results_class.py:131 - Average Return = 115.36418151855469
2025-08-03 22:46:02,919 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.533199310302734
2025-08-03 22:46:02,919 - evaluation_results_class.py:135 - Average Discounted Reward = 5.916093349456787
2025-08-03 22:46:02,919 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9983903420523139
2025-08-03 22:46:02,919 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:46:02,919 - evaluation_results_class.py:141 - Variance of Return = 16448.52734375
2025-08-03 22:46:02,919 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:46:02,919 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:46:02,919 - evaluation_results_class.py:147 - Average Episode Length = 109.72474849094567
2025-08-03 22:46:02,919 - evaluation_results_class.py:149 - Counted Episodes = 2485
2025-08-03 22:46:03,191 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:46:03,205 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:46:03,326 - father_agent.py:436 - Training agent on-policy
2025-08-03 22:46:12,074 - father_agent.py:386 - Step: 0, Training loss: 6.488650798797607
2025-08-03 22:46:14,012 - father_agent.py:386 - Step: 5, Training loss: 6.618887901306152
2025-08-03 22:46:15,974 - father_agent.py:386 - Step: 10, Training loss: 5.111416339874268
2025-08-03 22:46:17,946 - father_agent.py:386 - Step: 15, Training loss: 6.595612049102783
2025-08-03 22:46:19,917 - father_agent.py:386 - Step: 20, Training loss: 6.501930236816406
2025-08-03 22:46:21,898 - father_agent.py:386 - Step: 25, Training loss: 6.080930233001709
2025-08-03 22:46:23,875 - father_agent.py:386 - Step: 30, Training loss: 5.317494869232178
2025-08-03 22:46:25,886 - father_agent.py:386 - Step: 35, Training loss: 7.438205242156982
2025-08-03 22:46:27,849 - father_agent.py:386 - Step: 40, Training loss: 5.69981050491333
2025-08-03 22:46:29,820 - father_agent.py:386 - Step: 45, Training loss: 6.221301078796387
2025-08-03 22:46:31,881 - father_agent.py:386 - Step: 50, Training loss: 5.789542198181152
2025-08-03 22:46:33,901 - father_agent.py:386 - Step: 55, Training loss: 6.657585144042969
2025-08-03 22:46:35,935 - father_agent.py:386 - Step: 60, Training loss: 6.609440803527832
2025-08-03 22:46:37,974 - father_agent.py:386 - Step: 65, Training loss: 7.237948894500732
2025-08-03 22:46:40,085 - father_agent.py:386 - Step: 70, Training loss: 6.88327693939209
2025-08-03 22:46:42,203 - father_agent.py:386 - Step: 75, Training loss: 5.553605079650879
2025-08-03 22:46:44,286 - father_agent.py:386 - Step: 80, Training loss: 4.933440208435059
2025-08-03 22:46:46,369 - father_agent.py:386 - Step: 85, Training loss: 5.978973865509033
2025-08-03 22:46:48,421 - father_agent.py:386 - Step: 90, Training loss: 6.792059421539307
2025-08-03 22:46:50,425 - father_agent.py:386 - Step: 95, Training loss: 5.8737874031066895
2025-08-03 22:46:52,409 - father_agent.py:386 - Step: 100, Training loss: 6.504685401916504
2025-08-03 22:46:52,680 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:46:52,682 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:47:03,660 - evaluation_results_class.py:131 - Average Return = 103.15058135986328
2025-08-03 22:47:03,660 - evaluation_results_class.py:133 - Average Virtual Goal Value = 11.314285278320312
2025-08-03 22:47:03,661 - evaluation_results_class.py:135 - Average Discounted Reward = 5.646613597869873
2025-08-03 22:47:03,661 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9996138996138996
2025-08-03 22:47:03,661 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:47:03,661 - evaluation_results_class.py:141 - Variance of Return = 12902.197265625
2025-08-03 22:47:03,661 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:47:03,661 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:47:03,661 - evaluation_results_class.py:147 - Average Episode Length = 104.40579150579151
2025-08-03 22:47:03,661 - evaluation_results_class.py:149 - Counted Episodes = 2590
2025-08-03 22:47:03,930 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:47:03,942 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:47:08,141 - father_agent.py:386 - Step: 105, Training loss: 6.860536575317383
2025-08-03 22:47:10,095 - father_agent.py:386 - Step: 110, Training loss: 5.600076675415039
2025-08-03 22:47:12,039 - father_agent.py:386 - Step: 115, Training loss: 7.064000606536865
2025-08-03 22:47:14,022 - father_agent.py:386 - Step: 120, Training loss: 7.221163272857666
2025-08-03 22:47:15,992 - father_agent.py:386 - Step: 125, Training loss: 6.131716728210449
2025-08-03 22:47:17,969 - father_agent.py:386 - Step: 130, Training loss: 5.413084506988525
2025-08-03 22:47:19,964 - father_agent.py:386 - Step: 135, Training loss: 5.8039326667785645
2025-08-03 22:47:21,957 - father_agent.py:386 - Step: 140, Training loss: 7.791793346405029
2025-08-03 22:47:23,947 - father_agent.py:386 - Step: 145, Training loss: 7.784603595733643
2025-08-03 22:47:25,932 - father_agent.py:386 - Step: 150, Training loss: 5.599043369293213
2025-08-03 22:47:27,922 - father_agent.py:386 - Step: 155, Training loss: 6.016077995300293
2025-08-03 22:47:29,905 - father_agent.py:386 - Step: 160, Training loss: 6.842860698699951
2025-08-03 22:47:31,896 - father_agent.py:386 - Step: 165, Training loss: 6.162600517272949
2025-08-03 22:47:33,866 - father_agent.py:386 - Step: 170, Training loss: 6.1140031814575195
2025-08-03 22:47:35,868 - father_agent.py:386 - Step: 175, Training loss: 7.007221698760986
2025-08-03 22:47:37,844 - father_agent.py:386 - Step: 180, Training loss: 7.991151809692383
2025-08-03 22:47:39,785 - father_agent.py:386 - Step: 185, Training loss: 6.03402853012085
2025-08-03 22:47:41,739 - father_agent.py:386 - Step: 190, Training loss: 7.054358005523682
2025-08-03 22:47:43,705 - father_agent.py:386 - Step: 195, Training loss: 6.137218475341797
2025-08-03 22:47:45,656 - father_agent.py:386 - Step: 200, Training loss: 6.245273590087891
2025-08-03 22:47:45,921 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:47:45,924 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:47:56,671 - evaluation_results_class.py:131 - Average Return = 114.78191375732422
2025-08-03 22:47:56,672 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.473389625549316
2025-08-03 22:47:56,672 - evaluation_results_class.py:135 - Average Discounted Reward = 6.012010097503662
2025-08-03 22:47:56,672 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9975990396158463
2025-08-03 22:47:56,672 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:47:56,672 - evaluation_results_class.py:141 - Variance of Return = 17350.6015625
2025-08-03 22:47:56,672 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:47:56,672 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:47:56,672 - evaluation_results_class.py:147 - Average Episode Length = 107.80072028811524
2025-08-03 22:47:56,672 - evaluation_results_class.py:149 - Counted Episodes = 2499
2025-08-03 22:47:56,939 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:47:56,952 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:48:01,173 - father_agent.py:386 - Step: 205, Training loss: 6.121031761169434
2025-08-03 22:48:03,140 - father_agent.py:386 - Step: 210, Training loss: 6.054867744445801
2025-08-03 22:48:05,110 - father_agent.py:386 - Step: 215, Training loss: 6.248967170715332
2025-08-03 22:48:07,071 - father_agent.py:386 - Step: 220, Training loss: 5.661415100097656
2025-08-03 22:48:09,007 - father_agent.py:386 - Step: 225, Training loss: 7.045327186584473
2025-08-03 22:48:11,023 - father_agent.py:386 - Step: 230, Training loss: 6.121923446655273
2025-08-03 22:48:13,021 - father_agent.py:386 - Step: 235, Training loss: 6.6031389236450195
2025-08-03 22:48:15,008 - father_agent.py:386 - Step: 240, Training loss: 6.601532459259033
2025-08-03 22:48:16,991 - father_agent.py:386 - Step: 245, Training loss: 6.763672351837158
2025-08-03 22:48:19,013 - father_agent.py:386 - Step: 250, Training loss: 4.623969078063965
2025-08-03 22:48:20,979 - father_agent.py:386 - Step: 255, Training loss: 6.322731018066406
2025-08-03 22:48:22,940 - father_agent.py:386 - Step: 260, Training loss: 5.205433368682861
2025-08-03 22:48:24,910 - father_agent.py:386 - Step: 265, Training loss: 6.843305587768555
2025-08-03 22:48:26,866 - father_agent.py:386 - Step: 270, Training loss: 7.780742645263672
2025-08-03 22:48:28,806 - father_agent.py:386 - Step: 275, Training loss: 7.7087249755859375
2025-08-03 22:48:30,716 - father_agent.py:386 - Step: 280, Training loss: 6.187180042266846
2025-08-03 22:48:32,641 - father_agent.py:386 - Step: 285, Training loss: 6.097020626068115
2025-08-03 22:48:34,560 - father_agent.py:386 - Step: 290, Training loss: 6.2870097160339355
2025-08-03 22:48:36,532 - father_agent.py:386 - Step: 295, Training loss: 7.045931339263916
2025-08-03 22:48:38,468 - father_agent.py:386 - Step: 300, Training loss: 7.419173240661621
2025-08-03 22:48:38,741 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:48:38,744 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:48:49,374 - evaluation_results_class.py:131 - Average Return = 122.16606140136719
2025-08-03 22:48:49,374 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.211792945861816
2025-08-03 22:48:49,374 - evaluation_results_class.py:135 - Average Discounted Reward = 6.439157962799072
2025-08-03 22:48:49,374 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9975932611311673
2025-08-03 22:48:49,374 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:48:49,374 - evaluation_results_class.py:141 - Variance of Return = 17393.783203125
2025-08-03 22:48:49,374 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:48:49,374 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:48:49,374 - evaluation_results_class.py:147 - Average Episode Length = 107.61572402727637
2025-08-03 22:48:49,375 - evaluation_results_class.py:149 - Counted Episodes = 2493
2025-08-03 22:48:49,644 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:48:49,656 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:48:53,876 - father_agent.py:386 - Step: 305, Training loss: 7.604324817657471
2025-08-03 22:48:55,814 - father_agent.py:386 - Step: 310, Training loss: 5.372364521026611
2025-08-03 22:48:57,775 - father_agent.py:386 - Step: 315, Training loss: 6.1846699714660645
2025-08-03 22:48:59,707 - father_agent.py:386 - Step: 320, Training loss: 6.98467493057251
2025-08-03 22:49:01,643 - father_agent.py:386 - Step: 325, Training loss: 8.120018005371094
2025-08-03 22:49:03,623 - father_agent.py:386 - Step: 330, Training loss: 7.8830060958862305
2025-08-03 22:49:05,528 - father_agent.py:386 - Step: 335, Training loss: 6.827909469604492
2025-08-03 22:49:07,470 - father_agent.py:386 - Step: 340, Training loss: 6.647763252258301
2025-08-03 22:49:09,414 - father_agent.py:386 - Step: 345, Training loss: 5.850245475769043
2025-08-03 22:49:11,339 - father_agent.py:386 - Step: 350, Training loss: 6.269415855407715
2025-08-03 22:49:13,277 - father_agent.py:386 - Step: 355, Training loss: 6.897439956665039
2025-08-03 22:49:15,192 - father_agent.py:386 - Step: 360, Training loss: 5.826292514801025
2025-08-03 22:49:17,108 - father_agent.py:386 - Step: 365, Training loss: 5.6893157958984375
2025-08-03 22:49:19,022 - father_agent.py:386 - Step: 370, Training loss: 5.42591667175293
2025-08-03 22:49:20,930 - father_agent.py:386 - Step: 375, Training loss: 6.751382827758789
2025-08-03 22:49:22,837 - father_agent.py:386 - Step: 380, Training loss: 6.633932113647461
2025-08-03 22:49:24,770 - father_agent.py:386 - Step: 385, Training loss: 6.433431625366211
2025-08-03 22:49:26,685 - father_agent.py:386 - Step: 390, Training loss: 7.424229621887207
2025-08-03 22:49:28,632 - father_agent.py:386 - Step: 395, Training loss: 6.576507568359375
2025-08-03 22:49:30,574 - father_agent.py:386 - Step: 400, Training loss: 5.582583904266357
2025-08-03 22:49:30,859 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:49:30,861 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:49:41,442 - evaluation_results_class.py:131 - Average Return = 123.6455307006836
2025-08-03 22:49:41,442 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.36119270324707
2025-08-03 22:49:41,443 - evaluation_results_class.py:135 - Average Discounted Reward = 6.320181846618652
2025-08-03 22:49:41,443 - evaluation_results_class.py:137 - Goal Reach Probability = 0.998320033599328
2025-08-03 22:49:41,443 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:49:41,443 - evaluation_results_class.py:141 - Variance of Return = 18086.669921875
2025-08-03 22:49:41,443 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:49:41,443 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:49:41,443 - evaluation_results_class.py:147 - Average Episode Length = 114.4502309953801
2025-08-03 22:49:41,443 - evaluation_results_class.py:149 - Counted Episodes = 2381
2025-08-03 22:49:41,722 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:49:41,741 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:49:45,936 - father_agent.py:386 - Step: 405, Training loss: 6.188445568084717
2025-08-03 22:49:47,878 - father_agent.py:386 - Step: 410, Training loss: 7.634721755981445
2025-08-03 22:49:49,825 - father_agent.py:386 - Step: 415, Training loss: 7.300227165222168
2025-08-03 22:49:51,705 - father_agent.py:386 - Step: 420, Training loss: 6.6344075202941895
2025-08-03 22:49:53,615 - father_agent.py:386 - Step: 425, Training loss: 6.390604019165039
2025-08-03 22:49:55,523 - father_agent.py:386 - Step: 430, Training loss: 6.295106410980225
2025-08-03 22:49:57,416 - father_agent.py:386 - Step: 435, Training loss: 7.506646633148193
2025-08-03 22:49:59,315 - father_agent.py:386 - Step: 440, Training loss: 6.153892517089844
2025-08-03 22:50:01,212 - father_agent.py:386 - Step: 445, Training loss: 6.338796138763428
2025-08-03 22:50:03,100 - father_agent.py:386 - Step: 450, Training loss: 5.691067218780518
2025-08-03 22:50:04,979 - father_agent.py:386 - Step: 455, Training loss: 7.874696254730225
2025-08-03 22:50:06,866 - father_agent.py:386 - Step: 460, Training loss: 4.95296573638916
2025-08-03 22:50:08,758 - father_agent.py:386 - Step: 465, Training loss: 5.5396881103515625
2025-08-03 22:50:10,681 - father_agent.py:386 - Step: 470, Training loss: 5.422296524047852
2025-08-03 22:50:12,613 - father_agent.py:386 - Step: 475, Training loss: 5.254239082336426
2025-08-03 22:50:14,524 - father_agent.py:386 - Step: 480, Training loss: 5.833627700805664
2025-08-03 22:50:16,438 - father_agent.py:386 - Step: 485, Training loss: 7.0884857177734375
2025-08-03 22:50:18,337 - father_agent.py:386 - Step: 490, Training loss: 6.125461578369141
2025-08-03 22:50:20,262 - father_agent.py:386 - Step: 495, Training loss: 6.547722816467285
2025-08-03 22:50:22,196 - father_agent.py:386 - Step: 500, Training loss: 7.0306267738342285
2025-08-03 22:50:22,457 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:50:22,459 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:50:33,034 - evaluation_results_class.py:131 - Average Return = 121.12606811523438
2025-08-03 22:50:33,037 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.110158920288086
2025-08-03 22:50:33,037 - evaluation_results_class.py:135 - Average Discounted Reward = 6.286866188049316
2025-08-03 22:50:33,037 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9987760097919217
2025-08-03 22:50:33,037 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:50:33,037 - evaluation_results_class.py:141 - Variance of Return = 18342.91796875
2025-08-03 22:50:33,037 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:50:33,037 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:50:33,037 - evaluation_results_class.py:147 - Average Episode Length = 109.3141574867401
2025-08-03 22:50:33,037 - evaluation_results_class.py:149 - Counted Episodes = 2451
2025-08-03 22:50:33,299 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:50:33,311 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:50:33,429 - father_agent.py:547 - Training finished.
2025-08-03 22:50:33,585 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:50:33,588 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:50:33,591 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-03 22:50:44,174 - evaluation_results_class.py:131 - Average Return = 119.49076080322266
2025-08-03 22:50:44,174 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.946612358093262
2025-08-03 22:50:44,174 - evaluation_results_class.py:135 - Average Discounted Reward = 6.141231536865234
2025-08-03 22:50:44,174 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9987679671457905
2025-08-03 22:50:44,174 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:50:44,174 - evaluation_results_class.py:141 - Variance of Return = 17934.85546875
2025-08-03 22:50:44,174 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:50:44,174 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:50:44,174 - evaluation_results_class.py:147 - Average Episode Length = 110.44640657084189
2025-08-03 22:50:44,174 - evaluation_results_class.py:149 - Counted Episodes = 2435
2025-08-03 22:50:44,433 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:50:44,435 - self_interpretable_extractor.py:286 - True
2025-08-03 22:50:44,448 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:51:01,898 - evaluation_results_class.py:131 - Average Return = 139.5806427001953
2025-08-03 22:51:01,898 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.953226089477539
2025-08-03 22:51:01,898 - evaluation_results_class.py:135 - Average Discounted Reward = 6.590444564819336
2025-08-03 22:51:01,898 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9975806451612903
2025-08-03 22:51:01,898 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:51:01,898 - evaluation_results_class.py:141 - Variance of Return = 23211.4375
2025-08-03 22:51:01,898 - evaluation_results_class.py:143 - Current Best Return = 139.5806427001953
2025-08-03 22:51:01,898 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9975806451612903
2025-08-03 22:51:01,898 - evaluation_results_class.py:147 - Average Episode Length = 122.10887096774194
2025-08-03 22:51:01,898 - evaluation_results_class.py:149 - Counted Episodes = 2480
2025-08-03 22:51:01,898 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-03 22:51:01,898 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 11414 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 562, 564, 566, 568, 572, 574, 578, 580, 582, 584, 588, 594, 596, 598, 600, 602, 604, 606, 608, 612, 616, 618, 624, 626, 630, 634, 636, 638, 640, 642, 644, 646, 650, 651}
Buffer 1
2025-08-03 22:52:13,305 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 22900 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 612, 614, 616, 618, 622, 624, 626, 628, 630, 632, 634, 636, 638, 640, 642, 644, 646, 650, 651}
Buffer 2
2025-08-03 22:53:25,065 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 34286 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 622, 624, 626, 628, 630, 632, 634, 636, 638, 640, 642, 644, 646, 648, 650, 651}
All trajectories collected
2025-08-03 22:54:36,488 - self_interpretable_extractor.py:346 - Data sampled
2025-08-03 22:54:36,488 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 34286 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_9.dot.
Learned FSC of size 2
2025-08-03 22:55:10,230 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-03 22:55:26,615 - evaluation_results_class.py:131 - Average Return = 135.94439697265625
2025-08-03 22:55:26,615 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.587898254394531
2025-08-03 22:55:26,615 - evaluation_results_class.py:135 - Average Discounted Reward = 6.480329990386963
2025-08-03 22:55:26,615 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9967293540474244
2025-08-03 22:55:26,616 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:55:26,616 - evaluation_results_class.py:141 - Variance of Return = 25352.970703125
2025-08-03 22:55:26,616 - evaluation_results_class.py:143 - Current Best Return = 135.94439697265625
2025-08-03 22:55:26,616 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9967293540474244
2025-08-03 22:55:26,616 - evaluation_results_class.py:147 - Average Episode Length = 119.70645952575634
2025-08-03 22:55:26,616 - evaluation_results_class.py:149 - Counted Episodes = 2446
FSC Result: {'best_episode_return': 14.587898, 'best_return': 135.9444, 'goal_value': 0.0, 'returns_episodic': [14.587898], 'returns': [135.9444], 'reach_probs': [0.9967293540474244], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.9967293540474244, 'losses': [], 'best_updated': True, 'each_episode_variance': [25352.97], 'each_episode_virtual_variance': [252.67401], 'combined_variance': [30667.553], 'num_episodes': [2446], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [119.70645952575634], 'counted_episodes': [2446], 'discounted_rewards': [6.48033], 'new_pomdp_iteration_numbers': []}
[[1, 2, 3], [4], [0]]
2025-08-03 22:55:26,690 - statistic.py:67 - synthesis initiated, design space: 131220
2025-08-03 22:55:26,690 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-03 22:55:26,798 - synthesizer_ar.py:122 - value 1054.4274 achieved after 5722.34 seconds
2025-08-03 22:55:26,804 - synthesizer_ar.py:122 - value 1054.1942 achieved after 5722.34 seconds
2025-08-03 22:55:26,953 - synthesizer_ar.py:122 - value 1039.2169 achieved after 5722.49 seconds
2025-08-03 22:55:26,960 - synthesizer_ar.py:122 - value 1038.653 achieved after 5722.5 seconds
2025-08-03 22:55:26,985 - synthesizer_ar.py:122 - value 1037.4186 achieved after 5722.52 seconds
2025-08-03 22:55:26,992 - synthesizer_ar.py:122 - value 1036.8037 achieved after 5722.53 seconds
2025-08-03 22:55:27,204 - synthesizer_ar.py:122 - value 1012.2117 achieved after 5722.74 seconds
2025-08-03 22:55:27,210 - synthesizer_ar.py:122 - value 1011.9092 achieved after 5722.75 seconds
2025-08-03 22:55:27,216 - synthesizer_ar.py:122 - value 1011.8268 achieved after 5722.75 seconds
2025-08-03 22:55:27,376 - synthesizer_ar.py:122 - value 713.726 achieved after 5722.91 seconds
2025-08-03 22:55:27,382 - synthesizer_ar.py:122 - value 713.7195 achieved after 5722.92 seconds
2025-08-03 22:55:27,411 - synthesizer_ar.py:122 - value 713.5101 achieved after 5722.95 seconds
2025-08-03 22:55:27,417 - synthesizer_ar.py:122 - value 713.4953 achieved after 5722.95 seconds
2025-08-03 22:55:27,525 - synthesizer_ar.py:122 - value 525.8523 achieved after 5723.06 seconds
2025-08-03 22:55:27,530 - synthesizer_ar.py:122 - value 525.8486 achieved after 5723.07 seconds
2025-08-03 22:55:27,716 - synthesizer_ar.py:122 - value 147.9873 achieved after 5723.25 seconds
2025-08-03 22:55:27,794 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-03 22:55:27,794 - synthesizer.py:193 - P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 22:55:27,796 - synthesizer.py:198 - double-checking specification satisfiability:  : 147.98730732547781
--------------------
Synthesis summary:
optimality objective: R{"rew"}min=? [F (bat = 0)] 

method: AR, synthesis time: 1.1 s
number of holes: 9, family size: 131220, quotient: 2050 states / 11335 actions
explored: 100 %
MDP stats: avg MDP size: 2050, iterations: 164

optimum: 147.987307
--------------------
2025-08-03 22:55:27,796 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 22:55:27,816 - robust_rl_trainer.py:432 - Iteration 11 of pure RL loop
2025-08-03 22:55:27,857 - storm_vec_env.py:70 - Computing row map
2025-08-03 22:55:27,873 - storm_vec_env.py:97 - Computing transitions
2025-08-03 22:55:27,909 - storm_vec_env.py:100 - Computing allowed actions
2025-08-03 22:55:27,909 - storm_vec_env.py:114 - Computing sinks
2025-08-03 22:55:27,909 - storm_vec_env.py:119 - Computing raw rewards
2025-08-03 22:55:27,917 - storm_vec_env.py:143 - Computing labels
2025-08-03 22:55:27,917 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-03 22:55:27,918 - storm_vec_env.py:161 - Computing metalabels
2025-08-03 22:55:27,918 - storm_vec_env.py:175 - Computing observations
2025-08-03 22:55:28,091 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-03 22:55:28,091 - father_agent.py:540 - Before training evaluation.
2025-08-03 22:55:28,257 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:55:28,259 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:55:38,934 - evaluation_results_class.py:131 - Average Return = 116.8433609008789
2025-08-03 22:55:38,934 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.681147575378418
2025-08-03 22:55:38,934 - evaluation_results_class.py:135 - Average Discounted Reward = 6.011969566345215
2025-08-03 22:55:38,934 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9984057393383818
2025-08-03 22:55:38,934 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:55:38,934 - evaluation_results_class.py:141 - Variance of Return = 17646.791015625
2025-08-03 22:55:38,934 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:55:38,934 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:55:38,935 - evaluation_results_class.py:147 - Average Episode Length = 109.60860900757274
2025-08-03 22:55:38,935 - evaluation_results_class.py:149 - Counted Episodes = 2509
2025-08-03 22:55:39,198 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:55:39,211 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:55:39,329 - father_agent.py:436 - Training agent on-policy
2025-08-03 22:55:48,050 - father_agent.py:386 - Step: 0, Training loss: 6.143204689025879
2025-08-03 22:55:49,996 - father_agent.py:386 - Step: 5, Training loss: 6.361684799194336
2025-08-03 22:55:51,908 - father_agent.py:386 - Step: 10, Training loss: 5.16627836227417
2025-08-03 22:55:53,836 - father_agent.py:386 - Step: 15, Training loss: 7.891656875610352
2025-08-03 22:55:55,760 - father_agent.py:386 - Step: 20, Training loss: 7.843139171600342
2025-08-03 22:55:57,687 - father_agent.py:386 - Step: 25, Training loss: 6.310791015625
2025-08-03 22:55:59,647 - father_agent.py:386 - Step: 30, Training loss: 8.368231773376465
2025-08-03 22:56:01,592 - father_agent.py:386 - Step: 35, Training loss: 7.796811103820801
2025-08-03 22:56:03,543 - father_agent.py:386 - Step: 40, Training loss: 6.391146659851074
2025-08-03 22:56:05,460 - father_agent.py:386 - Step: 45, Training loss: 6.735849857330322
2025-08-03 22:56:07,391 - father_agent.py:386 - Step: 50, Training loss: 7.393636703491211
2025-08-03 22:56:09,344 - father_agent.py:386 - Step: 55, Training loss: 7.022668838500977
2025-08-03 22:56:11,260 - father_agent.py:386 - Step: 60, Training loss: 6.074535846710205
2025-08-03 22:56:13,186 - father_agent.py:386 - Step: 65, Training loss: 7.570723056793213
2025-08-03 22:56:15,164 - father_agent.py:386 - Step: 70, Training loss: 6.810173988342285
2025-08-03 22:56:17,128 - father_agent.py:386 - Step: 75, Training loss: 5.877283096313477
2025-08-03 22:56:19,066 - father_agent.py:386 - Step: 80, Training loss: 5.780975341796875
2025-08-03 22:56:20,999 - father_agent.py:386 - Step: 85, Training loss: 6.720086097717285
2025-08-03 22:56:22,924 - father_agent.py:386 - Step: 90, Training loss: 6.575045108795166
2025-08-03 22:56:24,844 - father_agent.py:386 - Step: 95, Training loss: 7.083106517791748
2025-08-03 22:56:26,782 - father_agent.py:386 - Step: 100, Training loss: 6.226620197296143
2025-08-03 22:56:27,044 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:56:27,046 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:56:37,671 - evaluation_results_class.py:131 - Average Return = 113.01438903808594
2025-08-03 22:56:37,671 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.2975492477417
2025-08-03 22:56:37,672 - evaluation_results_class.py:135 - Average Discounted Reward = 5.8914690017700195
2025-08-03 22:56:37,672 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9980552314274601
2025-08-03 22:56:37,672 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:56:37,672 - evaluation_results_class.py:141 - Variance of Return = 17045.87109375
2025-08-03 22:56:37,672 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:56:37,672 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:56:37,672 - evaluation_results_class.py:147 - Average Episode Length = 108.52936600544535
2025-08-03 22:56:37,672 - evaluation_results_class.py:149 - Counted Episodes = 2571
2025-08-03 22:56:37,941 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:56:37,953 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:56:42,112 - father_agent.py:386 - Step: 105, Training loss: 7.644628047943115
2025-08-03 22:56:44,003 - father_agent.py:386 - Step: 110, Training loss: 7.774749755859375
2025-08-03 22:56:45,917 - father_agent.py:386 - Step: 115, Training loss: 5.6314849853515625
2025-08-03 22:56:47,827 - father_agent.py:386 - Step: 120, Training loss: 6.440525054931641
2025-08-03 22:56:49,733 - father_agent.py:386 - Step: 125, Training loss: 5.899019241333008
2025-08-03 22:56:51,630 - father_agent.py:386 - Step: 130, Training loss: 6.852451324462891
2025-08-03 22:56:53,522 - father_agent.py:386 - Step: 135, Training loss: 5.915266990661621
2025-08-03 22:56:55,422 - father_agent.py:386 - Step: 140, Training loss: 6.45010232925415
2025-08-03 22:56:57,335 - father_agent.py:386 - Step: 145, Training loss: 6.970090389251709
2025-08-03 22:56:59,243 - father_agent.py:386 - Step: 150, Training loss: 7.2692179679870605
2025-08-03 22:57:01,161 - father_agent.py:386 - Step: 155, Training loss: 7.889278888702393
2025-08-03 22:57:03,053 - father_agent.py:386 - Step: 160, Training loss: 6.544458866119385
2025-08-03 22:57:04,946 - father_agent.py:386 - Step: 165, Training loss: 7.247042179107666
2025-08-03 22:57:06,842 - father_agent.py:386 - Step: 170, Training loss: 6.001193523406982
2025-08-03 22:57:08,785 - father_agent.py:386 - Step: 175, Training loss: 6.877914905548096
2025-08-03 22:57:10,712 - father_agent.py:386 - Step: 180, Training loss: 7.9216108322143555
2025-08-03 22:57:12,612 - father_agent.py:386 - Step: 185, Training loss: 7.470239162445068
2025-08-03 22:57:14,529 - father_agent.py:386 - Step: 190, Training loss: 6.188751697540283
2025-08-03 22:57:16,477 - father_agent.py:386 - Step: 195, Training loss: 5.804864883422852
2025-08-03 22:57:18,400 - father_agent.py:386 - Step: 200, Training loss: 7.711439609527588
2025-08-03 22:57:18,666 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:57:18,669 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:57:29,318 - evaluation_results_class.py:131 - Average Return = 110.83659362792969
2025-08-03 22:57:29,318 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.0820951461792
2025-08-03 22:57:29,318 - evaluation_results_class.py:135 - Average Discounted Reward = 5.939858913421631
2025-08-03 22:57:29,318 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9992181391712275
2025-08-03 22:57:29,318 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:57:29,318 - evaluation_results_class.py:141 - Variance of Return = 15633.310546875
2025-08-03 22:57:29,318 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:57:29,318 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:57:29,318 - evaluation_results_class.py:147 - Average Episode Length = 105.11415168100078
2025-08-03 22:57:29,318 - evaluation_results_class.py:149 - Counted Episodes = 2558
2025-08-03 22:57:29,588 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:57:29,601 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:57:33,791 - father_agent.py:386 - Step: 205, Training loss: 6.281462669372559
2025-08-03 22:57:35,710 - father_agent.py:386 - Step: 210, Training loss: 6.3668131828308105
2025-08-03 22:57:37,637 - father_agent.py:386 - Step: 215, Training loss: 6.405704021453857
2025-08-03 22:57:39,585 - father_agent.py:386 - Step: 220, Training loss: 6.791011333465576
2025-08-03 22:57:41,543 - father_agent.py:386 - Step: 225, Training loss: 7.129430770874023
2025-08-03 22:57:43,529 - father_agent.py:386 - Step: 230, Training loss: 8.014942169189453
2025-08-03 22:57:45,500 - father_agent.py:386 - Step: 235, Training loss: 6.189160346984863
2025-08-03 22:57:47,511 - father_agent.py:386 - Step: 240, Training loss: 7.654813766479492
2025-08-03 22:57:49,577 - father_agent.py:386 - Step: 245, Training loss: 8.346593856811523
2025-08-03 22:57:51,667 - father_agent.py:386 - Step: 250, Training loss: 7.1547465324401855
2025-08-03 22:57:53,706 - father_agent.py:386 - Step: 255, Training loss: 5.941201210021973
2025-08-03 22:57:55,709 - father_agent.py:386 - Step: 260, Training loss: 5.955663204193115
2025-08-03 22:57:57,702 - father_agent.py:386 - Step: 265, Training loss: 5.951587200164795
2025-08-03 22:57:59,728 - father_agent.py:386 - Step: 270, Training loss: 5.492903709411621
2025-08-03 22:58:01,759 - father_agent.py:386 - Step: 275, Training loss: 5.61572790145874
2025-08-03 22:58:03,791 - father_agent.py:386 - Step: 280, Training loss: 5.685940742492676
2025-08-03 22:58:05,820 - father_agent.py:386 - Step: 285, Training loss: 8.033370018005371
2025-08-03 22:58:07,873 - father_agent.py:386 - Step: 290, Training loss: 7.263963222503662
2025-08-03 22:58:09,939 - father_agent.py:386 - Step: 295, Training loss: 6.954576015472412
2025-08-03 22:58:11,970 - father_agent.py:386 - Step: 300, Training loss: 6.68278169631958
2025-08-03 22:58:12,236 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:58:12,239 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:58:23,402 - evaluation_results_class.py:131 - Average Return = 100.66692352294922
2025-08-03 22:58:23,402 - evaluation_results_class.py:133 - Average Virtual Goal Value = 11.063626289367676
2025-08-03 22:58:23,402 - evaluation_results_class.py:135 - Average Discounted Reward = 5.549717426300049
2025-08-03 22:58:23,402 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9984668455346877
2025-08-03 22:58:23,402 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:58:23,402 - evaluation_results_class.py:141 - Variance of Return = 12060.1142578125
2025-08-03 22:58:23,402 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:58:23,402 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:58:23,402 - evaluation_results_class.py:147 - Average Episode Length = 106.50977385971636
2025-08-03 22:58:23,402 - evaluation_results_class.py:149 - Counted Episodes = 2609
2025-08-03 22:58:23,670 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:58:23,682 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:58:27,935 - father_agent.py:386 - Step: 305, Training loss: 6.806983470916748
2025-08-03 22:58:30,033 - father_agent.py:386 - Step: 310, Training loss: 7.490300178527832
2025-08-03 22:58:32,093 - father_agent.py:386 - Step: 315, Training loss: 6.978446960449219
2025-08-03 22:58:34,108 - father_agent.py:386 - Step: 320, Training loss: 6.168977737426758
2025-08-03 22:58:36,135 - father_agent.py:386 - Step: 325, Training loss: 5.582935810089111
2025-08-03 22:58:38,149 - father_agent.py:386 - Step: 330, Training loss: 8.661189079284668
2025-08-03 22:58:40,238 - father_agent.py:386 - Step: 335, Training loss: 6.509245872497559
2025-08-03 22:58:42,278 - father_agent.py:386 - Step: 340, Training loss: 7.470003128051758
2025-08-03 22:58:44,300 - father_agent.py:386 - Step: 345, Training loss: 7.040144920349121
2025-08-03 22:58:46,312 - father_agent.py:386 - Step: 350, Training loss: 5.721590042114258
2025-08-03 22:58:48,318 - father_agent.py:386 - Step: 355, Training loss: 8.33019733428955
2025-08-03 22:58:50,331 - father_agent.py:386 - Step: 360, Training loss: 5.981539726257324
2025-08-03 22:58:52,295 - father_agent.py:386 - Step: 365, Training loss: 6.354608058929443
2025-08-03 22:58:54,309 - father_agent.py:386 - Step: 370, Training loss: 5.888282775878906
2025-08-03 22:58:56,298 - father_agent.py:386 - Step: 375, Training loss: 6.573917388916016
2025-08-03 22:58:58,248 - father_agent.py:386 - Step: 380, Training loss: 6.142518997192383
2025-08-03 22:59:00,193 - father_agent.py:386 - Step: 385, Training loss: 6.795281887054443
2025-08-03 22:59:02,107 - father_agent.py:386 - Step: 390, Training loss: 5.755914688110352
2025-08-03 22:59:04,011 - father_agent.py:386 - Step: 395, Training loss: 6.6205339431762695
2025-08-03 22:59:05,928 - father_agent.py:386 - Step: 400, Training loss: 8.130522727966309
2025-08-03 22:59:06,192 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:59:06,194 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:59:17,125 - evaluation_results_class.py:131 - Average Return = 121.95624542236328
2025-08-03 22:59:17,125 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.187044143676758
2025-08-03 22:59:17,125 - evaluation_results_class.py:135 - Average Discounted Reward = 6.113347053527832
2025-08-03 22:59:17,125 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9957099957099957
2025-08-03 22:59:17,125 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 22:59:17,125 - evaluation_results_class.py:141 - Variance of Return = 18012.818359375
2025-08-03 22:59:17,125 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 22:59:17,125 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 22:59:17,125 - evaluation_results_class.py:147 - Average Episode Length = 116.87001287001287
2025-08-03 22:59:17,125 - evaluation_results_class.py:149 - Counted Episodes = 2331
2025-08-03 22:59:17,394 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:59:17,407 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:59:21,637 - father_agent.py:386 - Step: 405, Training loss: 7.9662041664123535
2025-08-03 22:59:23,572 - father_agent.py:386 - Step: 410, Training loss: 5.144552707672119
2025-08-03 22:59:25,489 - father_agent.py:386 - Step: 415, Training loss: 4.646689414978027
2025-08-03 22:59:27,392 - father_agent.py:386 - Step: 420, Training loss: 8.992864608764648
2025-08-03 22:59:29,312 - father_agent.py:386 - Step: 425, Training loss: 6.305581092834473
2025-08-03 22:59:31,282 - father_agent.py:386 - Step: 430, Training loss: 5.950151443481445
2025-08-03 22:59:33,227 - father_agent.py:386 - Step: 435, Training loss: 7.541222095489502
2025-08-03 22:59:35,142 - father_agent.py:386 - Step: 440, Training loss: 8.517851829528809
2025-08-03 22:59:37,052 - father_agent.py:386 - Step: 445, Training loss: 5.9364848136901855
2025-08-03 22:59:38,994 - father_agent.py:386 - Step: 450, Training loss: 6.188415050506592
2025-08-03 22:59:40,891 - father_agent.py:386 - Step: 455, Training loss: 6.1324286460876465
2025-08-03 22:59:42,813 - father_agent.py:386 - Step: 460, Training loss: 5.517299175262451
2025-08-03 22:59:44,703 - father_agent.py:386 - Step: 465, Training loss: 5.374709129333496
2025-08-03 22:59:46,601 - father_agent.py:386 - Step: 470, Training loss: 6.135127067565918
2025-08-03 22:59:48,501 - father_agent.py:386 - Step: 475, Training loss: 5.728025913238525
2025-08-03 22:59:50,445 - father_agent.py:386 - Step: 480, Training loss: 6.602945327758789
2025-08-03 22:59:52,364 - father_agent.py:386 - Step: 485, Training loss: 5.6341094970703125
2025-08-03 22:59:54,281 - father_agent.py:386 - Step: 490, Training loss: 6.138915061950684
2025-08-03 22:59:56,209 - father_agent.py:386 - Step: 495, Training loss: 7.227317810058594
2025-08-03 22:59:58,120 - father_agent.py:386 - Step: 500, Training loss: 5.991231441497803
2025-08-03 22:59:58,391 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 22:59:58,393 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:00:09,007 - evaluation_results_class.py:131 - Average Return = 119.96775817871094
2025-08-03 23:00:09,007 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.991938591003418
2025-08-03 23:00:09,007 - evaluation_results_class.py:135 - Average Discounted Reward = 6.1005330085754395
2025-08-03 23:00:09,007 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9975816203143894
2025-08-03 23:00:09,007 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:00:09,007 - evaluation_results_class.py:141 - Variance of Return = 18391.29296875
2025-08-03 23:00:09,008 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:00:09,008 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:00:09,008 - evaluation_results_class.py:147 - Average Episode Length = 112.18460298266828
2025-08-03 23:00:09,008 - evaluation_results_class.py:149 - Counted Episodes = 2481
2025-08-03 23:00:09,278 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:00:09,291 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:00:09,409 - father_agent.py:547 - Training finished.
2025-08-03 23:00:09,568 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:00:09,571 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:00:09,573 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-03 23:00:20,187 - evaluation_results_class.py:131 - Average Return = 118.77715301513672
2025-08-03 23:00:20,187 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.876106262207031
2025-08-03 23:00:20,187 - evaluation_results_class.py:135 - Average Discounted Reward = 6.203005790710449
2025-08-03 23:00:20,188 - evaluation_results_class.py:137 - Goal Reach Probability = 0.999195494770716
2025-08-03 23:00:20,188 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:00:20,188 - evaluation_results_class.py:141 - Variance of Return = 16400.11328125
2025-08-03 23:00:20,188 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:00:20,188 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:00:20,188 - evaluation_results_class.py:147 - Average Episode Length = 109.70796460176992
2025-08-03 23:00:20,188 - evaluation_results_class.py:149 - Counted Episodes = 2486
2025-08-03 23:00:20,461 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:00:20,464 - self_interpretable_extractor.py:286 - True
2025-08-03 23:00:20,476 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:00:37,792 - evaluation_results_class.py:131 - Average Return = 129.63021850585938
2025-08-03 23:00:37,792 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.955153465270996
2025-08-03 23:00:37,792 - evaluation_results_class.py:135 - Average Discounted Reward = 6.29033899307251
2025-08-03 23:00:37,792 - evaluation_results_class.py:137 - Goal Reach Probability = 0.996066089693155
2025-08-03 23:00:37,792 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:00:37,792 - evaluation_results_class.py:141 - Variance of Return = 20461.783203125
2025-08-03 23:00:37,792 - evaluation_results_class.py:143 - Current Best Return = 129.63021850585938
2025-08-03 23:00:37,792 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.996066089693155
2025-08-03 23:00:37,792 - evaluation_results_class.py:147 - Average Episode Length = 118.24311565696303
2025-08-03 23:00:37,792 - evaluation_results_class.py:149 - Counted Episodes = 2542
2025-08-03 23:00:37,792 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-03 23:00:37,793 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 11579 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 516, 518, 522, 524, 526, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 552, 556, 558, 560, 564, 566, 568, 570, 572, 574, 576, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 628, 630, 632, 634, 636, 638, 644, 646, 651}
Buffer 1
2025-08-03 23:01:50,828 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 23164 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 630, 632, 634, 636, 638, 640, 642, 644, 646, 651}
Buffer 2
2025-08-03 23:03:03,184 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 34530 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 630, 632, 634, 636, 638, 640, 642, 644, 646, 648, 650, 651}
All trajectories collected
2025-08-03 23:04:15,570 - self_interpretable_extractor.py:346 - Data sampled
2025-08-03 23:04:15,570 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 34530 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_10.dot.
Learned FSC of size 2
2025-08-03 23:04:47,027 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-03 23:05:03,543 - evaluation_results_class.py:131 - Average Return = 134.32521057128906
2025-08-03 23:05:03,543 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.42276382446289
2025-08-03 23:05:03,544 - evaluation_results_class.py:135 - Average Discounted Reward = 6.437828063964844
2025-08-03 23:05:03,544 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9951219512195122
2025-08-03 23:05:03,544 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:05:03,544 - evaluation_results_class.py:141 - Variance of Return = 23756.416015625
2025-08-03 23:05:03,544 - evaluation_results_class.py:143 - Current Best Return = 134.32521057128906
2025-08-03 23:05:03,544 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9951219512195122
2025-08-03 23:05:03,544 - evaluation_results_class.py:147 - Average Episode Length = 123.13577235772358
2025-08-03 23:05:03,544 - evaluation_results_class.py:149 - Counted Episodes = 2460
FSC Result: {'best_episode_return': 14.422764, 'best_return': 134.32521, 'goal_value': 0.0, 'returns_episodic': [14.422764], 'returns': [134.32521], 'reach_probs': [0.9951219512195122], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.9951219512195122, 'losses': [], 'best_updated': True, 'each_episode_variance': [23756.416], 'each_episode_virtual_variance': [236.34322], 'combined_variance': [28731.639], 'num_episodes': [2460], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [123.13577235772358], 'counted_episodes': [2460], 'discounted_rewards': [6.437828], 'new_pomdp_iteration_numbers': []}
[[1, 2, 3], [4], [0]]
2025-08-03 23:05:03,614 - statistic.py:67 - synthesis initiated, design space: 131220
2025-08-03 23:05:03,615 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-03 23:05:03,698 - synthesizer_ar.py:122 - value 1000.1575 achieved after 6299.24 seconds
2025-08-03 23:05:03,704 - synthesizer_ar.py:122 - value 1000.086 achieved after 6299.24 seconds
2025-08-03 23:05:03,810 - synthesizer_ar.py:122 - value 143.4109 achieved after 6299.35 seconds
2025-08-03 23:05:03,866 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-03 23:05:03,867 - synthesizer.py:193 - P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 23:05:03,873 - synthesizer.py:198 - double-checking specification satisfiability:  : 143.41089211002875
--------------------
Synthesis summary:
optimality objective: R{"rew"}min=? [F (bat = 0)] 

method: AR, synthesis time: 0.25 s
number of holes: 9, family size: 131220, quotient: 2050 states / 11335 actions
explored: 100 %
MDP stats: avg MDP size: 2050, iterations: 40

optimum: 143.410892
--------------------
2025-08-03 23:05:03,874 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 23:05:03,895 - robust_rl_trainer.py:432 - Iteration 12 of pure RL loop
2025-08-03 23:05:03,936 - storm_vec_env.py:70 - Computing row map
2025-08-03 23:05:03,951 - storm_vec_env.py:97 - Computing transitions
2025-08-03 23:05:03,986 - storm_vec_env.py:100 - Computing allowed actions
2025-08-03 23:05:03,987 - storm_vec_env.py:114 - Computing sinks
2025-08-03 23:05:03,987 - storm_vec_env.py:119 - Computing raw rewards
2025-08-03 23:05:03,995 - storm_vec_env.py:143 - Computing labels
2025-08-03 23:05:03,995 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-03 23:05:03,995 - storm_vec_env.py:161 - Computing metalabels
2025-08-03 23:05:03,995 - storm_vec_env.py:175 - Computing observations
2025-08-03 23:05:04,179 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-03 23:05:04,181 - father_agent.py:540 - Before training evaluation.
2025-08-03 23:05:04,360 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:05:04,363 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:05:15,173 - evaluation_results_class.py:131 - Average Return = 118.84270477294922
2025-08-03 23:05:15,173 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.88182544708252
2025-08-03 23:05:15,174 - evaluation_results_class.py:135 - Average Discounted Reward = 6.010580062866211
2025-08-03 23:05:15,174 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9987775061124694
2025-08-03 23:05:15,174 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:05:15,174 - evaluation_results_class.py:141 - Variance of Return = 17148.130859375
2025-08-03 23:05:15,174 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:05:15,174 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:05:15,174 - evaluation_results_class.py:147 - Average Episode Length = 112.84881825590872
2025-08-03 23:05:15,174 - evaluation_results_class.py:149 - Counted Episodes = 2454
2025-08-03 23:05:15,444 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:05:15,458 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:05:15,576 - father_agent.py:436 - Training agent on-policy
2025-08-03 23:05:31,979 - father_agent.py:386 - Step: 0, Training loss: 6.096744537353516
2025-08-03 23:05:33,936 - father_agent.py:386 - Step: 5, Training loss: 6.906781196594238
2025-08-03 23:05:35,873 - father_agent.py:386 - Step: 10, Training loss: 7.056976795196533
2025-08-03 23:05:37,832 - father_agent.py:386 - Step: 15, Training loss: 6.192890167236328
2025-08-03 23:05:39,764 - father_agent.py:386 - Step: 20, Training loss: 6.241320610046387
2025-08-03 23:05:41,702 - father_agent.py:386 - Step: 25, Training loss: 6.241296291351318
2025-08-03 23:05:43,658 - father_agent.py:386 - Step: 30, Training loss: 6.155836582183838
2025-08-03 23:05:45,601 - father_agent.py:386 - Step: 35, Training loss: 7.228853225708008
2025-08-03 23:05:47,550 - father_agent.py:386 - Step: 40, Training loss: 5.337039470672607
2025-08-03 23:05:49,521 - father_agent.py:386 - Step: 45, Training loss: 6.090299129486084
2025-08-03 23:05:51,449 - father_agent.py:386 - Step: 50, Training loss: 6.829251766204834
2025-08-03 23:05:53,392 - father_agent.py:386 - Step: 55, Training loss: 5.809612274169922
2025-08-03 23:05:55,351 - father_agent.py:386 - Step: 60, Training loss: 7.182093143463135
2025-08-03 23:05:57,315 - father_agent.py:386 - Step: 65, Training loss: 4.993623733520508
2025-08-03 23:05:59,280 - father_agent.py:386 - Step: 70, Training loss: 7.837651252746582
2025-08-03 23:06:01,216 - father_agent.py:386 - Step: 75, Training loss: 6.480338096618652
2025-08-03 23:06:03,173 - father_agent.py:386 - Step: 80, Training loss: 6.367134094238281
2025-08-03 23:06:05,135 - father_agent.py:386 - Step: 85, Training loss: 6.087503910064697
2025-08-03 23:06:07,080 - father_agent.py:386 - Step: 90, Training loss: 6.353573799133301
2025-08-03 23:06:09,044 - father_agent.py:386 - Step: 95, Training loss: 5.550953388214111
2025-08-03 23:06:10,977 - father_agent.py:386 - Step: 100, Training loss: 7.07266092300415
2025-08-03 23:06:11,363 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:06:11,366 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:06:22,015 - evaluation_results_class.py:131 - Average Return = 123.02755737304688
2025-08-03 23:06:22,015 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.2986421585083
2025-08-03 23:06:22,015 - evaluation_results_class.py:135 - Average Discounted Reward = 6.287877559661865
2025-08-03 23:06:22,015 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9979432332373509
2025-08-03 23:06:22,015 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:06:22,016 - evaluation_results_class.py:141 - Variance of Return = 18050.890625
2025-08-03 23:06:22,016 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:06:22,016 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:06:22,016 - evaluation_results_class.py:147 - Average Episode Length = 111.19251336898395
2025-08-03 23:06:22,016 - evaluation_results_class.py:149 - Counted Episodes = 2431
2025-08-03 23:06:22,267 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:06:22,278 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:06:26,378 - father_agent.py:386 - Step: 105, Training loss: 5.802278518676758
2025-08-03 23:06:28,280 - father_agent.py:386 - Step: 110, Training loss: 6.425189971923828
2025-08-03 23:06:30,199 - father_agent.py:386 - Step: 115, Training loss: 7.497577667236328
2025-08-03 23:06:32,085 - father_agent.py:386 - Step: 120, Training loss: 7.330782413482666
2025-08-03 23:06:34,009 - father_agent.py:386 - Step: 125, Training loss: 6.676937580108643
2025-08-03 23:06:35,917 - father_agent.py:386 - Step: 130, Training loss: 6.480403900146484
2025-08-03 23:06:37,817 - father_agent.py:386 - Step: 135, Training loss: 5.653725624084473
2025-08-03 23:06:39,833 - father_agent.py:386 - Step: 140, Training loss: 6.552276134490967
2025-08-03 23:06:41,903 - father_agent.py:386 - Step: 145, Training loss: 5.765214920043945
2025-08-03 23:06:43,835 - father_agent.py:386 - Step: 150, Training loss: 7.07012414932251
2025-08-03 23:06:45,793 - father_agent.py:386 - Step: 155, Training loss: 5.569356441497803
2025-08-03 23:06:47,790 - father_agent.py:386 - Step: 160, Training loss: 6.793629169464111
2025-08-03 23:06:49,809 - father_agent.py:386 - Step: 165, Training loss: 7.6175055503845215
2025-08-03 23:06:51,780 - father_agent.py:386 - Step: 170, Training loss: 7.205887317657471
2025-08-03 23:06:53,730 - father_agent.py:386 - Step: 175, Training loss: 6.5498046875
2025-08-03 23:06:55,674 - father_agent.py:386 - Step: 180, Training loss: 7.073296070098877
2025-08-03 23:06:57,579 - father_agent.py:386 - Step: 185, Training loss: 5.959825038909912
2025-08-03 23:06:59,472 - father_agent.py:386 - Step: 190, Training loss: 6.780190467834473
2025-08-03 23:07:01,378 - father_agent.py:386 - Step: 195, Training loss: 6.372812271118164
2025-08-03 23:07:03,268 - father_agent.py:386 - Step: 200, Training loss: 6.453119277954102
2025-08-03 23:07:03,528 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:07:03,531 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:07:14,236 - evaluation_results_class.py:131 - Average Return = 112.03561401367188
2025-08-03 23:07:14,237 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.201132774353027
2025-08-03 23:07:14,237 - evaluation_results_class.py:135 - Average Discounted Reward = 5.891147613525391
2025-08-03 23:07:14,237 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9987859166329421
2025-08-03 23:07:14,237 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:07:14,237 - evaluation_results_class.py:141 - Variance of Return = 15880.8818359375
2025-08-03 23:07:14,237 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:07:14,237 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:07:14,237 - evaluation_results_class.py:147 - Average Episode Length = 110.2043707001214
2025-08-03 23:07:14,237 - evaluation_results_class.py:149 - Counted Episodes = 2471
2025-08-03 23:07:14,494 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:07:14,506 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:07:18,602 - father_agent.py:386 - Step: 205, Training loss: 6.956178665161133
2025-08-03 23:07:20,551 - father_agent.py:386 - Step: 210, Training loss: 6.724435806274414
2025-08-03 23:07:22,459 - father_agent.py:386 - Step: 215, Training loss: 8.074950218200684
2025-08-03 23:07:24,363 - father_agent.py:386 - Step: 220, Training loss: 6.505043029785156
2025-08-03 23:07:26,268 - father_agent.py:386 - Step: 225, Training loss: 7.906281471252441
2025-08-03 23:07:28,155 - father_agent.py:386 - Step: 230, Training loss: 6.762314319610596
2025-08-03 23:07:30,099 - father_agent.py:386 - Step: 235, Training loss: 7.745492935180664
2025-08-03 23:07:32,026 - father_agent.py:386 - Step: 240, Training loss: 7.346189975738525
2025-08-03 23:07:33,941 - father_agent.py:386 - Step: 245, Training loss: 7.695106029510498
2025-08-03 23:07:35,851 - father_agent.py:386 - Step: 250, Training loss: 5.982850551605225
2025-08-03 23:07:37,764 - father_agent.py:386 - Step: 255, Training loss: 6.88217830657959
2025-08-03 23:07:39,683 - father_agent.py:386 - Step: 260, Training loss: 6.538552284240723
2025-08-03 23:07:41,625 - father_agent.py:386 - Step: 265, Training loss: 6.039910316467285
2025-08-03 23:07:43,529 - father_agent.py:386 - Step: 270, Training loss: 6.6833815574646
2025-08-03 23:07:45,436 - father_agent.py:386 - Step: 275, Training loss: 6.328505516052246
2025-08-03 23:07:47,344 - father_agent.py:386 - Step: 280, Training loss: 6.375313758850098
2025-08-03 23:07:49,262 - father_agent.py:386 - Step: 285, Training loss: 7.1399946212768555
2025-08-03 23:07:51,190 - father_agent.py:386 - Step: 290, Training loss: 5.657207489013672
2025-08-03 23:07:53,137 - father_agent.py:386 - Step: 295, Training loss: 7.320072650909424
2025-08-03 23:07:55,043 - father_agent.py:386 - Step: 300, Training loss: 7.008924961090088
2025-08-03 23:07:55,298 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:07:55,300 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:08:06,267 - evaluation_results_class.py:131 - Average Return = 110.6259765625
2025-08-03 23:08:06,268 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.057120323181152
2025-08-03 23:08:06,268 - evaluation_results_class.py:135 - Average Discounted Reward = 5.780991077423096
2025-08-03 23:08:06,268 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9972613458528952
2025-08-03 23:08:06,268 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:08:06,268 - evaluation_results_class.py:141 - Variance of Return = 15920.265625
2025-08-03 23:08:06,268 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:08:06,268 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:08:06,268 - evaluation_results_class.py:147 - Average Episode Length = 108.05594679186228
2025-08-03 23:08:06,268 - evaluation_results_class.py:149 - Counted Episodes = 2556
2025-08-03 23:08:06,523 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:08:06,535 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:08:10,647 - father_agent.py:386 - Step: 305, Training loss: 5.152398586273193
2025-08-03 23:08:12,525 - father_agent.py:386 - Step: 310, Training loss: 5.6978559494018555
2025-08-03 23:08:14,432 - father_agent.py:386 - Step: 315, Training loss: 6.794781684875488
2025-08-03 23:08:16,323 - father_agent.py:386 - Step: 320, Training loss: 5.343448638916016
2025-08-03 23:08:18,225 - father_agent.py:386 - Step: 325, Training loss: 8.681769371032715
2025-08-03 23:08:20,119 - father_agent.py:386 - Step: 330, Training loss: 6.797784805297852
2025-08-03 23:08:22,006 - father_agent.py:386 - Step: 335, Training loss: 7.4016947746276855
2025-08-03 23:08:23,944 - father_agent.py:386 - Step: 340, Training loss: 7.418524265289307
2025-08-03 23:08:25,874 - father_agent.py:386 - Step: 345, Training loss: 7.0426411628723145
2025-08-03 23:08:27,808 - father_agent.py:386 - Step: 350, Training loss: 7.272720813751221
2025-08-03 23:08:29,710 - father_agent.py:386 - Step: 355, Training loss: 6.842064380645752
2025-08-03 23:08:31,590 - father_agent.py:386 - Step: 360, Training loss: 6.057755947113037
2025-08-03 23:08:33,485 - father_agent.py:386 - Step: 365, Training loss: 6.242598533630371
2025-08-03 23:08:35,377 - father_agent.py:386 - Step: 370, Training loss: 7.546728134155273
2025-08-03 23:08:37,263 - father_agent.py:386 - Step: 375, Training loss: 7.213719367980957
2025-08-03 23:08:39,231 - father_agent.py:386 - Step: 380, Training loss: 5.785607814788818
2025-08-03 23:08:41,270 - father_agent.py:386 - Step: 385, Training loss: 6.426328182220459
2025-08-03 23:08:43,282 - father_agent.py:386 - Step: 390, Training loss: 6.247206687927246
2025-08-03 23:08:45,293 - father_agent.py:386 - Step: 395, Training loss: 6.554479598999023
2025-08-03 23:08:47,303 - father_agent.py:386 - Step: 400, Training loss: 8.003137588500977
2025-08-03 23:08:47,561 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:08:47,564 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:08:58,184 - evaluation_results_class.py:131 - Average Return = 106.57353210449219
2025-08-03 23:08:58,185 - evaluation_results_class.py:133 - Average Virtual Goal Value = 11.657353401184082
2025-08-03 23:08:58,185 - evaluation_results_class.py:135 - Average Discounted Reward = 5.746702671051025
2025-08-03 23:08:58,185 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-03 23:08:58,185 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:08:58,185 - evaluation_results_class.py:141 - Variance of Return = 14218.486328125
2025-08-03 23:08:58,185 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:08:58,185 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:08:58,185 - evaluation_results_class.py:147 - Average Episode Length = 106.21420256111757
2025-08-03 23:08:58,185 - evaluation_results_class.py:149 - Counted Episodes = 2577
2025-08-03 23:08:58,448 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:08:58,459 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:09:02,560 - father_agent.py:386 - Step: 405, Training loss: 6.625840187072754
2025-08-03 23:09:04,473 - father_agent.py:386 - Step: 410, Training loss: 6.318264484405518
2025-08-03 23:09:06,376 - father_agent.py:386 - Step: 415, Training loss: 8.752245903015137
2025-08-03 23:09:08,272 - father_agent.py:386 - Step: 420, Training loss: 6.943580627441406
2025-08-03 23:09:10,187 - father_agent.py:386 - Step: 425, Training loss: 6.356225490570068
2025-08-03 23:09:12,115 - father_agent.py:386 - Step: 430, Training loss: 6.8781938552856445
2025-08-03 23:09:14,051 - father_agent.py:386 - Step: 435, Training loss: 6.529453754425049
2025-08-03 23:09:15,969 - father_agent.py:386 - Step: 440, Training loss: 6.384350776672363
2025-08-03 23:09:17,873 - father_agent.py:386 - Step: 445, Training loss: 6.778975009918213
2025-08-03 23:09:19,785 - father_agent.py:386 - Step: 450, Training loss: 6.184689521789551
2025-08-03 23:09:21,756 - father_agent.py:386 - Step: 455, Training loss: 7.959903240203857
2025-08-03 23:09:23,703 - father_agent.py:386 - Step: 460, Training loss: 6.22484016418457
2025-08-03 23:09:25,652 - father_agent.py:386 - Step: 465, Training loss: 7.052396774291992
2025-08-03 23:09:27,580 - father_agent.py:386 - Step: 470, Training loss: 5.250891208648682
2025-08-03 23:09:29,487 - father_agent.py:386 - Step: 475, Training loss: 5.564207553863525
2025-08-03 23:09:31,409 - father_agent.py:386 - Step: 480, Training loss: 5.685365676879883
2025-08-03 23:09:33,333 - father_agent.py:386 - Step: 485, Training loss: 7.186689376831055
2025-08-03 23:09:35,271 - father_agent.py:386 - Step: 490, Training loss: 6.1394362449646
2025-08-03 23:09:37,196 - father_agent.py:386 - Step: 495, Training loss: 7.278270721435547
2025-08-03 23:09:39,104 - father_agent.py:386 - Step: 500, Training loss: 6.772805213928223
2025-08-03 23:09:39,366 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:09:39,369 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:09:50,269 - evaluation_results_class.py:131 - Average Return = 117.4214859008789
2025-08-03 23:09:50,269 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.738016128540039
2025-08-03 23:09:50,269 - evaluation_results_class.py:135 - Average Discounted Reward = 6.023939609527588
2025-08-03 23:09:50,269 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9979338842975206
2025-08-03 23:09:50,269 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:09:50,270 - evaluation_results_class.py:141 - Variance of Return = 17396.328125
2025-08-03 23:09:50,270 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:09:50,270 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:09:50,270 - evaluation_results_class.py:147 - Average Episode Length = 110.22685950413224
2025-08-03 23:09:50,270 - evaluation_results_class.py:149 - Counted Episodes = 2420
2025-08-03 23:09:50,536 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:09:50,548 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:09:50,666 - father_agent.py:547 - Training finished.
2025-08-03 23:09:50,824 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:09:50,827 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:09:50,829 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-03 23:10:01,446 - evaluation_results_class.py:131 - Average Return = 116.52104949951172
2025-08-03 23:10:01,446 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.648133277893066
2025-08-03 23:10:01,446 - evaluation_results_class.py:135 - Average Discounted Reward = 6.025999546051025
2025-08-03 23:10:01,446 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9980142970611596
2025-08-03 23:10:01,446 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:10:01,446 - evaluation_results_class.py:141 - Variance of Return = 18118.474609375
2025-08-03 23:10:01,447 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:10:01,447 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:10:01,447 - evaluation_results_class.py:147 - Average Episode Length = 110.10762509928514
2025-08-03 23:10:01,447 - evaluation_results_class.py:149 - Counted Episodes = 2518
2025-08-03 23:10:01,706 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:10:01,708 - self_interpretable_extractor.py:286 - True
2025-08-03 23:10:01,721 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:10:19,369 - evaluation_results_class.py:131 - Average Return = 150.2460479736328
2025-08-03 23:10:19,369 - evaluation_results_class.py:133 - Average Virtual Goal Value = 16.00790786743164
2025-08-03 23:10:19,369 - evaluation_results_class.py:135 - Average Discounted Reward = 6.741505146026611
2025-08-03 23:10:19,369 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9916520210896309
2025-08-03 23:10:19,369 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:10:19,369 - evaluation_results_class.py:141 - Variance of Return = 27704.509765625
2025-08-03 23:10:19,369 - evaluation_results_class.py:143 - Current Best Return = 150.2460479736328
2025-08-03 23:10:19,370 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9916520210896309
2025-08-03 23:10:19,370 - evaluation_results_class.py:147 - Average Episode Length = 130.63488576449913
2025-08-03 23:10:19,370 - evaluation_results_class.py:149 - Counted Episodes = 2276
2025-08-03 23:10:19,370 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-03 23:10:19,370 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 11201 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 572, 574, 576, 578, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 606, 612, 616, 618, 622, 624, 628, 634, 636, 638, 640, 642, 646, 650, 651}
Buffer 1
2025-08-03 23:11:31,690 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 22643 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 572, 574, 576, 578, 580, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 630, 634, 636, 638, 640, 642, 644, 646, 648, 650, 651}
Buffer 2
2025-08-03 23:12:43,738 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 34025 trajectories
Learned trajectory lengths  {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518, 520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598, 600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 630, 632, 634, 636, 638, 640, 642, 644, 646, 648, 650, 651}
All trajectories collected
2025-08-03 23:13:55,272 - self_interpretable_extractor.py:346 - Data sampled
2025-08-03 23:13:55,273 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 34025 trajectories
DOF: 0
DOF: 0
DOF: 2
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_11.dot.
Learned FSC of size 2
2025-08-03 23:14:24,569 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-03 23:14:40,765 - evaluation_results_class.py:131 - Average Return = 131.46676635742188
2025-08-03 23:14:40,766 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.138026237487793
2025-08-03 23:14:40,766 - evaluation_results_class.py:135 - Average Discounted Reward = 6.309540748596191
2025-08-03 23:14:40,766 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9956744003145891
2025-08-03 23:14:40,766 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:14:40,766 - evaluation_results_class.py:141 - Variance of Return = 21833.75
2025-08-03 23:14:40,766 - evaluation_results_class.py:143 - Current Best Return = 131.46676635742188
2025-08-03 23:14:40,766 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9956744003145891
2025-08-03 23:14:40,766 - evaluation_results_class.py:147 - Average Episode Length = 119.85489579237121
2025-08-03 23:14:40,766 - evaluation_results_class.py:149 - Counted Episodes = 2543
FSC Result: {'best_episode_return': 14.138026, 'best_return': 131.46677, 'goal_value': 0.0, 'returns_episodic': [14.138026], 'returns': [131.46677], 'reach_probs': [0.9956744003145891], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.9956744003145891, 'losses': [], 'best_updated': True, 'each_episode_variance': [21833.75], 'each_episode_virtual_variance': [217.26094], 'combined_variance': [26406.824], 'num_episodes': [2543], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [119.85489579237121], 'counted_episodes': [2543], 'discounted_rewards': [6.3095407], 'new_pomdp_iteration_numbers': []}
[[1, 2, 3], [4], [0]]
2025-08-03 23:14:40,852 - statistic.py:67 - synthesis initiated, design space: 131220
2025-08-03 23:14:40,852 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-03 23:14:40,961 - synthesizer_ar.py:122 - value 1046.8053 achieved after 6876.5 seconds
2025-08-03 23:14:40,967 - synthesizer_ar.py:122 - value 1045.9133 achieved after 6876.5 seconds
2025-08-03 23:14:40,973 - synthesizer_ar.py:122 - value 1045.6698 achieved after 6876.51 seconds
2025-08-03 23:14:41,114 - synthesizer_ar.py:122 - value 1038.944 achieved after 6876.65 seconds
2025-08-03 23:14:41,121 - synthesizer_ar.py:122 - value 1038.3399 achieved after 6876.66 seconds
2025-08-03 23:14:41,161 - synthesizer_ar.py:122 - value 1036.2154 achieved after 6876.7 seconds
2025-08-03 23:14:41,167 - synthesizer_ar.py:122 - value 1035.5376 achieved after 6876.7 seconds
2025-08-03 23:14:41,380 - synthesizer_ar.py:122 - value 1002.7957 achieved after 6876.92 seconds
2025-08-03 23:14:41,386 - synthesizer_ar.py:122 - value 1002.4794 achieved after 6876.92 seconds
2025-08-03 23:14:41,392 - synthesizer_ar.py:122 - value 1002.3938 achieved after 6876.93 seconds
2025-08-03 23:14:41,546 - synthesizer_ar.py:122 - value 702.0339 achieved after 6877.08 seconds
2025-08-03 23:14:41,552 - synthesizer_ar.py:122 - value 702.0269 achieved after 6877.09 seconds
2025-08-03 23:14:41,582 - synthesizer_ar.py:122 - value 701.9262 achieved after 6877.12 seconds
2025-08-03 23:14:41,588 - synthesizer_ar.py:122 - value 701.9073 achieved after 6877.13 seconds
2025-08-03 23:14:41,695 - synthesizer_ar.py:122 - value 516.8043 achieved after 6877.23 seconds
2025-08-03 23:14:41,880 - synthesizer_ar.py:122 - value 477.8714 achieved after 6877.42 seconds
2025-08-03 23:14:41,886 - synthesizer_ar.py:122 - value 477.854 achieved after 6877.42 seconds
2025-08-03 23:14:41,892 - synthesizer_ar.py:122 - value 477.8493 achieved after 6877.43 seconds
2025-08-03 23:14:41,957 - synthesizer_ar.py:122 - value 408.3622 achieved after 6877.49 seconds
2025-08-03 23:14:42,073 - synthesizer_ar.py:122 - value 145.4414 achieved after 6877.61 seconds
2025-08-03 23:14:42,123 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-03 23:14:42,123 - synthesizer.py:193 - P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 23:14:42,125 - synthesizer.py:198 - double-checking specification satisfiability:  : 145.44140442607164
--------------------
Synthesis summary:
optimality objective: R{"rew"}min=? [F (bat = 0)] 

method: AR, synthesis time: 1.27 s
number of holes: 9, family size: 131220, quotient: 2050 states / 11335 actions
explored: 100 %
MDP stats: avg MDP size: 2050, iterations: 187

optimum: 145.441404
--------------------
2025-08-03 23:14:42,125 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
2025-08-03 23:14:42,152 - robust_rl_trainer.py:432 - Iteration 13 of pure RL loop
2025-08-03 23:14:42,192 - storm_vec_env.py:70 - Computing row map
2025-08-03 23:14:42,207 - storm_vec_env.py:97 - Computing transitions
2025-08-03 23:14:42,242 - storm_vec_env.py:100 - Computing allowed actions
2025-08-03 23:14:42,242 - storm_vec_env.py:114 - Computing sinks
2025-08-03 23:14:42,242 - storm_vec_env.py:119 - Computing raw rewards
2025-08-03 23:14:42,250 - storm_vec_env.py:143 - Computing labels
2025-08-03 23:14:42,251 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-03 23:14:42,251 - storm_vec_env.py:161 - Computing metalabels
2025-08-03 23:14:42,251 - storm_vec_env.py:175 - Computing observations
2025-08-03 23:14:42,434 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-03 23:14:42,435 - father_agent.py:540 - Before training evaluation.
2025-08-03 23:14:42,605 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:14:42,607 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:14:53,376 - evaluation_results_class.py:131 - Average Return = 115.98210906982422
2025-08-03 23:14:53,376 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.59495735168457
2025-08-03 23:14:53,376 - evaluation_results_class.py:135 - Average Discounted Reward = 6.125445365905762
2025-08-03 23:14:53,376 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9983733224888166
2025-08-03 23:14:53,376 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:14:53,376 - evaluation_results_class.py:141 - Variance of Return = 15461.3681640625
2025-08-03 23:14:53,376 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:14:53,376 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:14:53,377 - evaluation_results_class.py:147 - Average Episode Length = 109.72509150061
2025-08-03 23:14:53,377 - evaluation_results_class.py:149 - Counted Episodes = 2459
2025-08-03 23:14:53,641 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:14:53,653 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:14:53,771 - father_agent.py:436 - Training agent on-policy
2025-08-03 23:15:02,447 - father_agent.py:386 - Step: 0, Training loss: 6.732389450073242
2025-08-03 23:15:04,387 - father_agent.py:386 - Step: 5, Training loss: 7.429520606994629
2025-08-03 23:15:06,296 - father_agent.py:386 - Step: 10, Training loss: 6.002480983734131
2025-08-03 23:15:08,250 - father_agent.py:386 - Step: 15, Training loss: 5.852415084838867
2025-08-03 23:15:10,206 - father_agent.py:386 - Step: 20, Training loss: 5.4663286209106445
2025-08-03 23:15:12,136 - father_agent.py:386 - Step: 25, Training loss: 5.241010665893555
2025-08-03 23:15:14,056 - father_agent.py:386 - Step: 30, Training loss: 6.016822814941406
2025-08-03 23:15:15,993 - father_agent.py:386 - Step: 35, Training loss: 7.598845958709717
2025-08-03 23:15:17,950 - father_agent.py:386 - Step: 40, Training loss: 6.008291721343994
2025-08-03 23:15:19,901 - father_agent.py:386 - Step: 45, Training loss: 7.2215166091918945
2025-08-03 23:15:21,821 - father_agent.py:386 - Step: 50, Training loss: 5.622899532318115
2025-08-03 23:15:23,747 - father_agent.py:386 - Step: 55, Training loss: 6.425110816955566
2025-08-03 23:15:25,675 - father_agent.py:386 - Step: 60, Training loss: 7.463141918182373
2025-08-03 23:15:27,575 - father_agent.py:386 - Step: 65, Training loss: 7.367408275604248
2025-08-03 23:15:29,494 - father_agent.py:386 - Step: 70, Training loss: 7.10537052154541
2025-08-03 23:15:31,431 - father_agent.py:386 - Step: 75, Training loss: 5.5943074226379395
2025-08-03 23:15:33,350 - father_agent.py:386 - Step: 80, Training loss: 7.785660743713379
2025-08-03 23:15:35,277 - father_agent.py:386 - Step: 85, Training loss: 5.9544267654418945
2025-08-03 23:15:37,201 - father_agent.py:386 - Step: 90, Training loss: 6.521647930145264
2025-08-03 23:15:39,114 - father_agent.py:386 - Step: 95, Training loss: 7.292861461639404
2025-08-03 23:15:41,024 - father_agent.py:386 - Step: 100, Training loss: 7.939040184020996
2025-08-03 23:15:41,289 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:15:41,292 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:15:51,974 - evaluation_results_class.py:131 - Average Return = 112.13855743408203
2025-08-03 23:15:51,974 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.210843086242676
2025-08-03 23:15:51,974 - evaluation_results_class.py:135 - Average Discounted Reward = 6.069735050201416
2025-08-03 23:15:51,974 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9984939759036144
2025-08-03 23:15:51,974 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:15:51,974 - evaluation_results_class.py:141 - Variance of Return = 15814.1025390625
2025-08-03 23:15:51,974 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:15:51,974 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:15:51,975 - evaluation_results_class.py:147 - Average Episode Length = 103.09789156626506
2025-08-03 23:15:51,975 - evaluation_results_class.py:149 - Counted Episodes = 2656
2025-08-03 23:15:52,241 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:15:52,254 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:15:56,371 - father_agent.py:386 - Step: 105, Training loss: 7.385234355926514
2025-08-03 23:15:58,277 - father_agent.py:386 - Step: 110, Training loss: 6.373907089233398
2025-08-03 23:16:00,161 - father_agent.py:386 - Step: 115, Training loss: 5.797201156616211
2025-08-03 23:16:02,078 - father_agent.py:386 - Step: 120, Training loss: 5.698514461517334
2025-08-03 23:16:03,965 - father_agent.py:386 - Step: 125, Training loss: 6.218400001525879
2025-08-03 23:16:05,872 - father_agent.py:386 - Step: 130, Training loss: 5.969183444976807
2025-08-03 23:16:07,791 - father_agent.py:386 - Step: 135, Training loss: 7.432026386260986
2025-08-03 23:16:09,709 - father_agent.py:386 - Step: 140, Training loss: 6.370108127593994
2025-08-03 23:16:11,599 - father_agent.py:386 - Step: 145, Training loss: 6.960974216461182
2025-08-03 23:16:13,515 - father_agent.py:386 - Step: 150, Training loss: 7.835877418518066
2025-08-03 23:16:15,431 - father_agent.py:386 - Step: 155, Training loss: 6.072677135467529
2025-08-03 23:16:17,325 - father_agent.py:386 - Step: 160, Training loss: 6.537380218505859
2025-08-03 23:16:19,257 - father_agent.py:386 - Step: 165, Training loss: 6.646130084991455
2025-08-03 23:16:21,202 - father_agent.py:386 - Step: 170, Training loss: 6.815672397613525
2025-08-03 23:16:23,142 - father_agent.py:386 - Step: 175, Training loss: 6.508021354675293
2025-08-03 23:16:25,063 - father_agent.py:386 - Step: 180, Training loss: 7.909436225891113
2025-08-03 23:16:26,963 - father_agent.py:386 - Step: 185, Training loss: 6.1667280197143555
2025-08-03 23:16:28,881 - father_agent.py:386 - Step: 190, Training loss: 5.913687229156494
2025-08-03 23:16:30,779 - father_agent.py:386 - Step: 195, Training loss: 7.484854698181152
2025-08-03 23:16:32,686 - father_agent.py:386 - Step: 200, Training loss: 7.252766132354736
2025-08-03 23:16:32,963 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:16:32,966 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:16:43,690 - evaluation_results_class.py:131 - Average Return = 110.22816467285156
2025-08-03 23:16:43,690 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.018095970153809
2025-08-03 23:16:43,690 - evaluation_results_class.py:135 - Average Discounted Reward = 5.977365493774414
2025-08-03 23:16:43,690 - evaluation_results_class.py:137 - Goal Reach Probability = 0.997639653815893
2025-08-03 23:16:43,690 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:16:43,690 - evaluation_results_class.py:141 - Variance of Return = 14789.7998046875
2025-08-03 23:16:43,690 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:16:43,690 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:16:43,691 - evaluation_results_class.py:147 - Average Episode Length = 107.73249409913454
2025-08-03 23:16:43,691 - evaluation_results_class.py:149 - Counted Episodes = 2542
2025-08-03 23:16:43,954 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:16:43,967 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:16:48,137 - father_agent.py:386 - Step: 205, Training loss: 7.201142311096191
2025-08-03 23:16:50,060 - father_agent.py:386 - Step: 210, Training loss: 5.830002307891846
2025-08-03 23:16:51,980 - father_agent.py:386 - Step: 215, Training loss: 6.407938003540039
2025-08-03 23:16:53,869 - father_agent.py:386 - Step: 220, Training loss: 5.732900142669678
2025-08-03 23:16:55,766 - father_agent.py:386 - Step: 225, Training loss: 7.909572124481201
2025-08-03 23:16:57,681 - father_agent.py:386 - Step: 230, Training loss: 7.0483198165893555
2025-08-03 23:16:59,613 - father_agent.py:386 - Step: 235, Training loss: 6.928548336029053
2025-08-03 23:17:01,529 - father_agent.py:386 - Step: 240, Training loss: 6.5977654457092285
2025-08-03 23:17:03,437 - father_agent.py:386 - Step: 245, Training loss: 7.062981128692627
2025-08-03 23:17:05,358 - father_agent.py:386 - Step: 250, Training loss: 7.454076290130615
2025-08-03 23:17:07,263 - father_agent.py:386 - Step: 255, Training loss: 7.88364839553833
2025-08-03 23:17:09,188 - father_agent.py:386 - Step: 260, Training loss: 5.476229667663574
2025-08-03 23:17:11,132 - father_agent.py:386 - Step: 265, Training loss: 7.214114665985107
2025-08-03 23:17:13,142 - father_agent.py:386 - Step: 270, Training loss: 6.034788608551025
2025-08-03 23:17:15,179 - father_agent.py:386 - Step: 275, Training loss: 6.151771068572998
2025-08-03 23:17:17,190 - father_agent.py:386 - Step: 280, Training loss: 6.611561298370361
2025-08-03 23:17:19,163 - father_agent.py:386 - Step: 285, Training loss: 6.591022491455078
2025-08-03 23:17:21,169 - father_agent.py:386 - Step: 290, Training loss: 6.742881774902344
2025-08-03 23:17:23,182 - father_agent.py:386 - Step: 295, Training loss: 7.923271656036377
2025-08-03 23:17:25,191 - father_agent.py:386 - Step: 300, Training loss: 6.610879898071289
2025-08-03 23:17:25,464 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:17:25,466 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:17:36,357 - evaluation_results_class.py:131 - Average Return = 115.07396697998047
2025-08-03 23:17:36,357 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.504998207092285
2025-08-03 23:17:36,357 - evaluation_results_class.py:135 - Average Discounted Reward = 6.03063440322876
2025-08-03 23:17:36,357 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9988004798080767
2025-08-03 23:17:36,357 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:17:36,357 - evaluation_results_class.py:141 - Variance of Return = 16026.912109375
2025-08-03 23:17:36,357 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:17:36,357 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:17:36,357 - evaluation_results_class.py:147 - Average Episode Length = 110.92323070771691
2025-08-03 23:17:36,357 - evaluation_results_class.py:149 - Counted Episodes = 2501
2025-08-03 23:17:36,621 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:17:36,633 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:17:40,806 - father_agent.py:386 - Step: 305, Training loss: 6.235853672027588
2025-08-03 23:17:42,699 - father_agent.py:386 - Step: 310, Training loss: 6.269383430480957
2025-08-03 23:17:44,602 - father_agent.py:386 - Step: 315, Training loss: 7.084481716156006
2025-08-03 23:17:46,498 - father_agent.py:386 - Step: 320, Training loss: 7.283349990844727
2025-08-03 23:17:48,429 - father_agent.py:386 - Step: 325, Training loss: 7.396872043609619
2025-08-03 23:17:50,345 - father_agent.py:386 - Step: 330, Training loss: 7.028055191040039
2025-08-03 23:17:52,258 - father_agent.py:386 - Step: 335, Training loss: 7.48617696762085
2025-08-03 23:17:54,177 - father_agent.py:386 - Step: 340, Training loss: 7.434063911437988
2025-08-03 23:17:56,102 - father_agent.py:386 - Step: 345, Training loss: 6.863832950592041
2025-08-03 23:17:58,037 - father_agent.py:386 - Step: 350, Training loss: 7.087906837463379
2025-08-03 23:18:00,025 - father_agent.py:386 - Step: 355, Training loss: 6.95035457611084
2025-08-03 23:18:02,067 - father_agent.py:386 - Step: 360, Training loss: 6.605113506317139
2025-08-03 23:18:04,109 - father_agent.py:386 - Step: 365, Training loss: 7.602378845214844
2025-08-03 23:18:06,163 - father_agent.py:386 - Step: 370, Training loss: 7.451472282409668
2025-08-03 23:18:08,169 - father_agent.py:386 - Step: 375, Training loss: 6.709833145141602
2025-08-03 23:18:10,191 - father_agent.py:386 - Step: 380, Training loss: 5.990342140197754
2025-08-03 23:18:12,161 - father_agent.py:386 - Step: 385, Training loss: 6.466031551361084
2025-08-03 23:18:14,146 - father_agent.py:386 - Step: 390, Training loss: 5.692947864532471
2025-08-03 23:18:16,134 - father_agent.py:386 - Step: 395, Training loss: 6.805497169494629
2025-08-03 23:18:18,099 - father_agent.py:386 - Step: 400, Training loss: 8.732268333435059
2025-08-03 23:18:18,376 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:18:18,379 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:18:29,499 - evaluation_results_class.py:131 - Average Return = 128.65353393554688
2025-08-03 23:18:29,499 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.860426902770996
2025-08-03 23:18:29,499 - evaluation_results_class.py:135 - Average Discounted Reward = 6.438501358032227
2025-08-03 23:18:29,499 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9975369458128078
2025-08-03 23:18:29,499 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:18:29,499 - evaluation_results_class.py:141 - Variance of Return = 21365.509765625
2025-08-03 23:18:29,499 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:18:29,499 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:18:29,499 - evaluation_results_class.py:147 - Average Episode Length = 111.64860426929393
2025-08-03 23:18:29,499 - evaluation_results_class.py:149 - Counted Episodes = 2436
2025-08-03 23:18:29,778 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:18:29,791 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:18:34,085 - father_agent.py:386 - Step: 405, Training loss: 6.826513290405273
2025-08-03 23:18:36,103 - father_agent.py:386 - Step: 410, Training loss: 6.384995937347412
2025-08-03 23:18:38,110 - father_agent.py:386 - Step: 415, Training loss: 7.604390621185303
2025-08-03 23:18:40,233 - father_agent.py:386 - Step: 420, Training loss: 6.034328460693359
2025-08-03 23:18:42,307 - father_agent.py:386 - Step: 425, Training loss: 6.040173053741455
2025-08-03 23:18:44,225 - father_agent.py:386 - Step: 430, Training loss: 6.377780437469482
2025-08-03 23:18:46,151 - father_agent.py:386 - Step: 435, Training loss: 6.618552207946777
2025-08-03 23:18:48,071 - father_agent.py:386 - Step: 440, Training loss: 6.463851451873779
2025-08-03 23:18:50,012 - father_agent.py:386 - Step: 445, Training loss: 6.62870454788208
2025-08-03 23:18:51,932 - father_agent.py:386 - Step: 450, Training loss: 6.949215412139893
2025-08-03 23:18:53,838 - father_agent.py:386 - Step: 455, Training loss: 6.446895122528076
2025-08-03 23:18:55,758 - father_agent.py:386 - Step: 460, Training loss: 6.373429775238037
2025-08-03 23:18:57,679 - father_agent.py:386 - Step: 465, Training loss: 6.681194305419922
2025-08-03 23:18:59,628 - father_agent.py:386 - Step: 470, Training loss: 6.666213512420654
2025-08-03 23:19:01,568 - father_agent.py:386 - Step: 475, Training loss: 6.078893661499023
2025-08-03 23:19:03,499 - father_agent.py:386 - Step: 480, Training loss: 7.884890079498291
2025-08-03 23:19:05,409 - father_agent.py:386 - Step: 485, Training loss: 7.031137943267822
2025-08-03 23:19:07,345 - father_agent.py:386 - Step: 490, Training loss: 6.584571838378906
2025-08-03 23:19:09,288 - father_agent.py:386 - Step: 495, Training loss: 7.022798538208008
2025-08-03 23:19:11,194 - father_agent.py:386 - Step: 500, Training loss: 6.6572699546813965
2025-08-03 23:19:11,480 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:19:11,482 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:19:22,176 - evaluation_results_class.py:131 - Average Return = 120.80327606201172
2025-08-03 23:19:22,176 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.077049255371094
2025-08-03 23:19:22,176 - evaluation_results_class.py:135 - Average Discounted Reward = 6.253896713256836
2025-08-03 23:19:22,176 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9983606557377049
2025-08-03 23:19:22,176 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:19:22,176 - evaluation_results_class.py:141 - Variance of Return = 17866.240234375
2025-08-03 23:19:22,176 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:19:22,177 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:19:22,177 - evaluation_results_class.py:147 - Average Episode Length = 111.31393442622951
2025-08-03 23:19:22,177 - evaluation_results_class.py:149 - Counted Episodes = 2440
2025-08-03 23:19:22,447 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:19:22,460 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:19:22,580 - father_agent.py:547 - Training finished.
2025-08-03 23:19:22,744 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:19:22,747 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:19:22,750 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-03 23:19:33,390 - evaluation_results_class.py:131 - Average Return = 115.87761688232422
2025-08-03 23:19:33,390 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.586151123046875
2025-08-03 23:19:33,390 - evaluation_results_class.py:135 - Average Discounted Reward = 6.054206848144531
2025-08-03 23:19:33,390 - evaluation_results_class.py:137 - Goal Reach Probability = 0.999194847020934
2025-08-03 23:19:33,390 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:19:33,390 - evaluation_results_class.py:141 - Variance of Return = 15794.9208984375
2025-08-03 23:19:33,390 - evaluation_results_class.py:143 - Current Best Return = 226.02711486816406
2025-08-03 23:19:33,390 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-03 23:19:33,390 - evaluation_results_class.py:147 - Average Episode Length = 110.19323671497584
2025-08-03 23:19:33,390 - evaluation_results_class.py:149 - Counted Episodes = 2484
2025-08-03 23:19:33,665 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:19:33,667 - self_interpretable_extractor.py:286 - True
2025-08-03 23:19:33,681 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-03 23:19:50,909 - evaluation_results_class.py:131 - Average Return = 140.9405975341797
2025-08-03 23:19:50,910 - evaluation_results_class.py:133 - Average Virtual Goal Value = 15.082508087158203
2025-08-03 23:19:50,910 - evaluation_results_class.py:135 - Average Discounted Reward = 6.653700828552246
2025-08-03 23:19:50,910 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9942244224422442
2025-08-03 23:19:50,910 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-03 23:19:50,910 - evaluation_results_class.py:141 - Variance of Return = 24770.068359375
2025-08-03 23:19:50,910 - evaluation_results_class.py:143 - Current Best Return = 140.9405975341797
2025-08-03 23:19:50,910 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9942244224422442
2025-08-03 23:19:50,910 - evaluation_results_class.py:147 - Average Episode Length = 123.43894389438944
2025-08-03 23:19:50,910 - evaluation_results_class.py:149 - Counted Episodes = 2424
2025-08-03 23:19:50,910 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-03 23:19:50,910 - environment_wrapper_vec.py:518 - Resetting the environment.

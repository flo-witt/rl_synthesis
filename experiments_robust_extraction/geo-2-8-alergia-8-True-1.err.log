2025-08-03 23:20:02.996560: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-03 23:20:02.998419: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-03 23:20:03.028329: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-03 23:20:03.028375: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-03 23:20:03.029629: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-03 23:20:03.035453: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-03 23:20:03.035663: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-03 23:20:03.570830: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/models_robust_subset/geo-2-8/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/models_robust_subset/geo-2-8/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: Pmax=? ["notbad" U "goal"] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 45391 states and 820199 choices
INFO:paynt.verification.property:converting until formula to eventually...
INFO:paynt.parser.sketch:found the following specification optimality: Pmax=? [F "label_goal"] 
INFO:environment.vectorized_sim_initializer:Compiling model geo-2-8...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Iteration 1 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -22.80203628540039
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 0.7144282460212708
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.4191855192184448
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.07144282578327679
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.9285571742167232
INFO:tools.evaluation_results_class:Variance of Return = 764.4725341796875
INFO:tools.evaluation_results_class:Current Best Return = -22.80203628540039
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.07144282578327679
INFO:tools.evaluation_results_class:Average Episode Length = 31.85491917780882
INFO:tools.evaluation_results_class:Counted Episodes = 10022
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 1.2045540809631348
INFO:agents.father_agent:Step: 5, Training loss: 1.543976068496704
INFO:agents.father_agent:Step: 10, Training loss: 2.3044912815093994
INFO:agents.father_agent:Step: 15, Training loss: 3.4295735359191895
INFO:agents.father_agent:Step: 20, Training loss: 5.703583240509033
INFO:agents.father_agent:Step: 25, Training loss: 9.083881378173828
INFO:agents.father_agent:Step: 30, Training loss: 8.133955955505371
INFO:agents.father_agent:Step: 35, Training loss: 7.816848278045654
INFO:agents.father_agent:Step: 40, Training loss: 6.024970054626465
INFO:agents.father_agent:Step: 45, Training loss: 5.019925594329834
INFO:agents.father_agent:Step: 50, Training loss: 4.3181962966918945
INFO:agents.father_agent:Step: 55, Training loss: 3.571425199508667
INFO:agents.father_agent:Step: 60, Training loss: 3.1375346183776855
INFO:agents.father_agent:Step: 65, Training loss: 2.58711838722229
INFO:agents.father_agent:Step: 70, Training loss: 2.4952869415283203
INFO:agents.father_agent:Step: 75, Training loss: 2.1651151180267334
INFO:agents.father_agent:Step: 80, Training loss: 2.1574878692626953
INFO:agents.father_agent:Step: 85, Training loss: 2.06648588180542
INFO:agents.father_agent:Step: 90, Training loss: 1.9727952480316162
INFO:agents.father_agent:Step: 95, Training loss: 1.7612195014953613
INFO:agents.father_agent:Step: 100, Training loss: 1.8570337295532227
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -9.604681968688965
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 4.354252815246582
INFO:tools.evaluation_results_class:Average Discounted Reward = 3.491466522216797
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.43542528503682776
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.5645747149631722
INFO:tools.evaluation_results_class:Variance of Return = 54.041786193847656
INFO:tools.evaluation_results_class:Current Best Return = -9.604681968688965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.43542528503682776
INFO:tools.evaluation_results_class:Average Episode Length = 16.52845323378065
INFO:tools.evaluation_results_class:Counted Episodes = 19822
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 1.5754461288452148
INFO:agents.father_agent:Step: 110, Training loss: 1.6612706184387207
INFO:agents.father_agent:Step: 115, Training loss: 1.453940510749817
INFO:agents.father_agent:Step: 120, Training loss: 1.475255012512207
INFO:agents.father_agent:Step: 125, Training loss: 1.6225221157073975
INFO:agents.father_agent:Step: 130, Training loss: 1.4156445264816284
INFO:agents.father_agent:Step: 135, Training loss: 1.5031089782714844
INFO:agents.father_agent:Step: 140, Training loss: 1.4617881774902344
INFO:agents.father_agent:Step: 145, Training loss: 1.371717929840088
INFO:agents.father_agent:Step: 150, Training loss: 1.5176281929016113
INFO:agents.father_agent:Step: 155, Training loss: 1.2181718349456787
INFO:agents.father_agent:Step: 160, Training loss: 1.3975681066513062
INFO:agents.father_agent:Step: 165, Training loss: 1.2038787603378296
INFO:agents.father_agent:Step: 170, Training loss: 1.1977813243865967
INFO:agents.father_agent:Step: 175, Training loss: 1.4111733436584473
INFO:agents.father_agent:Step: 180, Training loss: 1.1486270427703857
INFO:agents.father_agent:Step: 185, Training loss: 1.286313772201538
INFO:agents.father_agent:Step: 190, Training loss: 1.2065269947052002
INFO:agents.father_agent:Step: 195, Training loss: 1.2499191761016846
INFO:agents.father_agent:Step: 200, Training loss: 1.3078819513320923
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -16.880786895751953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 4.759346008300781
INFO:tools.evaluation_results_class:Average Discounted Reward = 3.4968388080596924
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.47593459737468335
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.5240654026253166
INFO:tools.evaluation_results_class:Variance of Return = 143.02447509765625
INFO:tools.evaluation_results_class:Current Best Return = -9.604681968688965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.47593459737468335
INFO:tools.evaluation_results_class:Average Episode Length = 24.94020112074921
INFO:tools.evaluation_results_class:Counted Episodes = 13027
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 1.0987274646759033
INFO:agents.father_agent:Step: 210, Training loss: 1.0949571132659912
INFO:agents.father_agent:Step: 215, Training loss: 0.9521997570991516
INFO:agents.father_agent:Step: 220, Training loss: 0.984426736831665
INFO:agents.father_agent:Step: 225, Training loss: 0.986825704574585
INFO:agents.father_agent:Step: 230, Training loss: 0.8934086561203003
INFO:agents.father_agent:Step: 235, Training loss: 0.8908462524414062
INFO:agents.father_agent:Step: 240, Training loss: 0.891714334487915
INFO:agents.father_agent:Step: 245, Training loss: 0.8762136101722717
INFO:agents.father_agent:Step: 250, Training loss: 0.8899067044258118
INFO:agents.father_agent:Step: 255, Training loss: 0.7460371255874634
INFO:agents.father_agent:Step: 260, Training loss: 0.8687838315963745
INFO:agents.father_agent:Step: 265, Training loss: 0.9785701036453247
INFO:agents.father_agent:Step: 270, Training loss: 1.039893627166748
INFO:agents.father_agent:Step: 275, Training loss: 1.1245605945587158
INFO:agents.father_agent:Step: 280, Training loss: 0.9108535647392273
INFO:agents.father_agent:Step: 285, Training loss: 1.105332374572754
INFO:agents.father_agent:Step: 290, Training loss: 1.057768702507019
INFO:agents.father_agent:Step: 295, Training loss: 1.1489449739456177
INFO:agents.father_agent:Step: 300, Training loss: 0.9885455369949341
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -22.368261337280273
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 6.530941486358643
INFO:tools.evaluation_results_class:Average Discounted Reward = 4.6765570640563965
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6530941533788914
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.34690584662110857
INFO:tools.evaluation_results_class:Variance of Return = 408.7554016113281
INFO:tools.evaluation_results_class:Current Best Return = -9.604681968688965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6530941533788914
INFO:tools.evaluation_results_class:Average Episode Length = 30.48794608959757
INFO:tools.evaluation_results_class:Counted Episodes = 10536
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.0540741682052612
INFO:agents.father_agent:Step: 310, Training loss: 1.00661301612854
INFO:agents.father_agent:Step: 315, Training loss: 0.9217439293861389
INFO:agents.father_agent:Step: 320, Training loss: 1.0039311647415161
INFO:agents.father_agent:Step: 325, Training loss: 0.9678723812103271
INFO:agents.father_agent:Step: 330, Training loss: 1.018228530883789
INFO:agents.father_agent:Step: 335, Training loss: 0.9659050703048706
INFO:agents.father_agent:Step: 340, Training loss: 0.8613904118537903
INFO:agents.father_agent:Step: 345, Training loss: 0.9738147854804993
INFO:agents.father_agent:Step: 350, Training loss: 0.9119535684585571
INFO:agents.father_agent:Step: 355, Training loss: 1.0617055892944336
INFO:agents.father_agent:Step: 360, Training loss: 1.0695223808288574
INFO:agents.father_agent:Step: 365, Training loss: 0.9312083721160889
INFO:agents.father_agent:Step: 370, Training loss: 0.9362130165100098
INFO:agents.father_agent:Step: 375, Training loss: 0.9373252391815186
INFO:agents.father_agent:Step: 380, Training loss: 1.3436169624328613
INFO:agents.father_agent:Step: 385, Training loss: 0.9902442097663879
INFO:agents.father_agent:Step: 390, Training loss: 1.0275543928146362
INFO:agents.father_agent:Step: 395, Training loss: 1.1151474714279175
INFO:agents.father_agent:Step: 400, Training loss: 1.0099515914916992
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -24.199674606323242
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 6.5117692947387695
INFO:tools.evaluation_results_class:Average Discounted Reward = 4.550709247589111
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.651176948051948
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.34882305194805197
INFO:tools.evaluation_results_class:Variance of Return = 473.1788635253906
INFO:tools.evaluation_results_class:Current Best Return = -9.604681968688965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6530941533788914
INFO:tools.evaluation_results_class:Average Episode Length = 32.32102272727273
INFO:tools.evaluation_results_class:Counted Episodes = 9856
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.1197659969329834
INFO:agents.father_agent:Step: 410, Training loss: 1.0289896726608276
INFO:agents.father_agent:Step: 415, Training loss: 0.9319812655448914
INFO:agents.father_agent:Step: 420, Training loss: 1.0528677701950073
INFO:agents.father_agent:Step: 425, Training loss: 1.13705575466156
INFO:agents.father_agent:Step: 430, Training loss: 1.0251376628875732
INFO:agents.father_agent:Step: 435, Training loss: 1.0361162424087524
INFO:agents.father_agent:Step: 440, Training loss: 1.1161303520202637
INFO:agents.father_agent:Step: 445, Training loss: 0.9256917238235474
INFO:agents.father_agent:Step: 450, Training loss: 0.9929944276809692
INFO:agents.father_agent:Step: 455, Training loss: 1.071935772895813
INFO:agents.father_agent:Step: 460, Training loss: 1.144193172454834
INFO:agents.father_agent:Step: 465, Training loss: 1.1006495952606201
INFO:agents.father_agent:Step: 470, Training loss: 0.947079062461853
INFO:agents.father_agent:Step: 475, Training loss: 0.9110047817230225
INFO:agents.father_agent:Step: 480, Training loss: 1.1957290172576904
INFO:agents.father_agent:Step: 485, Training loss: 0.821196436882019
INFO:agents.father_agent:Step: 490, Training loss: 0.9896507263183594
INFO:agents.father_agent:Step: 495, Training loss: 1.1060383319854736
INFO:agents.father_agent:Step: 500, Training loss: 0.8049885034561157
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -25.731679916381836
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 6.514459609985352
INFO:tools.evaluation_results_class:Average Discounted Reward = 4.430379867553711
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6514459665144596
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.3485540334855403
INFO:tools.evaluation_results_class:Variance of Return = 345.9738464355469
INFO:tools.evaluation_results_class:Current Best Return = -9.604681968688965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6530941533788914
INFO:tools.evaluation_results_class:Average Episode Length = 34.96803652968037
INFO:tools.evaluation_results_class:Counted Episodes = 9198
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 505, Training loss: 0.9739621877670288
INFO:agents.father_agent:Step: 510, Training loss: 0.8099736571311951
INFO:agents.father_agent:Step: 515, Training loss: 0.9911308884620667
INFO:agents.father_agent:Step: 520, Training loss: 0.8839182257652283
INFO:agents.father_agent:Step: 525, Training loss: 0.8736295104026794
INFO:agents.father_agent:Step: 530, Training loss: 0.8997325301170349
INFO:agents.father_agent:Step: 535, Training loss: 0.9667550325393677
INFO:agents.father_agent:Step: 540, Training loss: 0.9546048045158386
INFO:agents.father_agent:Step: 545, Training loss: 0.9172534346580505
INFO:agents.father_agent:Step: 550, Training loss: 0.9550676345825195
INFO:agents.father_agent:Step: 555, Training loss: 0.8973649740219116
INFO:agents.father_agent:Step: 560, Training loss: 0.7901477813720703
INFO:agents.father_agent:Step: 565, Training loss: 0.862813413143158
INFO:agents.father_agent:Step: 570, Training loss: 0.7058212757110596
INFO:agents.father_agent:Step: 575, Training loss: 0.8344419002532959
INFO:agents.father_agent:Step: 580, Training loss: 0.7099040746688843
INFO:agents.father_agent:Step: 585, Training loss: 0.8993915319442749
INFO:agents.father_agent:Step: 590, Training loss: 0.7469258308410645
INFO:agents.father_agent:Step: 595, Training loss: 0.8607117533683777
INFO:agents.father_agent:Step: 600, Training loss: 0.8974111080169678
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -36.633453369140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 6.292183876037598
INFO:tools.evaluation_results_class:Average Discounted Reward = 3.7452330589294434
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6292184075967859
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.370781592403214
INFO:tools.evaluation_results_class:Variance of Return = 821.9303588867188
INFO:tools.evaluation_results_class:Current Best Return = -9.604681968688965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6530941533788914
INFO:tools.evaluation_results_class:Average Episode Length = 46.36742147552958
INFO:tools.evaluation_results_class:Counted Episodes = 6845
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 605, Training loss: 0.9758481383323669
INFO:agents.father_agent:Step: 610, Training loss: 0.9728201627731323
INFO:agents.father_agent:Step: 615, Training loss: 0.8815991878509521
INFO:agents.father_agent:Step: 620, Training loss: 0.790871262550354
INFO:agents.father_agent:Step: 625, Training loss: 0.9039954543113708
INFO:agents.father_agent:Step: 630, Training loss: 1.0349797010421753
INFO:agents.father_agent:Step: 635, Training loss: 0.9285524487495422
INFO:agents.father_agent:Step: 640, Training loss: 0.7634289860725403
INFO:agents.father_agent:Step: 645, Training loss: 0.9963792562484741
INFO:agents.father_agent:Step: 650, Training loss: 0.9256942868232727
INFO:agents.father_agent:Step: 655, Training loss: 0.971966564655304
INFO:agents.father_agent:Step: 660, Training loss: 1.1592540740966797
INFO:agents.father_agent:Step: 665, Training loss: 0.9479554295539856
INFO:agents.father_agent:Step: 670, Training loss: 0.9785900712013245
INFO:agents.father_agent:Step: 675, Training loss: 0.794411301612854
INFO:agents.father_agent:Step: 680, Training loss: 0.9772942066192627
INFO:agents.father_agent:Step: 685, Training loss: 0.9849044680595398
INFO:agents.father_agent:Step: 690, Training loss: 0.9536548256874084
INFO:agents.father_agent:Step: 695, Training loss: 1.0951647758483887
INFO:agents.father_agent:Step: 700, Training loss: 0.8494550585746765
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -122.95799255371094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 3.8143632411956787
INFO:tools.evaluation_results_class:Average Discounted Reward = 1.6626677513122559
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.3814363143631436
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.537940379403794
INFO:tools.evaluation_results_class:Variance of Return = 35907.453125
INFO:tools.evaluation_results_class:Current Best Return = -9.604681968688965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6530941533788914
INFO:tools.evaluation_results_class:Average Episode Length = 138.08739837398375
INFO:tools.evaluation_results_class:Counted Episodes = 1476
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 705, Training loss: 1.0168631076812744
INFO:agents.father_agent:Step: 710, Training loss: 0.8542850017547607
INFO:agents.father_agent:Step: 715, Training loss: 0.9408422708511353
INFO:agents.father_agent:Step: 720, Training loss: 0.9866130948066711
INFO:agents.father_agent:Step: 725, Training loss: 0.9725072383880615
INFO:agents.father_agent:Step: 730, Training loss: 1.1033521890640259
INFO:agents.father_agent:Step: 735, Training loss: 0.9471465945243835
INFO:agents.father_agent:Step: 740, Training loss: 0.9825090765953064
INFO:agents.father_agent:Step: 745, Training loss: 0.8956879377365112
INFO:agents.father_agent:Step: 750, Training loss: 0.9918595552444458
INFO:agents.father_agent:Step: 755, Training loss: 1.082124948501587
INFO:agents.father_agent:Step: 760, Training loss: 0.9779818654060364
INFO:agents.father_agent:Step: 765, Training loss: 0.9862670302391052
INFO:agents.father_agent:Step: 770, Training loss: 0.9667558670043945
INFO:agents.father_agent:Step: 775, Training loss: 1.0575083494186401
INFO:agents.father_agent:Step: 780, Training loss: 1.084154725074768
INFO:agents.father_agent:Step: 785, Training loss: 0.8009136319160461
INFO:agents.father_agent:Step: 790, Training loss: 0.8722167015075684
INFO:agents.father_agent:Step: 795, Training loss: 0.9612239003181458
INFO:agents.father_agent:Step: 800, Training loss: 0.9653797149658203
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -209.33587646484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 2.5326087474823
INFO:tools.evaluation_results_class:Average Discounted Reward = 1.0544627904891968
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.2532608695652174
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.48695652173913045
INFO:tools.evaluation_results_class:Variance of Return = 73139.5390625
INFO:tools.evaluation_results_class:Current Best Return = -9.604681968688965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6530941533788914
INFO:tools.evaluation_results_class:Average Episode Length = 226.11739130434782
INFO:tools.evaluation_results_class:Counted Episodes = 920
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 805, Training loss: 0.9079260230064392
INFO:agents.father_agent:Step: 810, Training loss: 0.8848354816436768
INFO:agents.father_agent:Step: 815, Training loss: 0.9637957811355591
INFO:agents.father_agent:Step: 820, Training loss: 0.73980712890625
INFO:agents.father_agent:Step: 825, Training loss: 0.7710381746292114
INFO:agents.father_agent:Step: 830, Training loss: 0.754553496837616
INFO:agents.father_agent:Step: 835, Training loss: 0.6553007364273071
INFO:agents.father_agent:Step: 840, Training loss: 0.6488070487976074
INFO:agents.father_agent:Step: 845, Training loss: 0.7109798192977905
INFO:agents.father_agent:Step: 850, Training loss: 0.6170818209648132
INFO:agents.father_agent:Step: 855, Training loss: 0.6838721632957458
INFO:agents.father_agent:Step: 860, Training loss: 0.5561485886573792
INFO:agents.father_agent:Step: 865, Training loss: 0.5412871837615967
INFO:agents.father_agent:Step: 870, Training loss: 0.5972306132316589
INFO:agents.father_agent:Step: 875, Training loss: 0.5555415749549866
INFO:agents.father_agent:Step: 880, Training loss: 0.48318397998809814
INFO:agents.father_agent:Step: 885, Training loss: 0.5596615076065063
INFO:agents.father_agent:Step: 890, Training loss: 0.6198475360870361
INFO:agents.father_agent:Step: 895, Training loss: 0.6457608938217163
INFO:agents.father_agent:Step: 900, Training loss: 0.6054462194442749
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -247.56185913085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 2.0317821502685547
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.7148556113243103
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.20317820658342792
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.4767309875141884
INFO:tools.evaluation_results_class:Variance of Return = 80726.3359375
INFO:tools.evaluation_results_class:Current Best Return = -9.604681968688965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6530941533788914
INFO:tools.evaluation_results_class:Average Episode Length = 263.6572077185017
INFO:tools.evaluation_results_class:Counted Episodes = 881
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 905, Training loss: 0.6576762199401855
INFO:agents.father_agent:Step: 910, Training loss: 0.5705838799476624
INFO:agents.father_agent:Step: 915, Training loss: 0.5991885662078857
INFO:agents.father_agent:Step: 920, Training loss: 0.4515364170074463
INFO:agents.father_agent:Step: 925, Training loss: 0.7706239223480225
INFO:agents.father_agent:Step: 930, Training loss: 0.9244320392608643
INFO:agents.father_agent:Step: 935, Training loss: 0.6770670413970947
INFO:agents.father_agent:Step: 940, Training loss: 0.7211823463439941
INFO:agents.father_agent:Step: 945, Training loss: 0.5898725986480713
INFO:agents.father_agent:Step: 950, Training loss: 0.4849778413772583
INFO:agents.father_agent:Step: 955, Training loss: 0.8340063095092773
INFO:agents.father_agent:Step: 960, Training loss: 0.9797888398170471
INFO:agents.father_agent:Step: 965, Training loss: 0.8375425934791565
INFO:agents.father_agent:Step: 970, Training loss: 0.6097826957702637
INFO:agents.father_agent:Step: 975, Training loss: 0.635083019733429
INFO:agents.father_agent:Step: 980, Training loss: 0.45643702149391174
INFO:agents.father_agent:Step: 985, Training loss: 0.5620325803756714
INFO:agents.father_agent:Step: 990, Training loss: 0.5509318113327026
INFO:agents.father_agent:Step: 995, Training loss: 0.5252107381820679
INFO:agents.father_agent:Step: 1000, Training loss: 0.6325501799583435
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -296.0192565917969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 0.7317073345184326
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.27892130613327026
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.07317073170731707
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.4942233632862644
INFO:tools.evaluation_results_class:Variance of Return = 95729.8203125
INFO:tools.evaluation_results_class:Current Best Return = -9.604681968688965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6530941533788914
INFO:tools.evaluation_results_class:Average Episode Length = 313.1463414634146
INFO:tools.evaluation_results_class:Counted Episodes = 779
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1005, Training loss: 0.573221743106842
INFO:agents.father_agent:Step: 1010, Training loss: 0.4276069402694702
INFO:agents.father_agent:Step: 1015, Training loss: 0.29254150390625
INFO:agents.father_agent:Step: 1020, Training loss: 0.253842830657959
INFO:agents.father_agent:Step: 1025, Training loss: 0.3654823303222656
INFO:agents.father_agent:Step: 1030, Training loss: 0.512819230556488
INFO:agents.father_agent:Step: 1035, Training loss: 0.9082295298576355
INFO:agents.father_agent:Step: 1040, Training loss: 1.2679297924041748
INFO:agents.father_agent:Step: 1045, Training loss: 1.283247947692871
INFO:agents.father_agent:Step: 1050, Training loss: 0.8252168297767639
INFO:agents.father_agent:Step: 1055, Training loss: 0.6293751001358032
INFO:agents.father_agent:Step: 1060, Training loss: 0.6162328720092773
INFO:agents.father_agent:Step: 1065, Training loss: 0.7265410423278809
INFO:agents.father_agent:Step: 1070, Training loss: 0.7721714973449707
INFO:agents.father_agent:Step: 1075, Training loss: 0.8377488255500793
INFO:agents.father_agent:Step: 1080, Training loss: 1.0009814500808716
INFO:agents.father_agent:Step: 1085, Training loss: 1.0065277814865112
INFO:agents.father_agent:Step: 1090, Training loss: 0.8835805058479309
INFO:agents.father_agent:Step: 1095, Training loss: 0.7676310539245605
INFO:agents.father_agent:Step: 1100, Training loss: 0.7826364636421204
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -265.19189453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 1.3653136491775513
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.44238829612731934
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.13653136531365315
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.5067650676506765
INFO:tools.evaluation_results_class:Variance of Return = 88872.6171875
INFO:tools.evaluation_results_class:Current Best Return = -9.604681968688965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6530941533788914
INFO:tools.evaluation_results_class:Average Episode Length = 283.01845018450183
INFO:tools.evaluation_results_class:Counted Episodes = 813
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1105, Training loss: 0.8357616066932678
INFO:agents.father_agent:Step: 1110, Training loss: 0.840272843837738
INFO:agents.father_agent:Step: 1115, Training loss: 0.8851988911628723
INFO:agents.father_agent:Step: 1120, Training loss: 0.9064050912857056
INFO:agents.father_agent:Step: 1125, Training loss: 1.1483577489852905
INFO:agents.father_agent:Step: 1130, Training loss: 1.0448737144470215
INFO:agents.father_agent:Step: 1135, Training loss: 0.842377245426178
INFO:agents.father_agent:Step: 1140, Training loss: 0.8227609992027283
INFO:agents.father_agent:Step: 1145, Training loss: 0.7589538097381592
INFO:agents.father_agent:Step: 1150, Training loss: 0.629673957824707
INFO:agents.father_agent:Step: 1155, Training loss: 0.8476092219352722
INFO:agents.father_agent:Step: 1160, Training loss: 1.0468955039978027
INFO:agents.father_agent:Step: 1165, Training loss: 1.2980008125305176
INFO:agents.father_agent:Step: 1170, Training loss: 1.2303400039672852
INFO:agents.father_agent:Step: 1175, Training loss: 1.1056385040283203
INFO:agents.father_agent:Step: 1180, Training loss: 0.8993702530860901
INFO:agents.father_agent:Step: 1185, Training loss: 0.7585418820381165
INFO:agents.father_agent:Step: 1190, Training loss: 0.7049081921577454
INFO:agents.father_agent:Step: 1195, Training loss: 0.7965350151062012
INFO:agents.father_agent:Step: 1200, Training loss: 1.0215272903442383
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -209.2956085205078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 2.1758241653442383
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.8938040733337402
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.2175824175824176
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.5131868131868131
INFO:tools.evaluation_results_class:Variance of Return = 75742.5546875
INFO:tools.evaluation_results_class:Current Best Return = -9.604681968688965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6530941533788914
INFO:tools.evaluation_results_class:Average Episode Length = 225.68791208791208
INFO:tools.evaluation_results_class:Counted Episodes = 910
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1205, Training loss: 1.264251470565796
INFO:agents.father_agent:Step: 1210, Training loss: 0.8163894414901733
INFO:agents.father_agent:Step: 1215, Training loss: 0.6242153644561768
INFO:agents.father_agent:Step: 1220, Training loss: 0.5668346881866455
INFO:agents.father_agent:Step: 1225, Training loss: 0.6505933403968811
INFO:agents.father_agent:Step: 1230, Training loss: 0.9870315790176392
INFO:agents.father_agent:Step: 1235, Training loss: 1.1018986701965332
INFO:agents.father_agent:Step: 1240, Training loss: 1.0541939735412598
INFO:agents.father_agent:Step: 1245, Training loss: 0.8035203218460083
INFO:agents.father_agent:Step: 1250, Training loss: 0.6361334323883057
INFO:agents.father_agent:Step: 1255, Training loss: 0.6094788908958435
INFO:agents.father_agent:Step: 1260, Training loss: 0.6324941515922546
INFO:agents.father_agent:Step: 1265, Training loss: 0.7509045600891113
INFO:agents.father_agent:Step: 1270, Training loss: 0.7755929231643677
INFO:agents.father_agent:Step: 1275, Training loss: 0.7928513288497925
INFO:agents.father_agent:Step: 1280, Training loss: 0.6918627619743347
INFO:agents.father_agent:Step: 1285, Training loss: 0.7090801000595093
INFO:agents.father_agent:Step: 1290, Training loss: 0.708203911781311
INFO:agents.father_agent:Step: 1295, Training loss: 0.8541490435600281
INFO:agents.father_agent:Step: 1300, Training loss: 1.004071593284607
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -200.56072998046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 2.462038993835449
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.9663634300231934
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.24620390455531455
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.5043383947939263
INFO:tools.evaluation_results_class:Variance of Return = 71027.4765625
INFO:tools.evaluation_results_class:Current Best Return = -9.604681968688965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6530941533788914
INFO:tools.evaluation_results_class:Average Episode Length = 218.07158351409979
INFO:tools.evaluation_results_class:Counted Episodes = 922
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1305, Training loss: 0.8904606699943542
INFO:agents.father_agent:Step: 1310, Training loss: 0.9593015313148499
INFO:agents.father_agent:Step: 1315, Training loss: 0.6146396994590759
INFO:agents.father_agent:Step: 1320, Training loss: 0.643510103225708
INFO:agents.father_agent:Step: 1325, Training loss: 0.6186936497688293
INFO:agents.father_agent:Step: 1330, Training loss: 0.789776086807251
INFO:agents.father_agent:Step: 1335, Training loss: 1.089032769203186
INFO:agents.father_agent:Step: 1340, Training loss: 0.7900576591491699
INFO:agents.father_agent:Step: 1345, Training loss: 0.7925214171409607
INFO:agents.father_agent:Step: 1350, Training loss: 0.6073904037475586
INFO:agents.father_agent:Step: 1355, Training loss: 0.7086317539215088
INFO:agents.father_agent:Step: 1360, Training loss: 0.8446568250656128
INFO:agents.father_agent:Step: 1365, Training loss: 0.8843328952789307
INFO:agents.father_agent:Step: 1370, Training loss: 0.7331157326698303
INFO:agents.father_agent:Step: 1375, Training loss: 0.7053788900375366
INFO:agents.father_agent:Step: 1380, Training loss: 0.7803425788879395
INFO:agents.father_agent:Step: 1385, Training loss: 0.764004111289978
INFO:agents.father_agent:Step: 1390, Training loss: 0.6313266158103943
INFO:agents.father_agent:Step: 1395, Training loss: 0.7383431792259216
INFO:agents.father_agent:Step: 1400, Training loss: 0.7211761474609375
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.24951171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 2.620889663696289
INFO:tools.evaluation_results_class:Average Discounted Reward = 1.0675510168075562
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.2620889748549323
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.52321083172147
INFO:tools.evaluation_results_class:Variance of Return = 64719.72265625
INFO:tools.evaluation_results_class:Current Best Return = -9.604681968688965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6530941533788914
INFO:tools.evaluation_results_class:Average Episode Length = 196.7852998065764
INFO:tools.evaluation_results_class:Counted Episodes = 1034
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -184.16616821289062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 2.5075528621673584
INFO:tools.evaluation_results_class:Average Discounted Reward = 1.0149683952331543
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.25075528700906347
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.527693856998993
INFO:tools.evaluation_results_class:Variance of Return = 65997.5390625
INFO:tools.evaluation_results_class:Current Best Return = -9.604681968688965
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6530941533788914
INFO:tools.evaluation_results_class:Average Episode Length = 200.1621349446123
INFO:tools.evaluation_results_class:Counted Episodes = 993
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -234.25865173339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 2.5967414379119873
INFO:tools.evaluation_results_class:Average Discounted Reward = 1.0981993675231934
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.25967413441955195
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.434826883910387
INFO:tools.evaluation_results_class:Variance of Return = 76608.546875
INFO:tools.evaluation_results_class:Current Best Return = -234.25865173339844
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.25967413441955195
INFO:tools.evaluation_results_class:Average Episode Length = 251.32281059063138
INFO:tools.evaluation_results_class:Counted Episodes = 982
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -277.3636474609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 0.15909090638160706
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.02769213542342186
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.015909090909090907
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.6193181818181818
INFO:tools.evaluation_results_class:Variance of Return = 87635.6328125
INFO:tools.evaluation_results_class:Current Best Return = -277.3636474609375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.015909090909090907
INFO:tools.evaluation_results_class:Average Episode Length = 294.7465909090909
INFO:tools.evaluation_results_class:Counted Episodes = 880

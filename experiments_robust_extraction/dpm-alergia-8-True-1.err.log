2025-08-03 21:20:02.120232: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-03 21:20:02.122088: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-03 21:20:02.152197: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-03 21:20:02.152240: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-03 21:20:02.153727: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-03 21:20:02.159397: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-03 21:20:02.159583: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-03 21:20:02.698702: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/models_robust_subset/dpm/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/models_robust_subset/dpm/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"rew"}max=? [F (bat = 0)] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 737 states and 10594 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"rew"}max=? [F (bat = 0)] 
INFO:environment.vectorized_sim_initializer:Compiling model dpm...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Iteration 1 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 226.02711486816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 23.59123992919922
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.835872173309326
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9942648592283628
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 79507.7890625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9942648592283628
INFO:tools.evaluation_results_class:Average Episode Length = 133.8806047966632
INFO:tools.evaluation_results_class:Counted Episodes = 1918
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 37.96004867553711
INFO:agents.father_agent:Step: 5, Training loss: 11.251276969909668
INFO:agents.father_agent:Step: 10, Training loss: 12.372895240783691
INFO:agents.father_agent:Step: 15, Training loss: 11.745393753051758
INFO:agents.father_agent:Step: 20, Training loss: 12.655396461486816
INFO:agents.father_agent:Step: 25, Training loss: 11.637170791625977
INFO:agents.father_agent:Step: 30, Training loss: 9.55413818359375
INFO:agents.father_agent:Step: 35, Training loss: 7.228248119354248
INFO:agents.father_agent:Step: 40, Training loss: 6.126197814941406
INFO:agents.father_agent:Step: 45, Training loss: 7.941475868225098
INFO:agents.father_agent:Step: 50, Training loss: 7.6086106300354
INFO:agents.father_agent:Step: 55, Training loss: 7.503852844238281
INFO:agents.father_agent:Step: 60, Training loss: 9.550846099853516
INFO:agents.father_agent:Step: 65, Training loss: 8.301970481872559
INFO:agents.father_agent:Step: 70, Training loss: 6.003207206726074
INFO:agents.father_agent:Step: 75, Training loss: 9.675264358520508
INFO:agents.father_agent:Step: 80, Training loss: 8.48681640625
INFO:agents.father_agent:Step: 85, Training loss: 6.572736740112305
INFO:agents.father_agent:Step: 90, Training loss: 7.056653022766113
INFO:agents.father_agent:Step: 95, Training loss: 7.356429576873779
INFO:agents.father_agent:Step: 100, Training loss: 8.84921932220459
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 156.0608673095703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 16.60186004638672
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.31040096282959
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9978867286559594
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 40507.22265625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9978867286559594
INFO:tools.evaluation_results_class:Average Episode Length = 113.67159763313609
INFO:tools.evaluation_results_class:Counted Episodes = 2366
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 7.808121204376221
INFO:agents.father_agent:Step: 110, Training loss: 9.982125282287598
INFO:agents.father_agent:Step: 115, Training loss: 6.931763172149658
INFO:agents.father_agent:Step: 120, Training loss: 6.667110919952393
INFO:agents.father_agent:Step: 125, Training loss: 8.225988388061523
INFO:agents.father_agent:Step: 130, Training loss: 6.764737606048584
INFO:agents.father_agent:Step: 135, Training loss: 7.69077205657959
INFO:agents.father_agent:Step: 140, Training loss: 7.5574798583984375
INFO:agents.father_agent:Step: 145, Training loss: 7.717491149902344
INFO:agents.father_agent:Step: 150, Training loss: 7.941234111785889
INFO:agents.father_agent:Step: 155, Training loss: 8.972490310668945
INFO:agents.father_agent:Step: 160, Training loss: 7.817333698272705
INFO:agents.father_agent:Step: 165, Training loss: 5.563429832458496
INFO:agents.father_agent:Step: 170, Training loss: 6.372718334197998
INFO:agents.father_agent:Step: 175, Training loss: 7.6417975425720215
INFO:agents.father_agent:Step: 180, Training loss: 8.218306541442871
INFO:agents.father_agent:Step: 185, Training loss: 7.605515480041504
INFO:agents.father_agent:Step: 190, Training loss: 8.118033409118652
INFO:agents.father_agent:Step: 195, Training loss: 8.861083030700684
INFO:agents.father_agent:Step: 200, Training loss: 10.376300811767578
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 133.5044708251953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.3465576171875
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.743219375610352
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9980552314274601
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 26790.482421875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9980552314274601
INFO:tools.evaluation_results_class:Average Episode Length = 105.72578763127188
INFO:tools.evaluation_results_class:Counted Episodes = 2571
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 6.972217082977295
INFO:agents.father_agent:Step: 210, Training loss: 11.310961723327637
INFO:agents.father_agent:Step: 215, Training loss: 7.744687080383301
INFO:agents.father_agent:Step: 220, Training loss: 6.836561679840088
INFO:agents.father_agent:Step: 225, Training loss: 6.6306891441345215
INFO:agents.father_agent:Step: 230, Training loss: 8.136712074279785
INFO:agents.father_agent:Step: 235, Training loss: 7.013694763183594
INFO:agents.father_agent:Step: 240, Training loss: 9.273505210876465
INFO:agents.father_agent:Step: 245, Training loss: 9.8651704788208
INFO:agents.father_agent:Step: 250, Training loss: 8.943145751953125
INFO:agents.father_agent:Step: 255, Training loss: 8.097127914428711
INFO:agents.father_agent:Step: 260, Training loss: 8.22326374053955
INFO:agents.father_agent:Step: 265, Training loss: 7.550997734069824
INFO:agents.father_agent:Step: 270, Training loss: 7.3089494705200195
INFO:agents.father_agent:Step: 275, Training loss: 7.178284645080566
INFO:agents.father_agent:Step: 280, Training loss: 7.043642997741699
INFO:agents.father_agent:Step: 285, Training loss: 7.098309516906738
INFO:agents.father_agent:Step: 290, Training loss: 6.283463954925537
INFO:agents.father_agent:Step: 295, Training loss: 7.3705220222473145
INFO:agents.father_agent:Step: 300, Training loss: 9.318581581115723
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 166.56639099121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 17.652341842651367
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.7403411865234375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9978513107004727
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 39974.0703125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9980552314274601
INFO:tools.evaluation_results_class:Average Episode Length = 115.11688869789428
INFO:tools.evaluation_results_class:Counted Episodes = 2327
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 8.321351051330566
INFO:agents.father_agent:Step: 310, Training loss: 9.227304458618164
INFO:agents.father_agent:Step: 315, Training loss: 9.697712898254395
INFO:agents.father_agent:Step: 320, Training loss: 7.836455345153809
INFO:agents.father_agent:Step: 325, Training loss: 8.673489570617676
INFO:agents.father_agent:Step: 330, Training loss: 10.603639602661133
INFO:agents.father_agent:Step: 335, Training loss: 9.14111042022705
INFO:agents.father_agent:Step: 340, Training loss: 8.401268005371094
INFO:agents.father_agent:Step: 345, Training loss: 5.747515678405762
INFO:agents.father_agent:Step: 350, Training loss: 9.693710327148438
INFO:agents.father_agent:Step: 355, Training loss: 6.5973286628723145
INFO:agents.father_agent:Step: 360, Training loss: 9.335103988647461
INFO:agents.father_agent:Step: 365, Training loss: 7.383171558380127
INFO:agents.father_agent:Step: 370, Training loss: 6.712879180908203
INFO:agents.father_agent:Step: 375, Training loss: 9.18985366821289
INFO:agents.father_agent:Step: 380, Training loss: 8.0091552734375
INFO:agents.father_agent:Step: 385, Training loss: 7.129428863525391
INFO:agents.father_agent:Step: 390, Training loss: 7.228371620178223
INFO:agents.father_agent:Step: 395, Training loss: 10.206661224365234
INFO:agents.father_agent:Step: 400, Training loss: 6.879978179931641
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 151.92417907714844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 16.1899471282959
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.268583297729492
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9987639060568603
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 32466.13671875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9987639060568603
INFO:tools.evaluation_results_class:Average Episode Length = 112.55006180469715
INFO:tools.evaluation_results_class:Counted Episodes = 2427
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 8.399024963378906
INFO:agents.father_agent:Step: 410, Training loss: 8.73102855682373
INFO:agents.father_agent:Step: 415, Training loss: 7.140547275543213
INFO:agents.father_agent:Step: 420, Training loss: 8.250758171081543
INFO:agents.father_agent:Step: 425, Training loss: 8.23679256439209
INFO:agents.father_agent:Step: 430, Training loss: 7.446712970733643
INFO:agents.father_agent:Step: 435, Training loss: 8.39621353149414
INFO:agents.father_agent:Step: 440, Training loss: 9.765633583068848
INFO:agents.father_agent:Step: 445, Training loss: 9.716935157775879
INFO:agents.father_agent:Step: 450, Training loss: 7.10959529876709
INFO:agents.father_agent:Step: 455, Training loss: 9.267923355102539
INFO:agents.father_agent:Step: 460, Training loss: 7.122754096984863
INFO:agents.father_agent:Step: 465, Training loss: 7.046923637390137
INFO:agents.father_agent:Step: 470, Training loss: 6.094211101531982
INFO:agents.father_agent:Step: 475, Training loss: 5.958868026733398
INFO:agents.father_agent:Step: 480, Training loss: 7.647620677947998
INFO:agents.father_agent:Step: 485, Training loss: 7.645079135894775
INFO:agents.father_agent:Step: 490, Training loss: 5.913303375244141
INFO:agents.father_agent:Step: 495, Training loss: 7.530720233917236
INFO:agents.father_agent:Step: 500, Training loss: 7.4594879150390625
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 160.2368927001953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 17.019458770751953
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.611466884613037
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9978849407783418
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35022.62109375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9987639060568603
INFO:tools.evaluation_results_class:Average Episode Length = 115.79568527918782
INFO:tools.evaluation_results_class:Counted Episodes = 2364
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 505, Training loss: 6.6797051429748535
INFO:agents.father_agent:Step: 510, Training loss: 6.767799377441406
INFO:agents.father_agent:Step: 515, Training loss: 6.438144683837891
INFO:agents.father_agent:Step: 520, Training loss: 9.755962371826172
INFO:agents.father_agent:Step: 525, Training loss: 7.217334747314453
INFO:agents.father_agent:Step: 530, Training loss: 8.899567604064941
INFO:agents.father_agent:Step: 535, Training loss: 7.767863750457764
INFO:agents.father_agent:Step: 540, Training loss: 7.476111888885498
INFO:agents.father_agent:Step: 545, Training loss: 8.889405250549316
INFO:agents.father_agent:Step: 550, Training loss: 8.79139518737793
INFO:agents.father_agent:Step: 555, Training loss: 6.588198184967041
INFO:agents.father_agent:Step: 560, Training loss: 9.721769332885742
INFO:agents.father_agent:Step: 565, Training loss: 8.910685539245605
INFO:agents.father_agent:Step: 570, Training loss: 10.485943794250488
INFO:agents.father_agent:Step: 575, Training loss: 9.87910270690918
INFO:agents.father_agent:Step: 580, Training loss: 7.503377914428711
INFO:agents.father_agent:Step: 585, Training loss: 9.261405944824219
INFO:agents.father_agent:Step: 590, Training loss: 7.146271705627441
INFO:agents.father_agent:Step: 595, Training loss: 8.019681930541992
INFO:agents.father_agent:Step: 600, Training loss: 8.3882417678833
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 151.98333740234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 16.19333267211914
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.375007152557373
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9975
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33249.734375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9987639060568603
INFO:tools.evaluation_results_class:Average Episode Length = 115.68166666666667
INFO:tools.evaluation_results_class:Counted Episodes = 2400
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 605, Training loss: 6.683036804199219
INFO:agents.father_agent:Step: 610, Training loss: 7.4821295738220215
INFO:agents.father_agent:Step: 615, Training loss: 7.327589988708496
INFO:agents.father_agent:Step: 620, Training loss: 6.453664779663086
INFO:agents.father_agent:Step: 625, Training loss: 6.544801235198975
INFO:agents.father_agent:Step: 630, Training loss: 8.064189910888672
INFO:agents.father_agent:Step: 635, Training loss: 7.726194858551025
INFO:agents.father_agent:Step: 640, Training loss: 7.668371677398682
INFO:agents.father_agent:Step: 645, Training loss: 7.843258857727051
INFO:agents.father_agent:Step: 650, Training loss: 7.541199207305908
INFO:agents.father_agent:Step: 655, Training loss: 7.132774353027344
INFO:agents.father_agent:Step: 660, Training loss: 7.287734508514404
INFO:agents.father_agent:Step: 665, Training loss: 7.4877119064331055
INFO:agents.father_agent:Step: 670, Training loss: 7.384328842163086
INFO:agents.father_agent:Step: 675, Training loss: 8.169261932373047
INFO:agents.father_agent:Step: 680, Training loss: 8.720375061035156
INFO:agents.father_agent:Step: 685, Training loss: 10.489691734313965
INFO:agents.father_agent:Step: 690, Training loss: 6.943974494934082
INFO:agents.father_agent:Step: 695, Training loss: 5.784489631652832
INFO:agents.father_agent:Step: 700, Training loss: 6.970915794372559
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 144.4928741455078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 15.444257736206055
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.135566234588623
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9974853310980721
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29419.8828125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9987639060568603
INFO:tools.evaluation_results_class:Average Episode Length = 114.18189438390611
INFO:tools.evaluation_results_class:Counted Episodes = 2386
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 705, Training loss: 8.065814018249512
INFO:agents.father_agent:Step: 710, Training loss: 7.610235691070557
INFO:agents.father_agent:Step: 715, Training loss: 8.141788482666016
INFO:agents.father_agent:Step: 720, Training loss: 10.36372184753418
INFO:agents.father_agent:Step: 725, Training loss: 6.679856777191162
INFO:agents.father_agent:Step: 730, Training loss: 8.220866203308105
INFO:agents.father_agent:Step: 735, Training loss: 6.601057052612305
INFO:agents.father_agent:Step: 740, Training loss: 7.471734523773193
INFO:agents.father_agent:Step: 745, Training loss: 7.269270420074463
INFO:agents.father_agent:Step: 750, Training loss: 8.142885208129883
INFO:agents.father_agent:Step: 755, Training loss: 7.2031941413879395
INFO:agents.father_agent:Step: 760, Training loss: 7.1241536140441895
INFO:agents.father_agent:Step: 765, Training loss: 8.08045768737793
INFO:agents.father_agent:Step: 770, Training loss: 7.643753528594971
INFO:agents.father_agent:Step: 775, Training loss: 6.956720352172852
INFO:agents.father_agent:Step: 780, Training loss: 9.793785095214844
INFO:agents.father_agent:Step: 785, Training loss: 8.861209869384766
INFO:agents.father_agent:Step: 790, Training loss: 7.506916046142578
INFO:agents.father_agent:Step: 795, Training loss: 6.105727672576904
INFO:agents.father_agent:Step: 800, Training loss: 10.368816375732422
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 130.63763427734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.05987548828125
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.783777236938477
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9980559875583204
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23538.47265625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9987639060568603
INFO:tools.evaluation_results_class:Average Episode Length = 106.59059097978228
INFO:tools.evaluation_results_class:Counted Episodes = 2572
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 805, Training loss: 7.208434104919434
INFO:agents.father_agent:Step: 810, Training loss: 7.24616003036499
INFO:agents.father_agent:Step: 815, Training loss: 5.369405269622803
INFO:agents.father_agent:Step: 820, Training loss: 10.065905570983887
INFO:agents.father_agent:Step: 825, Training loss: 6.951608180999756
INFO:agents.father_agent:Step: 830, Training loss: 9.223204612731934
INFO:agents.father_agent:Step: 835, Training loss: 8.973790168762207
INFO:agents.father_agent:Step: 840, Training loss: 6.4890456199646
INFO:agents.father_agent:Step: 845, Training loss: 8.336009979248047
INFO:agents.father_agent:Step: 850, Training loss: 7.014404296875
INFO:agents.father_agent:Step: 855, Training loss: 7.875685214996338
INFO:agents.father_agent:Step: 860, Training loss: 7.06516170501709
INFO:agents.father_agent:Step: 865, Training loss: 8.274192810058594
INFO:agents.father_agent:Step: 870, Training loss: 6.710543632507324
INFO:agents.father_agent:Step: 875, Training loss: 7.117908477783203
INFO:agents.father_agent:Step: 880, Training loss: 7.7391791343688965
INFO:agents.father_agent:Step: 885, Training loss: 7.1017351150512695
INFO:agents.father_agent:Step: 890, Training loss: 6.787240505218506
INFO:agents.father_agent:Step: 895, Training loss: 7.383685111999512
INFO:agents.father_agent:Step: 900, Training loss: 6.44141960144043
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 139.38296508789062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.935006141662598
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.927679061889648
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9983545865898807
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 29597.154296875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9987639060568603
INFO:tools.evaluation_results_class:Average Episode Length = 109.34594816947758
INFO:tools.evaluation_results_class:Counted Episodes = 2431
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 905, Training loss: 8.52224349975586
INFO:agents.father_agent:Step: 910, Training loss: 8.057341575622559
INFO:agents.father_agent:Step: 915, Training loss: 6.9738993644714355
INFO:agents.father_agent:Step: 920, Training loss: 9.281110763549805
INFO:agents.father_agent:Step: 925, Training loss: 7.947800636291504
INFO:agents.father_agent:Step: 930, Training loss: 7.025103569030762
INFO:agents.father_agent:Step: 935, Training loss: 7.858216762542725
INFO:agents.father_agent:Step: 940, Training loss: 8.29333209991455
INFO:agents.father_agent:Step: 945, Training loss: 7.170848369598389
INFO:agents.father_agent:Step: 950, Training loss: 9.615209579467773
INFO:agents.father_agent:Step: 955, Training loss: 8.230948448181152
INFO:agents.father_agent:Step: 960, Training loss: 8.326277732849121
INFO:agents.father_agent:Step: 965, Training loss: 7.2116522789001465
INFO:agents.father_agent:Step: 970, Training loss: 7.744943141937256
INFO:agents.father_agent:Step: 975, Training loss: 8.12475872039795
INFO:agents.father_agent:Step: 980, Training loss: 9.881497383117676
INFO:agents.father_agent:Step: 985, Training loss: 9.312029838562012
INFO:agents.father_agent:Step: 990, Training loss: 7.724441051483154
INFO:agents.father_agent:Step: 995, Training loss: 9.844173431396484
INFO:agents.father_agent:Step: 1000, Training loss: 7.66605281829834
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 114.92786407470703
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.48974895477295
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.140918254852295
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9984813971146546
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19340.712890625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9987639060568603
INFO:tools.evaluation_results_class:Average Episode Length = 103.09339407744875
INFO:tools.evaluation_results_class:Counted Episodes = 2634
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1005, Training loss: 9.977071762084961
INFO:agents.father_agent:Step: 1010, Training loss: 7.9350266456604
INFO:agents.father_agent:Step: 1015, Training loss: 8.150903701782227
INFO:agents.father_agent:Step: 1020, Training loss: 8.015682220458984
INFO:agents.father_agent:Step: 1025, Training loss: 8.093372344970703
INFO:agents.father_agent:Step: 1030, Training loss: 9.220870018005371
INFO:agents.father_agent:Step: 1035, Training loss: 7.635720252990723
INFO:agents.father_agent:Step: 1040, Training loss: 6.977456569671631
INFO:agents.father_agent:Step: 1045, Training loss: 7.573925018310547
INFO:agents.father_agent:Step: 1050, Training loss: 7.026193141937256
INFO:agents.father_agent:Step: 1055, Training loss: 9.55203628540039
INFO:agents.father_agent:Step: 1060, Training loss: 7.305907726287842
INFO:agents.father_agent:Step: 1065, Training loss: 7.752354145050049
INFO:agents.father_agent:Step: 1070, Training loss: 7.715574741363525
INFO:agents.father_agent:Step: 1075, Training loss: 7.158101558685303
INFO:agents.father_agent:Step: 1080, Training loss: 8.155915260314941
INFO:agents.father_agent:Step: 1085, Training loss: 7.575610160827637
INFO:agents.father_agent:Step: 1090, Training loss: 5.387800693511963
INFO:agents.father_agent:Step: 1095, Training loss: 9.720593452453613
INFO:agents.father_agent:Step: 1100, Training loss: 7.805112361907959
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 113.31204986572266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.329608917236328
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.949303150177002
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9992019154030327
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17456.310546875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9992019154030327
INFO:tools.evaluation_results_class:Average Episode Length = 110.06703910614524
INFO:tools.evaluation_results_class:Counted Episodes = 2506
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1105, Training loss: 7.0832695960998535
INFO:agents.father_agent:Step: 1110, Training loss: 9.476778984069824
INFO:agents.father_agent:Step: 1115, Training loss: 8.684754371643066
INFO:agents.father_agent:Step: 1120, Training loss: 9.722477912902832
INFO:agents.father_agent:Step: 1125, Training loss: 8.567627906799316
INFO:agents.father_agent:Step: 1130, Training loss: 7.8354082107543945
INFO:agents.father_agent:Step: 1135, Training loss: 9.03385066986084
INFO:agents.father_agent:Step: 1140, Training loss: 6.878799915313721
INFO:agents.father_agent:Step: 1145, Training loss: 7.62686824798584
INFO:agents.father_agent:Step: 1150, Training loss: 7.553801536560059
INFO:agents.father_agent:Step: 1155, Training loss: 7.210651874542236
INFO:agents.father_agent:Step: 1160, Training loss: 7.956497669219971
INFO:agents.father_agent:Step: 1165, Training loss: 5.961215496063232
INFO:agents.father_agent:Step: 1170, Training loss: 7.890841484069824
INFO:agents.father_agent:Step: 1175, Training loss: 8.800015449523926
INFO:agents.father_agent:Step: 1180, Training loss: 7.303991317749023
INFO:agents.father_agent:Step: 1185, Training loss: 7.706394672393799
INFO:agents.father_agent:Step: 1190, Training loss: 8.31718921661377
INFO:agents.father_agent:Step: 1195, Training loss: 7.059637546539307
INFO:agents.father_agent:Step: 1200, Training loss: 6.944428443908691
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 132.52284240722656
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.249106407165527
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.8324360847473145
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9984108065156932
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25229.591796875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9992019154030327
INFO:tools.evaluation_results_class:Average Episode Length = 108.12793007548669
INFO:tools.evaluation_results_class:Counted Episodes = 2517
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1205, Training loss: 7.564212322235107
INFO:agents.father_agent:Step: 1210, Training loss: 7.89857292175293
INFO:agents.father_agent:Step: 1215, Training loss: 7.251805305480957
INFO:agents.father_agent:Step: 1220, Training loss: 8.424781799316406
INFO:agents.father_agent:Step: 1225, Training loss: 8.569119453430176
INFO:agents.father_agent:Step: 1230, Training loss: 6.4384074211120605
INFO:agents.father_agent:Step: 1235, Training loss: 9.72879409790039
INFO:agents.father_agent:Step: 1240, Training loss: 7.457220554351807
INFO:agents.father_agent:Step: 1245, Training loss: 8.805194854736328
INFO:agents.father_agent:Step: 1250, Training loss: 7.454578399658203
INFO:agents.father_agent:Step: 1255, Training loss: 10.796558380126953
INFO:agents.father_agent:Step: 1260, Training loss: 7.63673734664917
INFO:agents.father_agent:Step: 1265, Training loss: 6.807850360870361
INFO:agents.father_agent:Step: 1270, Training loss: 6.875927448272705
INFO:agents.father_agent:Step: 1275, Training loss: 6.108994960784912
INFO:agents.father_agent:Step: 1280, Training loss: 6.391255855560303
INFO:agents.father_agent:Step: 1285, Training loss: 6.946505546569824
INFO:agents.father_agent:Step: 1290, Training loss: 8.124249458312988
INFO:agents.father_agent:Step: 1295, Training loss: 7.568055152893066
INFO:agents.father_agent:Step: 1300, Training loss: 9.108463287353516
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 134.92037963867188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.491242408752441
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.80042839050293
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9996019108280255
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24488.84765625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996019108280255
INFO:tools.evaluation_results_class:Average Episode Length = 109.03144904458598
INFO:tools.evaluation_results_class:Counted Episodes = 2512
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1305, Training loss: 7.241829872131348
INFO:agents.father_agent:Step: 1310, Training loss: 8.14247989654541
INFO:agents.father_agent:Step: 1315, Training loss: 7.830395698547363
INFO:agents.father_agent:Step: 1320, Training loss: 7.667534828186035
INFO:agents.father_agent:Step: 1325, Training loss: 7.869431018829346
INFO:agents.father_agent:Step: 1330, Training loss: 10.62478256225586
INFO:agents.father_agent:Step: 1335, Training loss: 8.459905624389648
INFO:agents.father_agent:Step: 1340, Training loss: 8.336135864257812
INFO:agents.father_agent:Step: 1345, Training loss: 7.260481357574463
INFO:agents.father_agent:Step: 1350, Training loss: 8.411896705627441
INFO:agents.father_agent:Step: 1355, Training loss: 7.961578845977783
INFO:agents.father_agent:Step: 1360, Training loss: 7.379025459289551
INFO:agents.father_agent:Step: 1365, Training loss: 7.320276737213135
INFO:agents.father_agent:Step: 1370, Training loss: 8.187365531921387
INFO:agents.father_agent:Step: 1375, Training loss: 7.7843451499938965
INFO:agents.father_agent:Step: 1380, Training loss: 8.83536148071289
INFO:agents.father_agent:Step: 1385, Training loss: 8.235529899597168
INFO:agents.father_agent:Step: 1390, Training loss: 9.49512004852295
INFO:agents.father_agent:Step: 1395, Training loss: 8.280694007873535
INFO:agents.father_agent:Step: 1400, Training loss: 10.183602333068848
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 127.32121276855469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.729697227478027
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.586504936218262
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9987878787878788
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21916.5
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996019108280255
INFO:tools.evaluation_results_class:Average Episode Length = 109.38868686868688
INFO:tools.evaluation_results_class:Counted Episodes = 2475
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 127.52177429199219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.745051383972168
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.593197345733643
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.996437054631829
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24009.9296875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996019108280255
INFO:tools.evaluation_results_class:Average Episode Length = 108.81353919239905
INFO:tools.evaluation_results_class:Counted Episodes = 2526
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 156.53085327148438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 16.645017623901367
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.2430100440979
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9959661153691004
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 34694.21484375
INFO:tools.evaluation_results_class:Current Best Return = 156.53085327148438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9959661153691004
INFO:tools.evaluation_results_class:Average Episode Length = 122.05889471561113
INFO:tools.evaluation_results_class:Counted Episodes = 2479
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 146.1468963623047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 15.606483459472656
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.77489709854126
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9958965941731637
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 30221.87890625
INFO:tools.evaluation_results_class:Current Best Return = 146.1468963623047
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9958965941731637
INFO:tools.evaluation_results_class:Average Episode Length = 124.40869922035289
INFO:tools.evaluation_results_class:Counted Episodes = 2437
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 131220
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 926.7153 achieved after 816.77 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 926.6041 achieved after 816.78 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 123.1723 achieved after 816.86 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:P1=0, P2=0, P3=0, P4=0, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 123.17234150429911
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=0, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:robust_rl.robust_rl_trainer:Iteration 2 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 109.5605239868164
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 11.95296859741211
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.669026851654053
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9984579799537394
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17539.74609375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996019108280255
INFO:tools.evaluation_results_class:Average Episode Length = 106.97995373939861
INFO:tools.evaluation_results_class:Counted Episodes = 2594
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.958368301391602
INFO:agents.father_agent:Step: 5, Training loss: 5.931599140167236
INFO:agents.father_agent:Step: 10, Training loss: 7.851141452789307
INFO:agents.father_agent:Step: 15, Training loss: 6.574592590332031
INFO:agents.father_agent:Step: 20, Training loss: 8.102743148803711
INFO:agents.father_agent:Step: 25, Training loss: 5.988490104675293
INFO:agents.father_agent:Step: 30, Training loss: 5.987133979797363
INFO:agents.father_agent:Step: 35, Training loss: 6.434316158294678
INFO:agents.father_agent:Step: 40, Training loss: 4.743882656097412
INFO:agents.father_agent:Step: 45, Training loss: 8.051478385925293
INFO:agents.father_agent:Step: 50, Training loss: 6.611484527587891
INFO:agents.father_agent:Step: 55, Training loss: 7.493902206420898
INFO:agents.father_agent:Step: 60, Training loss: 6.912318706512451
INFO:agents.father_agent:Step: 65, Training loss: 6.778632164001465
INFO:agents.father_agent:Step: 70, Training loss: 7.7073750495910645
INFO:agents.father_agent:Step: 75, Training loss: 7.214569568634033
INFO:agents.father_agent:Step: 80, Training loss: 6.717196464538574
INFO:agents.father_agent:Step: 85, Training loss: 6.772371292114258
INFO:agents.father_agent:Step: 90, Training loss: 5.946108818054199
INFO:agents.father_agent:Step: 95, Training loss: 6.804165840148926
INFO:agents.father_agent:Step: 100, Training loss: 6.628854751586914
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 120.71707153320312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.070927619934082
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.358819961547852
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9996102883865939
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19280.544921875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996102883865939
INFO:tools.evaluation_results_class:Average Episode Length = 106.90296180826189
INFO:tools.evaluation_results_class:Counted Episodes = 2566
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 8.15097713470459
INFO:agents.father_agent:Step: 110, Training loss: 6.067390441894531
INFO:agents.father_agent:Step: 115, Training loss: 6.496520042419434
INFO:agents.father_agent:Step: 120, Training loss: 5.8700432777404785
INFO:agents.father_agent:Step: 125, Training loss: 7.842804908752441
INFO:agents.father_agent:Step: 130, Training loss: 5.122694492340088
INFO:agents.father_agent:Step: 135, Training loss: 5.63112211227417
INFO:agents.father_agent:Step: 140, Training loss: 5.182641983032227
INFO:agents.father_agent:Step: 145, Training loss: 6.9067840576171875
INFO:agents.father_agent:Step: 150, Training loss: 6.836970806121826
INFO:agents.father_agent:Step: 155, Training loss: 6.146814346313477
INFO:agents.father_agent:Step: 160, Training loss: 5.199804782867432
INFO:agents.father_agent:Step: 165, Training loss: 6.116220474243164
INFO:agents.father_agent:Step: 170, Training loss: 5.952840805053711
INFO:agents.father_agent:Step: 175, Training loss: 6.337576866149902
INFO:agents.father_agent:Step: 180, Training loss: 6.197651386260986
INFO:agents.father_agent:Step: 185, Training loss: 5.934981346130371
INFO:agents.father_agent:Step: 190, Training loss: 7.434952259063721
INFO:agents.father_agent:Step: 195, Training loss: 7.69320821762085
INFO:agents.father_agent:Step: 200, Training loss: 6.304235458374023
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 134.63925170898438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.460700988769531
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.670820713043213
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9983877468762595
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 28402.541015625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996102883865939
INFO:tools.evaluation_results_class:Average Episode Length = 110.53929866989117
INFO:tools.evaluation_results_class:Counted Episodes = 2481
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 5.868895530700684
INFO:agents.father_agent:Step: 210, Training loss: 5.723267078399658
INFO:agents.father_agent:Step: 215, Training loss: 6.2966084480285645
INFO:agents.father_agent:Step: 220, Training loss: 7.082163333892822
INFO:agents.father_agent:Step: 225, Training loss: 7.642327308654785
INFO:agents.father_agent:Step: 230, Training loss: 7.227066516876221
INFO:agents.father_agent:Step: 235, Training loss: 6.223733901977539
INFO:agents.father_agent:Step: 240, Training loss: 6.291687965393066
INFO:agents.father_agent:Step: 245, Training loss: 6.516522407531738
INFO:agents.father_agent:Step: 250, Training loss: 7.672936916351318
INFO:agents.father_agent:Step: 255, Training loss: 5.950300693511963
INFO:agents.father_agent:Step: 260, Training loss: 5.326938152313232
INFO:agents.father_agent:Step: 265, Training loss: 5.973979473114014
INFO:agents.father_agent:Step: 270, Training loss: 6.273478031158447
INFO:agents.father_agent:Step: 275, Training loss: 6.10107946395874
INFO:agents.father_agent:Step: 280, Training loss: 5.338582992553711
INFO:agents.father_agent:Step: 285, Training loss: 6.354700565338135
INFO:agents.father_agent:Step: 290, Training loss: 6.371481418609619
INFO:agents.father_agent:Step: 295, Training loss: 6.67667293548584
INFO:agents.father_agent:Step: 300, Training loss: 7.3131513595581055
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 123.70486450195312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.363422393798828
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.376444339752197
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9964678178963893
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20114.373046875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996102883865939
INFO:tools.evaluation_results_class:Average Episode Length = 107.24215070643642
INFO:tools.evaluation_results_class:Counted Episodes = 2548
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 7.354188919067383
INFO:agents.father_agent:Step: 310, Training loss: 8.023994445800781
INFO:agents.father_agent:Step: 315, Training loss: 5.542382717132568
INFO:agents.father_agent:Step: 320, Training loss: 4.7895588874816895
INFO:agents.father_agent:Step: 325, Training loss: 5.482573986053467
INFO:agents.father_agent:Step: 330, Training loss: 7.147140026092529
INFO:agents.father_agent:Step: 335, Training loss: 7.423325538635254
INFO:agents.father_agent:Step: 340, Training loss: 6.5499444007873535
INFO:agents.father_agent:Step: 345, Training loss: 5.829080104827881
INFO:agents.father_agent:Step: 350, Training loss: 7.121680736541748
INFO:agents.father_agent:Step: 355, Training loss: 6.074365139007568
INFO:agents.father_agent:Step: 360, Training loss: 7.5625386238098145
INFO:agents.father_agent:Step: 365, Training loss: 6.637242794036865
INFO:agents.father_agent:Step: 370, Training loss: 5.6516289710998535
INFO:agents.father_agent:Step: 375, Training loss: 7.158607006072998
INFO:agents.father_agent:Step: 380, Training loss: 5.915329933166504
INFO:agents.father_agent:Step: 385, Training loss: 4.959895133972168
INFO:agents.father_agent:Step: 390, Training loss: 6.76713752746582
INFO:agents.father_agent:Step: 395, Training loss: 7.018497943878174
INFO:agents.father_agent:Step: 400, Training loss: 6.836668014526367
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 115.8863525390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.585433959960938
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.056753635406494
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9983993597438976
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18301.4453125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996102883865939
INFO:tools.evaluation_results_class:Average Episode Length = 107.02120848339335
INFO:tools.evaluation_results_class:Counted Episodes = 2499
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 6.178404331207275
INFO:agents.father_agent:Step: 410, Training loss: 6.815321445465088
INFO:agents.father_agent:Step: 415, Training loss: 6.201672554016113
INFO:agents.father_agent:Step: 420, Training loss: 6.256752967834473
INFO:agents.father_agent:Step: 425, Training loss: 7.423864364624023
INFO:agents.father_agent:Step: 430, Training loss: 6.566036701202393
INFO:agents.father_agent:Step: 435, Training loss: 6.349095821380615
INFO:agents.father_agent:Step: 440, Training loss: 5.6719465255737305
INFO:agents.father_agent:Step: 445, Training loss: 6.1530680656433105
INFO:agents.father_agent:Step: 450, Training loss: 7.789102554321289
INFO:agents.father_agent:Step: 455, Training loss: 6.750239849090576
INFO:agents.father_agent:Step: 460, Training loss: 6.069344997406006
INFO:agents.father_agent:Step: 465, Training loss: 5.136868953704834
INFO:agents.father_agent:Step: 470, Training loss: 6.925612926483154
INFO:agents.father_agent:Step: 475, Training loss: 6.499780178070068
INFO:agents.father_agent:Step: 480, Training loss: 6.358680725097656
INFO:agents.father_agent:Step: 485, Training loss: 6.5099687576293945
INFO:agents.father_agent:Step: 490, Training loss: 7.870728969573975
INFO:agents.father_agent:Step: 495, Training loss: 6.451694011688232
INFO:agents.father_agent:Step: 500, Training loss: 7.830277919769287
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 130.35116577148438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.029264450073242
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.521101474761963
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9970735785953178
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22494.19140625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996102883865939
INFO:tools.evaluation_results_class:Average Episode Length = 114.97951505016722
INFO:tools.evaluation_results_class:Counted Episodes = 2392
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 124.27580261230469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.425948143005371
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.332101821899414
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9991840065279478
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20102.158203125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996102883865939
INFO:tools.evaluation_results_class:Average Episode Length = 110.83476132190943
INFO:tools.evaluation_results_class:Counted Episodes = 2451
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 154.50457763671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 16.43463706970215
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.999415397644043
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.992089925062448
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 33764.3046875
INFO:tools.evaluation_results_class:Current Best Return = 154.50457763671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.992089925062448
INFO:tools.evaluation_results_class:Average Episode Length = 126.39175686927561
INFO:tools.evaluation_results_class:Counted Episodes = 2402
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 145.337890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 15.519247055053711
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.652951240539551
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9927288280581694
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27267.658203125
INFO:tools.evaluation_results_class:Current Best Return = 145.337890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9927288280581694
INFO:tools.evaluation_results_class:Average Episode Length = 129.03806672369547
INFO:tools.evaluation_results_class:Counted Episodes = 2338
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 131220
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 526.9639 achieved after 1283.53 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 526.95 achieved after 1283.54 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 141.6918 achieved after 1283.58 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 141.69178788034793
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:robust_rl.robust_rl_trainer:Iteration 3 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 115.37750244140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.535439491271973
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.1737380027771
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.99884437596302
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16329.94140625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996102883865939
INFO:tools.evaluation_results_class:Average Episode Length = 106.47881355932203
INFO:tools.evaluation_results_class:Counted Episodes = 2596
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.687198638916016
INFO:agents.father_agent:Step: 5, Training loss: 6.39762544631958
INFO:agents.father_agent:Step: 10, Training loss: 6.243222713470459
INFO:agents.father_agent:Step: 15, Training loss: 6.785177230834961
INFO:agents.father_agent:Step: 20, Training loss: 4.986632823944092
INFO:agents.father_agent:Step: 25, Training loss: 5.736464023590088
INFO:agents.father_agent:Step: 30, Training loss: 5.78009557723999
INFO:agents.father_agent:Step: 35, Training loss: 5.512851715087891
INFO:agents.father_agent:Step: 40, Training loss: 7.067266464233398
INFO:agents.father_agent:Step: 45, Training loss: 6.116495609283447
INFO:agents.father_agent:Step: 50, Training loss: 5.919405460357666
INFO:agents.father_agent:Step: 55, Training loss: 7.2298359870910645
INFO:agents.father_agent:Step: 60, Training loss: 7.219148635864258
INFO:agents.father_agent:Step: 65, Training loss: 5.461358547210693
INFO:agents.father_agent:Step: 70, Training loss: 5.808218002319336
INFO:agents.father_agent:Step: 75, Training loss: 6.349556922912598
INFO:agents.father_agent:Step: 80, Training loss: 6.4167799949646
INFO:agents.father_agent:Step: 85, Training loss: 5.765345096588135
INFO:agents.father_agent:Step: 90, Training loss: 6.171895503997803
INFO:agents.father_agent:Step: 95, Training loss: 6.144398212432861
INFO:agents.father_agent:Step: 100, Training loss: 6.611824035644531
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 113.15973663330078
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.312087059020996
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.982292652130127
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.998056743101438
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18237.861328125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996102883865939
INFO:tools.evaluation_results_class:Average Episode Length = 103.99339292654489
INFO:tools.evaluation_results_class:Counted Episodes = 2573
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 6.7089033126831055
INFO:agents.father_agent:Step: 110, Training loss: 6.998299598693848
INFO:agents.father_agent:Step: 115, Training loss: 5.6015095710754395
INFO:agents.father_agent:Step: 120, Training loss: 8.803396224975586
INFO:agents.father_agent:Step: 125, Training loss: 5.053330421447754
INFO:agents.father_agent:Step: 130, Training loss: 6.033458709716797
INFO:agents.father_agent:Step: 135, Training loss: 4.347520351409912
INFO:agents.father_agent:Step: 140, Training loss: 7.128139019012451
INFO:agents.father_agent:Step: 145, Training loss: 6.7676191329956055
INFO:agents.father_agent:Step: 150, Training loss: 4.988051891326904
INFO:agents.father_agent:Step: 155, Training loss: 6.482122898101807
INFO:agents.father_agent:Step: 160, Training loss: 7.1203227043151855
INFO:agents.father_agent:Step: 165, Training loss: 5.55412483215332
INFO:agents.father_agent:Step: 170, Training loss: 6.2996673583984375
INFO:agents.father_agent:Step: 175, Training loss: 7.2766218185424805
INFO:agents.father_agent:Step: 180, Training loss: 5.702676296234131
INFO:agents.father_agent:Step: 185, Training loss: 5.742833137512207
INFO:agents.father_agent:Step: 190, Training loss: 6.9815545082092285
INFO:agents.father_agent:Step: 195, Training loss: 5.737637042999268
INFO:agents.father_agent:Step: 200, Training loss: 6.178927898406982
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 119.52922058105469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.948863983154297
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.301487445831299
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9979707792207793
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16412.11328125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996102883865939
INFO:tools.evaluation_results_class:Average Episode Length = 110.60430194805195
INFO:tools.evaluation_results_class:Counted Episodes = 2464
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 5.984882831573486
INFO:agents.father_agent:Step: 210, Training loss: 7.111672878265381
INFO:agents.father_agent:Step: 215, Training loss: 7.872490882873535
INFO:agents.father_agent:Step: 220, Training loss: 5.856651782989502
INFO:agents.father_agent:Step: 225, Training loss: 8.14754867553711
INFO:agents.father_agent:Step: 230, Training loss: 6.721469879150391
INFO:agents.father_agent:Step: 235, Training loss: 6.065792560577393
INFO:agents.father_agent:Step: 240, Training loss: 5.23338508605957
INFO:agents.father_agent:Step: 245, Training loss: 5.825076103210449
INFO:agents.father_agent:Step: 250, Training loss: 6.251403331756592
INFO:agents.father_agent:Step: 255, Training loss: 6.906414985656738
INFO:agents.father_agent:Step: 260, Training loss: 7.524272918701172
INFO:agents.father_agent:Step: 265, Training loss: 6.496230125427246
INFO:agents.father_agent:Step: 270, Training loss: 6.169796466827393
INFO:agents.father_agent:Step: 275, Training loss: 6.007984161376953
INFO:agents.father_agent:Step: 280, Training loss: 8.285317420959473
INFO:agents.father_agent:Step: 285, Training loss: 7.094721794128418
INFO:agents.father_agent:Step: 290, Training loss: 6.592398643493652
INFO:agents.father_agent:Step: 295, Training loss: 5.234223365783691
INFO:agents.father_agent:Step: 300, Training loss: 7.2300262451171875
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 118.62238311767578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.86059856414795
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.2265472412109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.999179991799918
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16222.45703125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996102883865939
INFO:tools.evaluation_results_class:Average Episode Length = 110.52562525625257
INFO:tools.evaluation_results_class:Counted Episodes = 2439
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 6.051472187042236
INFO:agents.father_agent:Step: 310, Training loss: 5.4376654624938965
INFO:agents.father_agent:Step: 315, Training loss: 6.718698024749756
INFO:agents.father_agent:Step: 320, Training loss: 5.206575870513916
INFO:agents.father_agent:Step: 325, Training loss: 5.781589984893799
INFO:agents.father_agent:Step: 330, Training loss: 7.117973327636719
INFO:agents.father_agent:Step: 335, Training loss: 7.026289463043213
INFO:agents.father_agent:Step: 340, Training loss: 6.516311168670654
INFO:agents.father_agent:Step: 345, Training loss: 6.217396259307861
INFO:agents.father_agent:Step: 350, Training loss: 6.166128635406494
INFO:agents.father_agent:Step: 355, Training loss: 6.9233880043029785
INFO:agents.father_agent:Step: 360, Training loss: 6.802956581115723
INFO:agents.father_agent:Step: 365, Training loss: 5.9642014503479
INFO:agents.father_agent:Step: 370, Training loss: 5.785679817199707
INFO:agents.father_agent:Step: 375, Training loss: 7.897881031036377
INFO:agents.father_agent:Step: 380, Training loss: 6.211029529571533
INFO:agents.father_agent:Step: 385, Training loss: 4.795182704925537
INFO:agents.father_agent:Step: 390, Training loss: 5.76318359375
INFO:agents.father_agent:Step: 395, Training loss: 6.115501880645752
INFO:agents.father_agent:Step: 400, Training loss: 6.368417739868164
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 124.00662231445312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.396523475646973
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.3492751121521
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9979304635761589
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19068.71484375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9996102883865939
INFO:tools.evaluation_results_class:Average Episode Length = 111.27773178807946
INFO:tools.evaluation_results_class:Counted Episodes = 2416
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 5.884836673736572
INFO:agents.father_agent:Step: 410, Training loss: 7.013059139251709
INFO:agents.father_agent:Step: 415, Training loss: 6.8436994552612305
INFO:agents.father_agent:Step: 420, Training loss: 7.089013576507568
INFO:agents.father_agent:Step: 425, Training loss: 6.4420294761657715
INFO:agents.father_agent:Step: 430, Training loss: 5.510761737823486
INFO:agents.father_agent:Step: 435, Training loss: 6.238142490386963
INFO:agents.father_agent:Step: 440, Training loss: 6.199412822723389
INFO:agents.father_agent:Step: 445, Training loss: 6.31155252456665
INFO:agents.father_agent:Step: 450, Training loss: 6.527564525604248
INFO:agents.father_agent:Step: 455, Training loss: 7.554426670074463
INFO:agents.father_agent:Step: 460, Training loss: 7.161085605621338
INFO:agents.father_agent:Step: 465, Training loss: 7.485601425170898
INFO:agents.father_agent:Step: 470, Training loss: 6.507757663726807
INFO:agents.father_agent:Step: 475, Training loss: 6.852051734924316
INFO:agents.father_agent:Step: 480, Training loss: 6.233367919921875
INFO:agents.father_agent:Step: 485, Training loss: 6.19041633605957
INFO:agents.father_agent:Step: 490, Training loss: 5.366111755371094
INFO:agents.father_agent:Step: 495, Training loss: 6.718003749847412
INFO:agents.father_agent:Step: 500, Training loss: 5.720348834991455
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 110.6347427368164
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.062713623046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.023303031921387
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.999619916381604
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15208.0732421875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.999619916381604
INFO:tools.evaluation_results_class:Average Episode Length = 104.28240212846826
INFO:tools.evaluation_results_class:Counted Episodes = 2631
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 117.0102310180664
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.69630241394043
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.1193156242370605
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.997639653815893
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15948.2607421875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.999619916381604
INFO:tools.evaluation_results_class:Average Episode Length = 108.37136113296617
INFO:tools.evaluation_results_class:Counted Episodes = 2542
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 140.46817016601562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 15.039424896240234
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.644940376281738
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9963039014373717
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25985.326171875
INFO:tools.evaluation_results_class:Current Best Return = 140.46817016601562
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9963039014373717
INFO:tools.evaluation_results_class:Average Episode Length = 122.92689938398357
INFO:tools.evaluation_results_class:Counted Episodes = 2435
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 130.38107299804688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.029281616210938
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.220753192901611
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.99558764540714
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22002.78125
INFO:tools.evaluation_results_class:Current Best Return = 130.38107299804688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.99558764540714
INFO:tools.evaluation_results_class:Average Episode Length = 121.98596068993182
INFO:tools.evaluation_results_class:Counted Episodes = 2493
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 131220
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 1029.7583 achieved after 1780.13 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1028.83 achieved after 1780.13 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 986.0959 achieved after 1780.34 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 685.1649 achieved after 1780.43 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 685.1493 achieved after 1780.43 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 500.6874 achieved after 1780.51 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 500.6851 achieved after 1780.51 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 460.0304 achieved after 1780.73 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 392.6267 achieved after 1780.78 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 392.6195 achieved after 1780.78 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 138.908 achieved after 1780.9 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 138.90796767757942
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:robust_rl.robust_rl_trainer:Iteration 4 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 110.80632019042969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.07896900177002
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.7710466384887695
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.999168744804655
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15447.9765625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.999619916381604
INFO:tools.evaluation_results_class:Average Episode Length = 110.61263507896925
INFO:tools.evaluation_results_class:Counted Episodes = 2406
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 7.869278907775879
INFO:agents.father_agent:Step: 5, Training loss: 4.876047611236572
INFO:agents.father_agent:Step: 10, Training loss: 5.085029125213623
INFO:agents.father_agent:Step: 15, Training loss: 7.50900936126709
INFO:agents.father_agent:Step: 20, Training loss: 5.635898590087891
INFO:agents.father_agent:Step: 25, Training loss: 5.953700542449951
INFO:agents.father_agent:Step: 30, Training loss: 5.9442315101623535
INFO:agents.father_agent:Step: 35, Training loss: 6.04326057434082
INFO:agents.father_agent:Step: 40, Training loss: 5.5567545890808105
INFO:agents.father_agent:Step: 45, Training loss: 5.5578718185424805
INFO:agents.father_agent:Step: 50, Training loss: 6.153829574584961
INFO:agents.father_agent:Step: 55, Training loss: 4.887963771820068
INFO:agents.father_agent:Step: 60, Training loss: 6.548286437988281
INFO:agents.father_agent:Step: 65, Training loss: 5.281702995300293
INFO:agents.father_agent:Step: 70, Training loss: 5.306106090545654
INFO:agents.father_agent:Step: 75, Training loss: 6.14762544631958
INFO:agents.father_agent:Step: 80, Training loss: 7.324559688568115
INFO:agents.father_agent:Step: 85, Training loss: 4.937005519866943
INFO:agents.father_agent:Step: 90, Training loss: 5.456810474395752
INFO:agents.father_agent:Step: 95, Training loss: 6.208792209625244
INFO:agents.father_agent:Step: 100, Training loss: 6.324735164642334
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 118.39481353759766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.837860107421875
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.186552047729492
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.999189298743413
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16393.6953125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.999619916381604
INFO:tools.evaluation_results_class:Average Episode Length = 110.92501013376571
INFO:tools.evaluation_results_class:Counted Episodes = 2467
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 6.565121650695801
INFO:agents.father_agent:Step: 110, Training loss: 5.790286540985107
INFO:agents.father_agent:Step: 115, Training loss: 6.267333030700684
INFO:agents.father_agent:Step: 120, Training loss: 6.5969672203063965
INFO:agents.father_agent:Step: 125, Training loss: 5.989656925201416
INFO:agents.father_agent:Step: 130, Training loss: 5.300654411315918
INFO:agents.father_agent:Step: 135, Training loss: 5.767245769500732
INFO:agents.father_agent:Step: 140, Training loss: 5.657571315765381
INFO:agents.father_agent:Step: 145, Training loss: 5.901639938354492
INFO:agents.father_agent:Step: 150, Training loss: 6.098576068878174
INFO:agents.father_agent:Step: 155, Training loss: 5.047488689422607
INFO:agents.father_agent:Step: 160, Training loss: 5.840578079223633
INFO:agents.father_agent:Step: 165, Training loss: 6.693653583526611
INFO:agents.father_agent:Step: 170, Training loss: 6.002971649169922
INFO:agents.father_agent:Step: 175, Training loss: 5.204145431518555
INFO:agents.father_agent:Step: 180, Training loss: 6.161799907684326
INFO:agents.father_agent:Step: 185, Training loss: 5.568461894989014
INFO:agents.father_agent:Step: 190, Training loss: 6.134986400604248
INFO:agents.father_agent:Step: 195, Training loss: 6.916041374206543
INFO:agents.father_agent:Step: 200, Training loss: 6.2524237632751465
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 112.52782440185547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.252781867980957
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.8898091316223145
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16151.478515625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.09379968203497
INFO:tools.evaluation_results_class:Counted Episodes = 2516
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 6.65939474105835
INFO:agents.father_agent:Step: 210, Training loss: 5.742068290710449
INFO:agents.father_agent:Step: 215, Training loss: 5.679098129272461
INFO:agents.father_agent:Step: 220, Training loss: 5.596659183502197
INFO:agents.father_agent:Step: 225, Training loss: 6.777983665466309
INFO:agents.father_agent:Step: 230, Training loss: 4.7865753173828125
INFO:agents.father_agent:Step: 235, Training loss: 5.062992572784424
INFO:agents.father_agent:Step: 240, Training loss: 5.849362373352051
INFO:agents.father_agent:Step: 245, Training loss: 5.967111587524414
INFO:agents.father_agent:Step: 250, Training loss: 6.716022968292236
INFO:agents.father_agent:Step: 255, Training loss: 7.160442352294922
INFO:agents.father_agent:Step: 260, Training loss: 6.231734275817871
INFO:agents.father_agent:Step: 265, Training loss: 5.490668773651123
INFO:agents.father_agent:Step: 270, Training loss: 5.544924736022949
INFO:agents.father_agent:Step: 275, Training loss: 6.8782057762146
INFO:agents.father_agent:Step: 280, Training loss: 6.909847259521484
INFO:agents.father_agent:Step: 285, Training loss: 5.544562339782715
INFO:agents.father_agent:Step: 290, Training loss: 6.580197811126709
INFO:agents.father_agent:Step: 295, Training loss: 6.045860290527344
INFO:agents.father_agent:Step: 300, Training loss: 5.153740882873535
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 118.49325561523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.84615421295166
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.168502330780029
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9984139571768438
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17203.755859375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.53053132434576
INFO:tools.evaluation_results_class:Counted Episodes = 2522
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 5.892239570617676
INFO:agents.father_agent:Step: 310, Training loss: 6.266928195953369
INFO:agents.father_agent:Step: 315, Training loss: 5.625853061676025
INFO:agents.father_agent:Step: 320, Training loss: 6.903199672698975
INFO:agents.father_agent:Step: 325, Training loss: 6.478295803070068
INFO:agents.father_agent:Step: 330, Training loss: 7.5128021240234375
INFO:agents.father_agent:Step: 335, Training loss: 6.329571723937988
INFO:agents.father_agent:Step: 340, Training loss: 6.990399360656738
INFO:agents.father_agent:Step: 345, Training loss: 5.647259712219238
INFO:agents.father_agent:Step: 350, Training loss: 5.902486801147461
INFO:agents.father_agent:Step: 355, Training loss: 5.696816444396973
INFO:agents.father_agent:Step: 360, Training loss: 4.910031318664551
INFO:agents.father_agent:Step: 365, Training loss: 5.725644588470459
INFO:agents.father_agent:Step: 370, Training loss: 7.950364112854004
INFO:agents.father_agent:Step: 375, Training loss: 7.176943778991699
INFO:agents.father_agent:Step: 380, Training loss: 6.901458263397217
INFO:agents.father_agent:Step: 385, Training loss: 6.929762363433838
INFO:agents.father_agent:Step: 390, Training loss: 6.539932727813721
INFO:agents.father_agent:Step: 395, Training loss: 5.873274326324463
INFO:agents.father_agent:Step: 400, Training loss: 6.570406436920166
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 121.24494934082031
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.119644165039062
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.306064128875732
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9975747776879548
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17027.392578125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.26434923201293
INFO:tools.evaluation_results_class:Counted Episodes = 2474
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 5.918257713317871
INFO:agents.father_agent:Step: 410, Training loss: 6.176742076873779
INFO:agents.father_agent:Step: 415, Training loss: 6.00801944732666
INFO:agents.father_agent:Step: 420, Training loss: 5.708193302154541
INFO:agents.father_agent:Step: 425, Training loss: 5.90181827545166
INFO:agents.father_agent:Step: 430, Training loss: 7.232291221618652
INFO:agents.father_agent:Step: 435, Training loss: 7.510800838470459
INFO:agents.father_agent:Step: 440, Training loss: 6.609460830688477
INFO:agents.father_agent:Step: 445, Training loss: 6.411482334136963
INFO:agents.father_agent:Step: 450, Training loss: 7.70393180847168
INFO:agents.father_agent:Step: 455, Training loss: 6.833419322967529
INFO:agents.father_agent:Step: 460, Training loss: 5.853153228759766
INFO:agents.father_agent:Step: 465, Training loss: 7.471596717834473
INFO:agents.father_agent:Step: 470, Training loss: 4.985080718994141
INFO:agents.father_agent:Step: 475, Training loss: 5.888963222503662
INFO:agents.father_agent:Step: 480, Training loss: 6.08681058883667
INFO:agents.father_agent:Step: 485, Training loss: 5.9044928550720215
INFO:agents.father_agent:Step: 490, Training loss: 6.214938640594482
INFO:agents.father_agent:Step: 495, Training loss: 5.074743270874023
INFO:agents.father_agent:Step: 500, Training loss: 4.469131946563721
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 114.61869049072266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.457958221435547
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.005174160003662
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9980445834962847
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17635.4921875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.93273367227219
INFO:tools.evaluation_results_class:Counted Episodes = 2557
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 109.88272094726562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 11.986708641052246
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.868325233459473
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9992181391712275
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14834.935546875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.15871774824082
INFO:tools.evaluation_results_class:Counted Episodes = 2558
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 137.90335083007812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.781327247619629
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.632019519805908
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9954954954954955
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24361.53515625
INFO:tools.evaluation_results_class:Current Best Return = 137.90335083007812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9954954954954955
INFO:tools.evaluation_results_class:Average Episode Length = 123.38370188370189
INFO:tools.evaluation_results_class:Counted Episodes = 2442
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 128.95404052734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.882725715637207
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.171308517456055
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.993660855784469
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21704.29296875
INFO:tools.evaluation_results_class:Current Best Return = 128.95404052734375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.993660855784469
INFO:tools.evaluation_results_class:Average Episode Length = 120.5364500792393
INFO:tools.evaluation_results_class:Counted Episodes = 2524
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 131220
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 528.099 achieved after 2297.47 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 528.0479 achieved after 2297.47 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 528.0341 achieved after 2297.48 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 141.9851 achieved after 2297.55 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 141.98505654299393
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:robust_rl.robust_rl_trainer:Iteration 5 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 115.46900177001953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.545310020446777
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.054412841796875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9992050874403816
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17239.248046875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.44992050874404
INFO:tools.evaluation_results_class:Counted Episodes = 2516
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.077358245849609
INFO:agents.father_agent:Step: 5, Training loss: 6.194985866546631
INFO:agents.father_agent:Step: 10, Training loss: 5.951415061950684
INFO:agents.father_agent:Step: 15, Training loss: 5.147939205169678
INFO:agents.father_agent:Step: 20, Training loss: 5.766426086425781
INFO:agents.father_agent:Step: 25, Training loss: 5.922513961791992
INFO:agents.father_agent:Step: 30, Training loss: 5.404325008392334
INFO:agents.father_agent:Step: 35, Training loss: 5.831610679626465
INFO:agents.father_agent:Step: 40, Training loss: 5.476568222045898
INFO:agents.father_agent:Step: 45, Training loss: 5.891195774078369
INFO:agents.father_agent:Step: 50, Training loss: 6.168359756469727
INFO:agents.father_agent:Step: 55, Training loss: 6.699195384979248
INFO:agents.father_agent:Step: 60, Training loss: 6.0500898361206055
INFO:agents.father_agent:Step: 65, Training loss: 5.59852933883667
INFO:agents.father_agent:Step: 70, Training loss: 6.034692764282227
INFO:agents.father_agent:Step: 75, Training loss: 7.338404655456543
INFO:agents.father_agent:Step: 80, Training loss: 7.180629730224609
INFO:agents.father_agent:Step: 85, Training loss: 6.315247535705566
INFO:agents.father_agent:Step: 90, Training loss: 5.806336879730225
INFO:agents.father_agent:Step: 95, Training loss: 5.799797534942627
INFO:agents.father_agent:Step: 100, Training loss: 5.7159504890441895
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 105.54048919677734
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 11.553274154663086
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.68569803237915
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9996125532739248
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 13899.5615234375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.75590856257264
INFO:tools.evaluation_results_class:Counted Episodes = 2581
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 5.811309814453125
INFO:agents.father_agent:Step: 110, Training loss: 6.108206748962402
INFO:agents.father_agent:Step: 115, Training loss: 5.957052230834961
INFO:agents.father_agent:Step: 120, Training loss: 6.67857027053833
INFO:agents.father_agent:Step: 125, Training loss: 7.119571685791016
INFO:agents.father_agent:Step: 130, Training loss: 6.28584098815918
INFO:agents.father_agent:Step: 135, Training loss: 5.41318941116333
INFO:agents.father_agent:Step: 140, Training loss: 4.562588691711426
INFO:agents.father_agent:Step: 145, Training loss: 6.258906841278076
INFO:agents.father_agent:Step: 150, Training loss: 5.768538475036621
INFO:agents.father_agent:Step: 155, Training loss: 6.082139492034912
INFO:agents.father_agent:Step: 160, Training loss: 6.294673442840576
INFO:agents.father_agent:Step: 165, Training loss: 6.409134387969971
INFO:agents.father_agent:Step: 170, Training loss: 6.217860221862793
INFO:agents.father_agent:Step: 175, Training loss: 6.417717456817627
INFO:agents.father_agent:Step: 180, Training loss: 5.775248050689697
INFO:agents.father_agent:Step: 185, Training loss: 8.286101341247559
INFO:agents.father_agent:Step: 190, Training loss: 6.331557750701904
INFO:agents.father_agent:Step: 195, Training loss: 6.124590873718262
INFO:agents.father_agent:Step: 200, Training loss: 6.457214832305908
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 114.1976318359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.41264820098877
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.959761619567871
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9964426877470356
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16522.220703125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.9395256916996
INFO:tools.evaluation_results_class:Counted Episodes = 2530
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 7.055053234100342
INFO:agents.father_agent:Step: 210, Training loss: 6.579420566558838
INFO:agents.father_agent:Step: 215, Training loss: 6.699692726135254
INFO:agents.father_agent:Step: 220, Training loss: 5.739720821380615
INFO:agents.father_agent:Step: 225, Training loss: 5.249958038330078
INFO:agents.father_agent:Step: 230, Training loss: 8.036601066589355
INFO:agents.father_agent:Step: 235, Training loss: 6.128241062164307
INFO:agents.father_agent:Step: 240, Training loss: 6.812550067901611
INFO:agents.father_agent:Step: 245, Training loss: 5.625795841217041
INFO:agents.father_agent:Step: 250, Training loss: 6.139548301696777
INFO:agents.father_agent:Step: 255, Training loss: 6.723611831665039
INFO:agents.father_agent:Step: 260, Training loss: 6.779973030090332
INFO:agents.father_agent:Step: 265, Training loss: 6.163397312164307
INFO:agents.father_agent:Step: 270, Training loss: 6.072132110595703
INFO:agents.father_agent:Step: 275, Training loss: 5.510005474090576
INFO:agents.father_agent:Step: 280, Training loss: 5.820909023284912
INFO:agents.father_agent:Step: 285, Training loss: 6.0958027839660645
INFO:agents.father_agent:Step: 290, Training loss: 6.49618673324585
INFO:agents.father_agent:Step: 295, Training loss: 7.126001834869385
INFO:agents.father_agent:Step: 300, Training loss: 6.6654863357543945
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 118.65641784667969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.864022254943848
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.1905341148376465
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9991906110886281
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17776.90625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.4063132335087
INFO:tools.evaluation_results_class:Counted Episodes = 2471
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 6.160251617431641
INFO:agents.father_agent:Step: 310, Training loss: 6.026224136352539
INFO:agents.father_agent:Step: 315, Training loss: 6.661170959472656
INFO:agents.father_agent:Step: 320, Training loss: 6.038251876831055
INFO:agents.father_agent:Step: 325, Training loss: 5.564490795135498
INFO:agents.father_agent:Step: 330, Training loss: 6.487903594970703
INFO:agents.father_agent:Step: 335, Training loss: 7.216208457946777
INFO:agents.father_agent:Step: 340, Training loss: 7.445733547210693
INFO:agents.father_agent:Step: 345, Training loss: 7.336677551269531
INFO:agents.father_agent:Step: 350, Training loss: 5.71221399307251
INFO:agents.father_agent:Step: 355, Training loss: 6.172823905944824
INFO:agents.father_agent:Step: 360, Training loss: 5.4462690353393555
INFO:agents.father_agent:Step: 365, Training loss: 5.304385662078857
INFO:agents.father_agent:Step: 370, Training loss: 5.24939489364624
INFO:agents.father_agent:Step: 375, Training loss: 6.03795862197876
INFO:agents.father_agent:Step: 380, Training loss: 8.600322723388672
INFO:agents.father_agent:Step: 385, Training loss: 5.282834529876709
INFO:agents.father_agent:Step: 390, Training loss: 6.734582901000977
INFO:agents.father_agent:Step: 395, Training loss: 5.895063400268555
INFO:agents.father_agent:Step: 400, Training loss: 7.376665115356445
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 120.70318603515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.067866325378418
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.170750617980957
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9987735077677842
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17999.1796875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.79762878168438
INFO:tools.evaluation_results_class:Counted Episodes = 2446
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 5.866645812988281
INFO:agents.father_agent:Step: 410, Training loss: 5.985840797424316
INFO:agents.father_agent:Step: 415, Training loss: 5.705784320831299
INFO:agents.father_agent:Step: 420, Training loss: 6.902276992797852
INFO:agents.father_agent:Step: 425, Training loss: 6.416818618774414
INFO:agents.father_agent:Step: 430, Training loss: 6.690967082977295
INFO:agents.father_agent:Step: 435, Training loss: 5.833977222442627
INFO:agents.father_agent:Step: 440, Training loss: 6.094120025634766
INFO:agents.father_agent:Step: 445, Training loss: 6.731176853179932
INFO:agents.father_agent:Step: 450, Training loss: 4.8787922859191895
INFO:agents.father_agent:Step: 455, Training loss: 6.058764934539795
INFO:agents.father_agent:Step: 460, Training loss: 5.505537986755371
INFO:agents.father_agent:Step: 465, Training loss: 5.954837322235107
INFO:agents.father_agent:Step: 470, Training loss: 6.139859199523926
INFO:agents.father_agent:Step: 475, Training loss: 6.601536273956299
INFO:agents.father_agent:Step: 480, Training loss: 6.549734115600586
INFO:agents.father_agent:Step: 485, Training loss: 6.304845809936523
INFO:agents.father_agent:Step: 490, Training loss: 6.323292255401611
INFO:agents.father_agent:Step: 495, Training loss: 6.5635175704956055
INFO:agents.father_agent:Step: 500, Training loss: 5.629829406738281
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 119.83773040771484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.979716300964355
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.084364891052246
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9979716024340771
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18013.44140625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.76551724137931
INFO:tools.evaluation_results_class:Counted Episodes = 2465
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 121.9532241821289
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.192039489746094
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.176410675048828
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9983586376692655
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18162.126953125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.67952400492409
INFO:tools.evaluation_results_class:Counted Episodes = 2437
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 141.5841522216797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 15.14108943939209
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.568908214569092
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9913366336633663
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25650.294921875
INFO:tools.evaluation_results_class:Current Best Return = 141.5841522216797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9913366336633663
INFO:tools.evaluation_results_class:Average Episode Length = 125.7190594059406
INFO:tools.evaluation_results_class:Counted Episodes = 2424
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 133.88113403320312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.38093376159668
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.418830394744873
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9964100518548066
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23579.47265625
INFO:tools.evaluation_results_class:Current Best Return = 133.88113403320312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9964100518548066
INFO:tools.evaluation_results_class:Average Episode Length = 121.41244515357
INFO:tools.evaluation_results_class:Counted Episodes = 2507
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 131220
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 541.5963 achieved after 2842.52 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 541.5356 achieved after 2842.53 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 541.5191 achieved after 2842.53 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 145.8602 achieved after 2842.56 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 145.860238155256
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:robust_rl.robust_rl_trainer:Iteration 6 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 114.86690521240234
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.483511924743652
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.9656171798706055
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9984108065156932
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16879.4921875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.36551450139055
INFO:tools.evaluation_results_class:Counted Episodes = 2517
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.976659774780273
INFO:agents.father_agent:Step: 5, Training loss: 6.9290900230407715
INFO:agents.father_agent:Step: 10, Training loss: 6.074912071228027
INFO:agents.father_agent:Step: 15, Training loss: 6.105380535125732
INFO:agents.father_agent:Step: 20, Training loss: 5.3884477615356445
INFO:agents.father_agent:Step: 25, Training loss: 5.971008777618408
INFO:agents.father_agent:Step: 30, Training loss: 6.174515247344971
INFO:agents.father_agent:Step: 35, Training loss: 6.57791805267334
INFO:agents.father_agent:Step: 40, Training loss: 6.151031017303467
INFO:agents.father_agent:Step: 45, Training loss: 7.4029035568237305
INFO:agents.father_agent:Step: 50, Training loss: 6.348268032073975
INFO:agents.father_agent:Step: 55, Training loss: 6.105416297912598
INFO:agents.father_agent:Step: 60, Training loss: 6.182101249694824
INFO:agents.father_agent:Step: 65, Training loss: 5.57761812210083
INFO:agents.father_agent:Step: 70, Training loss: 6.455225944519043
INFO:agents.father_agent:Step: 75, Training loss: 6.251332759857178
INFO:agents.father_agent:Step: 80, Training loss: 7.165425777435303
INFO:agents.father_agent:Step: 85, Training loss: 5.712741374969482
INFO:agents.father_agent:Step: 90, Training loss: 6.261099815368652
INFO:agents.father_agent:Step: 95, Training loss: 5.727981090545654
INFO:agents.father_agent:Step: 100, Training loss: 6.102267742156982
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 121.70352172851562
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.169532775878906
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.11841344833374
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9995904995904996
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18379.41015625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.05364455364456
INFO:tools.evaluation_results_class:Counted Episodes = 2442
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 7.105896949768066
INFO:agents.father_agent:Step: 110, Training loss: 5.8697733879089355
INFO:agents.father_agent:Step: 115, Training loss: 6.631324291229248
INFO:agents.father_agent:Step: 120, Training loss: 6.315116882324219
INFO:agents.father_agent:Step: 125, Training loss: 7.289782524108887
INFO:agents.father_agent:Step: 130, Training loss: 5.7054057121276855
INFO:agents.father_agent:Step: 135, Training loss: 6.5667500495910645
INFO:agents.father_agent:Step: 140, Training loss: 5.3909525871276855
INFO:agents.father_agent:Step: 145, Training loss: 6.210834980010986
INFO:agents.father_agent:Step: 150, Training loss: 7.672468185424805
INFO:agents.father_agent:Step: 155, Training loss: 5.914921283721924
INFO:agents.father_agent:Step: 160, Training loss: 5.645034313201904
INFO:agents.father_agent:Step: 165, Training loss: 6.685416221618652
INFO:agents.father_agent:Step: 170, Training loss: 6.056153774261475
INFO:agents.father_agent:Step: 175, Training loss: 5.860410690307617
INFO:agents.father_agent:Step: 180, Training loss: 7.3307085037231445
INFO:agents.father_agent:Step: 185, Training loss: 7.4683990478515625
INFO:agents.father_agent:Step: 190, Training loss: 6.718390941619873
INFO:agents.father_agent:Step: 195, Training loss: 6.312425136566162
INFO:agents.father_agent:Step: 200, Training loss: 5.835049629211426
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 115.96923065185547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.59538459777832
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.085744380950928
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9992307692307693
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17299.4453125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.93923076923078
INFO:tools.evaluation_results_class:Counted Episodes = 2600
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 6.851756572723389
INFO:agents.father_agent:Step: 210, Training loss: 7.00126838684082
INFO:agents.father_agent:Step: 215, Training loss: 5.30430269241333
INFO:agents.father_agent:Step: 220, Training loss: 5.421182155609131
INFO:agents.father_agent:Step: 225, Training loss: 7.548310279846191
INFO:agents.father_agent:Step: 230, Training loss: 5.062644958496094
INFO:agents.father_agent:Step: 235, Training loss: 5.973336696624756
INFO:agents.father_agent:Step: 240, Training loss: 5.294588565826416
INFO:agents.father_agent:Step: 245, Training loss: 6.039906024932861
INFO:agents.father_agent:Step: 250, Training loss: 5.9215826988220215
INFO:agents.father_agent:Step: 255, Training loss: 6.951162815093994
INFO:agents.father_agent:Step: 260, Training loss: 5.824337959289551
INFO:agents.father_agent:Step: 265, Training loss: 6.0020670890808105
INFO:agents.father_agent:Step: 270, Training loss: 6.8010334968566895
INFO:agents.father_agent:Step: 275, Training loss: 5.071169853210449
INFO:agents.father_agent:Step: 280, Training loss: 5.778422832489014
INFO:agents.father_agent:Step: 285, Training loss: 5.438854694366455
INFO:agents.father_agent:Step: 290, Training loss: 5.961601734161377
INFO:agents.father_agent:Step: 295, Training loss: 6.4839348793029785
INFO:agents.father_agent:Step: 300, Training loss: 5.985664367675781
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 118.41191101074219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.839536666870117
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.036373615264893
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9991728701406121
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18652.234375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.09346567411083
INFO:tools.evaluation_results_class:Counted Episodes = 2418
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 5.055324554443359
INFO:agents.father_agent:Step: 310, Training loss: 6.43208122253418
INFO:agents.father_agent:Step: 315, Training loss: 6.508772373199463
INFO:agents.father_agent:Step: 320, Training loss: 5.16575813293457
INFO:agents.father_agent:Step: 325, Training loss: 5.589850902557373
INFO:agents.father_agent:Step: 330, Training loss: 5.773040294647217
INFO:agents.father_agent:Step: 335, Training loss: 5.3649210929870605
INFO:agents.father_agent:Step: 340, Training loss: 4.977837562561035
INFO:agents.father_agent:Step: 345, Training loss: 5.621018886566162
INFO:agents.father_agent:Step: 350, Training loss: 6.930901527404785
INFO:agents.father_agent:Step: 355, Training loss: 5.443011283874512
INFO:agents.father_agent:Step: 360, Training loss: 6.178966522216797
INFO:agents.father_agent:Step: 365, Training loss: 5.4770073890686035
INFO:agents.father_agent:Step: 370, Training loss: 5.781129837036133
INFO:agents.father_agent:Step: 375, Training loss: 7.1091437339782715
INFO:agents.father_agent:Step: 380, Training loss: 6.445742130279541
INFO:agents.father_agent:Step: 385, Training loss: 6.228840351104736
INFO:agents.father_agent:Step: 390, Training loss: 5.569300174713135
INFO:agents.father_agent:Step: 395, Training loss: 5.438747882843018
INFO:agents.father_agent:Step: 400, Training loss: 6.414996147155762
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 115.53045654296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.545784950256348
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.899240493774414
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9963695038321904
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17151.462890625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.09398951189996
INFO:tools.evaluation_results_class:Counted Episodes = 2479
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 6.34136438369751
INFO:agents.father_agent:Step: 410, Training loss: 7.005662441253662
INFO:agents.father_agent:Step: 415, Training loss: 6.82813835144043
INFO:agents.father_agent:Step: 420, Training loss: 6.445383548736572
INFO:agents.father_agent:Step: 425, Training loss: 5.789213180541992
INFO:agents.father_agent:Step: 430, Training loss: 5.611799240112305
INFO:agents.father_agent:Step: 435, Training loss: 5.972916603088379
INFO:agents.father_agent:Step: 440, Training loss: 5.183743000030518
INFO:agents.father_agent:Step: 445, Training loss: 5.842177391052246
INFO:agents.father_agent:Step: 450, Training loss: 6.767280101776123
INFO:agents.father_agent:Step: 455, Training loss: 5.27296781539917
INFO:agents.father_agent:Step: 460, Training loss: 6.209467887878418
INFO:agents.father_agent:Step: 465, Training loss: 7.150278091430664
INFO:agents.father_agent:Step: 470, Training loss: 6.338915824890137
INFO:agents.father_agent:Step: 475, Training loss: 6.800600051879883
INFO:agents.father_agent:Step: 480, Training loss: 6.718503475189209
INFO:agents.father_agent:Step: 485, Training loss: 5.587846279144287
INFO:agents.father_agent:Step: 490, Training loss: 5.165009498596191
INFO:agents.father_agent:Step: 495, Training loss: 6.114411354064941
INFO:agents.father_agent:Step: 500, Training loss: 4.585288047790527
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 120.64822387695312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.05928897857666
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.088019847869873
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9972332015810277
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18607.01171875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.69841897233202
INFO:tools.evaluation_results_class:Counted Episodes = 2530
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 114.50251770019531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.447928428649902
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.9181132316589355
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988385598141696
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16769.46875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.65543941153697
INFO:tools.evaluation_results_class:Counted Episodes = 2583
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 149.48992919921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 15.940420150756836
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.861371040344238
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9957136733819117
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25647.958984375
INFO:tools.evaluation_results_class:Current Best Return = 149.48992919921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9957136733819117
INFO:tools.evaluation_results_class:Average Episode Length = 130.2666095156451
INFO:tools.evaluation_results_class:Counted Episodes = 2333
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 135.70863342285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.564528465270996
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.536314487457275
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9968329374505146
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21984.751953125
INFO:tools.evaluation_results_class:Current Best Return = 135.70863342285156
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9968329374505146
INFO:tools.evaluation_results_class:Average Episode Length = 121.04275534441805
INFO:tools.evaluation_results_class:Counted Episodes = 2526
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 131220
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 1016.9827 achieved after 3416.45 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1016.656 achieved after 3416.46 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1016.5664 achieved after 3416.47 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 150.3959 achieved after 3416.6 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 150.39590192510946
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:robust_rl.robust_rl_trainer:Iteration 7 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 115.82112884521484
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.578947067260742
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.1367926597595215
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.998417095370004
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16381.4296875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.77522754254056
INFO:tools.evaluation_results_class:Counted Episodes = 2527
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.058713912963867
INFO:agents.father_agent:Step: 5, Training loss: 6.260735988616943
INFO:agents.father_agent:Step: 10, Training loss: 7.711390018463135
INFO:agents.father_agent:Step: 15, Training loss: 6.90140438079834
INFO:agents.father_agent:Step: 20, Training loss: 6.169919013977051
INFO:agents.father_agent:Step: 25, Training loss: 8.528006553649902
INFO:agents.father_agent:Step: 30, Training loss: 7.806458473205566
INFO:agents.father_agent:Step: 35, Training loss: 7.588463306427002
INFO:agents.father_agent:Step: 40, Training loss: 6.132648468017578
INFO:agents.father_agent:Step: 45, Training loss: 7.9506425857543945
INFO:agents.father_agent:Step: 50, Training loss: 5.966076374053955
INFO:agents.father_agent:Step: 55, Training loss: 5.661406993865967
INFO:agents.father_agent:Step: 60, Training loss: 5.614034175872803
INFO:agents.father_agent:Step: 65, Training loss: 6.4232892990112305
INFO:agents.father_agent:Step: 70, Training loss: 6.618957996368408
INFO:agents.father_agent:Step: 75, Training loss: 6.98015832901001
INFO:agents.father_agent:Step: 80, Training loss: 6.617681503295898
INFO:agents.father_agent:Step: 85, Training loss: 5.221291542053223
INFO:agents.father_agent:Step: 90, Training loss: 7.429337501525879
INFO:agents.father_agent:Step: 95, Training loss: 6.3651041984558105
INFO:agents.father_agent:Step: 100, Training loss: 6.396717548370361
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 110.53434753417969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.049417495727539
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.819885730743408
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9979911611088791
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15710.4833984375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.47569304941743
INFO:tools.evaluation_results_class:Counted Episodes = 2489
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 7.743549823760986
INFO:agents.father_agent:Step: 110, Training loss: 5.484925746917725
INFO:agents.father_agent:Step: 115, Training loss: 6.5245466232299805
INFO:agents.father_agent:Step: 120, Training loss: 5.087007999420166
INFO:agents.father_agent:Step: 125, Training loss: 7.071347713470459
INFO:agents.father_agent:Step: 130, Training loss: 5.899582386016846
INFO:agents.father_agent:Step: 135, Training loss: 6.418567180633545
INFO:agents.father_agent:Step: 140, Training loss: 6.049588680267334
INFO:agents.father_agent:Step: 145, Training loss: 7.290346622467041
INFO:agents.father_agent:Step: 150, Training loss: 6.3098673820495605
INFO:agents.father_agent:Step: 155, Training loss: 6.251559734344482
INFO:agents.father_agent:Step: 160, Training loss: 5.510388374328613
INFO:agents.father_agent:Step: 165, Training loss: 5.859035015106201
INFO:agents.father_agent:Step: 170, Training loss: 6.587639331817627
INFO:agents.father_agent:Step: 175, Training loss: 6.489724636077881
INFO:agents.father_agent:Step: 180, Training loss: 6.588868618011475
INFO:agents.father_agent:Step: 185, Training loss: 6.925520420074463
INFO:agents.father_agent:Step: 190, Training loss: 6.721151828765869
INFO:agents.father_agent:Step: 195, Training loss: 7.429965496063232
INFO:agents.father_agent:Step: 200, Training loss: 6.135411739349365
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 122.36576080322266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.234241485595703
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.316666126251221
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988326848249027
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18448.56640625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.58171206225681
INFO:tools.evaluation_results_class:Counted Episodes = 2570
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 7.592808723449707
INFO:agents.father_agent:Step: 210, Training loss: 7.17650842666626
INFO:agents.father_agent:Step: 215, Training loss: 6.9177470207214355
INFO:agents.father_agent:Step: 220, Training loss: 8.486549377441406
INFO:agents.father_agent:Step: 225, Training loss: 6.014070510864258
INFO:agents.father_agent:Step: 230, Training loss: 5.69135856628418
INFO:agents.father_agent:Step: 235, Training loss: 5.427296161651611
INFO:agents.father_agent:Step: 240, Training loss: 5.4213457107543945
INFO:agents.father_agent:Step: 245, Training loss: 6.719364166259766
INFO:agents.father_agent:Step: 250, Training loss: 6.482512474060059
INFO:agents.father_agent:Step: 255, Training loss: 5.848236560821533
INFO:agents.father_agent:Step: 260, Training loss: 5.66523551940918
INFO:agents.father_agent:Step: 265, Training loss: 6.4424357414245605
INFO:agents.father_agent:Step: 270, Training loss: 6.775234222412109
INFO:agents.father_agent:Step: 275, Training loss: 6.01603364944458
INFO:agents.father_agent:Step: 280, Training loss: 6.485769748687744
INFO:agents.father_agent:Step: 285, Training loss: 6.42323112487793
INFO:agents.father_agent:Step: 290, Training loss: 6.709132194519043
INFO:agents.father_agent:Step: 295, Training loss: 7.261381149291992
INFO:agents.father_agent:Step: 300, Training loss: 7.526224613189697
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 111.11813354492188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.107071876525879
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.903007984161377
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9976293954958514
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15118.267578125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.73528249703675
INFO:tools.evaluation_results_class:Counted Episodes = 2531
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 7.022862434387207
INFO:agents.father_agent:Step: 310, Training loss: 6.139416217803955
INFO:agents.father_agent:Step: 315, Training loss: 5.902683734893799
INFO:agents.father_agent:Step: 320, Training loss: 5.460123062133789
INFO:agents.father_agent:Step: 325, Training loss: 6.22254753112793
INFO:agents.father_agent:Step: 330, Training loss: 6.207090377807617
INFO:agents.father_agent:Step: 335, Training loss: 5.928768157958984
INFO:agents.father_agent:Step: 340, Training loss: 6.680691719055176
INFO:agents.father_agent:Step: 345, Training loss: 5.709138870239258
INFO:agents.father_agent:Step: 350, Training loss: 6.38005256652832
INFO:agents.father_agent:Step: 355, Training loss: 6.340807914733887
INFO:agents.father_agent:Step: 360, Training loss: 6.30653715133667
INFO:agents.father_agent:Step: 365, Training loss: 7.001504898071289
INFO:agents.father_agent:Step: 370, Training loss: 5.557754039764404
INFO:agents.father_agent:Step: 375, Training loss: 9.194478988647461
INFO:agents.father_agent:Step: 380, Training loss: 5.99130916595459
INFO:agents.father_agent:Step: 385, Training loss: 7.207483291625977
INFO:agents.father_agent:Step: 390, Training loss: 5.649862289428711
INFO:agents.father_agent:Step: 395, Training loss: 6.150219440460205
INFO:agents.father_agent:Step: 400, Training loss: 6.836012840270996
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 113.90214538574219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.387787818908691
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.007964611053467
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9987868985038415
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15024.03515625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.9708855640922
INFO:tools.evaluation_results_class:Counted Episodes = 2473
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 6.2979326248168945
INFO:agents.father_agent:Step: 410, Training loss: 6.394927024841309
INFO:agents.father_agent:Step: 415, Training loss: 6.032025337219238
INFO:agents.father_agent:Step: 420, Training loss: 5.616884708404541
INFO:agents.father_agent:Step: 425, Training loss: 7.099503993988037
INFO:agents.father_agent:Step: 430, Training loss: 4.767827987670898
INFO:agents.father_agent:Step: 435, Training loss: 7.223165512084961
INFO:agents.father_agent:Step: 440, Training loss: 7.407082557678223
INFO:agents.father_agent:Step: 445, Training loss: 6.641197204589844
INFO:agents.father_agent:Step: 450, Training loss: 6.323697566986084
INFO:agents.father_agent:Step: 455, Training loss: 5.815857410430908
INFO:agents.father_agent:Step: 460, Training loss: 6.138136386871338
INFO:agents.father_agent:Step: 465, Training loss: 6.299691200256348
INFO:agents.father_agent:Step: 470, Training loss: 5.666573524475098
INFO:agents.father_agent:Step: 475, Training loss: 5.924004077911377
INFO:agents.father_agent:Step: 480, Training loss: 6.411180019378662
INFO:agents.father_agent:Step: 485, Training loss: 6.174498081207275
INFO:agents.father_agent:Step: 490, Training loss: 6.333826541900635
INFO:agents.father_agent:Step: 495, Training loss: 5.600230693817139
INFO:agents.father_agent:Step: 500, Training loss: 5.9023051261901855
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 125.71195983886719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.568755149841309
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.496009349822998
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9987794955248169
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17605.779296875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.53580146460537
INFO:tools.evaluation_results_class:Counted Episodes = 2458
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 116.88716125488281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.684046745300293
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.032649040222168
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9976653696498055
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18615.05859375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.72140077821011
INFO:tools.evaluation_results_class:Counted Episodes = 2570
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 157.00563049316406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 16.685850143432617
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.02346658706665
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9926438771094764
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 31112.453125
INFO:tools.evaluation_results_class:Current Best Return = 157.00563049316406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9926438771094764
INFO:tools.evaluation_results_class:Average Episode Length = 130.5179575941151
INFO:tools.evaluation_results_class:Counted Episodes = 2311
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 139.1630096435547
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.913909912109375
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.63300085067749
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988043045037863
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23676.779296875
INFO:tools.evaluation_results_class:Current Best Return = 139.1630096435547
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9988043045037863
INFO:tools.evaluation_results_class:Average Episode Length = 120.74970107612594
INFO:tools.evaluation_results_class:Counted Episodes = 2509
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 131220
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 581.9424 achieved after 3993.98 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 581.9035 achieved after 3993.98 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 158.0575 achieved after 3994.03 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 158.05745827376853
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:robust_rl.robust_rl_trainer:Iteration 8 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 127.55646514892578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.753182411193848
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.474099159240723
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9987679671457905
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19801.296875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.05462012320328
INFO:tools.evaluation_results_class:Counted Episodes = 2435
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.930599689483643
INFO:agents.father_agent:Step: 5, Training loss: 5.954732418060303
INFO:agents.father_agent:Step: 10, Training loss: 5.588107109069824
INFO:agents.father_agent:Step: 15, Training loss: 5.576629638671875
INFO:agents.father_agent:Step: 20, Training loss: 6.475926399230957
INFO:agents.father_agent:Step: 25, Training loss: 7.389554023742676
INFO:agents.father_agent:Step: 30, Training loss: 7.4729132652282715
INFO:agents.father_agent:Step: 35, Training loss: 7.652044773101807
INFO:agents.father_agent:Step: 40, Training loss: 4.824005603790283
INFO:agents.father_agent:Step: 45, Training loss: 6.137105941772461
INFO:agents.father_agent:Step: 50, Training loss: 6.646721363067627
INFO:agents.father_agent:Step: 55, Training loss: 6.014156341552734
INFO:agents.father_agent:Step: 60, Training loss: 5.708845138549805
INFO:agents.father_agent:Step: 65, Training loss: 5.315739631652832
INFO:agents.father_agent:Step: 70, Training loss: 7.14202356338501
INFO:agents.father_agent:Step: 75, Training loss: 6.41470193862915
INFO:agents.father_agent:Step: 80, Training loss: 6.944883346557617
INFO:agents.father_agent:Step: 85, Training loss: 7.523043632507324
INFO:agents.father_agent:Step: 90, Training loss: 6.7422332763671875
INFO:agents.father_agent:Step: 95, Training loss: 6.3000006675720215
INFO:agents.father_agent:Step: 100, Training loss: 6.847054481506348
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 118.44613647460938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.843812942504883
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.1663970947265625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9995995194233079
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16685.12890625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.7537044453344
INFO:tools.evaluation_results_class:Counted Episodes = 2497
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 5.8883137702941895
INFO:agents.father_agent:Step: 110, Training loss: 6.386667728424072
INFO:agents.father_agent:Step: 115, Training loss: 5.4128265380859375
INFO:agents.father_agent:Step: 120, Training loss: 5.48704195022583
INFO:agents.father_agent:Step: 125, Training loss: 4.83997917175293
INFO:agents.father_agent:Step: 130, Training loss: 5.845348834991455
INFO:agents.father_agent:Step: 135, Training loss: 6.036131381988525
INFO:agents.father_agent:Step: 140, Training loss: 5.983754634857178
INFO:agents.father_agent:Step: 145, Training loss: 5.54878568649292
INFO:agents.father_agent:Step: 150, Training loss: 5.447357177734375
INFO:agents.father_agent:Step: 155, Training loss: 6.323862075805664
INFO:agents.father_agent:Step: 160, Training loss: 6.144177436828613
INFO:agents.father_agent:Step: 165, Training loss: 6.26200008392334
INFO:agents.father_agent:Step: 170, Training loss: 7.279195785522461
INFO:agents.father_agent:Step: 175, Training loss: 6.649927616119385
INFO:agents.father_agent:Step: 180, Training loss: 6.554861545562744
INFO:agents.father_agent:Step: 185, Training loss: 5.9053192138671875
INFO:agents.father_agent:Step: 190, Training loss: 5.070712566375732
INFO:agents.father_agent:Step: 195, Training loss: 6.816132068634033
INFO:agents.father_agent:Step: 200, Training loss: 5.826594829559326
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 112.10802459716797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.207801818847656
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.903228759765625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9984996249062266
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15781.677734375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.60765191297824
INFO:tools.evaluation_results_class:Counted Episodes = 2666
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 5.759581089019775
INFO:agents.father_agent:Step: 210, Training loss: 6.153250694274902
INFO:agents.father_agent:Step: 215, Training loss: 5.9987568855285645
INFO:agents.father_agent:Step: 220, Training loss: 6.044266700744629
INFO:agents.father_agent:Step: 225, Training loss: 6.653824329376221
INFO:agents.father_agent:Step: 230, Training loss: 7.658427715301514
INFO:agents.father_agent:Step: 235, Training loss: 5.87774658203125
INFO:agents.father_agent:Step: 240, Training loss: 6.69758939743042
INFO:agents.father_agent:Step: 245, Training loss: 8.365424156188965
INFO:agents.father_agent:Step: 250, Training loss: 6.637237548828125
INFO:agents.father_agent:Step: 255, Training loss: 5.967598915100098
INFO:agents.father_agent:Step: 260, Training loss: 6.078152656555176
INFO:agents.father_agent:Step: 265, Training loss: 5.704794883728027
INFO:agents.father_agent:Step: 270, Training loss: 7.049102783203125
INFO:agents.father_agent:Step: 275, Training loss: 5.942655563354492
INFO:agents.father_agent:Step: 280, Training loss: 5.9209747314453125
INFO:agents.father_agent:Step: 285, Training loss: 7.124337196350098
INFO:agents.father_agent:Step: 290, Training loss: 5.812422752380371
INFO:agents.father_agent:Step: 295, Training loss: 6.366081237792969
INFO:agents.father_agent:Step: 300, Training loss: 6.405941963195801
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 118.8268814086914
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.880244255065918
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.097870826721191
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9987780040733197
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18721.556640625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.949083503055
INFO:tools.evaluation_results_class:Counted Episodes = 2455
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 5.381060600280762
INFO:agents.father_agent:Step: 310, Training loss: 5.716531276702881
INFO:agents.father_agent:Step: 315, Training loss: 4.9923176765441895
INFO:agents.father_agent:Step: 320, Training loss: 6.836911201477051
INFO:agents.father_agent:Step: 325, Training loss: 5.779417037963867
INFO:agents.father_agent:Step: 330, Training loss: 7.089065074920654
INFO:agents.father_agent:Step: 335, Training loss: 8.049094200134277
INFO:agents.father_agent:Step: 340, Training loss: 5.878228664398193
INFO:agents.father_agent:Step: 345, Training loss: 5.406968593597412
INFO:agents.father_agent:Step: 350, Training loss: 6.076835632324219
INFO:agents.father_agent:Step: 355, Training loss: 6.898627758026123
INFO:agents.father_agent:Step: 360, Training loss: 6.181546211242676
INFO:agents.father_agent:Step: 365, Training loss: 5.778982639312744
INFO:agents.father_agent:Step: 370, Training loss: 6.898027420043945
INFO:agents.father_agent:Step: 375, Training loss: 7.5262227058410645
INFO:agents.father_agent:Step: 380, Training loss: 5.584316253662109
INFO:agents.father_agent:Step: 385, Training loss: 6.847114086151123
INFO:agents.father_agent:Step: 390, Training loss: 5.786708831787109
INFO:agents.father_agent:Step: 395, Training loss: 6.20557165145874
INFO:agents.father_agent:Step: 400, Training loss: 4.80797004699707
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 122.07897186279297
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.20386791229248
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.3425469398498535
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9979854955680902
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17618.23828125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.79170024174053
INFO:tools.evaluation_results_class:Counted Episodes = 2482
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 6.1401777267456055
INFO:agents.father_agent:Step: 410, Training loss: 5.870676517486572
INFO:agents.father_agent:Step: 415, Training loss: 6.316328525543213
INFO:agents.father_agent:Step: 420, Training loss: 6.837085723876953
INFO:agents.father_agent:Step: 425, Training loss: 6.716129779815674
INFO:agents.father_agent:Step: 430, Training loss: 5.174465179443359
INFO:agents.father_agent:Step: 435, Training loss: 5.596352577209473
INFO:agents.father_agent:Step: 440, Training loss: 6.37427282333374
INFO:agents.father_agent:Step: 445, Training loss: 6.123776435852051
INFO:agents.father_agent:Step: 450, Training loss: 6.29926061630249
INFO:agents.father_agent:Step: 455, Training loss: 6.989103317260742
INFO:agents.father_agent:Step: 460, Training loss: 6.915155410766602
INFO:agents.father_agent:Step: 465, Training loss: 6.394895076751709
INFO:agents.father_agent:Step: 470, Training loss: 6.853109836578369
INFO:agents.father_agent:Step: 475, Training loss: 5.753210544586182
INFO:agents.father_agent:Step: 480, Training loss: 6.371788024902344
INFO:agents.father_agent:Step: 485, Training loss: 6.378171920776367
INFO:agents.father_agent:Step: 490, Training loss: 6.27076530456543
INFO:agents.father_agent:Step: 495, Training loss: 5.817770004272461
INFO:agents.father_agent:Step: 500, Training loss: 6.354750633239746
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 122.58755493164062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.252986907958984
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.151284694671631
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9971157807993407
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18281.5625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 113.66831479192419
INFO:tools.evaluation_results_class:Counted Episodes = 2427
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 124.67874908447266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.463756561279297
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.182711124420166
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9979406919275123
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19775.14453125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.32413509060956
INFO:tools.evaluation_results_class:Counted Episodes = 2428
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 142.45962524414062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 15.236024856567383
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.611499786376953
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9950310559006211
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25791.21875
INFO:tools.evaluation_results_class:Current Best Return = 142.45962524414062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9950310559006211
INFO:tools.evaluation_results_class:Average Episode Length = 124.28488612836439
INFO:tools.evaluation_results_class:Counted Episodes = 2415
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 134.46707153320312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.440319061279297
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.489317417144775
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9968063872255489
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22427.033203125
INFO:tools.evaluation_results_class:Current Best Return = 134.46707153320312
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9968063872255489
INFO:tools.evaluation_results_class:Average Episode Length = 120.29540918163673
INFO:tools.evaluation_results_class:Counted Episodes = 2505
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 131220
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 1063.0548 achieved after 4570.83 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1062.1426 achieved after 4570.84 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1061.8879 achieved after 4570.84 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1034.2202 achieved after 4571.02 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1033.6844 achieved after 4571.03 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1031.4036 achieved after 4571.06 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1030.8244 achieved after 4571.07 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1020.7071 achieved after 4571.29 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1020.6097 achieved after 4571.3 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 998.9563 achieved after 4571.5 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 995.9717 achieved after 4571.51 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 995.5538 achieved after 4571.52 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 726.6142 achieved after 4571.69 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 726.6055 achieved after 4571.69 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 725.962 achieved after 4571.73 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 725.9435 achieved after 4571.74 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 538.0755 achieved after 4571.97 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 538.0704 achieved after 4571.98 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 499.3874 achieved after 4572.16 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 499.3812 achieved after 4572.16 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 426.9199 achieved after 4572.22 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 426.9154 achieved after 4572.23 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 152.7136 achieved after 4572.37 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 152.71357266925943
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:robust_rl.robust_rl_trainer:Iteration 9 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 117.79859161376953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.775176048278809
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.116440773010254
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9976580796252927
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 19455.576171875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.21779859484778
INFO:tools.evaluation_results_class:Counted Episodes = 2562
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.519786834716797
INFO:agents.father_agent:Step: 5, Training loss: 6.52091646194458
INFO:agents.father_agent:Step: 10, Training loss: 5.627841949462891
INFO:agents.father_agent:Step: 15, Training loss: 6.591888904571533
INFO:agents.father_agent:Step: 20, Training loss: 6.631446361541748
INFO:agents.father_agent:Step: 25, Training loss: 7.3277268409729
INFO:agents.father_agent:Step: 30, Training loss: 7.378269195556641
INFO:agents.father_agent:Step: 35, Training loss: 6.28031587600708
INFO:agents.father_agent:Step: 40, Training loss: 5.22119665145874
INFO:agents.father_agent:Step: 45, Training loss: 5.457557201385498
INFO:agents.father_agent:Step: 50, Training loss: 5.966043472290039
INFO:agents.father_agent:Step: 55, Training loss: 6.840553283691406
INFO:agents.father_agent:Step: 60, Training loss: 6.361926555633545
INFO:agents.father_agent:Step: 65, Training loss: 7.268738269805908
INFO:agents.father_agent:Step: 70, Training loss: 6.095438003540039
INFO:agents.father_agent:Step: 75, Training loss: 5.9321088790893555
INFO:agents.father_agent:Step: 80, Training loss: 7.287229061126709
INFO:agents.father_agent:Step: 85, Training loss: 5.709557056427002
INFO:agents.father_agent:Step: 90, Training loss: 7.027261734008789
INFO:agents.father_agent:Step: 95, Training loss: 5.761179447174072
INFO:agents.father_agent:Step: 100, Training loss: 6.562197208404541
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 125.43816375732422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.539721488952637
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.4251627922058105
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.997952497952498
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18676.404296875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.34111384111384
INFO:tools.evaluation_results_class:Counted Episodes = 2442
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 5.685607433319092
INFO:agents.father_agent:Step: 110, Training loss: 7.213993549346924
INFO:agents.father_agent:Step: 115, Training loss: 6.597235679626465
INFO:agents.father_agent:Step: 120, Training loss: 8.123100280761719
INFO:agents.father_agent:Step: 125, Training loss: 6.110349178314209
INFO:agents.father_agent:Step: 130, Training loss: 7.025285720825195
INFO:agents.father_agent:Step: 135, Training loss: 6.665767669677734
INFO:agents.father_agent:Step: 140, Training loss: 5.864353179931641
INFO:agents.father_agent:Step: 145, Training loss: 5.635279655456543
INFO:agents.father_agent:Step: 150, Training loss: 6.8617777824401855
INFO:agents.father_agent:Step: 155, Training loss: 6.800343990325928
INFO:agents.father_agent:Step: 160, Training loss: 5.658390045166016
INFO:agents.father_agent:Step: 165, Training loss: 6.599184036254883
INFO:agents.father_agent:Step: 170, Training loss: 6.017125129699707
INFO:agents.father_agent:Step: 175, Training loss: 7.577653408050537
INFO:agents.father_agent:Step: 180, Training loss: 7.741282939910889
INFO:agents.father_agent:Step: 185, Training loss: 7.158028602600098
INFO:agents.father_agent:Step: 190, Training loss: 6.213776588439941
INFO:agents.father_agent:Step: 195, Training loss: 6.657613754272461
INFO:agents.father_agent:Step: 200, Training loss: 6.431698799133301
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 112.89236450195312
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.289237022399902
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.907145977020264
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15751.5947265625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.24500978473581
INFO:tools.evaluation_results_class:Counted Episodes = 2555
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 4.669464588165283
INFO:agents.father_agent:Step: 210, Training loss: 6.212978839874268
INFO:agents.father_agent:Step: 215, Training loss: 5.854001998901367
INFO:agents.father_agent:Step: 220, Training loss: 6.189434051513672
INFO:agents.father_agent:Step: 225, Training loss: 5.696705341339111
INFO:agents.father_agent:Step: 230, Training loss: 7.8366875648498535
INFO:agents.father_agent:Step: 235, Training loss: 7.652747631072998
INFO:agents.father_agent:Step: 240, Training loss: 5.875597953796387
INFO:agents.father_agent:Step: 245, Training loss: 6.988555908203125
INFO:agents.father_agent:Step: 250, Training loss: 6.842817783355713
INFO:agents.father_agent:Step: 255, Training loss: 5.517460346221924
INFO:agents.father_agent:Step: 260, Training loss: 6.930553436279297
INFO:agents.father_agent:Step: 265, Training loss: 6.357874870300293
INFO:agents.father_agent:Step: 270, Training loss: 5.462675094604492
INFO:agents.father_agent:Step: 275, Training loss: 6.890106678009033
INFO:agents.father_agent:Step: 280, Training loss: 7.865045547485352
INFO:agents.father_agent:Step: 285, Training loss: 7.754081726074219
INFO:agents.father_agent:Step: 290, Training loss: 6.747724533081055
INFO:agents.father_agent:Step: 295, Training loss: 7.617092132568359
INFO:agents.father_agent:Step: 300, Training loss: 6.106090068817139
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 109.19731140136719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 11.9165678024292
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.857134819030762
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9984183471727955
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14469.3037109375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.28786081455121
INFO:tools.evaluation_results_class:Counted Episodes = 2529
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 7.309399604797363
INFO:agents.father_agent:Step: 310, Training loss: 5.250945568084717
INFO:agents.father_agent:Step: 315, Training loss: 5.956218242645264
INFO:agents.father_agent:Step: 320, Training loss: 6.9331183433532715
INFO:agents.father_agent:Step: 325, Training loss: 7.046900272369385
INFO:agents.father_agent:Step: 330, Training loss: 6.561826229095459
INFO:agents.father_agent:Step: 335, Training loss: 7.313136577606201
INFO:agents.father_agent:Step: 340, Training loss: 5.725825309753418
INFO:agents.father_agent:Step: 345, Training loss: 5.632630348205566
INFO:agents.father_agent:Step: 350, Training loss: 5.8628740310668945
INFO:agents.father_agent:Step: 355, Training loss: 7.148489475250244
INFO:agents.father_agent:Step: 360, Training loss: 7.030120372772217
INFO:agents.father_agent:Step: 365, Training loss: 6.40359354019165
INFO:agents.father_agent:Step: 370, Training loss: 5.333794593811035
INFO:agents.father_agent:Step: 375, Training loss: 6.539862155914307
INFO:agents.father_agent:Step: 380, Training loss: 7.762920379638672
INFO:agents.father_agent:Step: 385, Training loss: 7.805646896362305
INFO:agents.father_agent:Step: 390, Training loss: 7.33590841293335
INFO:agents.father_agent:Step: 395, Training loss: 6.594534397125244
INFO:agents.father_agent:Step: 400, Training loss: 7.227494239807129
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 121.20063018798828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.118483543395996
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.346732139587402
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9992101105845181
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17108.1953125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.07898894154819
INFO:tools.evaluation_results_class:Counted Episodes = 2532
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 6.834801197052002
INFO:agents.father_agent:Step: 410, Training loss: 7.433077335357666
INFO:agents.father_agent:Step: 415, Training loss: 6.813177108764648
INFO:agents.father_agent:Step: 420, Training loss: 6.381399631500244
INFO:agents.father_agent:Step: 425, Training loss: 6.7910542488098145
INFO:agents.father_agent:Step: 430, Training loss: 6.353574275970459
INFO:agents.father_agent:Step: 435, Training loss: 5.762293338775635
INFO:agents.father_agent:Step: 440, Training loss: 6.547077655792236
INFO:agents.father_agent:Step: 445, Training loss: 5.372221946716309
INFO:agents.father_agent:Step: 450, Training loss: 5.662865161895752
INFO:agents.father_agent:Step: 455, Training loss: 5.816431999206543
INFO:agents.father_agent:Step: 460, Training loss: 5.676017761230469
INFO:agents.father_agent:Step: 465, Training loss: 6.148895263671875
INFO:agents.father_agent:Step: 470, Training loss: 5.563199520111084
INFO:agents.father_agent:Step: 475, Training loss: 6.112864017486572
INFO:agents.father_agent:Step: 480, Training loss: 6.226208209991455
INFO:agents.father_agent:Step: 485, Training loss: 6.129925727844238
INFO:agents.father_agent:Step: 490, Training loss: 6.696139335632324
INFO:agents.father_agent:Step: 495, Training loss: 6.265610694885254
INFO:agents.father_agent:Step: 500, Training loss: 5.735601902008057
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 116.81942749023438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.679593086242676
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.138503074645996
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988249118683902
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17449.498046875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.82804543674109
INFO:tools.evaluation_results_class:Counted Episodes = 2553
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 123.8087387084961
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.374279022216797
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.172017574310303
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9967023907666942
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20185.98828125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.23495465787305
INFO:tools.evaluation_results_class:Counted Episodes = 2426
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 143.79452514648438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 15.370339393615723
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.555772304534912
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9954432477216238
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25895.458984375
INFO:tools.evaluation_results_class:Current Best Return = 143.79452514648438
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9954432477216238
INFO:tools.evaluation_results_class:Average Episode Length = 126.68392709196354
INFO:tools.evaluation_results_class:Counted Episodes = 2414
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 130.78233337402344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.066883087158203
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.216588973999023
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9943250912038913
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22892.740234375
INFO:tools.evaluation_results_class:Current Best Return = 130.78233337402344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9943250912038913
INFO:tools.evaluation_results_class:Average Episode Length = 121.98621807863802
INFO:tools.evaluation_results_class:Counted Episodes = 2467
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 131220
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 1047.8895 achieved after 5146.16 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1038.2984 achieved after 5146.3 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1037.7017 achieved after 5146.31 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1035.4535 achieved after 5146.32 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1034.7868 achieved after 5146.34 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1004.963 achieved after 5146.55 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1004.8749 achieved after 5146.56 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 705.4326 achieved after 5146.71 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 705.4253 achieved after 5146.72 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 705.2498 achieved after 5146.75 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 519.7552 achieved after 5146.88 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 519.751 achieved after 5146.88 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 480.7912 achieved after 5147.08 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 410.9234 achieved after 5147.12 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 410.9101 achieved after 5147.12 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 146.4377 achieved after 5147.24 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 146.43771082556665
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:robust_rl.robust_rl_trainer:Iteration 10 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 115.36418151855469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.533199310302734
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.916093349456787
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9983903420523139
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16448.52734375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.72474849094567
INFO:tools.evaluation_results_class:Counted Episodes = 2485
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.488650798797607
INFO:agents.father_agent:Step: 5, Training loss: 6.618887901306152
INFO:agents.father_agent:Step: 10, Training loss: 5.111416339874268
INFO:agents.father_agent:Step: 15, Training loss: 6.595612049102783
INFO:agents.father_agent:Step: 20, Training loss: 6.501930236816406
INFO:agents.father_agent:Step: 25, Training loss: 6.080930233001709
INFO:agents.father_agent:Step: 30, Training loss: 5.317494869232178
INFO:agents.father_agent:Step: 35, Training loss: 7.438205242156982
INFO:agents.father_agent:Step: 40, Training loss: 5.69981050491333
INFO:agents.father_agent:Step: 45, Training loss: 6.221301078796387
INFO:agents.father_agent:Step: 50, Training loss: 5.789542198181152
INFO:agents.father_agent:Step: 55, Training loss: 6.657585144042969
INFO:agents.father_agent:Step: 60, Training loss: 6.609440803527832
INFO:agents.father_agent:Step: 65, Training loss: 7.237948894500732
INFO:agents.father_agent:Step: 70, Training loss: 6.88327693939209
INFO:agents.father_agent:Step: 75, Training loss: 5.553605079650879
INFO:agents.father_agent:Step: 80, Training loss: 4.933440208435059
INFO:agents.father_agent:Step: 85, Training loss: 5.978973865509033
INFO:agents.father_agent:Step: 90, Training loss: 6.792059421539307
INFO:agents.father_agent:Step: 95, Training loss: 5.8737874031066895
INFO:agents.father_agent:Step: 100, Training loss: 6.504685401916504
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 103.15058135986328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 11.314285278320312
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.646613597869873
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9996138996138996
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12902.197265625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 104.40579150579151
INFO:tools.evaluation_results_class:Counted Episodes = 2590
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 6.860536575317383
INFO:agents.father_agent:Step: 110, Training loss: 5.600076675415039
INFO:agents.father_agent:Step: 115, Training loss: 7.064000606536865
INFO:agents.father_agent:Step: 120, Training loss: 7.221163272857666
INFO:agents.father_agent:Step: 125, Training loss: 6.131716728210449
INFO:agents.father_agent:Step: 130, Training loss: 5.413084506988525
INFO:agents.father_agent:Step: 135, Training loss: 5.8039326667785645
INFO:agents.father_agent:Step: 140, Training loss: 7.791793346405029
INFO:agents.father_agent:Step: 145, Training loss: 7.784603595733643
INFO:agents.father_agent:Step: 150, Training loss: 5.599043369293213
INFO:agents.father_agent:Step: 155, Training loss: 6.016077995300293
INFO:agents.father_agent:Step: 160, Training loss: 6.842860698699951
INFO:agents.father_agent:Step: 165, Training loss: 6.162600517272949
INFO:agents.father_agent:Step: 170, Training loss: 6.1140031814575195
INFO:agents.father_agent:Step: 175, Training loss: 7.007221698760986
INFO:agents.father_agent:Step: 180, Training loss: 7.991151809692383
INFO:agents.father_agent:Step: 185, Training loss: 6.03402853012085
INFO:agents.father_agent:Step: 190, Training loss: 7.054358005523682
INFO:agents.father_agent:Step: 195, Training loss: 6.137218475341797
INFO:agents.father_agent:Step: 200, Training loss: 6.245273590087891
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 114.78191375732422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.473389625549316
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.012010097503662
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9975990396158463
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17350.6015625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.80072028811524
INFO:tools.evaluation_results_class:Counted Episodes = 2499
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 6.121031761169434
INFO:agents.father_agent:Step: 210, Training loss: 6.054867744445801
INFO:agents.father_agent:Step: 215, Training loss: 6.248967170715332
INFO:agents.father_agent:Step: 220, Training loss: 5.661415100097656
INFO:agents.father_agent:Step: 225, Training loss: 7.045327186584473
INFO:agents.father_agent:Step: 230, Training loss: 6.121923446655273
INFO:agents.father_agent:Step: 235, Training loss: 6.6031389236450195
INFO:agents.father_agent:Step: 240, Training loss: 6.601532459259033
INFO:agents.father_agent:Step: 245, Training loss: 6.763672351837158
INFO:agents.father_agent:Step: 250, Training loss: 4.623969078063965
INFO:agents.father_agent:Step: 255, Training loss: 6.322731018066406
INFO:agents.father_agent:Step: 260, Training loss: 5.205433368682861
INFO:agents.father_agent:Step: 265, Training loss: 6.843305587768555
INFO:agents.father_agent:Step: 270, Training loss: 7.780742645263672
INFO:agents.father_agent:Step: 275, Training loss: 7.7087249755859375
INFO:agents.father_agent:Step: 280, Training loss: 6.187180042266846
INFO:agents.father_agent:Step: 285, Training loss: 6.097020626068115
INFO:agents.father_agent:Step: 290, Training loss: 6.2870097160339355
INFO:agents.father_agent:Step: 295, Training loss: 7.045931339263916
INFO:agents.father_agent:Step: 300, Training loss: 7.419173240661621
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 122.16606140136719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.211792945861816
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.439157962799072
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9975932611311673
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17393.783203125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.61572402727637
INFO:tools.evaluation_results_class:Counted Episodes = 2493
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 7.604324817657471
INFO:agents.father_agent:Step: 310, Training loss: 5.372364521026611
INFO:agents.father_agent:Step: 315, Training loss: 6.1846699714660645
INFO:agents.father_agent:Step: 320, Training loss: 6.98467493057251
INFO:agents.father_agent:Step: 325, Training loss: 8.120018005371094
INFO:agents.father_agent:Step: 330, Training loss: 7.8830060958862305
INFO:agents.father_agent:Step: 335, Training loss: 6.827909469604492
INFO:agents.father_agent:Step: 340, Training loss: 6.647763252258301
INFO:agents.father_agent:Step: 345, Training loss: 5.850245475769043
INFO:agents.father_agent:Step: 350, Training loss: 6.269415855407715
INFO:agents.father_agent:Step: 355, Training loss: 6.897439956665039
INFO:agents.father_agent:Step: 360, Training loss: 5.826292514801025
INFO:agents.father_agent:Step: 365, Training loss: 5.6893157958984375
INFO:agents.father_agent:Step: 370, Training loss: 5.42591667175293
INFO:agents.father_agent:Step: 375, Training loss: 6.751382827758789
INFO:agents.father_agent:Step: 380, Training loss: 6.633932113647461
INFO:agents.father_agent:Step: 385, Training loss: 6.433431625366211
INFO:agents.father_agent:Step: 390, Training loss: 7.424229621887207
INFO:agents.father_agent:Step: 395, Training loss: 6.576507568359375
INFO:agents.father_agent:Step: 400, Training loss: 5.582583904266357
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 123.6455307006836
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.36119270324707
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.320181846618652
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.998320033599328
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18086.669921875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 114.4502309953801
INFO:tools.evaluation_results_class:Counted Episodes = 2381
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 6.188445568084717
INFO:agents.father_agent:Step: 410, Training loss: 7.634721755981445
INFO:agents.father_agent:Step: 415, Training loss: 7.300227165222168
INFO:agents.father_agent:Step: 420, Training loss: 6.6344075202941895
INFO:agents.father_agent:Step: 425, Training loss: 6.390604019165039
INFO:agents.father_agent:Step: 430, Training loss: 6.295106410980225
INFO:agents.father_agent:Step: 435, Training loss: 7.506646633148193
INFO:agents.father_agent:Step: 440, Training loss: 6.153892517089844
INFO:agents.father_agent:Step: 445, Training loss: 6.338796138763428
INFO:agents.father_agent:Step: 450, Training loss: 5.691067218780518
INFO:agents.father_agent:Step: 455, Training loss: 7.874696254730225
INFO:agents.father_agent:Step: 460, Training loss: 4.95296573638916
INFO:agents.father_agent:Step: 465, Training loss: 5.5396881103515625
INFO:agents.father_agent:Step: 470, Training loss: 5.422296524047852
INFO:agents.father_agent:Step: 475, Training loss: 5.254239082336426
INFO:agents.father_agent:Step: 480, Training loss: 5.833627700805664
INFO:agents.father_agent:Step: 485, Training loss: 7.0884857177734375
INFO:agents.father_agent:Step: 490, Training loss: 6.125461578369141
INFO:agents.father_agent:Step: 495, Training loss: 6.547722816467285
INFO:agents.father_agent:Step: 500, Training loss: 7.0306267738342285
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 121.12606811523438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.110158920288086
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.286866188049316
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9987760097919217
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18342.91796875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.3141574867401
INFO:tools.evaluation_results_class:Counted Episodes = 2451
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 119.49076080322266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.946612358093262
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.141231536865234
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9987679671457905
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17934.85546875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.44640657084189
INFO:tools.evaluation_results_class:Counted Episodes = 2435
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 139.5806427001953
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.953226089477539
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.590444564819336
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9975806451612903
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23211.4375
INFO:tools.evaluation_results_class:Current Best Return = 139.5806427001953
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9975806451612903
INFO:tools.evaluation_results_class:Average Episode Length = 122.10887096774194
INFO:tools.evaluation_results_class:Counted Episodes = 2480
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 135.94439697265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.587898254394531
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.480329990386963
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9967293540474244
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25352.970703125
INFO:tools.evaluation_results_class:Current Best Return = 135.94439697265625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9967293540474244
INFO:tools.evaluation_results_class:Average Episode Length = 119.70645952575634
INFO:tools.evaluation_results_class:Counted Episodes = 2446
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 131220
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 1054.4274 achieved after 5722.34 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1054.1942 achieved after 5722.34 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1039.2169 achieved after 5722.49 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1038.653 achieved after 5722.5 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1037.4186 achieved after 5722.52 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1036.8037 achieved after 5722.53 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1012.2117 achieved after 5722.74 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1011.9092 achieved after 5722.75 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1011.8268 achieved after 5722.75 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 713.726 achieved after 5722.91 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 713.7195 achieved after 5722.92 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 713.5101 achieved after 5722.95 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 713.4953 achieved after 5722.95 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 525.8523 achieved after 5723.06 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 525.8486 achieved after 5723.07 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 147.9873 achieved after 5723.25 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 147.98730732547781
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:robust_rl.robust_rl_trainer:Iteration 11 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 116.8433609008789
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.681147575378418
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.011969566345215
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9984057393383818
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17646.791015625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.60860900757274
INFO:tools.evaluation_results_class:Counted Episodes = 2509
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.143204689025879
INFO:agents.father_agent:Step: 5, Training loss: 6.361684799194336
INFO:agents.father_agent:Step: 10, Training loss: 5.16627836227417
INFO:agents.father_agent:Step: 15, Training loss: 7.891656875610352
INFO:agents.father_agent:Step: 20, Training loss: 7.843139171600342
INFO:agents.father_agent:Step: 25, Training loss: 6.310791015625
INFO:agents.father_agent:Step: 30, Training loss: 8.368231773376465
INFO:agents.father_agent:Step: 35, Training loss: 7.796811103820801
INFO:agents.father_agent:Step: 40, Training loss: 6.391146659851074
INFO:agents.father_agent:Step: 45, Training loss: 6.735849857330322
INFO:agents.father_agent:Step: 50, Training loss: 7.393636703491211
INFO:agents.father_agent:Step: 55, Training loss: 7.022668838500977
INFO:agents.father_agent:Step: 60, Training loss: 6.074535846710205
INFO:agents.father_agent:Step: 65, Training loss: 7.570723056793213
INFO:agents.father_agent:Step: 70, Training loss: 6.810173988342285
INFO:agents.father_agent:Step: 75, Training loss: 5.877283096313477
INFO:agents.father_agent:Step: 80, Training loss: 5.780975341796875
INFO:agents.father_agent:Step: 85, Training loss: 6.720086097717285
INFO:agents.father_agent:Step: 90, Training loss: 6.575045108795166
INFO:agents.father_agent:Step: 95, Training loss: 7.083106517791748
INFO:agents.father_agent:Step: 100, Training loss: 6.226620197296143
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 113.01438903808594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.2975492477417
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.8914690017700195
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9980552314274601
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17045.87109375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.52936600544535
INFO:tools.evaluation_results_class:Counted Episodes = 2571
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 7.644628047943115
INFO:agents.father_agent:Step: 110, Training loss: 7.774749755859375
INFO:agents.father_agent:Step: 115, Training loss: 5.6314849853515625
INFO:agents.father_agent:Step: 120, Training loss: 6.440525054931641
INFO:agents.father_agent:Step: 125, Training loss: 5.899019241333008
INFO:agents.father_agent:Step: 130, Training loss: 6.852451324462891
INFO:agents.father_agent:Step: 135, Training loss: 5.915266990661621
INFO:agents.father_agent:Step: 140, Training loss: 6.45010232925415
INFO:agents.father_agent:Step: 145, Training loss: 6.970090389251709
INFO:agents.father_agent:Step: 150, Training loss: 7.2692179679870605
INFO:agents.father_agent:Step: 155, Training loss: 7.889278888702393
INFO:agents.father_agent:Step: 160, Training loss: 6.544458866119385
INFO:agents.father_agent:Step: 165, Training loss: 7.247042179107666
INFO:agents.father_agent:Step: 170, Training loss: 6.001193523406982
INFO:agents.father_agent:Step: 175, Training loss: 6.877914905548096
INFO:agents.father_agent:Step: 180, Training loss: 7.9216108322143555
INFO:agents.father_agent:Step: 185, Training loss: 7.470239162445068
INFO:agents.father_agent:Step: 190, Training loss: 6.188751697540283
INFO:agents.father_agent:Step: 195, Training loss: 5.804864883422852
INFO:agents.father_agent:Step: 200, Training loss: 7.711439609527588
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 110.83659362792969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.0820951461792
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.939858913421631
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9992181391712275
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15633.310546875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.11415168100078
INFO:tools.evaluation_results_class:Counted Episodes = 2558
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 6.281462669372559
INFO:agents.father_agent:Step: 210, Training loss: 6.3668131828308105
INFO:agents.father_agent:Step: 215, Training loss: 6.405704021453857
INFO:agents.father_agent:Step: 220, Training loss: 6.791011333465576
INFO:agents.father_agent:Step: 225, Training loss: 7.129430770874023
INFO:agents.father_agent:Step: 230, Training loss: 8.014942169189453
INFO:agents.father_agent:Step: 235, Training loss: 6.189160346984863
INFO:agents.father_agent:Step: 240, Training loss: 7.654813766479492
INFO:agents.father_agent:Step: 245, Training loss: 8.346593856811523
INFO:agents.father_agent:Step: 250, Training loss: 7.1547465324401855
INFO:agents.father_agent:Step: 255, Training loss: 5.941201210021973
INFO:agents.father_agent:Step: 260, Training loss: 5.955663204193115
INFO:agents.father_agent:Step: 265, Training loss: 5.951587200164795
INFO:agents.father_agent:Step: 270, Training loss: 5.492903709411621
INFO:agents.father_agent:Step: 275, Training loss: 5.61572790145874
INFO:agents.father_agent:Step: 280, Training loss: 5.685940742492676
INFO:agents.father_agent:Step: 285, Training loss: 8.033370018005371
INFO:agents.father_agent:Step: 290, Training loss: 7.263963222503662
INFO:agents.father_agent:Step: 295, Training loss: 6.954576015472412
INFO:agents.father_agent:Step: 300, Training loss: 6.68278169631958
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 100.66692352294922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 11.063626289367676
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.549717426300049
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9984668455346877
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12060.1142578125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.50977385971636
INFO:tools.evaluation_results_class:Counted Episodes = 2609
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 6.806983470916748
INFO:agents.father_agent:Step: 310, Training loss: 7.490300178527832
INFO:agents.father_agent:Step: 315, Training loss: 6.978446960449219
INFO:agents.father_agent:Step: 320, Training loss: 6.168977737426758
INFO:agents.father_agent:Step: 325, Training loss: 5.582935810089111
INFO:agents.father_agent:Step: 330, Training loss: 8.661189079284668
INFO:agents.father_agent:Step: 335, Training loss: 6.509245872497559
INFO:agents.father_agent:Step: 340, Training loss: 7.470003128051758
INFO:agents.father_agent:Step: 345, Training loss: 7.040144920349121
INFO:agents.father_agent:Step: 350, Training loss: 5.721590042114258
INFO:agents.father_agent:Step: 355, Training loss: 8.33019733428955
INFO:agents.father_agent:Step: 360, Training loss: 5.981539726257324
INFO:agents.father_agent:Step: 365, Training loss: 6.354608058929443
INFO:agents.father_agent:Step: 370, Training loss: 5.888282775878906
INFO:agents.father_agent:Step: 375, Training loss: 6.573917388916016
INFO:agents.father_agent:Step: 380, Training loss: 6.142518997192383
INFO:agents.father_agent:Step: 385, Training loss: 6.795281887054443
INFO:agents.father_agent:Step: 390, Training loss: 5.755914688110352
INFO:agents.father_agent:Step: 395, Training loss: 6.6205339431762695
INFO:agents.father_agent:Step: 400, Training loss: 8.130522727966309
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 121.95624542236328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.187044143676758
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.113347053527832
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9957099957099957
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18012.818359375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 116.87001287001287
INFO:tools.evaluation_results_class:Counted Episodes = 2331
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 7.9662041664123535
INFO:agents.father_agent:Step: 410, Training loss: 5.144552707672119
INFO:agents.father_agent:Step: 415, Training loss: 4.646689414978027
INFO:agents.father_agent:Step: 420, Training loss: 8.992864608764648
INFO:agents.father_agent:Step: 425, Training loss: 6.305581092834473
INFO:agents.father_agent:Step: 430, Training loss: 5.950151443481445
INFO:agents.father_agent:Step: 435, Training loss: 7.541222095489502
INFO:agents.father_agent:Step: 440, Training loss: 8.517851829528809
INFO:agents.father_agent:Step: 445, Training loss: 5.9364848136901855
INFO:agents.father_agent:Step: 450, Training loss: 6.188415050506592
INFO:agents.father_agent:Step: 455, Training loss: 6.1324286460876465
INFO:agents.father_agent:Step: 460, Training loss: 5.517299175262451
INFO:agents.father_agent:Step: 465, Training loss: 5.374709129333496
INFO:agents.father_agent:Step: 470, Training loss: 6.135127067565918
INFO:agents.father_agent:Step: 475, Training loss: 5.728025913238525
INFO:agents.father_agent:Step: 480, Training loss: 6.602945327758789
INFO:agents.father_agent:Step: 485, Training loss: 5.6341094970703125
INFO:agents.father_agent:Step: 490, Training loss: 6.138915061950684
INFO:agents.father_agent:Step: 495, Training loss: 7.227317810058594
INFO:agents.father_agent:Step: 500, Training loss: 5.991231441497803
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 119.96775817871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.991938591003418
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.1005330085754395
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9975816203143894
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18391.29296875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.18460298266828
INFO:tools.evaluation_results_class:Counted Episodes = 2481
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 118.77715301513672
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.876106262207031
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.203005790710449
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.999195494770716
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16400.11328125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.70796460176992
INFO:tools.evaluation_results_class:Counted Episodes = 2486
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 129.63021850585938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.955153465270996
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.29033899307251
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.996066089693155
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 20461.783203125
INFO:tools.evaluation_results_class:Current Best Return = 129.63021850585938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.996066089693155
INFO:tools.evaluation_results_class:Average Episode Length = 118.24311565696303
INFO:tools.evaluation_results_class:Counted Episodes = 2542
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 134.32521057128906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.42276382446289
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.437828063964844
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9951219512195122
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 23756.416015625
INFO:tools.evaluation_results_class:Current Best Return = 134.32521057128906
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9951219512195122
INFO:tools.evaluation_results_class:Average Episode Length = 123.13577235772358
INFO:tools.evaluation_results_class:Counted Episodes = 2460
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 131220
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 1000.1575 achieved after 6299.24 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1000.086 achieved after 6299.24 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 143.4109 achieved after 6299.35 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 143.41089211002875
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=1, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:robust_rl.robust_rl_trainer:Iteration 12 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 118.84270477294922
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.88182544708252
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.010580062866211
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9987775061124694
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17148.130859375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 112.84881825590872
INFO:tools.evaluation_results_class:Counted Episodes = 2454
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.096744537353516
INFO:agents.father_agent:Step: 5, Training loss: 6.906781196594238
INFO:agents.father_agent:Step: 10, Training loss: 7.056976795196533
INFO:agents.father_agent:Step: 15, Training loss: 6.192890167236328
INFO:agents.father_agent:Step: 20, Training loss: 6.241320610046387
INFO:agents.father_agent:Step: 25, Training loss: 6.241296291351318
INFO:agents.father_agent:Step: 30, Training loss: 6.155836582183838
INFO:agents.father_agent:Step: 35, Training loss: 7.228853225708008
INFO:agents.father_agent:Step: 40, Training loss: 5.337039470672607
INFO:agents.father_agent:Step: 45, Training loss: 6.090299129486084
INFO:agents.father_agent:Step: 50, Training loss: 6.829251766204834
INFO:agents.father_agent:Step: 55, Training loss: 5.809612274169922
INFO:agents.father_agent:Step: 60, Training loss: 7.182093143463135
INFO:agents.father_agent:Step: 65, Training loss: 4.993623733520508
INFO:agents.father_agent:Step: 70, Training loss: 7.837651252746582
INFO:agents.father_agent:Step: 75, Training loss: 6.480338096618652
INFO:agents.father_agent:Step: 80, Training loss: 6.367134094238281
INFO:agents.father_agent:Step: 85, Training loss: 6.087503910064697
INFO:agents.father_agent:Step: 90, Training loss: 6.353573799133301
INFO:agents.father_agent:Step: 95, Training loss: 5.550953388214111
INFO:agents.father_agent:Step: 100, Training loss: 7.07266092300415
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 123.02755737304688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.2986421585083
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.287877559661865
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9979432332373509
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18050.890625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.19251336898395
INFO:tools.evaluation_results_class:Counted Episodes = 2431
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 5.802278518676758
INFO:agents.father_agent:Step: 110, Training loss: 6.425189971923828
INFO:agents.father_agent:Step: 115, Training loss: 7.497577667236328
INFO:agents.father_agent:Step: 120, Training loss: 7.330782413482666
INFO:agents.father_agent:Step: 125, Training loss: 6.676937580108643
INFO:agents.father_agent:Step: 130, Training loss: 6.480403900146484
INFO:agents.father_agent:Step: 135, Training loss: 5.653725624084473
INFO:agents.father_agent:Step: 140, Training loss: 6.552276134490967
INFO:agents.father_agent:Step: 145, Training loss: 5.765214920043945
INFO:agents.father_agent:Step: 150, Training loss: 7.07012414932251
INFO:agents.father_agent:Step: 155, Training loss: 5.569356441497803
INFO:agents.father_agent:Step: 160, Training loss: 6.793629169464111
INFO:agents.father_agent:Step: 165, Training loss: 7.6175055503845215
INFO:agents.father_agent:Step: 170, Training loss: 7.205887317657471
INFO:agents.father_agent:Step: 175, Training loss: 6.5498046875
INFO:agents.father_agent:Step: 180, Training loss: 7.073296070098877
INFO:agents.father_agent:Step: 185, Training loss: 5.959825038909912
INFO:agents.father_agent:Step: 190, Training loss: 6.780190467834473
INFO:agents.father_agent:Step: 195, Training loss: 6.372812271118164
INFO:agents.father_agent:Step: 200, Training loss: 6.453119277954102
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 112.03561401367188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.201132774353027
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.891147613525391
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9987859166329421
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15880.8818359375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.2043707001214
INFO:tools.evaluation_results_class:Counted Episodes = 2471
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 6.956178665161133
INFO:agents.father_agent:Step: 210, Training loss: 6.724435806274414
INFO:agents.father_agent:Step: 215, Training loss: 8.074950218200684
INFO:agents.father_agent:Step: 220, Training loss: 6.505043029785156
INFO:agents.father_agent:Step: 225, Training loss: 7.906281471252441
INFO:agents.father_agent:Step: 230, Training loss: 6.762314319610596
INFO:agents.father_agent:Step: 235, Training loss: 7.745492935180664
INFO:agents.father_agent:Step: 240, Training loss: 7.346189975738525
INFO:agents.father_agent:Step: 245, Training loss: 7.695106029510498
INFO:agents.father_agent:Step: 250, Training loss: 5.982850551605225
INFO:agents.father_agent:Step: 255, Training loss: 6.88217830657959
INFO:agents.father_agent:Step: 260, Training loss: 6.538552284240723
INFO:agents.father_agent:Step: 265, Training loss: 6.039910316467285
INFO:agents.father_agent:Step: 270, Training loss: 6.6833815574646
INFO:agents.father_agent:Step: 275, Training loss: 6.328505516052246
INFO:agents.father_agent:Step: 280, Training loss: 6.375313758850098
INFO:agents.father_agent:Step: 285, Training loss: 7.1399946212768555
INFO:agents.father_agent:Step: 290, Training loss: 5.657207489013672
INFO:agents.father_agent:Step: 295, Training loss: 7.320072650909424
INFO:agents.father_agent:Step: 300, Training loss: 7.008924961090088
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 110.6259765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.057120323181152
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.780991077423096
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9972613458528952
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15920.265625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 108.05594679186228
INFO:tools.evaluation_results_class:Counted Episodes = 2556
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 5.152398586273193
INFO:agents.father_agent:Step: 310, Training loss: 5.6978559494018555
INFO:agents.father_agent:Step: 315, Training loss: 6.794781684875488
INFO:agents.father_agent:Step: 320, Training loss: 5.343448638916016
INFO:agents.father_agent:Step: 325, Training loss: 8.681769371032715
INFO:agents.father_agent:Step: 330, Training loss: 6.797784805297852
INFO:agents.father_agent:Step: 335, Training loss: 7.4016947746276855
INFO:agents.father_agent:Step: 340, Training loss: 7.418524265289307
INFO:agents.father_agent:Step: 345, Training loss: 7.0426411628723145
INFO:agents.father_agent:Step: 350, Training loss: 7.272720813751221
INFO:agents.father_agent:Step: 355, Training loss: 6.842064380645752
INFO:agents.father_agent:Step: 360, Training loss: 6.057755947113037
INFO:agents.father_agent:Step: 365, Training loss: 6.242598533630371
INFO:agents.father_agent:Step: 370, Training loss: 7.546728134155273
INFO:agents.father_agent:Step: 375, Training loss: 7.213719367980957
INFO:agents.father_agent:Step: 380, Training loss: 5.785607814788818
INFO:agents.father_agent:Step: 385, Training loss: 6.426328182220459
INFO:agents.father_agent:Step: 390, Training loss: 6.247206687927246
INFO:agents.father_agent:Step: 395, Training loss: 6.554479598999023
INFO:agents.father_agent:Step: 400, Training loss: 8.003137588500977
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 106.57353210449219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 11.657353401184082
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.746702671051025
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14218.486328125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.21420256111757
INFO:tools.evaluation_results_class:Counted Episodes = 2577
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 6.625840187072754
INFO:agents.father_agent:Step: 410, Training loss: 6.318264484405518
INFO:agents.father_agent:Step: 415, Training loss: 8.752245903015137
INFO:agents.father_agent:Step: 420, Training loss: 6.943580627441406
INFO:agents.father_agent:Step: 425, Training loss: 6.356225490570068
INFO:agents.father_agent:Step: 430, Training loss: 6.8781938552856445
INFO:agents.father_agent:Step: 435, Training loss: 6.529453754425049
INFO:agents.father_agent:Step: 440, Training loss: 6.384350776672363
INFO:agents.father_agent:Step: 445, Training loss: 6.778975009918213
INFO:agents.father_agent:Step: 450, Training loss: 6.184689521789551
INFO:agents.father_agent:Step: 455, Training loss: 7.959903240203857
INFO:agents.father_agent:Step: 460, Training loss: 6.22484016418457
INFO:agents.father_agent:Step: 465, Training loss: 7.052396774291992
INFO:agents.father_agent:Step: 470, Training loss: 5.250891208648682
INFO:agents.father_agent:Step: 475, Training loss: 5.564207553863525
INFO:agents.father_agent:Step: 480, Training loss: 5.685365676879883
INFO:agents.father_agent:Step: 485, Training loss: 7.186689376831055
INFO:agents.father_agent:Step: 490, Training loss: 6.1394362449646
INFO:agents.father_agent:Step: 495, Training loss: 7.278270721435547
INFO:agents.father_agent:Step: 500, Training loss: 6.772805213928223
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 117.4214859008789
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.738016128540039
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.023939609527588
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9979338842975206
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17396.328125
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.22685950413224
INFO:tools.evaluation_results_class:Counted Episodes = 2420
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 116.52104949951172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.648133277893066
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.025999546051025
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9980142970611596
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 18118.474609375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.10762509928514
INFO:tools.evaluation_results_class:Counted Episodes = 2518
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 150.2460479736328
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 16.00790786743164
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.741505146026611
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9916520210896309
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27704.509765625
INFO:tools.evaluation_results_class:Current Best Return = 150.2460479736328
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9916520210896309
INFO:tools.evaluation_results_class:Average Episode Length = 130.63488576449913
INFO:tools.evaluation_results_class:Counted Episodes = 2276
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 131.46676635742188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 14.138026237487793
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.309540748596191
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9956744003145891
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21833.75
INFO:tools.evaluation_results_class:Current Best Return = 131.46676635742188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9956744003145891
INFO:tools.evaluation_results_class:Average Episode Length = 119.85489579237121
INFO:tools.evaluation_results_class:Counted Episodes = 2543
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 131220
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 1046.8053 achieved after 6876.5 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1045.9133 achieved after 6876.5 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1045.6698 achieved after 6876.51 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1038.944 achieved after 6876.65 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1038.3399 achieved after 6876.66 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1036.2154 achieved after 6876.7 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1035.5376 achieved after 6876.7 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1002.7957 achieved after 6876.92 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1002.4794 achieved after 6876.92 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1002.3938 achieved after 6876.93 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 702.0339 achieved after 6877.08 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 702.0269 achieved after 6877.09 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 701.9262 achieved after 6877.12 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 701.9073 achieved after 6877.13 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 516.8043 achieved after 6877.23 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 477.8714 achieved after 6877.42 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 477.854 achieved after 6877.42 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 477.8493 achieved after 6877.43 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 408.3622 achieved after 6877.49 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 145.4414 achieved after 6877.61 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 145.44140442607164
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: P1=0, P2=0, P3=0, P4=2, T1=4, T2=5, T3=9, WP1=9/10, WP2=9/10
INFO:robust_rl.robust_rl_trainer:Iteration 13 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 115.98210906982422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.59495735168457
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.125445365905762
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9983733224888166
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15461.3681640625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 109.72509150061
INFO:tools.evaluation_results_class:Counted Episodes = 2459
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.732389450073242
INFO:agents.father_agent:Step: 5, Training loss: 7.429520606994629
INFO:agents.father_agent:Step: 10, Training loss: 6.002480983734131
INFO:agents.father_agent:Step: 15, Training loss: 5.852415084838867
INFO:agents.father_agent:Step: 20, Training loss: 5.4663286209106445
INFO:agents.father_agent:Step: 25, Training loss: 5.241010665893555
INFO:agents.father_agent:Step: 30, Training loss: 6.016822814941406
INFO:agents.father_agent:Step: 35, Training loss: 7.598845958709717
INFO:agents.father_agent:Step: 40, Training loss: 6.008291721343994
INFO:agents.father_agent:Step: 45, Training loss: 7.2215166091918945
INFO:agents.father_agent:Step: 50, Training loss: 5.622899532318115
INFO:agents.father_agent:Step: 55, Training loss: 6.425110816955566
INFO:agents.father_agent:Step: 60, Training loss: 7.463141918182373
INFO:agents.father_agent:Step: 65, Training loss: 7.367408275604248
INFO:agents.father_agent:Step: 70, Training loss: 7.10537052154541
INFO:agents.father_agent:Step: 75, Training loss: 5.5943074226379395
INFO:agents.father_agent:Step: 80, Training loss: 7.785660743713379
INFO:agents.father_agent:Step: 85, Training loss: 5.9544267654418945
INFO:agents.father_agent:Step: 90, Training loss: 6.521647930145264
INFO:agents.father_agent:Step: 95, Training loss: 7.292861461639404
INFO:agents.father_agent:Step: 100, Training loss: 7.939040184020996
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 112.13855743408203
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.210843086242676
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.069735050201416
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9984939759036144
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15814.1025390625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 103.09789156626506
INFO:tools.evaluation_results_class:Counted Episodes = 2656
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 7.385234355926514
INFO:agents.father_agent:Step: 110, Training loss: 6.373907089233398
INFO:agents.father_agent:Step: 115, Training loss: 5.797201156616211
INFO:agents.father_agent:Step: 120, Training loss: 5.698514461517334
INFO:agents.father_agent:Step: 125, Training loss: 6.218400001525879
INFO:agents.father_agent:Step: 130, Training loss: 5.969183444976807
INFO:agents.father_agent:Step: 135, Training loss: 7.432026386260986
INFO:agents.father_agent:Step: 140, Training loss: 6.370108127593994
INFO:agents.father_agent:Step: 145, Training loss: 6.960974216461182
INFO:agents.father_agent:Step: 150, Training loss: 7.835877418518066
INFO:agents.father_agent:Step: 155, Training loss: 6.072677135467529
INFO:agents.father_agent:Step: 160, Training loss: 6.537380218505859
INFO:agents.father_agent:Step: 165, Training loss: 6.646130084991455
INFO:agents.father_agent:Step: 170, Training loss: 6.815672397613525
INFO:agents.father_agent:Step: 175, Training loss: 6.508021354675293
INFO:agents.father_agent:Step: 180, Training loss: 7.909436225891113
INFO:agents.father_agent:Step: 185, Training loss: 6.1667280197143555
INFO:agents.father_agent:Step: 190, Training loss: 5.913687229156494
INFO:agents.father_agent:Step: 195, Training loss: 7.484854698181152
INFO:agents.father_agent:Step: 200, Training loss: 7.252766132354736
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 110.22816467285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.018095970153809
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.977365493774414
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.997639653815893
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 14789.7998046875
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.73249409913454
INFO:tools.evaluation_results_class:Counted Episodes = 2542
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 7.201142311096191
INFO:agents.father_agent:Step: 210, Training loss: 5.830002307891846
INFO:agents.father_agent:Step: 215, Training loss: 6.407938003540039
INFO:agents.father_agent:Step: 220, Training loss: 5.732900142669678
INFO:agents.father_agent:Step: 225, Training loss: 7.909572124481201
INFO:agents.father_agent:Step: 230, Training loss: 7.0483198165893555
INFO:agents.father_agent:Step: 235, Training loss: 6.928548336029053
INFO:agents.father_agent:Step: 240, Training loss: 6.5977654457092285
INFO:agents.father_agent:Step: 245, Training loss: 7.062981128692627
INFO:agents.father_agent:Step: 250, Training loss: 7.454076290130615
INFO:agents.father_agent:Step: 255, Training loss: 7.88364839553833
INFO:agents.father_agent:Step: 260, Training loss: 5.476229667663574
INFO:agents.father_agent:Step: 265, Training loss: 7.214114665985107
INFO:agents.father_agent:Step: 270, Training loss: 6.034788608551025
INFO:agents.father_agent:Step: 275, Training loss: 6.151771068572998
INFO:agents.father_agent:Step: 280, Training loss: 6.611561298370361
INFO:agents.father_agent:Step: 285, Training loss: 6.591022491455078
INFO:agents.father_agent:Step: 290, Training loss: 6.742881774902344
INFO:agents.father_agent:Step: 295, Training loss: 7.923271656036377
INFO:agents.father_agent:Step: 300, Training loss: 6.610879898071289
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 115.07396697998047
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.504998207092285
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.03063440322876
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9988004798080767
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 16026.912109375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.92323070771691
INFO:tools.evaluation_results_class:Counted Episodes = 2501
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 6.235853672027588
INFO:agents.father_agent:Step: 310, Training loss: 6.269383430480957
INFO:agents.father_agent:Step: 315, Training loss: 7.084481716156006
INFO:agents.father_agent:Step: 320, Training loss: 7.283349990844727
INFO:agents.father_agent:Step: 325, Training loss: 7.396872043609619
INFO:agents.father_agent:Step: 330, Training loss: 7.028055191040039
INFO:agents.father_agent:Step: 335, Training loss: 7.48617696762085
INFO:agents.father_agent:Step: 340, Training loss: 7.434063911437988
INFO:agents.father_agent:Step: 345, Training loss: 6.863832950592041
INFO:agents.father_agent:Step: 350, Training loss: 7.087906837463379
INFO:agents.father_agent:Step: 355, Training loss: 6.95035457611084
INFO:agents.father_agent:Step: 360, Training loss: 6.605113506317139
INFO:agents.father_agent:Step: 365, Training loss: 7.602378845214844
INFO:agents.father_agent:Step: 370, Training loss: 7.451472282409668
INFO:agents.father_agent:Step: 375, Training loss: 6.709833145141602
INFO:agents.father_agent:Step: 380, Training loss: 5.990342140197754
INFO:agents.father_agent:Step: 385, Training loss: 6.466031551361084
INFO:agents.father_agent:Step: 390, Training loss: 5.692947864532471
INFO:agents.father_agent:Step: 395, Training loss: 6.805497169494629
INFO:agents.father_agent:Step: 400, Training loss: 8.732268333435059
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 128.65353393554688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.860426902770996
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.438501358032227
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9975369458128078
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 21365.509765625
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.64860426929393
INFO:tools.evaluation_results_class:Counted Episodes = 2436
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 6.826513290405273
INFO:agents.father_agent:Step: 410, Training loss: 6.384995937347412
INFO:agents.father_agent:Step: 415, Training loss: 7.604390621185303
INFO:agents.father_agent:Step: 420, Training loss: 6.034328460693359
INFO:agents.father_agent:Step: 425, Training loss: 6.040173053741455
INFO:agents.father_agent:Step: 430, Training loss: 6.377780437469482
INFO:agents.father_agent:Step: 435, Training loss: 6.618552207946777
INFO:agents.father_agent:Step: 440, Training loss: 6.463851451873779
INFO:agents.father_agent:Step: 445, Training loss: 6.62870454788208
INFO:agents.father_agent:Step: 450, Training loss: 6.949215412139893
INFO:agents.father_agent:Step: 455, Training loss: 6.446895122528076
INFO:agents.father_agent:Step: 460, Training loss: 6.373429775238037
INFO:agents.father_agent:Step: 465, Training loss: 6.681194305419922
INFO:agents.father_agent:Step: 470, Training loss: 6.666213512420654
INFO:agents.father_agent:Step: 475, Training loss: 6.078893661499023
INFO:agents.father_agent:Step: 480, Training loss: 7.884890079498291
INFO:agents.father_agent:Step: 485, Training loss: 7.031137943267822
INFO:agents.father_agent:Step: 490, Training loss: 6.584571838378906
INFO:agents.father_agent:Step: 495, Training loss: 7.022798538208008
INFO:agents.father_agent:Step: 500, Training loss: 6.6572699546813965
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 120.80327606201172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 13.077049255371094
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.253896713256836
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9983606557377049
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 17866.240234375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 111.31393442622951
INFO:tools.evaluation_results_class:Counted Episodes = 2440
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = 115.87761688232422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 12.586151123046875
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.054206848144531
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.999194847020934
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 15794.9208984375
INFO:tools.evaluation_results_class:Current Best Return = 226.02711486816406
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 110.19323671497584
INFO:tools.evaluation_results_class:Counted Episodes = 2484
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = 140.9405975341797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 15.082508087158203
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.653700828552246
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9942244224422442
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 24770.068359375
INFO:tools.evaluation_results_class:Current Best Return = 140.9405975341797
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9942244224422442
INFO:tools.evaluation_results_class:Average Episode Length = 123.43894389438944
INFO:tools.evaluation_results_class:Counted Episodes = 2424
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.

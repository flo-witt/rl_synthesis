2025-08-04 03:36:20,746 - sketch.py:80 - loading sketch from /home/ihudak/synthesis/models_robust_subset/obstacles-10-2/sketch.templ ...
2025-08-04 03:36:20,746 - sketch.py:84 - assuming sketch in PRISM format...
2025-08-04 03:36:20,750 - prism_parser.py:31 - PRISM model type: POMDP
2025-08-04 03:36:20,750 - prism_parser.py:40 - processing hole definitions...
2025-08-04 03:36:20,750 - prism_parser.py:220 - loading properties from /home/ihudak/synthesis/models_robust_subset/obstacles-10-2/sketch.props ...
2025-08-04 03:36:20,751 - prism_parser.py:236 - found the following specification: optimality: R{"penalty"}min=? [F ((x = 10) & (y = 10))] 
2025-08-04 03:36:20,751 - jani.py:41 - constructing JANI program...
2025-08-04 03:36:20,754 - jani.py:61 - constructing the quotient...
2025-08-04 03:36:20,776 - jani.py:66 - associating choices of the quotient with hole assignments...
2025-08-04 03:36:20,778 - sketch.py:135 - sketch parsing OK
2025-08-04 03:36:20,778 - sketch.py:144 - WARNING: choice labeling for the quotient is not canonic
2025-08-04 03:36:20,779 - sketch.py:148 - constructed explicit quotient having 326 states and 829 choices
2025-08-04 03:36:20,779 - sketch.py:154 - found the following specification optimality: R{"rewardmodel_penalty"}min=? [F ((x = 10) & (y = 10))] 
2025-08-04 03:36:20,783 - vectorized_sim_initializer.py:61 - Compiling model obstacles-10-2...
Hello <stormpy.storage.storage.StateValuation object at 0x70d99a7d9330>
2025-08-04 03:36:20,839 - storm_vec_env.py:70 - Computing row map
2025-08-04 03:36:20,844 - storm_vec_env.py:97 - Computing transitions
2025-08-04 03:36:20,857 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 03:36:20,857 - storm_vec_env.py:114 - Computing sinks
2025-08-04 03:36:20,857 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 03:36:20,861 - storm_vec_env.py:143 - Computing labels
2025-08-04 03:36:20,861 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 03:36:20,861 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 03:36:20,861 - storm_vec_env.py:175 - Computing observations
2025-08-04 03:36:21,108 - environment_wrapper_vec.py:146 - Grid-like renderer not possible to initialize.
2025-08-04 03:36:21,217 - environment_wrapper_vec.py:153 - Vectorized simulator initialized with 256 environments.
2025-08-04 03:36:21,436 - recurrent_ppo_agent.py:98 - Agent initialized
2025-08-04 03:36:21,459 - recurrent_ppo_agent.py:100 - Replay buffer initialized
Using unmasked training with policy wrapper.
2025-08-04 03:36:21,468 - recurrent_ppo_agent.py:112 - Collector driver initialized
2025-08-04 03:36:21,469 - robust_rl_trainer.py:432 - Iteration 1 of pure RL loop
2025-08-04 03:36:21,505 - storm_vec_env.py:70 - Computing row map
2025-08-04 03:36:21,510 - storm_vec_env.py:97 - Computing transitions
2025-08-04 03:36:21,523 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 03:36:21,523 - storm_vec_env.py:114 - Computing sinks
2025-08-04 03:36:21,523 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 03:36:21,527 - storm_vec_env.py:143 - Computing labels
2025-08-04 03:36:21,527 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 03:36:21,527 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 03:36:21,527 - storm_vec_env.py:175 - Computing observations
2025-08-04 03:36:21,611 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 03:36:21,611 - father_agent.py:540 - Before training evaluation.
2025-08-04 03:36:21,731 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:36:21,740 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:36:29,239 - evaluation_results_class.py:131 - Average Return = -456.903564453125
2025-08-04 03:36:29,239 - evaluation_results_class.py:133 - Average Virtual Goal Value = -415.0464172363281
2025-08-04 03:36:29,239 - evaluation_results_class.py:135 - Average Discounted Reward = -91.93956756591797
2025-08-04 03:36:29,239 - evaluation_results_class.py:137 - Goal Reach Probability = 0.5232142857142857
2025-08-04 03:36:29,239 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:36:29,239 - evaluation_results_class.py:141 - Variance of Return = 77211.0234375
2025-08-04 03:36:29,239 - evaluation_results_class.py:143 - Current Best Return = -456.903564453125
2025-08-04 03:36:29,239 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.5232142857142857
2025-08-04 03:36:29,239 - evaluation_results_class.py:147 - Average Episode Length = 494.9875
2025-08-04 03:36:29,239 - evaluation_results_class.py:149 - Counted Episodes = 560
2025-08-04 03:36:29,376 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:36:29,387 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:36:29,484 - father_agent.py:436 - Training agent on-policy
2025-08-04 03:36:43,797 - father_agent.py:386 - Step: 0, Training loss: 821.4912719726562
2025-08-04 03:36:45,394 - father_agent.py:386 - Step: 5, Training loss: 3.0214102268218994
2025-08-04 03:36:46,977 - father_agent.py:386 - Step: 10, Training loss: 3.9139227867126465
2025-08-04 03:36:48,545 - father_agent.py:386 - Step: 15, Training loss: 3.3443312644958496
2025-08-04 03:36:50,171 - father_agent.py:386 - Step: 20, Training loss: 3.6727378368377686
2025-08-04 03:36:51,845 - father_agent.py:386 - Step: 25, Training loss: 2.8836522102355957
2025-08-04 03:36:53,482 - father_agent.py:386 - Step: 30, Training loss: 4.145596027374268
2025-08-04 03:36:55,092 - father_agent.py:386 - Step: 35, Training loss: 4.468220233917236
2025-08-04 03:36:56,665 - father_agent.py:386 - Step: 40, Training loss: 2.764862537384033
2025-08-04 03:36:58,235 - father_agent.py:386 - Step: 45, Training loss: 2.9878547191619873
2025-08-04 03:36:59,820 - father_agent.py:386 - Step: 50, Training loss: 4.522129058837891
2025-08-04 03:37:01,397 - father_agent.py:386 - Step: 55, Training loss: 5.023496627807617
2025-08-04 03:37:02,968 - father_agent.py:386 - Step: 60, Training loss: 5.065096378326416
2025-08-04 03:37:04,547 - father_agent.py:386 - Step: 65, Training loss: 4.868790149688721
2025-08-04 03:37:06,145 - father_agent.py:386 - Step: 70, Training loss: 3.83225154876709
2025-08-04 03:37:07,717 - father_agent.py:386 - Step: 75, Training loss: 2.34279203414917
2025-08-04 03:37:09,290 - father_agent.py:386 - Step: 80, Training loss: 2.8010449409484863
2025-08-04 03:37:10,883 - father_agent.py:386 - Step: 85, Training loss: 1.6876325607299805
2025-08-04 03:37:12,446 - father_agent.py:386 - Step: 90, Training loss: 2.31612491607666
2025-08-04 03:37:14,022 - father_agent.py:386 - Step: 95, Training loss: 1.5779807567596436
2025-08-04 03:37:15,596 - father_agent.py:386 - Step: 100, Training loss: 1.9602490663528442
2025-08-04 03:37:15,746 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:37:15,749 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:37:22,812 - evaluation_results_class.py:131 - Average Return = -186.83912658691406
2025-08-04 03:37:22,812 - evaluation_results_class.py:133 - Average Virtual Goal Value = -155.24862670898438
2025-08-04 03:37:22,812 - evaluation_results_class.py:135 - Average Discounted Reward = -30.780536651611328
2025-08-04 03:37:22,812 - evaluation_results_class.py:137 - Goal Reach Probability = 0.39488117001828155
2025-08-04 03:37:22,812 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:37:22,812 - evaluation_results_class.py:141 - Variance of Return = 6658.43505859375
2025-08-04 03:37:22,812 - evaluation_results_class.py:143 - Current Best Return = -186.83912658691406
2025-08-04 03:37:22,812 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.5232142857142857
2025-08-04 03:37:22,812 - evaluation_results_class.py:147 - Average Episode Length = 508.18098720292505
2025-08-04 03:37:22,812 - evaluation_results_class.py:149 - Counted Episodes = 547
2025-08-04 03:37:22,966 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:37:22,974 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:37:32,604 - father_agent.py:386 - Step: 105, Training loss: 2.3242809772491455
2025-08-04 03:37:34,169 - father_agent.py:386 - Step: 110, Training loss: 3.062309503555298
2025-08-04 03:37:35,735 - father_agent.py:386 - Step: 115, Training loss: 1.894438624382019
2025-08-04 03:37:37,322 - father_agent.py:386 - Step: 120, Training loss: 1.6216646432876587
2025-08-04 03:37:38,913 - father_agent.py:386 - Step: 125, Training loss: 1.7114770412445068
2025-08-04 03:37:40,491 - father_agent.py:386 - Step: 130, Training loss: 2.401435613632202
2025-08-04 03:37:42,077 - father_agent.py:386 - Step: 135, Training loss: 2.0192887783050537
2025-08-04 03:37:43,653 - father_agent.py:386 - Step: 140, Training loss: 1.962235689163208
2025-08-04 03:37:45,229 - father_agent.py:386 - Step: 145, Training loss: 2.0133581161499023
2025-08-04 03:37:46,799 - father_agent.py:386 - Step: 150, Training loss: 2.043372392654419
2025-08-04 03:37:48,365 - father_agent.py:386 - Step: 155, Training loss: 2.167464256286621
2025-08-04 03:37:49,947 - father_agent.py:386 - Step: 160, Training loss: 1.7956650257110596
2025-08-04 03:37:51,538 - father_agent.py:386 - Step: 165, Training loss: 2.0221054553985596
2025-08-04 03:37:53,129 - father_agent.py:386 - Step: 170, Training loss: 2.170037031173706
2025-08-04 03:37:54,697 - father_agent.py:386 - Step: 175, Training loss: 2.7425425052642822
2025-08-04 03:37:56,269 - father_agent.py:386 - Step: 180, Training loss: 4.335172176361084
2025-08-04 03:37:57,845 - father_agent.py:386 - Step: 185, Training loss: 3.7007288932800293
2025-08-04 03:37:59,414 - father_agent.py:386 - Step: 190, Training loss: 3.1554770469665527
2025-08-04 03:38:00,981 - father_agent.py:386 - Step: 195, Training loss: 2.9577255249023438
2025-08-04 03:38:02,530 - father_agent.py:386 - Step: 200, Training loss: 2.942009210586548
2025-08-04 03:38:02,691 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:38:02,694 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:38:09,753 - evaluation_results_class.py:131 - Average Return = -42.42390823364258
2025-08-04 03:38:09,753 - evaluation_results_class.py:133 - Average Virtual Goal Value = 37.57609176635742
2025-08-04 03:38:09,753 - evaluation_results_class.py:135 - Average Discounted Reward = 14.99871826171875
2025-08-04 03:38:09,753 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:38:09,753 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:38:09,754 - evaluation_results_class.py:141 - Variance of Return = 1870.708740234375
2025-08-04 03:38:09,754 - evaluation_results_class.py:143 - Current Best Return = -42.42390823364258
2025-08-04 03:38:09,754 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:38:09,754 - evaluation_results_class.py:147 - Average Episode Length = 53.960991126736985
2025-08-04 03:38:09,754 - evaluation_results_class.py:149 - Counted Episodes = 5973
2025-08-04 03:38:09,913 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:38:09,922 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:38:12,354 - father_agent.py:386 - Step: 205, Training loss: 2.7764387130737305
2025-08-04 03:38:13,911 - father_agent.py:386 - Step: 210, Training loss: 2.6915950775146484
2025-08-04 03:38:15,477 - father_agent.py:386 - Step: 215, Training loss: 3.2278831005096436
2025-08-04 03:38:17,026 - father_agent.py:386 - Step: 220, Training loss: 3.7765583992004395
2025-08-04 03:38:18,594 - father_agent.py:386 - Step: 225, Training loss: 3.0926496982574463
2025-08-04 03:38:20,160 - father_agent.py:386 - Step: 230, Training loss: 2.8483355045318604
2025-08-04 03:38:21,723 - father_agent.py:386 - Step: 235, Training loss: 1.959585428237915
2025-08-04 03:38:23,288 - father_agent.py:386 - Step: 240, Training loss: 3.4218790531158447
2025-08-04 03:38:24,847 - father_agent.py:386 - Step: 245, Training loss: 2.9802656173706055
2025-08-04 03:38:26,430 - father_agent.py:386 - Step: 250, Training loss: 2.9387147426605225
2025-08-04 03:38:27,995 - father_agent.py:386 - Step: 255, Training loss: 3.1863958835601807
2025-08-04 03:38:29,572 - father_agent.py:386 - Step: 260, Training loss: 3.7790913581848145
2025-08-04 03:38:31,148 - father_agent.py:386 - Step: 265, Training loss: 3.176362991333008
2025-08-04 03:38:32,708 - father_agent.py:386 - Step: 270, Training loss: 3.3919003009796143
2025-08-04 03:38:34,274 - father_agent.py:386 - Step: 275, Training loss: 2.8083395957946777
2025-08-04 03:38:35,837 - father_agent.py:386 - Step: 280, Training loss: 3.151308298110962
2025-08-04 03:38:37,411 - father_agent.py:386 - Step: 285, Training loss: 2.1654584407806396
2025-08-04 03:38:38,982 - father_agent.py:386 - Step: 290, Training loss: 2.7942862510681152
2025-08-04 03:38:40,550 - father_agent.py:386 - Step: 295, Training loss: 1.8846819400787354
2025-08-04 03:38:42,104 - father_agent.py:386 - Step: 300, Training loss: 3.1383213996887207
2025-08-04 03:38:42,266 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:38:42,269 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:38:49,323 - evaluation_results_class.py:131 - Average Return = -47.478721618652344
2025-08-04 03:38:49,324 - evaluation_results_class.py:133 - Average Virtual Goal Value = 32.521278381347656
2025-08-04 03:38:49,324 - evaluation_results_class.py:135 - Average Discounted Reward = 10.97391128540039
2025-08-04 03:38:49,324 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:38:49,324 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:38:49,324 - evaluation_results_class.py:141 - Variance of Return = 2105.032470703125
2025-08-04 03:38:49,324 - evaluation_results_class.py:143 - Current Best Return = -42.42390823364258
2025-08-04 03:38:49,324 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:38:49,324 - evaluation_results_class.py:147 - Average Episode Length = 57.03687943262411
2025-08-04 03:38:49,324 - evaluation_results_class.py:149 - Counted Episodes = 5640
2025-08-04 03:38:49,488 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:38:49,497 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:38:51,890 - father_agent.py:386 - Step: 305, Training loss: 3.1246743202209473
2025-08-04 03:38:53,455 - father_agent.py:386 - Step: 310, Training loss: 2.4418749809265137
2025-08-04 03:38:55,026 - father_agent.py:386 - Step: 315, Training loss: 2.915898084640503
2025-08-04 03:38:56,594 - father_agent.py:386 - Step: 320, Training loss: 3.397451162338257
2025-08-04 03:38:58,169 - father_agent.py:386 - Step: 325, Training loss: 2.473215103149414
2025-08-04 03:38:59,729 - father_agent.py:386 - Step: 330, Training loss: 2.07234787940979
2025-08-04 03:39:01,301 - father_agent.py:386 - Step: 335, Training loss: 2.4428954124450684
2025-08-04 03:39:02,869 - father_agent.py:386 - Step: 340, Training loss: 2.956813335418701
2025-08-04 03:39:04,441 - father_agent.py:386 - Step: 345, Training loss: 1.5982152223587036
2025-08-04 03:39:06,008 - father_agent.py:386 - Step: 350, Training loss: 2.2987356185913086
2025-08-04 03:39:07,586 - father_agent.py:386 - Step: 355, Training loss: 2.4115655422210693
2025-08-04 03:39:09,170 - father_agent.py:386 - Step: 360, Training loss: 1.2821694612503052
2025-08-04 03:39:10,748 - father_agent.py:386 - Step: 365, Training loss: 1.8690438270568848
2025-08-04 03:39:12,313 - father_agent.py:386 - Step: 370, Training loss: 2.044766426086426
2025-08-04 03:39:13,903 - father_agent.py:386 - Step: 375, Training loss: 1.911030650138855
2025-08-04 03:39:15,480 - father_agent.py:386 - Step: 380, Training loss: 1.7753751277923584
2025-08-04 03:39:17,052 - father_agent.py:386 - Step: 385, Training loss: 1.6494512557983398
2025-08-04 03:39:18,630 - father_agent.py:386 - Step: 390, Training loss: 1.2277222871780396
2025-08-04 03:39:20,219 - father_agent.py:386 - Step: 395, Training loss: 0.823540449142456
2025-08-04 03:39:21,793 - father_agent.py:386 - Step: 400, Training loss: 1.7572553157806396
2025-08-04 03:39:21,957 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:39:21,960 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:39:29,318 - evaluation_results_class.py:131 - Average Return = -26.13637351989746
2025-08-04 03:39:29,319 - evaluation_results_class.py:133 - Average Virtual Goal Value = 53.86362838745117
2025-08-04 03:39:29,319 - evaluation_results_class.py:135 - Average Discounted Reward = 23.116214752197266
2025-08-04 03:39:29,319 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:39:29,319 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:39:29,319 - evaluation_results_class.py:141 - Variance of Return = 442.71685791015625
2025-08-04 03:39:29,319 - evaluation_results_class.py:143 - Current Best Return = -26.13637351989746
2025-08-04 03:39:29,319 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:39:29,319 - evaluation_results_class.py:147 - Average Episode Length = 63.80943811237753
2025-08-04 03:39:29,319 - evaluation_results_class.py:149 - Counted Episodes = 5001
2025-08-04 03:39:29,478 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:39:29,488 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:39:31,831 - father_agent.py:386 - Step: 405, Training loss: 0.9146985411643982
2025-08-04 03:39:33,406 - father_agent.py:386 - Step: 410, Training loss: 2.1802589893341064
2025-08-04 03:39:34,975 - father_agent.py:386 - Step: 415, Training loss: 1.700276494026184
2025-08-04 03:39:36,557 - father_agent.py:386 - Step: 420, Training loss: 1.8520243167877197
2025-08-04 03:39:38,124 - father_agent.py:386 - Step: 425, Training loss: 1.429463267326355
2025-08-04 03:39:39,699 - father_agent.py:386 - Step: 430, Training loss: 1.7113327980041504
2025-08-04 03:39:41,275 - father_agent.py:386 - Step: 435, Training loss: 1.9980788230895996
2025-08-04 03:39:42,842 - father_agent.py:386 - Step: 440, Training loss: 1.1029856204986572
2025-08-04 03:39:44,398 - father_agent.py:386 - Step: 445, Training loss: 1.1210592985153198
2025-08-04 03:39:45,954 - father_agent.py:386 - Step: 450, Training loss: 2.329023599624634
2025-08-04 03:39:47,517 - father_agent.py:386 - Step: 455, Training loss: 1.0843405723571777
2025-08-04 03:39:49,083 - father_agent.py:386 - Step: 460, Training loss: 0.9028455018997192
2025-08-04 03:39:50,639 - father_agent.py:386 - Step: 465, Training loss: 1.2193682193756104
2025-08-04 03:39:52,198 - father_agent.py:386 - Step: 470, Training loss: 1.5251128673553467
2025-08-04 03:39:53,756 - father_agent.py:386 - Step: 475, Training loss: 1.038661241531372
2025-08-04 03:39:55,311 - father_agent.py:386 - Step: 480, Training loss: 1.5240802764892578
2025-08-04 03:39:56,857 - father_agent.py:386 - Step: 485, Training loss: 0.7092015147209167
2025-08-04 03:39:58,417 - father_agent.py:386 - Step: 490, Training loss: 0.7121933102607727
2025-08-04 03:40:00,020 - father_agent.py:386 - Step: 495, Training loss: 1.4252054691314697
2025-08-04 03:40:01,600 - father_agent.py:386 - Step: 500, Training loss: 1.504014015197754
2025-08-04 03:40:01,764 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:40:01,768 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:40:08,705 - evaluation_results_class.py:131 - Average Return = -34.789955139160156
2025-08-04 03:40:08,705 - evaluation_results_class.py:133 - Average Virtual Goal Value = 45.210044860839844
2025-08-04 03:40:08,705 - evaluation_results_class.py:135 - Average Discounted Reward = 8.618751525878906
2025-08-04 03:40:08,705 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:40:08,706 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:40:08,706 - evaluation_results_class.py:141 - Variance of Return = 334.34619140625
2025-08-04 03:40:08,706 - evaluation_results_class.py:143 - Current Best Return = -26.13637351989746
2025-08-04 03:40:08,706 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:40:08,706 - evaluation_results_class.py:147 - Average Episode Length = 94.08158295281584
2025-08-04 03:40:08,706 - evaluation_results_class.py:149 - Counted Episodes = 3285
2025-08-04 03:40:08,869 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:40:08,878 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:40:11,254 - father_agent.py:386 - Step: 505, Training loss: 1.052660584449768
2025-08-04 03:40:12,824 - father_agent.py:386 - Step: 510, Training loss: 1.2574067115783691
2025-08-04 03:40:14,399 - father_agent.py:386 - Step: 515, Training loss: 1.0274244546890259
2025-08-04 03:40:15,965 - father_agent.py:386 - Step: 520, Training loss: 1.6159160137176514
2025-08-04 03:40:17,585 - father_agent.py:386 - Step: 525, Training loss: 0.7404887676239014
2025-08-04 03:40:19,239 - father_agent.py:386 - Step: 530, Training loss: 0.7449870705604553
2025-08-04 03:40:20,868 - father_agent.py:386 - Step: 535, Training loss: 0.8847125768661499
2025-08-04 03:40:22,514 - father_agent.py:386 - Step: 540, Training loss: 0.9414108991622925
2025-08-04 03:40:24,154 - father_agent.py:386 - Step: 545, Training loss: 0.5504487752914429
2025-08-04 03:40:25,795 - father_agent.py:386 - Step: 550, Training loss: 1.3383030891418457
2025-08-04 03:40:27,416 - father_agent.py:386 - Step: 555, Training loss: 0.737942636013031
2025-08-04 03:40:29,078 - father_agent.py:386 - Step: 560, Training loss: 0.8565859198570251
2025-08-04 03:40:30,690 - father_agent.py:386 - Step: 565, Training loss: 0.8770067691802979
2025-08-04 03:40:32,301 - father_agent.py:386 - Step: 570, Training loss: 0.8565278053283691
2025-08-04 03:40:33,911 - father_agent.py:386 - Step: 575, Training loss: 0.9571089744567871
2025-08-04 03:40:35,528 - father_agent.py:386 - Step: 580, Training loss: 0.7942892909049988
2025-08-04 03:40:37,137 - father_agent.py:386 - Step: 585, Training loss: 0.9803413152694702
2025-08-04 03:40:38,746 - father_agent.py:386 - Step: 590, Training loss: 0.5932608842849731
2025-08-04 03:40:40,364 - father_agent.py:386 - Step: 595, Training loss: 0.9356982111930847
2025-08-04 03:40:41,992 - father_agent.py:386 - Step: 600, Training loss: 1.204413652420044
2025-08-04 03:40:42,158 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:40:42,161 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:40:49,465 - evaluation_results_class.py:131 - Average Return = -31.593732833862305
2025-08-04 03:40:49,466 - evaluation_results_class.py:133 - Average Virtual Goal Value = 48.40626525878906
2025-08-04 03:40:49,466 - evaluation_results_class.py:135 - Average Discounted Reward = 12.714205741882324
2025-08-04 03:40:49,466 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:40:49,466 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:40:49,466 - evaluation_results_class.py:141 - Variance of Return = 275.10113525390625
2025-08-04 03:40:49,466 - evaluation_results_class.py:143 - Current Best Return = -26.13637351989746
2025-08-04 03:40:49,466 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:40:49,466 - evaluation_results_class.py:147 - Average Episode Length = 85.7125340599455
2025-08-04 03:40:49,466 - evaluation_results_class.py:149 - Counted Episodes = 3670
2025-08-04 03:40:49,654 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:40:49,664 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:40:52,149 - father_agent.py:386 - Step: 605, Training loss: 1.0122188329696655
2025-08-04 03:40:53,815 - father_agent.py:386 - Step: 610, Training loss: 1.065401554107666
2025-08-04 03:40:55,535 - father_agent.py:386 - Step: 615, Training loss: 0.27576977014541626
2025-08-04 03:40:57,222 - father_agent.py:386 - Step: 620, Training loss: 1.1544914245605469
2025-08-04 03:40:58,876 - father_agent.py:386 - Step: 625, Training loss: 0.7790568470954895
2025-08-04 03:41:00,636 - father_agent.py:386 - Step: 630, Training loss: 0.8852759003639221
2025-08-04 03:41:02,406 - father_agent.py:386 - Step: 635, Training loss: 1.0465295314788818
2025-08-04 03:41:04,077 - father_agent.py:386 - Step: 640, Training loss: 1.4874463081359863
2025-08-04 03:41:05,705 - father_agent.py:386 - Step: 645, Training loss: 0.7171218395233154
2025-08-04 03:41:07,350 - father_agent.py:386 - Step: 650, Training loss: 1.2960959672927856
2025-08-04 03:41:08,998 - father_agent.py:386 - Step: 655, Training loss: 0.9284986853599548
2025-08-04 03:41:10,629 - father_agent.py:386 - Step: 660, Training loss: 0.33941197395324707
2025-08-04 03:41:12,274 - father_agent.py:386 - Step: 665, Training loss: 0.708771288394928
2025-08-04 03:41:13,933 - father_agent.py:386 - Step: 670, Training loss: 0.5613738894462585
2025-08-04 03:41:15,578 - father_agent.py:386 - Step: 675, Training loss: 0.7075448036193848
2025-08-04 03:41:17,220 - father_agent.py:386 - Step: 680, Training loss: 0.8208730816841125
2025-08-04 03:41:18,882 - father_agent.py:386 - Step: 685, Training loss: 1.0267164707183838
2025-08-04 03:41:20,539 - father_agent.py:386 - Step: 690, Training loss: 0.31068167090415955
2025-08-04 03:41:22,179 - father_agent.py:386 - Step: 695, Training loss: 0.8082998991012573
2025-08-04 03:41:23,841 - father_agent.py:386 - Step: 700, Training loss: 0.7705374956130981
2025-08-04 03:41:24,009 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:41:24,012 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:41:31,295 - evaluation_results_class.py:131 - Average Return = -34.85066223144531
2025-08-04 03:41:31,295 - evaluation_results_class.py:133 - Average Virtual Goal Value = 45.14933776855469
2025-08-04 03:41:31,295 - evaluation_results_class.py:135 - Average Discounted Reward = 8.296334266662598
2025-08-04 03:41:31,295 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:41:31,295 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:41:31,295 - evaluation_results_class.py:141 - Variance of Return = 279.3630676269531
2025-08-04 03:41:31,295 - evaluation_results_class.py:143 - Current Best Return = -26.13637351989746
2025-08-04 03:41:31,295 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:41:31,295 - evaluation_results_class.py:147 - Average Episode Length = 95.34094415303919
2025-08-04 03:41:31,295 - evaluation_results_class.py:149 - Counted Episodes = 3241
2025-08-04 03:41:31,454 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:41:31,463 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:41:33,892 - father_agent.py:386 - Step: 705, Training loss: 0.6967599391937256
2025-08-04 03:41:35,524 - father_agent.py:386 - Step: 710, Training loss: 1.654590368270874
2025-08-04 03:41:37,155 - father_agent.py:386 - Step: 715, Training loss: 0.6645647287368774
2025-08-04 03:41:38,767 - father_agent.py:386 - Step: 720, Training loss: 0.5483865737915039
2025-08-04 03:41:40,335 - father_agent.py:386 - Step: 725, Training loss: 1.0535106658935547
2025-08-04 03:41:41,911 - father_agent.py:386 - Step: 730, Training loss: 0.7705332040786743
2025-08-04 03:41:43,479 - father_agent.py:386 - Step: 735, Training loss: 0.6243098378181458
2025-08-04 03:41:45,045 - father_agent.py:386 - Step: 740, Training loss: 1.6262378692626953
2025-08-04 03:41:46,618 - father_agent.py:386 - Step: 745, Training loss: 0.760197639465332
2025-08-04 03:41:48,206 - father_agent.py:386 - Step: 750, Training loss: 1.07864511013031
2025-08-04 03:41:49,778 - father_agent.py:386 - Step: 755, Training loss: 0.9427209496498108
2025-08-04 03:41:51,424 - father_agent.py:386 - Step: 760, Training loss: 0.761974036693573
2025-08-04 03:41:53,080 - father_agent.py:386 - Step: 765, Training loss: 1.0880228281021118
2025-08-04 03:41:54,728 - father_agent.py:386 - Step: 770, Training loss: 0.6979993581771851
2025-08-04 03:41:56,367 - father_agent.py:386 - Step: 775, Training loss: 1.0718857049942017
2025-08-04 03:41:58,017 - father_agent.py:386 - Step: 780, Training loss: 0.4456765055656433
2025-08-04 03:41:59,724 - father_agent.py:386 - Step: 785, Training loss: 0.7432185411453247
2025-08-04 03:42:01,397 - father_agent.py:386 - Step: 790, Training loss: 0.647698700428009
2025-08-04 03:42:03,075 - father_agent.py:386 - Step: 795, Training loss: 0.9301589131355286
2025-08-04 03:42:04,715 - father_agent.py:386 - Step: 800, Training loss: 0.8538400530815125
2025-08-04 03:42:04,918 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:42:04,922 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:42:12,097 - evaluation_results_class.py:131 - Average Return = -29.150049209594727
2025-08-04 03:42:12,097 - evaluation_results_class.py:133 - Average Virtual Goal Value = 50.84994888305664
2025-08-04 03:42:12,097 - evaluation_results_class.py:135 - Average Discounted Reward = 16.11651039123535
2025-08-04 03:42:12,097 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:42:12,097 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:42:12,097 - evaluation_results_class.py:141 - Variance of Return = 251.94467163085938
2025-08-04 03:42:12,097 - evaluation_results_class.py:143 - Current Best Return = -26.13637351989746
2025-08-04 03:42:12,098 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:42:12,098 - evaluation_results_class.py:147 - Average Episode Length = 79.10746492985972
2025-08-04 03:42:12,098 - evaluation_results_class.py:149 - Counted Episodes = 3992
2025-08-04 03:42:12,257 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:42:12,266 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:42:14,651 - father_agent.py:386 - Step: 805, Training loss: 1.470744252204895
2025-08-04 03:42:16,219 - father_agent.py:386 - Step: 810, Training loss: 0.9363499283790588
2025-08-04 03:42:17,791 - father_agent.py:386 - Step: 815, Training loss: 0.5063703656196594
2025-08-04 03:42:19,359 - father_agent.py:386 - Step: 820, Training loss: 0.9960811734199524
2025-08-04 03:42:20,924 - father_agent.py:386 - Step: 825, Training loss: 0.6032091975212097
2025-08-04 03:42:22,479 - father_agent.py:386 - Step: 830, Training loss: 0.4170677661895752
2025-08-04 03:42:24,045 - father_agent.py:386 - Step: 835, Training loss: 0.9318838119506836
2025-08-04 03:42:25,603 - father_agent.py:386 - Step: 840, Training loss: 1.1153855323791504
2025-08-04 03:42:27,154 - father_agent.py:386 - Step: 845, Training loss: 0.78374844789505
2025-08-04 03:42:28,714 - father_agent.py:386 - Step: 850, Training loss: 0.5924923419952393
2025-08-04 03:42:30,275 - father_agent.py:386 - Step: 855, Training loss: 0.8646931052207947
2025-08-04 03:42:31,834 - father_agent.py:386 - Step: 860, Training loss: 0.6485036611557007
2025-08-04 03:42:33,426 - father_agent.py:386 - Step: 865, Training loss: 0.44857290387153625
2025-08-04 03:42:34,982 - father_agent.py:386 - Step: 870, Training loss: 1.19978928565979
2025-08-04 03:42:36,534 - father_agent.py:386 - Step: 875, Training loss: 0.47636717557907104
2025-08-04 03:42:38,093 - father_agent.py:386 - Step: 880, Training loss: 0.9857136607170105
2025-08-04 03:42:39,670 - father_agent.py:386 - Step: 885, Training loss: 1.1088807582855225
2025-08-04 03:42:41,303 - father_agent.py:386 - Step: 890, Training loss: 0.3836032450199127
2025-08-04 03:42:42,943 - father_agent.py:386 - Step: 895, Training loss: 0.6290953159332275
2025-08-04 03:42:44,513 - father_agent.py:386 - Step: 900, Training loss: 0.7602649331092834
2025-08-04 03:42:44,678 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:42:44,681 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:42:51,890 - evaluation_results_class.py:131 - Average Return = -36.39439392089844
2025-08-04 03:42:51,890 - evaluation_results_class.py:133 - Average Virtual Goal Value = 43.60560607910156
2025-08-04 03:42:51,890 - evaluation_results_class.py:135 - Average Discounted Reward = 6.525973796844482
2025-08-04 03:42:51,890 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:42:51,890 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:42:51,890 - evaluation_results_class.py:141 - Variance of Return = 307.1894836425781
2025-08-04 03:42:51,890 - evaluation_results_class.py:143 - Current Best Return = -26.13637351989746
2025-08-04 03:42:51,890 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:42:51,890 - evaluation_results_class.py:147 - Average Episode Length = 99.32303281299777
2025-08-04 03:42:51,890 - evaluation_results_class.py:149 - Counted Episodes = 3139
2025-08-04 03:42:52,049 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:42:52,058 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:42:54,411 - father_agent.py:386 - Step: 905, Training loss: 1.0976206064224243
2025-08-04 03:42:55,995 - father_agent.py:386 - Step: 910, Training loss: 0.7035269737243652
2025-08-04 03:42:57,571 - father_agent.py:386 - Step: 915, Training loss: 1.1239290237426758
2025-08-04 03:42:59,152 - father_agent.py:386 - Step: 920, Training loss: 0.7741888165473938
2025-08-04 03:43:00,718 - father_agent.py:386 - Step: 925, Training loss: 1.0696840286254883
2025-08-04 03:43:02,271 - father_agent.py:386 - Step: 930, Training loss: 0.6892836093902588
2025-08-04 03:43:03,834 - father_agent.py:386 - Step: 935, Training loss: 0.20484063029289246
2025-08-04 03:43:05,404 - father_agent.py:386 - Step: 940, Training loss: 0.6319203972816467
2025-08-04 03:43:06,982 - father_agent.py:386 - Step: 945, Training loss: 1.504528522491455
2025-08-04 03:43:08,555 - father_agent.py:386 - Step: 950, Training loss: 0.4402570128440857
2025-08-04 03:43:10,131 - father_agent.py:386 - Step: 955, Training loss: 0.6177151203155518
2025-08-04 03:43:11,704 - father_agent.py:386 - Step: 960, Training loss: 0.4672442376613617
2025-08-04 03:43:13,281 - father_agent.py:386 - Step: 965, Training loss: 0.46701210737228394
2025-08-04 03:43:14,866 - father_agent.py:386 - Step: 970, Training loss: 0.4966133236885071
2025-08-04 03:43:16,447 - father_agent.py:386 - Step: 975, Training loss: 0.2723139524459839
2025-08-04 03:43:18,023 - father_agent.py:386 - Step: 980, Training loss: 0.5775010585784912
2025-08-04 03:43:19,603 - father_agent.py:386 - Step: 985, Training loss: 0.8618367910385132
2025-08-04 03:43:21,171 - father_agent.py:386 - Step: 990, Training loss: 0.4614229202270508
2025-08-04 03:43:22,729 - father_agent.py:386 - Step: 995, Training loss: 0.6436258554458618
2025-08-04 03:43:24,290 - father_agent.py:386 - Step: 1000, Training loss: 0.5245781540870667
2025-08-04 03:43:24,455 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:43:24,458 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:43:31,439 - evaluation_results_class.py:131 - Average Return = -33.04679870605469
2025-08-04 03:43:31,439 - evaluation_results_class.py:133 - Average Virtual Goal Value = 46.95320129394531
2025-08-04 03:43:31,439 - evaluation_results_class.py:135 - Average Discounted Reward = 10.486032485961914
2025-08-04 03:43:31,439 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:43:31,439 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:43:31,439 - evaluation_results_class.py:141 - Variance of Return = 241.2484588623047
2025-08-04 03:43:31,439 - evaluation_results_class.py:143 - Current Best Return = -26.13637351989746
2025-08-04 03:43:31,439 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:43:31,439 - evaluation_results_class.py:147 - Average Episode Length = 91.23427902895584
2025-08-04 03:43:31,439 - evaluation_results_class.py:149 - Counted Episodes = 3419
2025-08-04 03:43:31,602 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:43:31,611 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:43:33,989 - father_agent.py:386 - Step: 1005, Training loss: 0.76268470287323
2025-08-04 03:43:35,566 - father_agent.py:386 - Step: 1010, Training loss: 0.6529328227043152
2025-08-04 03:43:37,142 - father_agent.py:386 - Step: 1015, Training loss: 0.46688830852508545
2025-08-04 03:43:38,715 - father_agent.py:386 - Step: 1020, Training loss: 0.3231843411922455
2025-08-04 03:43:40,267 - father_agent.py:386 - Step: 1025, Training loss: 0.8091237545013428
2025-08-04 03:43:41,830 - father_agent.py:386 - Step: 1030, Training loss: 0.5071648359298706
2025-08-04 03:43:43,393 - father_agent.py:386 - Step: 1035, Training loss: 0.5254585146903992
2025-08-04 03:43:44,964 - father_agent.py:386 - Step: 1040, Training loss: 1.012128472328186
2025-08-04 03:43:46,541 - father_agent.py:386 - Step: 1045, Training loss: 0.8342239856719971
2025-08-04 03:43:48,092 - father_agent.py:386 - Step: 1050, Training loss: 0.6566317081451416
2025-08-04 03:43:49,641 - father_agent.py:386 - Step: 1055, Training loss: 0.6299517154693604
2025-08-04 03:43:51,211 - father_agent.py:386 - Step: 1060, Training loss: 0.3117360472679138
2025-08-04 03:43:52,773 - father_agent.py:386 - Step: 1065, Training loss: 0.9373279809951782
2025-08-04 03:43:54,335 - father_agent.py:386 - Step: 1070, Training loss: 0.6757712364196777
2025-08-04 03:43:55,882 - father_agent.py:386 - Step: 1075, Training loss: 0.5133483409881592
2025-08-04 03:43:57,436 - father_agent.py:386 - Step: 1080, Training loss: 0.4364948570728302
2025-08-04 03:43:58,990 - father_agent.py:386 - Step: 1085, Training loss: 1.135042667388916
2025-08-04 03:44:00,538 - father_agent.py:386 - Step: 1090, Training loss: 0.6302093267440796
2025-08-04 03:44:02,095 - father_agent.py:386 - Step: 1095, Training loss: 0.6698460578918457
2025-08-04 03:44:03,653 - father_agent.py:386 - Step: 1100, Training loss: 0.6668384075164795
2025-08-04 03:44:03,822 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:44:03,826 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:44:10,903 - evaluation_results_class.py:131 - Average Return = -31.289430618286133
2025-08-04 03:44:10,903 - evaluation_results_class.py:133 - Average Virtual Goal Value = 48.710567474365234
2025-08-04 03:44:10,903 - evaluation_results_class.py:135 - Average Discounted Reward = 12.941533088684082
2025-08-04 03:44:10,903 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:44:10,903 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:44:10,903 - evaluation_results_class.py:141 - Variance of Return = 247.70973205566406
2025-08-04 03:44:10,903 - evaluation_results_class.py:143 - Current Best Return = -26.13637351989746
2025-08-04 03:44:10,903 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:44:10,903 - evaluation_results_class.py:147 - Average Episode Length = 85.65636856368563
2025-08-04 03:44:10,903 - evaluation_results_class.py:149 - Counted Episodes = 3690
2025-08-04 03:44:11,065 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:44:11,075 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:44:13,492 - father_agent.py:386 - Step: 1105, Training loss: 0.8580142259597778
2025-08-04 03:44:15,162 - father_agent.py:386 - Step: 1110, Training loss: 0.350737988948822
2025-08-04 03:44:16,813 - father_agent.py:386 - Step: 1115, Training loss: 1.0411157608032227
2025-08-04 03:44:18,453 - father_agent.py:386 - Step: 1120, Training loss: 0.29192882776260376
2025-08-04 03:44:20,080 - father_agent.py:386 - Step: 1125, Training loss: 0.9416301250457764
2025-08-04 03:44:21,713 - father_agent.py:386 - Step: 1130, Training loss: 0.5220678448677063
2025-08-04 03:44:23,342 - father_agent.py:386 - Step: 1135, Training loss: 0.769143283367157
2025-08-04 03:44:25,008 - father_agent.py:386 - Step: 1140, Training loss: 0.6724891066551208
2025-08-04 03:44:26,641 - father_agent.py:386 - Step: 1145, Training loss: 0.6516963243484497
2025-08-04 03:44:28,280 - father_agent.py:386 - Step: 1150, Training loss: 0.44208815693855286
2025-08-04 03:44:29,991 - father_agent.py:386 - Step: 1155, Training loss: 0.7073726654052734
2025-08-04 03:44:31,676 - father_agent.py:386 - Step: 1160, Training loss: 0.5382922887802124
2025-08-04 03:44:33,336 - father_agent.py:386 - Step: 1165, Training loss: 0.9732933640480042
2025-08-04 03:44:35,051 - father_agent.py:386 - Step: 1170, Training loss: 0.2520190477371216
2025-08-04 03:44:36,808 - father_agent.py:386 - Step: 1175, Training loss: 0.7121171951293945
2025-08-04 03:44:38,533 - father_agent.py:386 - Step: 1180, Training loss: 0.36728471517562866
2025-08-04 03:44:40,272 - father_agent.py:386 - Step: 1185, Training loss: 0.5438033938407898
2025-08-04 03:44:42,038 - father_agent.py:386 - Step: 1190, Training loss: 0.10005998611450195
2025-08-04 03:44:43,757 - father_agent.py:386 - Step: 1195, Training loss: 0.5065471529960632
2025-08-04 03:44:45,511 - father_agent.py:386 - Step: 1200, Training loss: 0.6741078495979309
2025-08-04 03:44:45,782 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:44:45,786 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:44:53,332 - evaluation_results_class.py:131 - Average Return = -31.40583610534668
2025-08-04 03:44:53,332 - evaluation_results_class.py:133 - Average Virtual Goal Value = 48.59416198730469
2025-08-04 03:44:53,332 - evaluation_results_class.py:135 - Average Discounted Reward = 12.937702178955078
2025-08-04 03:44:53,332 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:44:53,332 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:44:53,332 - evaluation_results_class.py:141 - Variance of Return = 272.431884765625
2025-08-04 03:44:53,333 - evaluation_results_class.py:143 - Current Best Return = -26.13637351989746
2025-08-04 03:44:53,333 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:44:53,333 - evaluation_results_class.py:147 - Average Episode Length = 85.21642799243448
2025-08-04 03:44:53,333 - evaluation_results_class.py:149 - Counted Episodes = 3701
2025-08-04 03:44:53,555 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:44:53,568 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:44:56,128 - father_agent.py:386 - Step: 1205, Training loss: 0.869452714920044
2025-08-04 03:44:57,849 - father_agent.py:386 - Step: 1210, Training loss: 0.4420675039291382
2025-08-04 03:44:59,526 - father_agent.py:386 - Step: 1215, Training loss: 0.3154870569705963
2025-08-04 03:45:01,184 - father_agent.py:386 - Step: 1220, Training loss: 0.45126795768737793
2025-08-04 03:45:02,938 - father_agent.py:386 - Step: 1225, Training loss: 0.5870457291603088
2025-08-04 03:45:04,683 - father_agent.py:386 - Step: 1230, Training loss: 0.22024983167648315
2025-08-04 03:45:06,352 - father_agent.py:386 - Step: 1235, Training loss: 0.7151726484298706
2025-08-04 03:45:08,013 - father_agent.py:386 - Step: 1240, Training loss: 0.3985612094402313
2025-08-04 03:45:09,674 - father_agent.py:386 - Step: 1245, Training loss: 0.5529044270515442
2025-08-04 03:45:11,293 - father_agent.py:386 - Step: 1250, Training loss: 0.7520943880081177
2025-08-04 03:45:12,900 - father_agent.py:386 - Step: 1255, Training loss: 1.1062911748886108
2025-08-04 03:45:14,537 - father_agent.py:386 - Step: 1260, Training loss: 0.18297415971755981
2025-08-04 03:45:16,223 - father_agent.py:386 - Step: 1265, Training loss: 1.035939335823059
2025-08-04 03:45:17,921 - father_agent.py:386 - Step: 1270, Training loss: 0.45911285281181335
2025-08-04 03:45:19,636 - father_agent.py:386 - Step: 1275, Training loss: 0.38746902346611023
2025-08-04 03:45:21,218 - father_agent.py:386 - Step: 1280, Training loss: 0.5736092329025269
2025-08-04 03:45:22,801 - father_agent.py:386 - Step: 1285, Training loss: 0.9745641946792603
2025-08-04 03:45:24,380 - father_agent.py:386 - Step: 1290, Training loss: 0.36730852723121643
2025-08-04 03:45:25,975 - father_agent.py:386 - Step: 1295, Training loss: 0.8820961713790894
2025-08-04 03:45:27,564 - father_agent.py:386 - Step: 1300, Training loss: 0.6877776384353638
2025-08-04 03:45:27,733 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:45:27,736 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:45:35,038 - evaluation_results_class.py:131 - Average Return = -31.11370277404785
2025-08-04 03:45:35,038 - evaluation_results_class.py:133 - Average Virtual Goal Value = 48.88629913330078
2025-08-04 03:45:35,038 - evaluation_results_class.py:135 - Average Discounted Reward = 13.553126335144043
2025-08-04 03:45:35,038 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:45:35,038 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:45:35,038 - evaluation_results_class.py:141 - Variance of Return = 308.0260314941406
2025-08-04 03:45:35,038 - evaluation_results_class.py:143 - Current Best Return = -26.13637351989746
2025-08-04 03:45:35,039 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:45:35,039 - evaluation_results_class.py:147 - Average Episode Length = 83.30002650410813
2025-08-04 03:45:35,039 - evaluation_results_class.py:149 - Counted Episodes = 3773
2025-08-04 03:45:35,203 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:45:35,213 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:45:37,733 - father_agent.py:386 - Step: 1305, Training loss: 0.4376998841762543
2025-08-04 03:45:39,477 - father_agent.py:386 - Step: 1310, Training loss: 0.6756040453910828
2025-08-04 03:45:41,229 - father_agent.py:386 - Step: 1315, Training loss: 0.49504661560058594
2025-08-04 03:45:42,948 - father_agent.py:386 - Step: 1320, Training loss: 0.27816349267959595
2025-08-04 03:45:44,652 - father_agent.py:386 - Step: 1325, Training loss: 0.8467023372650146
2025-08-04 03:45:46,421 - father_agent.py:386 - Step: 1330, Training loss: 0.4299667775630951
2025-08-04 03:45:48,145 - father_agent.py:386 - Step: 1335, Training loss: 0.593269407749176
2025-08-04 03:45:49,905 - father_agent.py:386 - Step: 1340, Training loss: 0.7625445127487183
2025-08-04 03:45:51,645 - father_agent.py:386 - Step: 1345, Training loss: 0.6456772089004517
2025-08-04 03:45:53,361 - father_agent.py:386 - Step: 1350, Training loss: 0.2687259912490845
2025-08-04 03:45:55,085 - father_agent.py:386 - Step: 1355, Training loss: 0.8701675534248352
2025-08-04 03:45:56,803 - father_agent.py:386 - Step: 1360, Training loss: 0.6414409279823303
2025-08-04 03:45:58,542 - father_agent.py:386 - Step: 1365, Training loss: 0.709823727607727
2025-08-04 03:46:00,271 - father_agent.py:386 - Step: 1370, Training loss: 1.0095930099487305
2025-08-04 03:46:01,981 - father_agent.py:386 - Step: 1375, Training loss: 0.8265507221221924
2025-08-04 03:46:03,703 - father_agent.py:386 - Step: 1380, Training loss: 0.5970216989517212
2025-08-04 03:46:05,377 - father_agent.py:386 - Step: 1385, Training loss: 1.0955557823181152
2025-08-04 03:46:07,035 - father_agent.py:386 - Step: 1390, Training loss: 0.5684275031089783
2025-08-04 03:46:08,685 - father_agent.py:386 - Step: 1395, Training loss: 0.4691044092178345
2025-08-04 03:46:10,305 - father_agent.py:386 - Step: 1400, Training loss: 0.7203890681266785
2025-08-04 03:46:10,473 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:46:10,476 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:46:18,272 - evaluation_results_class.py:131 - Average Return = -31.388019561767578
2025-08-04 03:46:18,272 - evaluation_results_class.py:133 - Average Virtual Goal Value = 48.61198043823242
2025-08-04 03:46:18,272 - evaluation_results_class.py:135 - Average Discounted Reward = 12.569693565368652
2025-08-04 03:46:18,272 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:46:18,272 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:46:18,272 - evaluation_results_class.py:141 - Variance of Return = 214.93051147460938
2025-08-04 03:46:18,272 - evaluation_results_class.py:143 - Current Best Return = -26.13637351989746
2025-08-04 03:46:18,272 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:46:18,272 - evaluation_results_class.py:147 - Average Episode Length = 87.0052212146194
2025-08-04 03:46:18,272 - evaluation_results_class.py:149 - Counted Episodes = 3639
2025-08-04 03:46:18,470 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:46:18,483 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:46:18,608 - father_agent.py:547 - Training finished.
2025-08-04 03:46:18,769 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:46:18,773 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:46:18,775 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 03:46:26,354 - evaluation_results_class.py:131 - Average Return = -31.64809799194336
2025-08-04 03:46:26,354 - evaluation_results_class.py:133 - Average Virtual Goal Value = 48.35190200805664
2025-08-04 03:46:26,354 - evaluation_results_class.py:135 - Average Discounted Reward = 12.299276351928711
2025-08-04 03:46:26,355 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:46:26,355 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:46:26,355 - evaluation_results_class.py:141 - Variance of Return = 228.76585388183594
2025-08-04 03:46:26,355 - evaluation_results_class.py:143 - Current Best Return = -26.13637351989746
2025-08-04 03:46:26,355 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:46:26,355 - evaluation_results_class.py:147 - Average Episode Length = 87.26696083838941
2025-08-04 03:46:26,355 - evaluation_results_class.py:149 - Counted Episodes = 3626
2025-08-04 03:46:26,518 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:46:26,521 - self_interpretable_extractor.py:286 - True
2025-08-04 03:46:26,531 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:46:39,421 - evaluation_results_class.py:131 - Average Return = -31.880685806274414
2025-08-04 03:46:39,422 - evaluation_results_class.py:133 - Average Virtual Goal Value = 48.0975227355957
2025-08-04 03:46:39,422 - evaluation_results_class.py:135 - Average Discounted Reward = 12.064054489135742
2025-08-04 03:46:39,422 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9997275946608554
2025-08-04 03:46:39,422 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:46:39,422 - evaluation_results_class.py:141 - Variance of Return = 245.91932678222656
2025-08-04 03:46:39,422 - evaluation_results_class.py:143 - Current Best Return = -31.880685806274414
2025-08-04 03:46:39,422 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9997275946608554
2025-08-04 03:46:39,422 - evaluation_results_class.py:147 - Average Episode Length = 87.70798147643694
2025-08-04 03:46:39,422 - evaluation_results_class.py:149 - Counted Episodes = 3671
2025-08-04 03:46:39,422 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 03:46:39,422 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 17259 trajectories
Learned trajectory lengths  {128, 131, 651, 158, 167, 50, 179, 53, 182, 56, 59, 572, 443, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 122, 125}
Buffer 1
2025-08-04 03:47:36,317 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 34548 trajectories
Learned trajectory lengths  {128, 131, 134, 651, 140, 158, 548, 167, 47, 50, 179, 53, 182, 56, 59, 572, 443, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 350, 95, 98, 101, 104, 107, 110, 113, 116, 374, 119, 122, 125}
Buffer 2
2025-08-04 03:48:33,463 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 51851 trajectories
Learned trajectory lengths  {128, 131, 134, 137, 651, 140, 146, 158, 548, 167, 47, 50, 179, 53, 182, 56, 59, 572, 443, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 350, 95, 98, 101, 104, 107, 110, 113, 116, 374, 119, 122, 125}
All trajectories collected
2025-08-04 03:49:30,297 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 03:49:30,297 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 51851 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_0.dot.
Learned FSC of size 2
2025-08-04 03:49:32,493 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 03:49:44,623 - evaluation_results_class.py:131 - Average Return = -50.10845184326172
2025-08-04 03:49:44,623 - evaluation_results_class.py:133 - Average Virtual Goal Value = 29.89154624938965
2025-08-04 03:49:44,623 - evaluation_results_class.py:135 - Average Discounted Reward = 0.5597752332687378
2025-08-04 03:49:44,623 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:49:44,623 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:49:44,623 - evaluation_results_class.py:141 - Variance of Return = 1594.7034912109375
2025-08-04 03:49:44,623 - evaluation_results_class.py:143 - Current Best Return = -50.10845184326172
2025-08-04 03:49:44,623 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:49:44,623 - evaluation_results_class.py:147 - Average Episode Length = 87.28241369937483
2025-08-04 03:49:44,623 - evaluation_results_class.py:149 - Counted Episodes = 3679
FSC Result: {'best_episode_return': 29.891546, 'best_return': -50.10845, 'goal_value': 0.0, 'returns_episodic': [29.891546], 'returns': [-50.10845], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [1594.7035], 'each_episode_virtual_variance': [1594.7035], 'combined_variance': [6378.814], 'num_episodes': [3679], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [87.28241369937483], 'counted_episodes': [3679], 'discounted_rewards': [0.55977523], 'new_pomdp_iteration_numbers': []}
[[5], [3, 4, 6, 7], [3, 4, 6, 7], [1], [1], [1], [1], [2], [2], [2], [2], [3, 4, 6, 7], [3, 4, 6, 7], [0]]
2025-08-04 03:49:44,724 - statistic.py:67 - synthesis initiated, design space: 81
2025-08-04 03:49:44,724 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 03:49:44,739 - synthesizer_ar.py:122 - value 120.6467 achieved after 804.0 seconds
2025-08-04 03:49:44,750 - synthesizer_ar.py:122 - value 130.7542 achieved after 804.01 seconds
2025-08-04 03:49:44,761 - synthesizer_ar.py:122 - value 146.5208 achieved after 804.02 seconds
2025-08-04 03:49:44,786 - synthesizer_ar.py:122 - value 147.0217 achieved after 804.04 seconds
2025-08-04 03:49:44,812 - synthesizer_ar.py:122 - value 147.354 achieved after 804.07 seconds
2025-08-04 03:49:44,815 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 03:49:44,815 - synthesizer.py:193 - o1x=3, o1y=5, o2x=6, o2y=8
2025-08-04 03:49:44,816 - synthesizer.py:198 - double-checking specification satisfiability:  : 147.35403944280887
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 10) & (y = 10))] 

method: AR, synthesis time: 0.09 s
number of holes: 4, family size: 81, quotient: 1048 states / 1200 actions
explored: 100 %
MDP stats: avg MDP size: 934, iterations: 55

optimum: 147.354039
--------------------
2025-08-04 03:49:44,816 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
2025-08-04 03:49:44,818 - robust_rl_trainer.py:432 - Iteration 2 of pure RL loop
2025-08-04 03:49:44,860 - storm_vec_env.py:70 - Computing row map
2025-08-04 03:49:44,865 - storm_vec_env.py:97 - Computing transitions
2025-08-04 03:49:44,878 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 03:49:44,878 - storm_vec_env.py:114 - Computing sinks
2025-08-04 03:49:44,878 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 03:49:44,882 - storm_vec_env.py:143 - Computing labels
2025-08-04 03:49:44,882 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 03:49:44,882 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 03:49:44,882 - storm_vec_env.py:175 - Computing observations
2025-08-04 03:49:44,995 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 03:49:44,996 - father_agent.py:540 - Before training evaluation.
2025-08-04 03:49:45,135 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:49:45,138 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:49:53,192 - evaluation_results_class.py:131 - Average Return = -86.97844696044922
2025-08-04 03:49:53,192 - evaluation_results_class.py:133 - Average Virtual Goal Value = -6.978444576263428
2025-08-04 03:49:53,192 - evaluation_results_class.py:135 - Average Discounted Reward = -34.959510803222656
2025-08-04 03:49:53,192 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:49:53,192 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:49:53,192 - evaluation_results_class.py:141 - Variance of Return = 3341.884033203125
2025-08-04 03:49:53,192 - evaluation_results_class.py:143 - Current Best Return = -26.13637351989746
2025-08-04 03:49:53,192 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:49:53,192 - evaluation_results_class.py:147 - Average Episode Length = 86.42592087312414
2025-08-04 03:49:53,192 - evaluation_results_class.py:149 - Counted Episodes = 3665
2025-08-04 03:49:53,362 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:49:53,372 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:49:53,482 - father_agent.py:436 - Training agent on-policy
2025-08-04 03:50:00,030 - father_agent.py:386 - Step: 0, Training loss: 6.567180156707764
2025-08-04 03:50:01,674 - father_agent.py:386 - Step: 5, Training loss: 1.8351260423660278
2025-08-04 03:50:03,315 - father_agent.py:386 - Step: 10, Training loss: 1.641623616218567
2025-08-04 03:50:04,956 - father_agent.py:386 - Step: 15, Training loss: 0.8677101731300354
2025-08-04 03:50:06,633 - father_agent.py:386 - Step: 20, Training loss: 0.6471579670906067
2025-08-04 03:50:08,350 - father_agent.py:386 - Step: 25, Training loss: 0.3719930648803711
2025-08-04 03:50:10,005 - father_agent.py:386 - Step: 30, Training loss: 0.4058099687099457
2025-08-04 03:50:11,659 - father_agent.py:386 - Step: 35, Training loss: 0.6723889708518982
2025-08-04 03:50:13,312 - father_agent.py:386 - Step: 40, Training loss: 0.7178596258163452
2025-08-04 03:50:14,965 - father_agent.py:386 - Step: 45, Training loss: 0.3692004680633545
2025-08-04 03:50:16,635 - father_agent.py:386 - Step: 50, Training loss: 0.5697238445281982
2025-08-04 03:50:18,287 - father_agent.py:386 - Step: 55, Training loss: 0.1695096492767334
2025-08-04 03:50:19,961 - father_agent.py:386 - Step: 60, Training loss: 0.8646591901779175
2025-08-04 03:50:21,617 - father_agent.py:386 - Step: 65, Training loss: 0.9094987511634827
2025-08-04 03:50:23,278 - father_agent.py:386 - Step: 70, Training loss: 0.7023414969444275
2025-08-04 03:50:24,947 - father_agent.py:386 - Step: 75, Training loss: 0.46521809697151184
2025-08-04 03:50:26,604 - father_agent.py:386 - Step: 80, Training loss: 0.7092491388320923
2025-08-04 03:50:28,281 - father_agent.py:386 - Step: 85, Training loss: 0.6899308562278748
2025-08-04 03:50:29,949 - father_agent.py:386 - Step: 90, Training loss: 0.6179941296577454
2025-08-04 03:50:31,604 - father_agent.py:386 - Step: 95, Training loss: 0.7482850551605225
2025-08-04 03:50:33,258 - father_agent.py:386 - Step: 100, Training loss: 0.7549356818199158
2025-08-04 03:50:33,438 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:50:33,441 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:50:41,127 - evaluation_results_class.py:131 - Average Return = -28.69614028930664
2025-08-04 03:50:41,127 - evaluation_results_class.py:133 - Average Virtual Goal Value = 51.30385971069336
2025-08-04 03:50:41,127 - evaluation_results_class.py:135 - Average Discounted Reward = 18.80882453918457
2025-08-04 03:50:41,127 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:50:41,127 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:50:41,127 - evaluation_results_class.py:141 - Variance of Return = 468.80517578125
2025-08-04 03:50:41,127 - evaluation_results_class.py:143 - Current Best Return = -26.13637351989746
2025-08-04 03:50:41,127 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:50:41,127 - evaluation_results_class.py:147 - Average Episode Length = 70.28974641675855
2025-08-04 03:50:41,127 - evaluation_results_class.py:149 - Counted Episodes = 4535
2025-08-04 03:50:41,308 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:50:41,318 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:50:47,129 - father_agent.py:386 - Step: 105, Training loss: 1.1068159341812134
2025-08-04 03:50:48,861 - father_agent.py:386 - Step: 110, Training loss: 1.0421234369277954
2025-08-04 03:50:50,710 - father_agent.py:386 - Step: 115, Training loss: 0.6439429521560669
2025-08-04 03:50:52,495 - father_agent.py:386 - Step: 120, Training loss: 0.46966230869293213
2025-08-04 03:50:54,260 - father_agent.py:386 - Step: 125, Training loss: 0.6064272522926331
2025-08-04 03:50:56,003 - father_agent.py:386 - Step: 130, Training loss: 0.5265712141990662
2025-08-04 03:50:57,754 - father_agent.py:386 - Step: 135, Training loss: 0.5014077425003052
2025-08-04 03:50:59,512 - father_agent.py:386 - Step: 140, Training loss: 0.9797109365463257
2025-08-04 03:51:01,343 - father_agent.py:386 - Step: 145, Training loss: 1.039454460144043
2025-08-04 03:51:03,098 - father_agent.py:386 - Step: 150, Training loss: 0.9312633872032166
2025-08-04 03:51:04,841 - father_agent.py:386 - Step: 155, Training loss: 0.909500241279602
2025-08-04 03:51:06,582 - father_agent.py:386 - Step: 160, Training loss: 0.24888183176517487
2025-08-04 03:51:08,314 - father_agent.py:386 - Step: 165, Training loss: 0.7422457337379456
2025-08-04 03:51:10,019 - father_agent.py:386 - Step: 170, Training loss: 0.5762810707092285
2025-08-04 03:51:11,725 - father_agent.py:386 - Step: 175, Training loss: 0.9858123064041138
2025-08-04 03:51:13,439 - father_agent.py:386 - Step: 180, Training loss: 0.5128993391990662
2025-08-04 03:51:15,144 - father_agent.py:386 - Step: 185, Training loss: 0.5827155113220215
2025-08-04 03:51:16,838 - father_agent.py:386 - Step: 190, Training loss: 0.2591018080711365
2025-08-04 03:51:18,538 - father_agent.py:386 - Step: 195, Training loss: 0.5452839136123657
2025-08-04 03:51:20,273 - father_agent.py:386 - Step: 200, Training loss: 0.5930113792419434
2025-08-04 03:51:20,455 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:51:20,459 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:51:28,469 - evaluation_results_class.py:131 - Average Return = -25.66155433654785
2025-08-04 03:51:28,470 - evaluation_results_class.py:133 - Average Virtual Goal Value = 54.324344635009766
2025-08-04 03:51:28,470 - evaluation_results_class.py:135 - Average Discounted Reward = 25.69943618774414
2025-08-04 03:51:28,470 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9998237264234091
2025-08-04 03:51:28,470 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:51:28,470 - evaluation_results_class.py:141 - Variance of Return = 598.9163208007812
2025-08-04 03:51:28,470 - evaluation_results_class.py:143 - Current Best Return = -25.66155433654785
2025-08-04 03:51:28,470 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:51:28,470 - evaluation_results_class.py:147 - Average Episode Length = 56.751277983430285
2025-08-04 03:51:28,470 - evaluation_results_class.py:149 - Counted Episodes = 5673
2025-08-04 03:51:28,652 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:51:28,662 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:51:31,515 - father_agent.py:386 - Step: 205, Training loss: 1.3941993713378906
2025-08-04 03:51:33,229 - father_agent.py:386 - Step: 210, Training loss: 0.77767413854599
2025-08-04 03:51:34,912 - father_agent.py:386 - Step: 215, Training loss: 0.3105565309524536
2025-08-04 03:51:36,631 - father_agent.py:386 - Step: 220, Training loss: 0.5685692429542542
2025-08-04 03:51:38,336 - father_agent.py:386 - Step: 225, Training loss: 0.5864629745483398
2025-08-04 03:51:39,997 - father_agent.py:386 - Step: 230, Training loss: 0.38908320665359497
2025-08-04 03:51:41,646 - father_agent.py:386 - Step: 235, Training loss: 0.9326236844062805
2025-08-04 03:51:43,293 - father_agent.py:386 - Step: 240, Training loss: 0.5957992076873779
2025-08-04 03:51:44,930 - father_agent.py:386 - Step: 245, Training loss: 0.7531166672706604
2025-08-04 03:51:46,559 - father_agent.py:386 - Step: 250, Training loss: 0.8481775522232056
2025-08-04 03:51:48,176 - father_agent.py:386 - Step: 255, Training loss: 1.0042146444320679
2025-08-04 03:51:49,819 - father_agent.py:386 - Step: 260, Training loss: 0.66734379529953
2025-08-04 03:51:51,461 - father_agent.py:386 - Step: 265, Training loss: 0.5928881168365479
2025-08-04 03:51:53,119 - father_agent.py:386 - Step: 270, Training loss: 0.6014263033866882
2025-08-04 03:51:54,794 - father_agent.py:386 - Step: 275, Training loss: 0.3716817796230316
2025-08-04 03:51:56,437 - father_agent.py:386 - Step: 280, Training loss: 0.9362181425094604
2025-08-04 03:51:58,083 - father_agent.py:386 - Step: 285, Training loss: 0.8758100867271423
2025-08-04 03:51:59,725 - father_agent.py:386 - Step: 290, Training loss: 0.6909381747245789
2025-08-04 03:52:01,375 - father_agent.py:386 - Step: 295, Training loss: 0.656492292881012
2025-08-04 03:52:03,024 - father_agent.py:386 - Step: 300, Training loss: 0.7839717268943787
2025-08-04 03:52:03,209 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:52:03,212 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:52:11,441 - evaluation_results_class.py:131 - Average Return = -25.038421630859375
2025-08-04 03:52:11,442 - evaluation_results_class.py:133 - Average Virtual Goal Value = 54.961578369140625
2025-08-04 03:52:11,442 - evaluation_results_class.py:135 - Average Discounted Reward = 26.987564086914062
2025-08-04 03:52:11,442 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:52:11,442 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:52:11,442 - evaluation_results_class.py:141 - Variance of Return = 605.3135375976562
2025-08-04 03:52:11,442 - evaluation_results_class.py:143 - Current Best Return = -25.038421630859375
2025-08-04 03:52:11,442 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:52:11,442 - evaluation_results_class.py:147 - Average Episode Length = 54.58039945836154
2025-08-04 03:52:11,442 - evaluation_results_class.py:149 - Counted Episodes = 5908
2025-08-04 03:52:11,621 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:52:11,630 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:52:14,422 - father_agent.py:386 - Step: 305, Training loss: 1.2725039720535278
2025-08-04 03:52:16,096 - father_agent.py:386 - Step: 310, Training loss: 1.5029362440109253
2025-08-04 03:52:17,759 - father_agent.py:386 - Step: 315, Training loss: 0.44823944568634033
2025-08-04 03:52:19,425 - father_agent.py:386 - Step: 320, Training loss: 0.3962676525115967
2025-08-04 03:52:21,073 - father_agent.py:386 - Step: 325, Training loss: 0.9192372560501099
2025-08-04 03:52:22,744 - father_agent.py:386 - Step: 330, Training loss: 1.06813645362854
2025-08-04 03:52:24,392 - father_agent.py:386 - Step: 335, Training loss: 0.6562293767929077
2025-08-04 03:52:26,038 - father_agent.py:386 - Step: 340, Training loss: 0.7201664447784424
2025-08-04 03:52:27,679 - father_agent.py:386 - Step: 345, Training loss: 0.5635547637939453
2025-08-04 03:52:29,333 - father_agent.py:386 - Step: 350, Training loss: 0.5497996807098389
2025-08-04 03:52:30,995 - father_agent.py:386 - Step: 355, Training loss: 0.8939631581306458
2025-08-04 03:52:32,642 - father_agent.py:386 - Step: 360, Training loss: 0.8597396016120911
2025-08-04 03:52:34,292 - father_agent.py:386 - Step: 365, Training loss: 1.1154381036758423
2025-08-04 03:52:35,929 - father_agent.py:386 - Step: 370, Training loss: 1.080445647239685
2025-08-04 03:52:37,565 - father_agent.py:386 - Step: 375, Training loss: 0.8049513101577759
2025-08-04 03:52:39,215 - father_agent.py:386 - Step: 380, Training loss: 0.6782037019729614
2025-08-04 03:52:40,878 - father_agent.py:386 - Step: 385, Training loss: 1.065622091293335
2025-08-04 03:52:42,553 - father_agent.py:386 - Step: 390, Training loss: 1.2369863986968994
2025-08-04 03:52:44,237 - father_agent.py:386 - Step: 395, Training loss: 0.65807044506073
2025-08-04 03:52:45,929 - father_agent.py:386 - Step: 400, Training loss: 0.9744332432746887
2025-08-04 03:52:46,114 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:52:46,117 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:52:54,085 - evaluation_results_class.py:131 - Average Return = -23.299257278442383
2025-08-04 03:52:54,085 - evaluation_results_class.py:133 - Average Virtual Goal Value = 56.700740814208984
2025-08-04 03:52:54,085 - evaluation_results_class.py:135 - Average Discounted Reward = 29.815153121948242
2025-08-04 03:52:54,085 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:52:54,085 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:52:54,085 - evaluation_results_class.py:141 - Variance of Return = 560.06787109375
2025-08-04 03:52:54,085 - evaluation_results_class.py:143 - Current Best Return = -23.299257278442383
2025-08-04 03:52:54,085 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:52:54,085 - evaluation_results_class.py:147 - Average Episode Length = 50.851951335124035
2025-08-04 03:52:54,085 - evaluation_results_class.py:149 - Counted Episodes = 6329
2025-08-04 03:52:54,264 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:52:54,275 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:52:57,131 - father_agent.py:386 - Step: 405, Training loss: 1.0725927352905273
2025-08-04 03:52:58,872 - father_agent.py:386 - Step: 410, Training loss: 0.9634861946105957
2025-08-04 03:53:00,576 - father_agent.py:386 - Step: 415, Training loss: 0.9203646779060364
2025-08-04 03:53:02,267 - father_agent.py:386 - Step: 420, Training loss: 0.9433776140213013
2025-08-04 03:53:03,970 - father_agent.py:386 - Step: 425, Training loss: 0.828937828540802
2025-08-04 03:53:05,713 - father_agent.py:386 - Step: 430, Training loss: 0.5436557531356812
2025-08-04 03:53:07,423 - father_agent.py:386 - Step: 435, Training loss: 1.284661889076233
2025-08-04 03:53:09,136 - father_agent.py:386 - Step: 440, Training loss: 0.9171848893165588
2025-08-04 03:53:10,912 - father_agent.py:386 - Step: 445, Training loss: 0.9509739279747009
2025-08-04 03:53:12,627 - father_agent.py:386 - Step: 450, Training loss: 0.5793873071670532
2025-08-04 03:53:14,314 - father_agent.py:386 - Step: 455, Training loss: 1.0647268295288086
2025-08-04 03:53:16,027 - father_agent.py:386 - Step: 460, Training loss: 0.5602774024009705
2025-08-04 03:53:17,735 - father_agent.py:386 - Step: 465, Training loss: 0.6111209392547607
2025-08-04 03:53:19,422 - father_agent.py:386 - Step: 470, Training loss: 0.8091838955879211
2025-08-04 03:53:21,063 - father_agent.py:386 - Step: 475, Training loss: 0.7993966341018677
2025-08-04 03:53:22,717 - father_agent.py:386 - Step: 480, Training loss: 0.6414588689804077
2025-08-04 03:53:24,381 - father_agent.py:386 - Step: 485, Training loss: 0.608406126499176
2025-08-04 03:53:26,045 - father_agent.py:386 - Step: 490, Training loss: 0.7101970911026001
2025-08-04 03:53:27,697 - father_agent.py:386 - Step: 495, Training loss: 0.8781863451004028
2025-08-04 03:53:29,362 - father_agent.py:386 - Step: 500, Training loss: 0.7071856260299683
2025-08-04 03:53:29,566 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:53:29,570 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:53:37,463 - evaluation_results_class.py:131 - Average Return = -23.910497665405273
2025-08-04 03:53:37,463 - evaluation_results_class.py:133 - Average Virtual Goal Value = 56.08950424194336
2025-08-04 03:53:37,463 - evaluation_results_class.py:135 - Average Discounted Reward = 28.447317123413086
2025-08-04 03:53:37,463 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:53:37,463 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:53:37,463 - evaluation_results_class.py:141 - Variance of Return = 548.685302734375
2025-08-04 03:53:37,463 - evaluation_results_class.py:143 - Current Best Return = -23.299257278442383
2025-08-04 03:53:37,463 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:53:37,463 - evaluation_results_class.py:147 - Average Episode Length = 53.1250411319513
2025-08-04 03:53:37,463 - evaluation_results_class.py:149 - Counted Episodes = 6078
2025-08-04 03:53:37,644 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:53:37,654 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:53:37,765 - father_agent.py:547 - Training finished.
2025-08-04 03:53:37,909 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:53:37,912 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:53:37,914 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 03:53:45,693 - evaluation_results_class.py:131 - Average Return = -24.34024429321289
2025-08-04 03:53:45,693 - evaluation_results_class.py:133 - Average Virtual Goal Value = 55.65975570678711
2025-08-04 03:53:45,693 - evaluation_results_class.py:135 - Average Discounted Reward = 28.122146606445312
2025-08-04 03:53:45,693 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:53:45,693 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:53:45,693 - evaluation_results_class.py:141 - Variance of Return = 590.0142211914062
2025-08-04 03:53:45,693 - evaluation_results_class.py:143 - Current Best Return = -23.299257278442383
2025-08-04 03:53:45,693 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:53:45,693 - evaluation_results_class.py:147 - Average Episode Length = 53.03224744981902
2025-08-04 03:53:45,693 - evaluation_results_class.py:149 - Counted Episodes = 6078
2025-08-04 03:53:45,875 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:53:45,878 - self_interpretable_extractor.py:286 - True
2025-08-04 03:53:45,888 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:53:59,655 - evaluation_results_class.py:131 - Average Return = -24.324840545654297
2025-08-04 03:53:59,655 - evaluation_results_class.py:133 - Average Virtual Goal Value = 55.6751594543457
2025-08-04 03:53:59,655 - evaluation_results_class.py:135 - Average Discounted Reward = 28.047821044921875
2025-08-04 03:53:59,655 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:53:59,655 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:53:59,655 - evaluation_results_class.py:141 - Variance of Return = 583.7374877929688
2025-08-04 03:53:59,655 - evaluation_results_class.py:143 - Current Best Return = -24.324840545654297
2025-08-04 03:53:59,655 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:53:59,655 - evaluation_results_class.py:147 - Average Episode Length = 53.314388371713214
2025-08-04 03:53:59,656 - evaluation_results_class.py:149 - Counted Episodes = 6123
2025-08-04 03:53:59,656 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 03:53:59,656 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 28154 trajectories
Learned trajectory lengths  {651, 29, 32, 35, 38, 41, 44, 47, 50, 179, 53, 56, 59, 188, 62, 65, 68, 71, 74, 77, 80, 83, 89, 92, 98}
Buffer 1
2025-08-04 03:54:59,289 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 56343 trajectories
Learned trajectory lengths  {651, 29, 32, 35, 38, 41, 44, 47, 50, 179, 53, 56, 59, 188, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 98}
Buffer 2
2025-08-04 03:55:58,987 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 84464 trajectories
Learned trajectory lengths  {650, 651, 29, 32, 35, 38, 41, 44, 47, 50, 179, 53, 56, 59, 188, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 98, 371, 125, 254}
All trajectories collected
2025-08-04 03:56:59,026 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 03:56:59,026 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 84464 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_1.dot.
Learned FSC of size 2
2025-08-04 03:57:00,697 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 03:57:13,371 - evaluation_results_class.py:131 - Average Return = -72.86067962646484
2025-08-04 03:57:13,371 - evaluation_results_class.py:133 - Average Virtual Goal Value = 7.139317035675049
2025-08-04 03:57:13,371 - evaluation_results_class.py:135 - Average Discounted Reward = -10.461804389953613
2025-08-04 03:57:13,371 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:57:13,371 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:57:13,371 - evaluation_results_class.py:141 - Variance of Return = 4035.777587890625
2025-08-04 03:57:13,371 - evaluation_results_class.py:143 - Current Best Return = -72.86067962646484
2025-08-04 03:57:13,371 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:57:13,372 - evaluation_results_class.py:147 - Average Episode Length = 54.729403884795715
2025-08-04 03:57:13,372 - evaluation_results_class.py:149 - Counted Episodes = 5972
FSC Result: {'best_episode_return': 7.139317, 'best_return': -72.86068, 'goal_value': 0.0, 'returns_episodic': [7.139317], 'returns': [-72.86068], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [4035.7776], 'each_episode_virtual_variance': [4035.7776], 'combined_variance': [16143.11], 'num_episodes': [5972], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [54.729403884795715], 'counted_episodes': [5972], 'discounted_rewards': [-10.461804], 'new_pomdp_iteration_numbers': []}
[[5], [3, 4, 6, 7], [3, 4, 6, 7], [1], [1], [1], [1], [2], [2], [2], [2], [3, 4, 6, 7], [3, 4, 6, 7], [0]]
2025-08-04 03:57:13,482 - statistic.py:67 - synthesis initiated, design space: 81
2025-08-04 03:57:13,482 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 03:57:13,498 - synthesizer_ar.py:122 - value 61.2941 achieved after 1252.75 seconds
2025-08-04 03:57:13,520 - synthesizer_ar.py:122 - value 68.7342 achieved after 1252.78 seconds
2025-08-04 03:57:13,542 - synthesizer_ar.py:122 - value 75.4439 achieved after 1252.8 seconds
2025-08-04 03:57:13,548 - synthesizer_ar.py:122 - value 75.9595 achieved after 1252.81 seconds
2025-08-04 03:57:13,602 - synthesizer_ar.py:122 - value 82.4796 achieved after 1252.86 seconds
2025-08-04 03:57:13,637 - synthesizer_ar.py:122 - value 83.9458 achieved after 1252.89 seconds
2025-08-04 03:57:13,652 - synthesizer_ar.py:122 - value 91.0734 achieved after 1252.91 seconds
2025-08-04 03:57:13,665 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 03:57:13,665 - synthesizer.py:193 - o1x=3, o1y=5, o2x=6, o2y=8
2025-08-04 03:57:13,666 - synthesizer.py:198 - double-checking specification satisfiability:  : 91.07340897558599
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 10) & (y = 10))] 

method: AR, synthesis time: 0.18 s
number of holes: 4, family size: 81, quotient: 1063 states / 1215 actions
explored: 100 %
MDP stats: avg MDP size: 929, iterations: 103

optimum: 91.073409
--------------------
2025-08-04 03:57:13,666 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=8
2025-08-04 03:57:13,669 - robust_rl_trainer.py:432 - Iteration 3 of pure RL loop
2025-08-04 03:57:13,709 - storm_vec_env.py:70 - Computing row map
2025-08-04 03:57:13,715 - storm_vec_env.py:97 - Computing transitions
2025-08-04 03:57:13,728 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 03:57:13,728 - storm_vec_env.py:114 - Computing sinks
2025-08-04 03:57:13,728 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 03:57:13,731 - storm_vec_env.py:143 - Computing labels
2025-08-04 03:57:13,731 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 03:57:13,731 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 03:57:13,731 - storm_vec_env.py:175 - Computing observations
2025-08-04 03:57:13,853 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 03:57:13,855 - father_agent.py:540 - Before training evaluation.
2025-08-04 03:57:13,993 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:57:13,995 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:57:22,319 - evaluation_results_class.py:131 - Average Return = -25.037616729736328
2025-08-04 03:57:22,320 - evaluation_results_class.py:133 - Average Virtual Goal Value = 54.96238327026367
2025-08-04 03:57:22,320 - evaluation_results_class.py:135 - Average Discounted Reward = 27.593801498413086
2025-08-04 03:57:22,320 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:57:22,320 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:57:22,320 - evaluation_results_class.py:141 - Variance of Return = 644.015869140625
2025-08-04 03:57:22,320 - evaluation_results_class.py:143 - Current Best Return = -23.299257278442383
2025-08-04 03:57:22,320 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:57:22,320 - evaluation_results_class.py:147 - Average Episode Length = 53.14073585216961
2025-08-04 03:57:22,320 - evaluation_results_class.py:149 - Counted Episodes = 6061
2025-08-04 03:57:22,501 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:57:22,511 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:57:22,623 - father_agent.py:436 - Training agent on-policy
2025-08-04 03:57:29,941 - father_agent.py:386 - Step: 0, Training loss: 0.4464908540248871
2025-08-04 03:57:31,646 - father_agent.py:386 - Step: 5, Training loss: 0.7412898540496826
2025-08-04 03:57:33,372 - father_agent.py:386 - Step: 10, Training loss: 1.0807381868362427
2025-08-04 03:57:35,082 - father_agent.py:386 - Step: 15, Training loss: 0.5006498098373413
2025-08-04 03:57:36,781 - father_agent.py:386 - Step: 20, Training loss: 1.0337011814117432
2025-08-04 03:57:38,478 - father_agent.py:386 - Step: 25, Training loss: 0.6768626570701599
2025-08-04 03:57:40,183 - father_agent.py:386 - Step: 30, Training loss: 0.49160921573638916
2025-08-04 03:57:41,914 - father_agent.py:386 - Step: 35, Training loss: 1.0131884813308716
2025-08-04 03:57:43,627 - father_agent.py:386 - Step: 40, Training loss: 1.2320114374160767
2025-08-04 03:57:45,343 - father_agent.py:386 - Step: 45, Training loss: 0.6175118684768677
2025-08-04 03:57:47,068 - father_agent.py:386 - Step: 50, Training loss: 1.241675853729248
2025-08-04 03:57:48,766 - father_agent.py:386 - Step: 55, Training loss: 0.8679813146591187
2025-08-04 03:57:50,478 - father_agent.py:386 - Step: 60, Training loss: 0.6197696924209595
2025-08-04 03:57:52,190 - father_agent.py:386 - Step: 65, Training loss: 0.7700261473655701
2025-08-04 03:57:53,890 - father_agent.py:386 - Step: 70, Training loss: 0.8634192943572998
2025-08-04 03:57:55,600 - father_agent.py:386 - Step: 75, Training loss: 1.1275973320007324
2025-08-04 03:57:57,316 - father_agent.py:386 - Step: 80, Training loss: 1.3425754308700562
2025-08-04 03:57:59,031 - father_agent.py:386 - Step: 85, Training loss: 0.8996008634567261
2025-08-04 03:58:00,761 - father_agent.py:386 - Step: 90, Training loss: 1.0311411619186401
2025-08-04 03:58:02,478 - father_agent.py:386 - Step: 95, Training loss: 1.0256887674331665
2025-08-04 03:58:04,205 - father_agent.py:386 - Step: 100, Training loss: 1.035688877105713
2025-08-04 03:58:04,404 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:58:04,407 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:58:12,838 - evaluation_results_class.py:131 - Average Return = -23.92400360107422
2025-08-04 03:58:12,838 - evaluation_results_class.py:133 - Average Virtual Goal Value = 56.07599639892578
2025-08-04 03:58:12,838 - evaluation_results_class.py:135 - Average Discounted Reward = 30.34345054626465
2025-08-04 03:58:12,838 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:58:12,839 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:58:12,839 - evaluation_results_class.py:141 - Variance of Return = 677.6021728515625
2025-08-04 03:58:12,839 - evaluation_results_class.py:143 - Current Best Return = -23.299257278442383
2025-08-04 03:58:12,839 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:58:12,839 - evaluation_results_class.py:147 - Average Episode Length = 48.52791572610986
2025-08-04 03:58:12,839 - evaluation_results_class.py:149 - Counted Episodes = 6645
2025-08-04 03:58:13,028 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:58:13,038 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:58:19,173 - father_agent.py:386 - Step: 105, Training loss: 1.1678965091705322
2025-08-04 03:58:20,905 - father_agent.py:386 - Step: 110, Training loss: 1.0833617448806763
2025-08-04 03:58:22,644 - father_agent.py:386 - Step: 115, Training loss: 0.9810499548912048
2025-08-04 03:58:24,351 - father_agent.py:386 - Step: 120, Training loss: 1.0723971128463745
2025-08-04 03:58:26,048 - father_agent.py:386 - Step: 125, Training loss: 0.9046056270599365
2025-08-04 03:58:27,746 - father_agent.py:386 - Step: 130, Training loss: 0.8081060647964478
2025-08-04 03:58:29,464 - father_agent.py:386 - Step: 135, Training loss: 0.9142946600914001
2025-08-04 03:58:31,176 - father_agent.py:386 - Step: 140, Training loss: 1.1082266569137573
2025-08-04 03:58:32,867 - father_agent.py:386 - Step: 145, Training loss: 0.5155843496322632
2025-08-04 03:58:34,563 - father_agent.py:386 - Step: 150, Training loss: 0.2418418824672699
2025-08-04 03:58:36,252 - father_agent.py:386 - Step: 155, Training loss: 0.7488009929656982
2025-08-04 03:58:37,942 - father_agent.py:386 - Step: 160, Training loss: 1.2384079694747925
2025-08-04 03:58:39,660 - father_agent.py:386 - Step: 165, Training loss: 0.8719473481178284
2025-08-04 03:58:41,375 - father_agent.py:386 - Step: 170, Training loss: 0.6995364427566528
2025-08-04 03:58:43,088 - father_agent.py:386 - Step: 175, Training loss: 0.9330131411552429
2025-08-04 03:58:44,796 - father_agent.py:386 - Step: 180, Training loss: 0.39401060342788696
2025-08-04 03:58:46,492 - father_agent.py:386 - Step: 185, Training loss: 0.7549097537994385
2025-08-04 03:58:48,191 - father_agent.py:386 - Step: 190, Training loss: 0.8467925786972046
2025-08-04 03:58:49,892 - father_agent.py:386 - Step: 195, Training loss: 1.1819243431091309
2025-08-04 03:58:51,587 - father_agent.py:386 - Step: 200, Training loss: 1.008623480796814
2025-08-04 03:58:51,785 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:58:51,787 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:59:00,235 - evaluation_results_class.py:131 - Average Return = -23.758153915405273
2025-08-04 03:59:00,235 - evaluation_results_class.py:133 - Average Virtual Goal Value = 56.24184799194336
2025-08-04 03:59:00,235 - evaluation_results_class.py:135 - Average Discounted Reward = 31.33767318725586
2025-08-04 03:59:00,235 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 03:59:00,235 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:59:00,235 - evaluation_results_class.py:141 - Variance of Return = 724.0811157226562
2025-08-04 03:59:00,235 - evaluation_results_class.py:143 - Current Best Return = -23.299257278442383
2025-08-04 03:59:00,235 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:59:00,235 - evaluation_results_class.py:147 - Average Episode Length = 46.568578916715204
2025-08-04 03:59:00,235 - evaluation_results_class.py:149 - Counted Episodes = 6868
2025-08-04 03:59:00,430 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:59:00,441 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:59:03,603 - father_agent.py:386 - Step: 205, Training loss: 0.8074364066123962
2025-08-04 03:59:05,281 - father_agent.py:386 - Step: 210, Training loss: 0.5835699439048767
2025-08-04 03:59:07,003 - father_agent.py:386 - Step: 215, Training loss: 1.116683006286621
2025-08-04 03:59:08,724 - father_agent.py:386 - Step: 220, Training loss: 0.5303449034690857
2025-08-04 03:59:10,436 - father_agent.py:386 - Step: 225, Training loss: 0.9351117610931396
2025-08-04 03:59:12,155 - father_agent.py:386 - Step: 230, Training loss: 0.8765801787376404
2025-08-04 03:59:13,863 - father_agent.py:386 - Step: 235, Training loss: 0.9609589576721191
2025-08-04 03:59:15,558 - father_agent.py:386 - Step: 240, Training loss: 1.3208160400390625
2025-08-04 03:59:17,242 - father_agent.py:386 - Step: 245, Training loss: 1.5998022556304932
2025-08-04 03:59:18,942 - father_agent.py:386 - Step: 250, Training loss: 1.412240982055664
2025-08-04 03:59:20,628 - father_agent.py:386 - Step: 255, Training loss: 1.432714581489563
2025-08-04 03:59:22,313 - father_agent.py:386 - Step: 260, Training loss: 1.3079931735992432
2025-08-04 03:59:23,985 - father_agent.py:386 - Step: 265, Training loss: 0.5210921764373779
2025-08-04 03:59:25,678 - father_agent.py:386 - Step: 270, Training loss: 0.570036768913269
2025-08-04 03:59:27,351 - father_agent.py:386 - Step: 275, Training loss: 0.4561464190483093
2025-08-04 03:59:29,046 - father_agent.py:386 - Step: 280, Training loss: 1.1038728952407837
2025-08-04 03:59:30,820 - father_agent.py:386 - Step: 285, Training loss: 0.9219238758087158
2025-08-04 03:59:32,567 - father_agent.py:386 - Step: 290, Training loss: 0.9162325859069824
2025-08-04 03:59:34,321 - father_agent.py:386 - Step: 295, Training loss: 0.7513157725334167
2025-08-04 03:59:36,065 - father_agent.py:386 - Step: 300, Training loss: 1.5815917253494263
2025-08-04 03:59:36,272 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:59:36,274 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:59:45,121 - evaluation_results_class.py:131 - Average Return = -27.23635482788086
2025-08-04 03:59:45,121 - evaluation_results_class.py:133 - Average Virtual Goal Value = 52.28292465209961
2025-08-04 03:59:45,121 - evaluation_results_class.py:135 - Average Discounted Reward = 29.977209091186523
2025-08-04 03:59:45,121 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9939909864797196
2025-08-04 03:59:45,121 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 03:59:45,121 - evaluation_results_class.py:141 - Variance of Return = 1315.669189453125
2025-08-04 03:59:45,121 - evaluation_results_class.py:143 - Current Best Return = -23.299257278442383
2025-08-04 03:59:45,121 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 03:59:45,121 - evaluation_results_class.py:147 - Average Episode Length = 59.48022033049574
2025-08-04 03:59:45,121 - evaluation_results_class.py:149 - Counted Episodes = 3994
2025-08-04 03:59:45,314 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:59:45,325 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 03:59:48,909 - father_agent.py:386 - Step: 305, Training loss: 1.598863124847412
2025-08-04 03:59:50,665 - father_agent.py:386 - Step: 310, Training loss: 0.7916561365127563
2025-08-04 03:59:52,463 - father_agent.py:386 - Step: 315, Training loss: 0.8034625053405762
2025-08-04 03:59:54,254 - father_agent.py:386 - Step: 320, Training loss: 0.5581011772155762
2025-08-04 03:59:56,089 - father_agent.py:386 - Step: 325, Training loss: 0.9031638503074646
2025-08-04 03:59:57,885 - father_agent.py:386 - Step: 330, Training loss: 0.5729680061340332
2025-08-04 03:59:59,673 - father_agent.py:386 - Step: 335, Training loss: 0.3543500304222107
2025-08-04 04:00:01,453 - father_agent.py:386 - Step: 340, Training loss: 1.239853024482727
2025-08-04 04:00:03,242 - father_agent.py:386 - Step: 345, Training loss: 1.1505720615386963
2025-08-04 04:00:04,979 - father_agent.py:386 - Step: 350, Training loss: 1.1083961725234985
2025-08-04 04:00:06,716 - father_agent.py:386 - Step: 355, Training loss: 0.9599428176879883
2025-08-04 04:00:08,457 - father_agent.py:386 - Step: 360, Training loss: 0.8705167770385742
2025-08-04 04:00:10,207 - father_agent.py:386 - Step: 365, Training loss: 0.7203623652458191
2025-08-04 04:00:11,959 - father_agent.py:386 - Step: 370, Training loss: 0.8730189204216003
2025-08-04 04:00:13,712 - father_agent.py:386 - Step: 375, Training loss: 0.8046340942382812
2025-08-04 04:00:15,468 - father_agent.py:386 - Step: 380, Training loss: 0.7871213555335999
2025-08-04 04:00:17,253 - father_agent.py:386 - Step: 385, Training loss: 0.8024860620498657
2025-08-04 04:00:19,011 - father_agent.py:386 - Step: 390, Training loss: 0.6715999841690063
2025-08-04 04:00:20,699 - father_agent.py:386 - Step: 395, Training loss: 0.761167585849762
2025-08-04 04:00:22,391 - father_agent.py:386 - Step: 400, Training loss: 0.8546303510665894
2025-08-04 04:00:22,595 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:00:22,598 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:00:30,902 - evaluation_results_class.py:131 - Average Return = -26.153162002563477
2025-08-04 04:00:30,902 - evaluation_results_class.py:133 - Average Virtual Goal Value = 53.482887268066406
2025-08-04 04:00:30,902 - evaluation_results_class.py:135 - Average Discounted Reward = 30.727699279785156
2025-08-04 04:00:30,902 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9954506065857885
2025-08-04 04:00:30,902 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:00:30,902 - evaluation_results_class.py:141 - Variance of Return = 1108.7037353515625
2025-08-04 04:00:30,902 - evaluation_results_class.py:143 - Current Best Return = -23.299257278442383
2025-08-04 04:00:30,902 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:00:30,902 - evaluation_results_class.py:147 - Average Episode Length = 52.72140381282496
2025-08-04 04:00:30,902 - evaluation_results_class.py:149 - Counted Episodes = 4616
2025-08-04 04:00:31,097 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:00:31,107 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:00:34,261 - father_agent.py:386 - Step: 405, Training loss: 1.2395069599151611
2025-08-04 04:00:35,978 - father_agent.py:386 - Step: 410, Training loss: 1.1354769468307495
2025-08-04 04:00:37,698 - father_agent.py:386 - Step: 415, Training loss: 0.6395463943481445
2025-08-04 04:00:39,406 - father_agent.py:386 - Step: 420, Training loss: 0.9393300414085388
2025-08-04 04:00:41,101 - father_agent.py:386 - Step: 425, Training loss: 1.1783838272094727
2025-08-04 04:00:42,798 - father_agent.py:386 - Step: 430, Training loss: 1.1829382181167603
2025-08-04 04:00:44,497 - father_agent.py:386 - Step: 435, Training loss: 1.102807879447937
2025-08-04 04:00:46,202 - father_agent.py:386 - Step: 440, Training loss: 1.2554253339767456
2025-08-04 04:00:47,893 - father_agent.py:386 - Step: 445, Training loss: 1.0890918970108032
2025-08-04 04:00:49,595 - father_agent.py:386 - Step: 450, Training loss: 1.088565468788147
2025-08-04 04:00:51,290 - father_agent.py:386 - Step: 455, Training loss: 0.6459505558013916
2025-08-04 04:00:52,990 - father_agent.py:386 - Step: 460, Training loss: 0.9613896608352661
2025-08-04 04:00:54,674 - father_agent.py:386 - Step: 465, Training loss: 0.389210969209671
2025-08-04 04:00:56,368 - father_agent.py:386 - Step: 470, Training loss: 0.7300471067428589
2025-08-04 04:00:58,071 - father_agent.py:386 - Step: 475, Training loss: 0.869290292263031
2025-08-04 04:00:59,783 - father_agent.py:386 - Step: 480, Training loss: 1.1591992378234863
2025-08-04 04:01:01,493 - father_agent.py:386 - Step: 485, Training loss: 1.4057284593582153
2025-08-04 04:01:03,242 - father_agent.py:386 - Step: 490, Training loss: 1.1392015218734741
2025-08-04 04:01:04,995 - father_agent.py:386 - Step: 495, Training loss: 0.825799286365509
2025-08-04 04:01:06,705 - father_agent.py:386 - Step: 500, Training loss: 0.9348001480102539
2025-08-04 04:01:06,908 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:01:06,911 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:01:15,338 - evaluation_results_class.py:131 - Average Return = -23.48757553100586
2025-08-04 04:01:15,338 - evaluation_results_class.py:133 - Average Virtual Goal Value = 56.51242446899414
2025-08-04 04:01:15,338 - evaluation_results_class.py:135 - Average Discounted Reward = 31.73406982421875
2025-08-04 04:01:15,338 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:01:15,338 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:01:15,338 - evaluation_results_class.py:141 - Variance of Return = 717.0634155273438
2025-08-04 04:01:15,338 - evaluation_results_class.py:143 - Current Best Return = -23.299257278442383
2025-08-04 04:01:15,338 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:01:15,338 - evaluation_results_class.py:147 - Average Episode Length = 46.63522103438313
2025-08-04 04:01:15,339 - evaluation_results_class.py:149 - Counted Episodes = 6922
2025-08-04 04:01:15,535 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:01:15,545 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:01:15,656 - father_agent.py:547 - Training finished.
2025-08-04 04:01:15,805 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:01:15,808 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:01:15,811 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 04:01:24,350 - evaluation_results_class.py:131 - Average Return = -23.18852424621582
2025-08-04 04:01:24,350 - evaluation_results_class.py:133 - Average Virtual Goal Value = 56.81147766113281
2025-08-04 04:01:24,350 - evaluation_results_class.py:135 - Average Discounted Reward = 32.05842590332031
2025-08-04 04:01:24,350 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:01:24,350 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:01:24,350 - evaluation_results_class.py:141 - Variance of Return = 714.8699340820312
2025-08-04 04:01:24,351 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:01:24,351 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:01:24,351 - evaluation_results_class.py:147 - Average Episode Length = 46.32067816258513
2025-08-04 04:01:24,351 - evaluation_results_class.py:149 - Counted Episodes = 6901
2025-08-04 04:01:24,547 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:01:24,549 - self_interpretable_extractor.py:286 - True
2025-08-04 04:01:24,560 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:01:39,092 - evaluation_results_class.py:131 - Average Return = -23.850080490112305
2025-08-04 04:01:39,092 - evaluation_results_class.py:133 - Average Virtual Goal Value = 56.09175491333008
2025-08-04 04:01:39,092 - evaluation_results_class.py:135 - Average Discounted Reward = 31.38721466064453
2025-08-04 04:01:39,092 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9992729387814454
2025-08-04 04:01:39,092 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:01:39,092 - evaluation_results_class.py:141 - Variance of Return = 787.2034912109375
2025-08-04 04:01:39,092 - evaluation_results_class.py:143 - Current Best Return = -23.850080490112305
2025-08-04 04:01:39,092 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9992729387814454
2025-08-04 04:01:39,092 - evaluation_results_class.py:147 - Average Episode Length = 47.4368183801076
2025-08-04 04:01:39,092 - evaluation_results_class.py:149 - Counted Episodes = 6877
2025-08-04 04:01:39,093 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 04:01:39,093 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 31530 trajectories
Learned trajectory lengths  {128, 641, 386, 257, 389, 137, 266, 651, 395, 272, 530, 149, 278, 536, 281, 29, 158, 287, 32, 35, 164, 38, 41, 425, 44, 173, 47, 176, 431, 50, 434, 308, 53, 311, 56, 314, 59, 188, 62, 65, 578, 68, 581, 197, 71, 200, 74, 458, 590, 335, 464, 209, 80, 467, 83, 338, 86, 212, 89, 92, 95, 608, 353, 482, 101, 491, 107, 116, 377, 506, 125, 383}
Buffer 1
2025-08-04 04:02:41,668 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 62874 trajectories
Learned trajectory lengths  {530, 533, 536, 26, 29, 32, 35, 38, 41, 554, 44, 557, 47, 50, 53, 56, 59, 62, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 83, 596, 86, 89, 92, 95, 608, 101, 614, 107, 110, 116, 119, 632, 635, 125, 128, 641, 131, 137, 650, 651, 149, 158, 164, 167, 170, 173, 176, 188, 191, 197, 200, 206, 209, 212, 218, 221, 227, 233, 242, 251, 257, 266, 269, 272, 275, 278, 281, 287, 293, 302, 308, 311, 314, 317, 326, 335, 338, 341, 353, 365, 368, 377, 383, 386, 389, 395, 425, 428, 431, 434, 443, 455, 458, 464, 467, 479, 482, 491, 494, 506, 509}
Buffer 2
2025-08-04 04:03:44,472 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 94305 trajectories
Learned trajectory lengths  {512, 518, 524, 530, 533, 536, 26, 29, 32, 35, 38, 41, 554, 44, 557, 47, 560, 50, 563, 53, 56, 569, 59, 62, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 83, 596, 86, 89, 92, 605, 95, 608, 101, 614, 104, 107, 620, 110, 116, 119, 632, 122, 635, 125, 128, 641, 131, 134, 137, 650, 651, 146, 149, 158, 164, 167, 170, 173, 176, 188, 191, 197, 200, 206, 209, 212, 218, 221, 227, 233, 239, 242, 245, 248, 251, 254, 257, 266, 269, 272, 275, 278, 281, 287, 293, 302, 308, 311, 314, 317, 326, 335, 338, 341, 353, 365, 368, 371, 377, 380, 383, 386, 389, 392, 395, 410, 425, 428, 431, 434, 443, 446, 449, 452, 455, 458, 461, 464, 467, 479, 482, 488, 491, 494, 506, 509}
All trajectories collected
2025-08-04 04:04:47,073 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 04:04:47,074 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 94305 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_2.dot.
Learned FSC of size 2
2025-08-04 04:04:49,402 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 04:05:03,145 - evaluation_results_class.py:131 - Average Return = -83.43518829345703
2025-08-04 04:05:03,145 - evaluation_results_class.py:133 - Average Virtual Goal Value = -3.4351866245269775
2025-08-04 04:05:03,145 - evaluation_results_class.py:135 - Average Discounted Reward = -18.15460777282715
2025-08-04 04:05:03,145 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:05:03,145 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:05:03,145 - evaluation_results_class.py:141 - Variance of Return = 4117.5830078125
2025-08-04 04:05:03,145 - evaluation_results_class.py:143 - Current Best Return = -83.43518829345703
2025-08-04 04:05:03,145 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:05:03,146 - evaluation_results_class.py:147 - Average Episode Length = 50.78364565587734
2025-08-04 04:05:03,146 - evaluation_results_class.py:149 - Counted Episodes = 6457
FSC Result: {'best_episode_return': -3.4351866, 'best_return': -83.43519, 'goal_value': 0.0, 'returns_episodic': [-3.4351866], 'returns': [-83.43519], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [4117.583], 'each_episode_virtual_variance': [4117.583], 'combined_variance': [16470.332], 'num_episodes': [6457], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [50.78364565587734], 'counted_episodes': [6457], 'discounted_rewards': [-18.154608], 'new_pomdp_iteration_numbers': []}
[[5], [3, 4, 6, 7], [3, 4, 6, 7], [1], [1], [1], [1], [2], [2], [2], [2], [3, 4, 6, 7], [3, 4, 6, 7], [0]]
2025-08-04 04:05:03,255 - statistic.py:67 - synthesis initiated, design space: 81
2025-08-04 04:05:03,255 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 04:05:03,271 - synthesizer_ar.py:122 - value 70.5217 achieved after 1722.53 seconds
2025-08-04 04:05:03,292 - synthesizer_ar.py:122 - value 73.8971 achieved after 1722.55 seconds
2025-08-04 04:05:03,299 - synthesizer_ar.py:122 - value 74.209 achieved after 1722.56 seconds
2025-08-04 04:05:03,315 - synthesizer_ar.py:122 - value 74.5745 achieved after 1722.57 seconds
2025-08-04 04:05:03,321 - synthesizer_ar.py:122 - value 80.5753 achieved after 1722.58 seconds
2025-08-04 04:05:03,383 - synthesizer_ar.py:122 - value 81.6129 achieved after 1722.64 seconds
2025-08-04 04:05:03,390 - synthesizer_ar.py:122 - value 86.7879 achieved after 1722.65 seconds
2025-08-04 04:05:03,421 - synthesizer_ar.py:122 - value 88.314 achieved after 1722.68 seconds
2025-08-04 04:05:03,442 - synthesizer_ar.py:122 - value 89.7042 achieved after 1722.7 seconds
2025-08-04 04:05:03,449 - synthesizer_ar.py:122 - value 94.1527 achieved after 1722.71 seconds
2025-08-04 04:05:03,456 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 04:05:03,456 - synthesizer.py:193 - o1x=3, o1y=5, o2x=6, o2y=7
2025-08-04 04:05:03,457 - synthesizer.py:198 - double-checking specification satisfiability:  : 94.15272224787209
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 10) & (y = 10))] 

method: AR, synthesis time: 0.2 s
number of holes: 4, family size: 81, quotient: 1063 states / 1215 actions
explored: 100 %
MDP stats: avg MDP size: 928, iterations: 115

optimum: 94.152722
--------------------
2025-08-04 04:05:03,458 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
2025-08-04 04:05:03,460 - robust_rl_trainer.py:432 - Iteration 4 of pure RL loop
2025-08-04 04:05:03,498 - storm_vec_env.py:70 - Computing row map
2025-08-04 04:05:03,504 - storm_vec_env.py:97 - Computing transitions
2025-08-04 04:05:03,517 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 04:05:03,517 - storm_vec_env.py:114 - Computing sinks
2025-08-04 04:05:03,517 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 04:05:03,520 - storm_vec_env.py:143 - Computing labels
2025-08-04 04:05:03,520 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 04:05:03,521 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 04:05:03,521 - storm_vec_env.py:175 - Computing observations
2025-08-04 04:05:03,647 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 04:05:03,648 - father_agent.py:540 - Before training evaluation.
2025-08-04 04:05:03,796 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:05:03,799 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:05:12,678 - evaluation_results_class.py:131 - Average Return = -23.667293548583984
2025-08-04 04:05:12,678 - evaluation_results_class.py:133 - Average Virtual Goal Value = 56.332706451416016
2025-08-04 04:05:12,678 - evaluation_results_class.py:135 - Average Discounted Reward = 31.443340301513672
2025-08-04 04:05:12,678 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:05:12,678 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:05:12,678 - evaluation_results_class.py:141 - Variance of Return = 726.0284423828125
2025-08-04 04:05:12,678 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:05:12,678 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:05:12,678 - evaluation_results_class.py:147 - Average Episode Length = 46.406914508896286
2025-08-04 04:05:12,678 - evaluation_results_class.py:149 - Counted Episodes = 6913
2025-08-04 04:05:12,872 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:05:12,882 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:05:12,990 - father_agent.py:436 - Training agent on-policy
2025-08-04 04:05:20,322 - father_agent.py:386 - Step: 0, Training loss: 0.6465719938278198
2025-08-04 04:05:22,151 - father_agent.py:386 - Step: 5, Training loss: 1.120727300643921
2025-08-04 04:05:24,014 - father_agent.py:386 - Step: 10, Training loss: 0.8180440664291382
2025-08-04 04:05:25,877 - father_agent.py:386 - Step: 15, Training loss: 0.7036291360855103
2025-08-04 04:05:27,729 - father_agent.py:386 - Step: 20, Training loss: 0.559269368648529
2025-08-04 04:05:29,582 - father_agent.py:386 - Step: 25, Training loss: 0.4500480890274048
2025-08-04 04:05:31,424 - father_agent.py:386 - Step: 30, Training loss: 1.3255894184112549
2025-08-04 04:05:33,270 - father_agent.py:386 - Step: 35, Training loss: 0.960817813873291
2025-08-04 04:05:35,126 - father_agent.py:386 - Step: 40, Training loss: 0.5653374791145325
2025-08-04 04:05:36,970 - father_agent.py:386 - Step: 45, Training loss: 0.47757744789123535
2025-08-04 04:05:38,862 - father_agent.py:386 - Step: 50, Training loss: 0.7281105518341064
2025-08-04 04:05:40,721 - father_agent.py:386 - Step: 55, Training loss: 0.5671036243438721
2025-08-04 04:05:42,598 - father_agent.py:386 - Step: 60, Training loss: 0.6859616637229919
2025-08-04 04:05:44,439 - father_agent.py:386 - Step: 65, Training loss: 1.1464649438858032
2025-08-04 04:05:46,245 - father_agent.py:386 - Step: 70, Training loss: 0.9136024117469788
2025-08-04 04:05:48,071 - father_agent.py:386 - Step: 75, Training loss: 0.8283168077468872
2025-08-04 04:05:49,919 - father_agent.py:386 - Step: 80, Training loss: 1.1652339696884155
2025-08-04 04:05:51,747 - father_agent.py:386 - Step: 85, Training loss: 1.0558949708938599
2025-08-04 04:05:53,578 - father_agent.py:386 - Step: 90, Training loss: 1.088740587234497
2025-08-04 04:05:55,332 - father_agent.py:386 - Step: 95, Training loss: 1.2382893562316895
2025-08-04 04:05:57,093 - father_agent.py:386 - Step: 100, Training loss: 0.47396886348724365
2025-08-04 04:05:57,302 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:05:57,305 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:06:06,261 - evaluation_results_class.py:131 - Average Return = -23.95132064819336
2025-08-04 04:06:06,261 - evaluation_results_class.py:133 - Average Virtual Goal Value = 56.04867935180664
2025-08-04 04:06:06,261 - evaluation_results_class.py:135 - Average Discounted Reward = 31.526233673095703
2025-08-04 04:06:06,261 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:06:06,261 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:06:06,261 - evaluation_results_class.py:141 - Variance of Return = 761.75390625
2025-08-04 04:06:06,261 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:06:06,261 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:06:06,261 - evaluation_results_class.py:147 - Average Episode Length = 46.05327398047099
2025-08-04 04:06:06,261 - evaluation_results_class.py:149 - Counted Episodes = 6964
2025-08-04 04:06:06,467 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:06:06,477 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:06:12,997 - father_agent.py:386 - Step: 105, Training loss: 0.7536150217056274
2025-08-04 04:06:14,784 - father_agent.py:386 - Step: 110, Training loss: 0.5341546535491943
2025-08-04 04:06:16,541 - father_agent.py:386 - Step: 115, Training loss: 1.0933672189712524
2025-08-04 04:06:18,288 - father_agent.py:386 - Step: 120, Training loss: 0.7201458215713501
2025-08-04 04:06:20,065 - father_agent.py:386 - Step: 125, Training loss: 0.5634926557540894
2025-08-04 04:06:21,839 - father_agent.py:386 - Step: 130, Training loss: 0.8219398856163025
2025-08-04 04:06:23,618 - father_agent.py:386 - Step: 135, Training loss: 1.2362189292907715
2025-08-04 04:06:25,382 - father_agent.py:386 - Step: 140, Training loss: 1.4508157968521118
2025-08-04 04:06:27,137 - father_agent.py:386 - Step: 145, Training loss: 0.8161950707435608
2025-08-04 04:06:28,912 - father_agent.py:386 - Step: 150, Training loss: 0.8143762946128845
2025-08-04 04:06:30,668 - father_agent.py:386 - Step: 155, Training loss: 1.1878743171691895
2025-08-04 04:06:32,420 - father_agent.py:386 - Step: 160, Training loss: 0.7141710519790649
2025-08-04 04:06:34,186 - father_agent.py:386 - Step: 165, Training loss: 0.6915185451507568
2025-08-04 04:06:35,963 - father_agent.py:386 - Step: 170, Training loss: 1.0188506841659546
2025-08-04 04:06:37,738 - father_agent.py:386 - Step: 175, Training loss: 0.5822637677192688
2025-08-04 04:06:39,583 - father_agent.py:386 - Step: 180, Training loss: 0.8167675137519836
2025-08-04 04:06:41,448 - father_agent.py:386 - Step: 185, Training loss: 1.0817453861236572
2025-08-04 04:06:43,308 - father_agent.py:386 - Step: 190, Training loss: 0.7198036909103394
2025-08-04 04:06:45,163 - father_agent.py:386 - Step: 195, Training loss: 0.5788917541503906
2025-08-04 04:06:46,979 - father_agent.py:386 - Step: 200, Training loss: 1.2996922731399536
2025-08-04 04:06:47,198 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:06:47,201 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:06:56,663 - evaluation_results_class.py:131 - Average Return = -26.158281326293945
2025-08-04 04:06:56,663 - evaluation_results_class.py:133 - Average Virtual Goal Value = 53.52928924560547
2025-08-04 04:06:56,663 - evaluation_results_class.py:135 - Average Discounted Reward = 30.473186492919922
2025-08-04 04:06:56,663 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9960946473696302
2025-08-04 04:06:56,663 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:06:56,663 - evaluation_results_class.py:141 - Variance of Return = 1106.8780517578125
2025-08-04 04:06:56,663 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:06:56,663 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:06:56,663 - evaluation_results_class.py:147 - Average Episode Length = 52.964162646450724
2025-08-04 04:06:56,663 - evaluation_results_class.py:149 - Counted Episodes = 4353
2025-08-04 04:06:56,869 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:06:56,879 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:07:00,372 - father_agent.py:386 - Step: 205, Training loss: 0.714532196521759
2025-08-04 04:07:02,133 - father_agent.py:386 - Step: 210, Training loss: 1.1473084688186646
2025-08-04 04:07:03,882 - father_agent.py:386 - Step: 215, Training loss: 0.8071632385253906
2025-08-04 04:07:05,636 - father_agent.py:386 - Step: 220, Training loss: 0.6427500247955322
2025-08-04 04:07:07,398 - father_agent.py:386 - Step: 225, Training loss: 1.1992504596710205
2025-08-04 04:07:09,172 - father_agent.py:386 - Step: 230, Training loss: 0.7205471992492676
2025-08-04 04:07:10,927 - father_agent.py:386 - Step: 235, Training loss: 0.7938882112503052
2025-08-04 04:07:12,687 - father_agent.py:386 - Step: 240, Training loss: 1.0574240684509277
2025-08-04 04:07:14,452 - father_agent.py:386 - Step: 245, Training loss: 0.4819030165672302
2025-08-04 04:07:16,215 - father_agent.py:386 - Step: 250, Training loss: 0.7174651026725769
2025-08-04 04:07:17,974 - father_agent.py:386 - Step: 255, Training loss: 1.2867118120193481
2025-08-04 04:07:19,755 - father_agent.py:386 - Step: 260, Training loss: 1.0590895414352417
2025-08-04 04:07:21,493 - father_agent.py:386 - Step: 265, Training loss: 0.6275410056114197
2025-08-04 04:07:23,255 - father_agent.py:386 - Step: 270, Training loss: 0.9928041100502014
2025-08-04 04:07:25,042 - father_agent.py:386 - Step: 275, Training loss: 0.6903204321861267
2025-08-04 04:07:26,831 - father_agent.py:386 - Step: 280, Training loss: 1.17044997215271
2025-08-04 04:07:28,617 - father_agent.py:386 - Step: 285, Training loss: 0.6923427581787109
2025-08-04 04:07:30,369 - father_agent.py:386 - Step: 290, Training loss: 0.9465541243553162
2025-08-04 04:07:32,125 - father_agent.py:386 - Step: 295, Training loss: 0.5070623159408569
2025-08-04 04:07:33,867 - father_agent.py:386 - Step: 300, Training loss: 0.7689145803451538
2025-08-04 04:07:34,085 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:07:34,087 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:07:42,990 - evaluation_results_class.py:131 - Average Return = -32.27606201171875
2025-08-04 04:07:42,991 - evaluation_results_class.py:133 - Average Virtual Goal Value = 47.18811798095703
2025-08-04 04:07:42,991 - evaluation_results_class.py:135 - Average Discounted Reward = 25.772146224975586
2025-08-04 04:07:42,991 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9933022714036109
2025-08-04 04:07:42,991 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:07:42,991 - evaluation_results_class.py:141 - Variance of Return = 1674.203857421875
2025-08-04 04:07:42,991 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:07:42,991 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:07:42,991 - evaluation_results_class.py:147 - Average Episode Length = 68.86575422248107
2025-08-04 04:07:42,991 - evaluation_results_class.py:149 - Counted Episodes = 3434
2025-08-04 04:07:43,203 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:07:43,214 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:07:46,689 - father_agent.py:386 - Step: 305, Training loss: 0.8644077777862549
2025-08-04 04:07:48,440 - father_agent.py:386 - Step: 310, Training loss: 0.4983742833137512
2025-08-04 04:07:50,195 - father_agent.py:386 - Step: 315, Training loss: 0.7794856429100037
2025-08-04 04:07:51,946 - father_agent.py:386 - Step: 320, Training loss: 0.7326577305793762
2025-08-04 04:07:53,709 - father_agent.py:386 - Step: 325, Training loss: 1.008626937866211
2025-08-04 04:07:55,484 - father_agent.py:386 - Step: 330, Training loss: 0.8219413161277771
2025-08-04 04:07:57,262 - father_agent.py:386 - Step: 335, Training loss: 1.017004132270813
2025-08-04 04:07:59,037 - father_agent.py:386 - Step: 340, Training loss: 0.8817166686058044
2025-08-04 04:08:00,796 - father_agent.py:386 - Step: 345, Training loss: 0.5239824056625366
2025-08-04 04:08:02,575 - father_agent.py:386 - Step: 350, Training loss: 1.0225470066070557
2025-08-04 04:08:04,340 - father_agent.py:386 - Step: 355, Training loss: 0.9047946929931641
2025-08-04 04:08:06,120 - father_agent.py:386 - Step: 360, Training loss: 0.6764410734176636
2025-08-04 04:08:07,879 - father_agent.py:386 - Step: 365, Training loss: 0.4369845390319824
2025-08-04 04:08:09,644 - father_agent.py:386 - Step: 370, Training loss: 1.0076727867126465
2025-08-04 04:08:11,394 - father_agent.py:386 - Step: 375, Training loss: 0.5021458864212036
2025-08-04 04:08:13,156 - father_agent.py:386 - Step: 380, Training loss: 0.916817307472229
2025-08-04 04:08:14,924 - father_agent.py:386 - Step: 385, Training loss: 1.0800046920776367
2025-08-04 04:08:16,686 - father_agent.py:386 - Step: 390, Training loss: 0.5554584264755249
2025-08-04 04:08:18,443 - father_agent.py:386 - Step: 395, Training loss: 0.486506849527359
2025-08-04 04:08:20,219 - father_agent.py:386 - Step: 400, Training loss: 1.2844289541244507
2025-08-04 04:08:20,445 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:08:20,448 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:08:29,278 - evaluation_results_class.py:131 - Average Return = -57.35984802246094
2025-08-04 04:08:29,278 - evaluation_results_class.py:133 - Average Virtual Goal Value = 15.306818008422852
2025-08-04 04:08:29,279 - evaluation_results_class.py:135 - Average Discounted Reward = 15.718717575073242
2025-08-04 04:08:29,279 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9083333333333333
2025-08-04 04:08:29,279 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:08:29,279 - evaluation_results_class.py:141 - Variance of Return = 5431.3349609375
2025-08-04 04:08:29,279 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:08:29,279 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:08:29,279 - evaluation_results_class.py:147 - Average Episode Length = 144.19242424242424
2025-08-04 04:08:29,279 - evaluation_results_class.py:149 - Counted Episodes = 1320
2025-08-04 04:08:29,496 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:08:29,507 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:08:32,969 - father_agent.py:386 - Step: 405, Training loss: 1.1243840456008911
2025-08-04 04:08:34,741 - father_agent.py:386 - Step: 410, Training loss: 0.6128526926040649
2025-08-04 04:08:36,502 - father_agent.py:386 - Step: 415, Training loss: 0.89003986120224
2025-08-04 04:08:38,265 - father_agent.py:386 - Step: 420, Training loss: 0.46624186635017395
2025-08-04 04:08:40,037 - father_agent.py:386 - Step: 425, Training loss: 0.9054962396621704
2025-08-04 04:08:41,803 - father_agent.py:386 - Step: 430, Training loss: 0.46546024084091187
2025-08-04 04:08:43,564 - father_agent.py:386 - Step: 435, Training loss: 1.0547610521316528
2025-08-04 04:08:45,323 - father_agent.py:386 - Step: 440, Training loss: 1.2526638507843018
2025-08-04 04:08:47,080 - father_agent.py:386 - Step: 445, Training loss: 0.6718364953994751
2025-08-04 04:08:48,839 - father_agent.py:386 - Step: 450, Training loss: 0.8717169165611267
2025-08-04 04:08:50,601 - father_agent.py:386 - Step: 455, Training loss: 0.6334988474845886
2025-08-04 04:08:52,340 - father_agent.py:386 - Step: 460, Training loss: 1.0838364362716675
2025-08-04 04:08:54,090 - father_agent.py:386 - Step: 465, Training loss: 0.6588353514671326
2025-08-04 04:08:55,852 - father_agent.py:386 - Step: 470, Training loss: 0.8632464408874512
2025-08-04 04:08:57,606 - father_agent.py:386 - Step: 475, Training loss: 0.4888131320476532
2025-08-04 04:08:59,367 - father_agent.py:386 - Step: 480, Training loss: 0.5878636837005615
2025-08-04 04:09:01,117 - father_agent.py:386 - Step: 485, Training loss: 1.388835072517395
2025-08-04 04:09:02,876 - father_agent.py:386 - Step: 490, Training loss: 0.3496951460838318
2025-08-04 04:09:04,606 - father_agent.py:386 - Step: 495, Training loss: 0.7568030953407288
2025-08-04 04:09:06,350 - father_agent.py:386 - Step: 500, Training loss: 0.8286377191543579
2025-08-04 04:09:06,581 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:09:06,584 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:09:15,858 - evaluation_results_class.py:131 - Average Return = -25.210142135620117
2025-08-04 04:09:15,859 - evaluation_results_class.py:133 - Average Virtual Goal Value = 54.643924713134766
2025-08-04 04:09:15,859 - evaluation_results_class.py:135 - Average Discounted Reward = 31.05341339111328
2025-08-04 04:09:15,859 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9981758482305728
2025-08-04 04:09:15,859 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:09:15,859 - evaluation_results_class.py:141 - Variance of Return = 961.2174682617188
2025-08-04 04:09:15,859 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:09:15,859 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:09:15,859 - evaluation_results_class.py:147 - Average Episode Length = 49.639182780007296
2025-08-04 04:09:15,859 - evaluation_results_class.py:149 - Counted Episodes = 5482
2025-08-04 04:09:16,071 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:09:16,082 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:09:16,196 - father_agent.py:547 - Training finished.
2025-08-04 04:09:16,347 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:09:16,350 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:09:16,352 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 04:09:25,221 - evaluation_results_class.py:131 - Average Return = -24.983877182006836
2025-08-04 04:09:25,222 - evaluation_results_class.py:133 - Average Virtual Goal Value = 54.83821487426758
2025-08-04 04:09:25,222 - evaluation_results_class.py:135 - Average Discounted Reward = 31.229721069335938
2025-08-04 04:09:25,222 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9977761304670126
2025-08-04 04:09:25,222 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:09:25,222 - evaluation_results_class.py:141 - Variance of Return = 933.421630859375
2025-08-04 04:09:25,222 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:09:25,222 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:09:25,222 - evaluation_results_class.py:147 - Average Episode Length = 50.06356560415122
2025-08-04 04:09:25,222 - evaluation_results_class.py:149 - Counted Episodes = 5396
2025-08-04 04:09:25,432 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:09:25,434 - self_interpretable_extractor.py:286 - True
2025-08-04 04:09:25,446 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:09:40,817 - evaluation_results_class.py:131 - Average Return = -28.172704696655273
2025-08-04 04:09:40,817 - evaluation_results_class.py:133 - Average Virtual Goal Value = 50.63677215576172
2025-08-04 04:09:40,817 - evaluation_results_class.py:135 - Average Discounted Reward = 29.990114212036133
2025-08-04 04:09:40,817 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9851184648521637
2025-08-04 04:09:40,817 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:09:40,818 - evaluation_results_class.py:141 - Variance of Return = 1553.860595703125
2025-08-04 04:09:40,818 - evaluation_results_class.py:143 - Current Best Return = -28.172704696655273
2025-08-04 04:09:40,818 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9851184648521637
2025-08-04 04:09:40,818 - evaluation_results_class.py:147 - Average Episode Length = 59.57450558057568
2025-08-04 04:09:40,818 - evaluation_results_class.py:149 - Counted Episodes = 5107
2025-08-04 04:09:40,818 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 04:09:40,818 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 22228 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 65, 578, 68, 581, 584, 74, 587, 77, 590, 80, 593, 83, 86, 599, 89, 602, 92, 605, 95, 98, 101, 614, 104, 617, 107, 110, 113, 626, 116, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 350, 353, 356, 359, 362, 365, 371, 374, 377, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 428, 431, 434, 437, 440, 443, 446, 449, 452, 458, 461, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
Buffer 1
2025-08-04 04:10:45,559 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 44798 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
Buffer 2
2025-08-04 04:11:50,862 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 67792 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
All trajectories collected
2025-08-04 04:12:55,684 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 04:12:55,684 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 67792 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_3.dot.
Learned FSC of size 2
2025-08-04 04:12:58,101 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 04:13:12,405 - evaluation_results_class.py:131 - Average Return = -87.16053009033203
2025-08-04 04:13:12,405 - evaluation_results_class.py:133 - Average Virtual Goal Value = -7.160531520843506
2025-08-04 04:13:12,405 - evaluation_results_class.py:135 - Average Discounted Reward = -25.396015167236328
2025-08-04 04:13:12,405 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:13:12,405 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:13:12,405 - evaluation_results_class.py:141 - Variance of Return = 3728.7158203125
2025-08-04 04:13:12,405 - evaluation_results_class.py:143 - Current Best Return = -87.16053009033203
2025-08-04 04:13:12,405 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:13:12,405 - evaluation_results_class.py:147 - Average Episode Length = 61.81821631878558
2025-08-04 04:13:12,405 - evaluation_results_class.py:149 - Counted Episodes = 5270
FSC Result: {'best_episode_return': -7.1605315, 'best_return': -87.16053, 'goal_value': 0.0, 'returns_episodic': [-7.1605315], 'returns': [-87.16053], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [3728.7158], 'each_episode_virtual_variance': [3728.7158], 'combined_variance': [14914.863], 'num_episodes': [5270], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [61.81821631878558], 'counted_episodes': [5270], 'discounted_rewards': [-25.396015], 'new_pomdp_iteration_numbers': []}
[[5], [3, 4, 6, 7], [3, 4, 6, 7], [1], [1], [1], [1], [2], [2], [2], [2], [3, 4, 6, 7], [3, 4, 6, 7], [0]]
2025-08-04 04:13:12,504 - statistic.py:67 - synthesis initiated, design space: 81
2025-08-04 04:13:12,505 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 04:13:12,521 - synthesizer_ar.py:122 - value 72.3182 achieved after 2211.78 seconds
2025-08-04 04:13:12,528 - synthesizer_ar.py:122 - value 78.8636 achieved after 2211.78 seconds
2025-08-04 04:13:12,549 - synthesizer_ar.py:122 - value 79.5758 achieved after 2211.81 seconds
2025-08-04 04:13:12,556 - synthesizer_ar.py:122 - value 83.3718 achieved after 2211.81 seconds
2025-08-04 04:13:12,573 - synthesizer_ar.py:122 - value 88.7873 achieved after 2211.83 seconds
2025-08-04 04:13:12,641 - synthesizer_ar.py:122 - value 93.1171 achieved after 2211.9 seconds
2025-08-04 04:13:12,683 - synthesizer_ar.py:122 - value 93.1658 achieved after 2211.94 seconds
2025-08-04 04:13:12,690 - synthesizer_ar.py:122 - value 94.6626 achieved after 2211.95 seconds
2025-08-04 04:13:12,711 - synthesizer_ar.py:122 - value 99.0814 achieved after 2211.97 seconds
2025-08-04 04:13:12,713 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 04:13:12,713 - synthesizer.py:193 - o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 04:13:12,714 - synthesizer.py:198 - double-checking specification satisfiability:  : 99.08138628252831
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 10) & (y = 10))] 

method: AR, synthesis time: 0.21 s
number of holes: 4, family size: 81, quotient: 1063 states / 1215 actions
explored: 100 %
MDP stats: avg MDP size: 929, iterations: 118

optimum: 99.081386
--------------------
2025-08-04 04:13:12,714 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 04:13:12,717 - robust_rl_trainer.py:432 - Iteration 5 of pure RL loop
2025-08-04 04:13:12,755 - storm_vec_env.py:70 - Computing row map
2025-08-04 04:13:12,761 - storm_vec_env.py:97 - Computing transitions
2025-08-04 04:13:12,774 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 04:13:12,774 - storm_vec_env.py:114 - Computing sinks
2025-08-04 04:13:12,774 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 04:13:12,777 - storm_vec_env.py:143 - Computing labels
2025-08-04 04:13:12,778 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 04:13:12,778 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 04:13:12,778 - storm_vec_env.py:175 - Computing observations
2025-08-04 04:13:12,921 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 04:13:12,921 - father_agent.py:540 - Before training evaluation.
2025-08-04 04:13:13,072 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:13:13,075 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:13:23,256 - evaluation_results_class.py:131 - Average Return = -24.360546112060547
2025-08-04 04:13:23,256 - evaluation_results_class.py:133 - Average Virtual Goal Value = 55.53601837158203
2025-08-04 04:13:23,256 - evaluation_results_class.py:135 - Average Discounted Reward = 31.586915969848633
2025-08-04 04:13:23,256 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9987070557813077
2025-08-04 04:13:23,256 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:13:23,256 - evaluation_results_class.py:141 - Variance of Return = 894.434814453125
2025-08-04 04:13:23,256 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:13:23,257 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:13:23,257 - evaluation_results_class.py:147 - Average Episode Length = 48.82619135574436
2025-08-04 04:13:23,257 - evaluation_results_class.py:149 - Counted Episodes = 5414
2025-08-04 04:13:23,532 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:13:23,544 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:13:23,658 - father_agent.py:436 - Training agent on-policy
2025-08-04 04:13:31,341 - father_agent.py:386 - Step: 0, Training loss: 0.7005826830863953
2025-08-04 04:13:33,190 - father_agent.py:386 - Step: 5, Training loss: 0.5820774435997009
2025-08-04 04:13:35,029 - father_agent.py:386 - Step: 10, Training loss: 1.0505034923553467
2025-08-04 04:13:36,853 - father_agent.py:386 - Step: 15, Training loss: 0.318267822265625
2025-08-04 04:13:38,730 - father_agent.py:386 - Step: 20, Training loss: 0.7283228635787964
2025-08-04 04:13:40,690 - father_agent.py:386 - Step: 25, Training loss: 0.8791389465332031
2025-08-04 04:13:42,608 - father_agent.py:386 - Step: 30, Training loss: 0.7360301613807678
2025-08-04 04:13:44,511 - father_agent.py:386 - Step: 35, Training loss: 0.4678735136985779
2025-08-04 04:13:46,409 - father_agent.py:386 - Step: 40, Training loss: 0.8219345808029175
2025-08-04 04:13:48,351 - father_agent.py:386 - Step: 45, Training loss: 0.47149181365966797
2025-08-04 04:13:50,285 - father_agent.py:386 - Step: 50, Training loss: 0.8137354254722595
2025-08-04 04:13:52,233 - father_agent.py:386 - Step: 55, Training loss: 0.6014567613601685
2025-08-04 04:13:54,174 - father_agent.py:386 - Step: 60, Training loss: 1.0490881204605103
2025-08-04 04:13:56,106 - father_agent.py:386 - Step: 65, Training loss: 0.8493905067443848
2025-08-04 04:13:58,037 - father_agent.py:386 - Step: 70, Training loss: 0.6297627687454224
2025-08-04 04:13:59,970 - father_agent.py:386 - Step: 75, Training loss: 0.637319803237915
2025-08-04 04:14:01,917 - father_agent.py:386 - Step: 80, Training loss: 0.9415143728256226
2025-08-04 04:14:03,831 - father_agent.py:386 - Step: 85, Training loss: 0.6837412118911743
2025-08-04 04:14:05,797 - father_agent.py:386 - Step: 90, Training loss: 0.508483350276947
2025-08-04 04:14:07,736 - father_agent.py:386 - Step: 95, Training loss: 0.9208418726921082
2025-08-04 04:14:09,690 - father_agent.py:386 - Step: 100, Training loss: 0.6874780654907227
2025-08-04 04:14:09,925 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:14:09,928 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:14:19,704 - evaluation_results_class.py:131 - Average Return = -33.106712341308594
2025-08-04 04:14:19,704 - evaluation_results_class.py:133 - Average Virtual Goal Value = 45.70912170410156
2025-08-04 04:14:19,704 - evaluation_results_class.py:135 - Average Discounted Reward = 26.132057189941406
2025-08-04 04:14:19,704 - evaluation_results_class.py:137 - Goal Reach Probability = 0.985197934595525
2025-08-04 04:14:19,705 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:14:19,705 - evaluation_results_class.py:141 - Variance of Return = 1981.8702392578125
2025-08-04 04:14:19,705 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:14:19,705 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:14:19,705 - evaluation_results_class.py:147 - Average Episode Length = 75.02616179001721
2025-08-04 04:14:19,705 - evaluation_results_class.py:149 - Counted Episodes = 2905
2025-08-04 04:14:19,925 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:14:19,936 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:14:26,754 - father_agent.py:386 - Step: 105, Training loss: 1.5342859029769897
2025-08-04 04:14:28,578 - father_agent.py:386 - Step: 110, Training loss: 0.39249101281166077
2025-08-04 04:14:30,413 - father_agent.py:386 - Step: 115, Training loss: 0.5525894165039062
2025-08-04 04:14:32,258 - father_agent.py:386 - Step: 120, Training loss: 0.380148708820343
2025-08-04 04:14:34,096 - father_agent.py:386 - Step: 125, Training loss: 0.4317968785762787
2025-08-04 04:14:35,928 - father_agent.py:386 - Step: 130, Training loss: 0.14820392429828644
2025-08-04 04:14:37,770 - father_agent.py:386 - Step: 135, Training loss: 0.40517914295196533
2025-08-04 04:14:39,651 - father_agent.py:386 - Step: 140, Training loss: 0.6927687525749207
2025-08-04 04:14:41,546 - father_agent.py:386 - Step: 145, Training loss: 0.6225085258483887
2025-08-04 04:14:43,438 - father_agent.py:386 - Step: 150, Training loss: 0.821397602558136
2025-08-04 04:14:45,365 - father_agent.py:386 - Step: 155, Training loss: 0.641179621219635
2025-08-04 04:14:47,236 - father_agent.py:386 - Step: 160, Training loss: 0.818541407585144
2025-08-04 04:14:49,126 - father_agent.py:386 - Step: 165, Training loss: 0.722947895526886
2025-08-04 04:14:50,945 - father_agent.py:386 - Step: 170, Training loss: 0.9072346687316895
2025-08-04 04:14:52,775 - father_agent.py:386 - Step: 175, Training loss: 0.38904792070388794
2025-08-04 04:14:54,604 - father_agent.py:386 - Step: 180, Training loss: 0.41409027576446533
2025-08-04 04:14:56,439 - father_agent.py:386 - Step: 185, Training loss: 1.1321165561676025
2025-08-04 04:14:58,261 - father_agent.py:386 - Step: 190, Training loss: 0.8144036531448364
2025-08-04 04:15:00,093 - father_agent.py:386 - Step: 195, Training loss: 0.6481221914291382
2025-08-04 04:15:01,912 - father_agent.py:386 - Step: 200, Training loss: 1.1192846298217773
2025-08-04 04:15:02,143 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:15:02,145 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:15:11,474 - evaluation_results_class.py:131 - Average Return = -27.08980941772461
2025-08-04 04:15:11,474 - evaluation_results_class.py:133 - Average Virtual Goal Value = 52.436466217041016
2025-08-04 04:15:11,474 - evaluation_results_class.py:135 - Average Discounted Reward = 29.816251754760742
2025-08-04 04:15:11,474 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9940784603997039
2025-08-04 04:15:11,474 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:15:11,474 - evaluation_results_class.py:141 - Variance of Return = 1245.0875244140625
2025-08-04 04:15:11,474 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:15:11,474 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:15:11,474 - evaluation_results_class.py:147 - Average Episode Length = 59.64569454724895
2025-08-04 04:15:11,474 - evaluation_results_class.py:149 - Counted Episodes = 4053
2025-08-04 04:15:11,700 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:15:11,711 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:15:15,830 - father_agent.py:386 - Step: 205, Training loss: 1.1663670539855957
2025-08-04 04:15:17,662 - father_agent.py:386 - Step: 210, Training loss: 0.33920276165008545
2025-08-04 04:15:19,464 - father_agent.py:386 - Step: 215, Training loss: 0.43686431646347046
2025-08-04 04:15:21,283 - father_agent.py:386 - Step: 220, Training loss: 0.5993301868438721
2025-08-04 04:15:23,080 - father_agent.py:386 - Step: 225, Training loss: 0.6813490390777588
2025-08-04 04:15:24,869 - father_agent.py:386 - Step: 230, Training loss: 0.2863868474960327
2025-08-04 04:15:26,684 - father_agent.py:386 - Step: 235, Training loss: 0.5367139577865601
2025-08-04 04:15:28,478 - father_agent.py:386 - Step: 240, Training loss: 0.7593909502029419
2025-08-04 04:15:30,284 - father_agent.py:386 - Step: 245, Training loss: 0.5656421184539795
2025-08-04 04:15:32,095 - father_agent.py:386 - Step: 250, Training loss: 0.8174137473106384
2025-08-04 04:15:33,928 - father_agent.py:386 - Step: 255, Training loss: 0.802059531211853
2025-08-04 04:15:35,743 - father_agent.py:386 - Step: 260, Training loss: 0.5632645487785339
2025-08-04 04:15:37,568 - father_agent.py:386 - Step: 265, Training loss: 0.8192456960678101
2025-08-04 04:15:39,401 - father_agent.py:386 - Step: 270, Training loss: 0.9047231078147888
2025-08-04 04:15:41,228 - father_agent.py:386 - Step: 275, Training loss: 0.5227960348129272
2025-08-04 04:15:43,044 - father_agent.py:386 - Step: 280, Training loss: 0.46343928575515747
2025-08-04 04:15:44,853 - father_agent.py:386 - Step: 285, Training loss: 1.0915629863739014
2025-08-04 04:15:46,655 - father_agent.py:386 - Step: 290, Training loss: 0.284322589635849
2025-08-04 04:15:48,461 - father_agent.py:386 - Step: 295, Training loss: 0.3909262418746948
2025-08-04 04:15:50,273 - father_agent.py:386 - Step: 300, Training loss: 0.6107856631278992
2025-08-04 04:15:50,508 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:15:50,511 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:15:59,943 - evaluation_results_class.py:131 - Average Return = -45.17332458496094
2025-08-04 04:15:59,943 - evaluation_results_class.py:133 - Average Virtual Goal Value = 32.9584846496582
2025-08-04 04:15:59,943 - evaluation_results_class.py:135 - Average Discounted Reward = 17.91579246520996
2025-08-04 04:15:59,943 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9766476388168137
2025-08-04 04:15:59,943 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:15:59,943 - evaluation_results_class.py:141 - Variance of Return = 3045.333740234375
2025-08-04 04:15:59,943 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:15:59,943 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:15:59,943 - evaluation_results_class.py:147 - Average Episode Length = 114.3502854177478
2025-08-04 04:15:59,943 - evaluation_results_class.py:149 - Counted Episodes = 1927
2025-08-04 04:16:00,180 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:16:00,191 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:16:03,942 - father_agent.py:386 - Step: 305, Training loss: 0.3783048689365387
2025-08-04 04:16:05,768 - father_agent.py:386 - Step: 310, Training loss: 0.4105592370033264
2025-08-04 04:16:07,599 - father_agent.py:386 - Step: 315, Training loss: 0.9984550476074219
2025-08-04 04:16:09,471 - father_agent.py:386 - Step: 320, Training loss: 0.7677310705184937
2025-08-04 04:16:11,368 - father_agent.py:386 - Step: 325, Training loss: 0.8319313526153564
2025-08-04 04:16:13,268 - father_agent.py:386 - Step: 330, Training loss: 0.43258175253868103
2025-08-04 04:16:15,158 - father_agent.py:386 - Step: 335, Training loss: 0.4525470435619354
2025-08-04 04:16:17,040 - father_agent.py:386 - Step: 340, Training loss: 0.7820382714271545
2025-08-04 04:16:18,865 - father_agent.py:386 - Step: 345, Training loss: 0.2143520563840866
2025-08-04 04:16:20,669 - father_agent.py:386 - Step: 350, Training loss: 0.8059551119804382
2025-08-04 04:16:22,476 - father_agent.py:386 - Step: 355, Training loss: 1.1749926805496216
2025-08-04 04:16:24,282 - father_agent.py:386 - Step: 360, Training loss: 0.7025068998336792
2025-08-04 04:16:26,097 - father_agent.py:386 - Step: 365, Training loss: 0.6101481318473816
2025-08-04 04:16:27,925 - father_agent.py:386 - Step: 370, Training loss: 0.7616912722587585
2025-08-04 04:16:29,753 - father_agent.py:386 - Step: 375, Training loss: 0.4922810196876526
2025-08-04 04:16:31,562 - father_agent.py:386 - Step: 380, Training loss: 0.5365475416183472
2025-08-04 04:16:33,365 - father_agent.py:386 - Step: 385, Training loss: 0.6463811993598938
2025-08-04 04:16:35,186 - father_agent.py:386 - Step: 390, Training loss: 0.43602603673934937
2025-08-04 04:16:37,029 - father_agent.py:386 - Step: 395, Training loss: 0.5971863865852356
2025-08-04 04:16:38,854 - father_agent.py:386 - Step: 400, Training loss: 0.700708270072937
2025-08-04 04:16:39,086 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:16:39,089 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:16:48,492 - evaluation_results_class.py:131 - Average Return = -62.4095458984375
2025-08-04 04:16:48,492 - evaluation_results_class.py:133 - Average Virtual Goal Value = 8.612227439880371
2025-08-04 04:16:48,492 - evaluation_results_class.py:135 - Average Discounted Reward = 13.402894973754883
2025-08-04 04:16:48,492 - evaluation_results_class.py:137 - Goal Reach Probability = 0.8877721943048577
2025-08-04 04:16:48,492 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:16:48,492 - evaluation_results_class.py:141 - Variance of Return = 5788.1396484375
2025-08-04 04:16:48,493 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:16:48,493 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:16:48,493 - evaluation_results_class.py:147 - Average Episode Length = 164.80653266331657
2025-08-04 04:16:48,493 - evaluation_results_class.py:149 - Counted Episodes = 1194
2025-08-04 04:16:48,720 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:16:48,732 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:16:52,531 - father_agent.py:386 - Step: 405, Training loss: 1.0952731370925903
2025-08-04 04:16:54,346 - father_agent.py:386 - Step: 410, Training loss: 0.31863582134246826
2025-08-04 04:16:56,162 - father_agent.py:386 - Step: 415, Training loss: 0.28682011365890503
2025-08-04 04:16:57,988 - father_agent.py:386 - Step: 420, Training loss: 0.7747572064399719
2025-08-04 04:16:59,826 - father_agent.py:386 - Step: 425, Training loss: 0.35390591621398926
2025-08-04 04:17:01,628 - father_agent.py:386 - Step: 430, Training loss: 0.3318130671977997
2025-08-04 04:17:03,453 - father_agent.py:386 - Step: 435, Training loss: 1.1157560348510742
2025-08-04 04:17:05,290 - father_agent.py:386 - Step: 440, Training loss: 0.614318311214447
2025-08-04 04:17:07,104 - father_agent.py:386 - Step: 445, Training loss: 0.5145055651664734
2025-08-04 04:17:08,926 - father_agent.py:386 - Step: 450, Training loss: 0.4212074279785156
2025-08-04 04:17:10,768 - father_agent.py:386 - Step: 455, Training loss: 0.8767852187156677
2025-08-04 04:17:12,588 - father_agent.py:386 - Step: 460, Training loss: 0.42055755853652954
2025-08-04 04:17:14,416 - father_agent.py:386 - Step: 465, Training loss: 0.5232343673706055
2025-08-04 04:17:16,241 - father_agent.py:386 - Step: 470, Training loss: 0.9129458069801331
2025-08-04 04:17:18,076 - father_agent.py:386 - Step: 475, Training loss: 0.7827206254005432
2025-08-04 04:17:19,902 - father_agent.py:386 - Step: 480, Training loss: 0.7483072876930237
2025-08-04 04:17:21,732 - father_agent.py:386 - Step: 485, Training loss: 0.5558714270591736
2025-08-04 04:17:23,573 - father_agent.py:386 - Step: 490, Training loss: 0.5277666449546814
2025-08-04 04:17:25,398 - father_agent.py:386 - Step: 495, Training loss: 0.5051989555358887
2025-08-04 04:17:27,208 - father_agent.py:386 - Step: 500, Training loss: 1.1761037111282349
2025-08-04 04:17:27,443 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:17:27,446 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:17:37,100 - evaluation_results_class.py:131 - Average Return = -35.338356018066406
2025-08-04 04:17:37,100 - evaluation_results_class.py:133 - Average Virtual Goal Value = 43.13843536376953
2025-08-04 04:17:37,100 - evaluation_results_class.py:135 - Average Discounted Reward = 24.824172973632812
2025-08-04 04:17:37,100 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9809599365331217
2025-08-04 04:17:37,100 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:17:37,100 - evaluation_results_class.py:141 - Variance of Return = 2274.01318359375
2025-08-04 04:17:37,100 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:17:37,100 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:17:37,100 - evaluation_results_class.py:147 - Average Episode Length = 83.64894882982944
2025-08-04 04:17:37,100 - evaluation_results_class.py:149 - Counted Episodes = 2521
2025-08-04 04:17:37,330 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:17:37,342 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:17:37,456 - father_agent.py:547 - Training finished.
2025-08-04 04:17:37,609 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:17:37,612 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:17:37,614 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 04:17:47,020 - evaluation_results_class.py:131 - Average Return = -37.6894416809082
2025-08-04 04:17:47,020 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.28985595703125
2025-08-04 04:17:47,020 - evaluation_results_class.py:135 - Average Discounted Reward = 23.640012741088867
2025-08-04 04:17:47,020 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9747412008281573
2025-08-04 04:17:47,020 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:17:47,020 - evaluation_results_class.py:141 - Variance of Return = 2578.12060546875
2025-08-04 04:17:47,020 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:17:47,021 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:17:47,021 - evaluation_results_class.py:147 - Average Episode Length = 89.14865424430641
2025-08-04 04:17:47,021 - evaluation_results_class.py:149 - Counted Episodes = 2415
2025-08-04 04:17:47,250 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:17:47,252 - self_interpretable_extractor.py:286 - True
2025-08-04 04:17:47,264 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:18:03,349 - evaluation_results_class.py:131 - Average Return = -47.23689651489258
2025-08-04 04:18:03,349 - evaluation_results_class.py:133 - Average Virtual Goal Value = 27.54793357849121
2025-08-04 04:18:03,349 - evaluation_results_class.py:135 - Average Discounted Reward = 20.068904876708984
2025-08-04 04:18:03,349 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9348103962505326
2025-08-04 04:18:03,349 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:18:03,349 - evaluation_results_class.py:141 - Variance of Return = 4114.49658203125
2025-08-04 04:18:03,349 - evaluation_results_class.py:143 - Current Best Return = -47.23689651489258
2025-08-04 04:18:03,349 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9348103962505326
2025-08-04 04:18:03,349 - evaluation_results_class.py:147 - Average Episode Length = 118.68129527055815
2025-08-04 04:18:03,349 - evaluation_results_class.py:149 - Counted Episodes = 2347
2025-08-04 04:18:03,349 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 04:18:03,349 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 10838 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 503, 506, 509}
Buffer 1
2025-08-04 04:19:11,028 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 21442 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
Buffer 2
2025-08-04 04:20:18,493 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 32047 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
All trajectories collected
2025-08-04 04:21:26,324 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 04:21:26,325 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 32047 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_4.dot.
Learned FSC of size 2
2025-08-04 04:21:28,713 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 04:21:43,614 - evaluation_results_class.py:131 - Average Return = -111.01930236816406
2025-08-04 04:21:43,614 - evaluation_results_class.py:133 - Average Virtual Goal Value = -31.019302368164062
2025-08-04 04:21:43,615 - evaluation_results_class.py:135 - Average Discounted Reward = -51.46391677856445
2025-08-04 04:21:43,615 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:21:43,615 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:21:43,615 - evaluation_results_class.py:141 - Variance of Return = 4922.61669921875
2025-08-04 04:21:43,615 - evaluation_results_class.py:143 - Current Best Return = -111.01930236816406
2025-08-04 04:21:43,615 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:21:43,615 - evaluation_results_class.py:147 - Average Episode Length = 128.0513347022587
2025-08-04 04:21:43,615 - evaluation_results_class.py:149 - Counted Episodes = 2435
FSC Result: {'best_episode_return': -31.019302, 'best_return': -111.0193, 'goal_value': 0.0, 'returns_episodic': [-31.019302], 'returns': [-111.0193], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [4922.6167], 'each_episode_virtual_variance': [4922.6167], 'combined_variance': [19690.467], 'num_episodes': [2435], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [128.0513347022587], 'counted_episodes': [2435], 'discounted_rewards': [-51.463917], 'new_pomdp_iteration_numbers': []}
[[5], [3, 4, 6, 7], [3, 4, 6, 7], [1], [1], [1], [1], [2], [2], [2], [2], [3, 4, 6, 7], [3, 4, 6, 7], [0]]
2025-08-04 04:21:43,703 - statistic.py:67 - synthesis initiated, design space: 81
2025-08-04 04:21:43,703 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 04:21:43,719 - synthesizer_ar.py:122 - value 84.8663 achieved after 2722.98 seconds
2025-08-04 04:21:43,726 - synthesizer_ar.py:122 - value 96.0385 achieved after 2722.98 seconds
2025-08-04 04:21:43,733 - synthesizer_ar.py:122 - value 107.9674 achieved after 2722.99 seconds
2025-08-04 04:21:43,750 - synthesizer_ar.py:122 - value 109.4087 achieved after 2723.01 seconds
2025-08-04 04:21:43,762 - synthesizer_ar.py:122 - value 110.8444 achieved after 2723.02 seconds
2025-08-04 04:21:43,783 - synthesizer_ar.py:122 - value 111.9897 achieved after 2723.04 seconds
2025-08-04 04:21:43,803 - synthesizer_ar.py:122 - value 113.2162 achieved after 2723.06 seconds
2025-08-04 04:21:43,820 - synthesizer_ar.py:122 - value 114.3889 achieved after 2723.08 seconds
2025-08-04 04:21:43,835 - synthesizer_ar.py:122 - value 117.7045 achieved after 2723.09 seconds
2025-08-04 04:21:43,848 - synthesizer_ar.py:122 - value 118.7525 achieved after 2723.1 seconds
2025-08-04 04:21:43,860 - synthesizer_ar.py:122 - value 119.7325 achieved after 2723.12 seconds
2025-08-04 04:21:43,862 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 04:21:43,862 - synthesizer.py:193 - o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 04:21:43,863 - synthesizer.py:198 - double-checking specification satisfiability:  : 119.73254856863112
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 10) & (y = 10))] 

method: AR, synthesis time: 0.16 s
number of holes: 4, family size: 81, quotient: 1018 states / 1170 actions
explored: 100 %
MDP stats: avg MDP size: 912, iterations: 85

optimum: 119.732549
--------------------
2025-08-04 04:21:43,863 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 04:21:43,868 - robust_rl_trainer.py:432 - Iteration 6 of pure RL loop
2025-08-04 04:21:43,905 - storm_vec_env.py:70 - Computing row map
2025-08-04 04:21:43,911 - storm_vec_env.py:97 - Computing transitions
2025-08-04 04:21:43,924 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 04:21:43,924 - storm_vec_env.py:114 - Computing sinks
2025-08-04 04:21:43,924 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 04:21:43,927 - storm_vec_env.py:143 - Computing labels
2025-08-04 04:21:43,928 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 04:21:43,928 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 04:21:43,928 - storm_vec_env.py:175 - Computing observations
2025-08-04 04:21:44,080 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 04:21:44,081 - father_agent.py:540 - Before training evaluation.
2025-08-04 04:21:44,234 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:21:44,237 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:21:54,077 - evaluation_results_class.py:131 - Average Return = -37.66695022583008
2025-08-04 04:21:54,078 - evaluation_results_class.py:133 - Average Virtual Goal Value = 40.249359130859375
2025-08-04 04:21:54,078 - evaluation_results_class.py:135 - Average Discounted Reward = 23.748865127563477
2025-08-04 04:21:54,078 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9739538855678906
2025-08-04 04:21:54,078 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:21:54,078 - evaluation_results_class.py:141 - Variance of Return = 2553.67822265625
2025-08-04 04:21:54,078 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:21:54,078 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:21:54,078 - evaluation_results_class.py:147 - Average Episode Length = 90.03202391118703
2025-08-04 04:21:54,078 - evaluation_results_class.py:149 - Counted Episodes = 2342
2025-08-04 04:21:54,313 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:21:54,325 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:21:54,439 - father_agent.py:436 - Training agent on-policy
2025-08-04 04:22:03,002 - father_agent.py:386 - Step: 0, Training loss: 0.5735496282577515
2025-08-04 04:22:04,885 - father_agent.py:386 - Step: 5, Training loss: 0.8264581561088562
2025-08-04 04:22:06,774 - father_agent.py:386 - Step: 10, Training loss: 0.33505135774612427
2025-08-04 04:22:08,656 - father_agent.py:386 - Step: 15, Training loss: 0.8679193258285522
2025-08-04 04:22:10,519 - father_agent.py:386 - Step: 20, Training loss: 0.9997605085372925
2025-08-04 04:22:12,383 - father_agent.py:386 - Step: 25, Training loss: 0.634768009185791
2025-08-04 04:22:14,256 - father_agent.py:386 - Step: 30, Training loss: 0.7841170430183411
2025-08-04 04:22:16,123 - father_agent.py:386 - Step: 35, Training loss: 0.6253477334976196
2025-08-04 04:22:18,002 - father_agent.py:386 - Step: 40, Training loss: 0.5090810060501099
2025-08-04 04:22:19,868 - father_agent.py:386 - Step: 45, Training loss: 0.6186739206314087
2025-08-04 04:22:21,756 - father_agent.py:386 - Step: 50, Training loss: 0.8667507171630859
2025-08-04 04:22:23,614 - father_agent.py:386 - Step: 55, Training loss: 0.5695807933807373
2025-08-04 04:22:25,479 - father_agent.py:386 - Step: 60, Training loss: 0.6645211577415466
2025-08-04 04:22:27,333 - father_agent.py:386 - Step: 65, Training loss: 0.3656107783317566
2025-08-04 04:22:29,187 - father_agent.py:386 - Step: 70, Training loss: 0.7517320513725281
2025-08-04 04:22:31,071 - father_agent.py:386 - Step: 75, Training loss: 0.8245739340782166
2025-08-04 04:22:32,966 - father_agent.py:386 - Step: 80, Training loss: 0.49445176124572754
2025-08-04 04:22:34,864 - father_agent.py:386 - Step: 85, Training loss: 0.7354831695556641
2025-08-04 04:22:36,751 - father_agent.py:386 - Step: 90, Training loss: 0.6067373752593994
2025-08-04 04:22:38,620 - father_agent.py:386 - Step: 95, Training loss: 0.6679027080535889
2025-08-04 04:22:40,524 - father_agent.py:386 - Step: 100, Training loss: 0.927295982837677
2025-08-04 04:22:40,758 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:22:40,761 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:22:50,764 - evaluation_results_class.py:131 - Average Return = -40.01715087890625
2025-08-04 04:22:50,764 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.492855072021484
2025-08-04 04:22:50,764 - evaluation_results_class.py:135 - Average Discounted Reward = 18.136123657226562
2025-08-04 04:22:50,764 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9938750510412413
2025-08-04 04:22:50,764 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:22:50,764 - evaluation_results_class.py:141 - Variance of Return = 2127.98828125
2025-08-04 04:22:50,764 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:22:50,764 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:22:50,764 - evaluation_results_class.py:147 - Average Episode Length = 100.47366271947733
2025-08-04 04:22:50,764 - evaluation_results_class.py:149 - Counted Episodes = 2449
2025-08-04 04:22:51,002 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:22:51,013 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:22:58,218 - father_agent.py:386 - Step: 105, Training loss: 0.6011142134666443
2025-08-04 04:23:00,226 - father_agent.py:386 - Step: 110, Training loss: 0.5046910047531128
2025-08-04 04:23:02,170 - father_agent.py:386 - Step: 115, Training loss: 0.44401323795318604
2025-08-04 04:23:04,116 - father_agent.py:386 - Step: 120, Training loss: 0.6851841807365417
2025-08-04 04:23:06,067 - father_agent.py:386 - Step: 125, Training loss: 0.626371443271637
2025-08-04 04:23:07,993 - father_agent.py:386 - Step: 130, Training loss: 0.6583694219589233
2025-08-04 04:23:09,924 - father_agent.py:386 - Step: 135, Training loss: 0.3346351981163025
2025-08-04 04:23:11,829 - father_agent.py:386 - Step: 140, Training loss: 0.8534538149833679
2025-08-04 04:23:13,732 - father_agent.py:386 - Step: 145, Training loss: 0.48203757405281067
2025-08-04 04:23:15,668 - father_agent.py:386 - Step: 150, Training loss: 0.6569960117340088
2025-08-04 04:23:17,577 - father_agent.py:386 - Step: 155, Training loss: 0.6890914440155029
2025-08-04 04:23:19,458 - father_agent.py:386 - Step: 160, Training loss: 0.5561501979827881
2025-08-04 04:23:21,315 - father_agent.py:386 - Step: 165, Training loss: 0.7385324239730835
2025-08-04 04:23:23,179 - father_agent.py:386 - Step: 170, Training loss: 0.7884504199028015
2025-08-04 04:23:25,047 - father_agent.py:386 - Step: 175, Training loss: 0.39688926935195923
2025-08-04 04:23:26,919 - father_agent.py:386 - Step: 180, Training loss: 0.31227684020996094
2025-08-04 04:23:28,806 - father_agent.py:386 - Step: 185, Training loss: 0.7671366333961487
2025-08-04 04:23:30,684 - father_agent.py:386 - Step: 190, Training loss: 0.6544680595397949
2025-08-04 04:23:32,556 - father_agent.py:386 - Step: 195, Training loss: 0.502979576587677
2025-08-04 04:23:34,439 - father_agent.py:386 - Step: 200, Training loss: 0.3711656332015991
2025-08-04 04:23:34,687 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:23:34,690 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:23:44,548 - evaluation_results_class.py:131 - Average Return = -35.23671340942383
2025-08-04 04:23:44,548 - evaluation_results_class.py:133 - Average Virtual Goal Value = 44.74030303955078
2025-08-04 04:23:44,548 - evaluation_results_class.py:135 - Average Discounted Reward = 18.792749404907227
2025-08-04 04:23:44,548 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9997127262280954
2025-08-04 04:23:44,548 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:23:44,548 - evaluation_results_class.py:141 - Variance of Return = 1385.03564453125
2025-08-04 04:23:44,548 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:23:44,548 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:23:44,548 - evaluation_results_class.py:147 - Average Episode Length = 83.27635736857225
2025-08-04 04:23:44,548 - evaluation_results_class.py:149 - Counted Episodes = 3481
2025-08-04 04:23:44,790 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:23:44,802 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:23:49,169 - father_agent.py:386 - Step: 205, Training loss: 0.5940588116645813
2025-08-04 04:23:51,042 - father_agent.py:386 - Step: 210, Training loss: 0.34750235080718994
2025-08-04 04:23:52,894 - father_agent.py:386 - Step: 215, Training loss: 0.374562531709671
2025-08-04 04:23:54,761 - father_agent.py:386 - Step: 220, Training loss: 0.3618745803833008
2025-08-04 04:23:56,630 - father_agent.py:386 - Step: 225, Training loss: 0.5486212968826294
2025-08-04 04:23:58,560 - father_agent.py:386 - Step: 230, Training loss: 0.42378920316696167
2025-08-04 04:24:00,590 - father_agent.py:386 - Step: 235, Training loss: 0.28037160634994507
2025-08-04 04:24:02,568 - father_agent.py:386 - Step: 240, Training loss: 0.3678169250488281
2025-08-04 04:24:04,521 - father_agent.py:386 - Step: 245, Training loss: 0.2614779472351074
2025-08-04 04:24:06,469 - father_agent.py:386 - Step: 250, Training loss: 0.5242992639541626
2025-08-04 04:24:08,438 - father_agent.py:386 - Step: 255, Training loss: 0.4225626289844513
2025-08-04 04:24:10,429 - father_agent.py:386 - Step: 260, Training loss: 0.14425499737262726
2025-08-04 04:24:12,418 - father_agent.py:386 - Step: 265, Training loss: 0.6098071336746216
2025-08-04 04:24:14,419 - father_agent.py:386 - Step: 270, Training loss: 0.5721339583396912
2025-08-04 04:24:16,381 - father_agent.py:386 - Step: 275, Training loss: 0.8601009845733643
2025-08-04 04:24:18,338 - father_agent.py:386 - Step: 280, Training loss: 0.3991110324859619
2025-08-04 04:24:20,310 - father_agent.py:386 - Step: 285, Training loss: 0.6244003772735596
2025-08-04 04:24:22,331 - father_agent.py:386 - Step: 290, Training loss: 0.8436881303787231
2025-08-04 04:24:24,304 - father_agent.py:386 - Step: 295, Training loss: 0.5053443312644958
2025-08-04 04:24:26,238 - father_agent.py:386 - Step: 300, Training loss: 0.25324708223342896
2025-08-04 04:24:26,494 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:24:26,497 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:24:36,964 - evaluation_results_class.py:131 - Average Return = -28.745487213134766
2025-08-04 04:24:36,964 - evaluation_results_class.py:133 - Average Virtual Goal Value = 51.2364616394043
2025-08-04 04:24:36,964 - evaluation_results_class.py:135 - Average Discounted Reward = 26.136627197265625
2025-08-04 04:24:36,965 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9997743682310469
2025-08-04 04:24:36,965 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:24:36,965 - evaluation_results_class.py:141 - Variance of Return = 1037.2606201171875
2025-08-04 04:24:36,965 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:24:36,965 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:24:36,965 - evaluation_results_class.py:147 - Average Episode Length = 65.42351083032491
2025-08-04 04:24:36,965 - evaluation_results_class.py:149 - Counted Episodes = 4432
2025-08-04 04:24:37,217 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:24:37,229 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:24:41,333 - father_agent.py:386 - Step: 305, Training loss: 0.17818990349769592
2025-08-04 04:24:43,249 - father_agent.py:386 - Step: 310, Training loss: 0.26354092359542847
2025-08-04 04:24:45,141 - father_agent.py:386 - Step: 315, Training loss: 0.5698585510253906
2025-08-04 04:24:47,037 - father_agent.py:386 - Step: 320, Training loss: 0.6744924187660217
2025-08-04 04:24:48,945 - father_agent.py:386 - Step: 325, Training loss: 0.4288148283958435
2025-08-04 04:24:50,843 - father_agent.py:386 - Step: 330, Training loss: 0.590219259262085
2025-08-04 04:24:52,714 - father_agent.py:386 - Step: 335, Training loss: 0.5009886622428894
2025-08-04 04:24:54,594 - father_agent.py:386 - Step: 340, Training loss: 0.6531557440757751
2025-08-04 04:24:56,469 - father_agent.py:386 - Step: 345, Training loss: 0.243098646402359
2025-08-04 04:24:58,365 - father_agent.py:386 - Step: 350, Training loss: 0.292598694562912
2025-08-04 04:25:00,280 - father_agent.py:386 - Step: 355, Training loss: 1.0050091743469238
2025-08-04 04:25:02,193 - father_agent.py:386 - Step: 360, Training loss: 0.6601279973983765
2025-08-04 04:25:04,078 - father_agent.py:386 - Step: 365, Training loss: 0.7268872261047363
2025-08-04 04:25:05,955 - father_agent.py:386 - Step: 370, Training loss: 0.4128841757774353
2025-08-04 04:25:07,826 - father_agent.py:386 - Step: 375, Training loss: 0.5388475060462952
2025-08-04 04:25:09,720 - father_agent.py:386 - Step: 380, Training loss: 0.7046909928321838
2025-08-04 04:25:11,612 - father_agent.py:386 - Step: 385, Training loss: 0.518302857875824
2025-08-04 04:25:13,500 - father_agent.py:386 - Step: 390, Training loss: 0.7942972779273987
2025-08-04 04:25:15,394 - father_agent.py:386 - Step: 395, Training loss: 0.6590454578399658
2025-08-04 04:25:17,278 - father_agent.py:386 - Step: 400, Training loss: 0.6082282066345215
2025-08-04 04:25:17,529 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:25:17,533 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:25:27,640 - evaluation_results_class.py:131 - Average Return = -24.716127395629883
2025-08-04 04:25:27,640 - evaluation_results_class.py:133 - Average Virtual Goal Value = 55.283870697021484
2025-08-04 04:25:27,640 - evaluation_results_class.py:135 - Average Discounted Reward = 28.145177841186523
2025-08-04 04:25:27,640 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:25:27,640 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:25:27,641 - evaluation_results_class.py:141 - Variance of Return = 638.9351806640625
2025-08-04 04:25:27,641 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:25:27,641 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:25:27,641 - evaluation_results_class.py:147 - Average Episode Length = 58.92450009344048
2025-08-04 04:25:27,641 - evaluation_results_class.py:149 - Counted Episodes = 5351
2025-08-04 04:25:27,894 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:25:27,906 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:25:31,959 - father_agent.py:386 - Step: 405, Training loss: 0.3450607657432556
2025-08-04 04:25:33,820 - father_agent.py:386 - Step: 410, Training loss: 0.49687111377716064
2025-08-04 04:25:35,723 - father_agent.py:386 - Step: 415, Training loss: 0.6376422643661499
2025-08-04 04:25:37,621 - father_agent.py:386 - Step: 420, Training loss: 0.630325973033905
2025-08-04 04:25:39,498 - father_agent.py:386 - Step: 425, Training loss: 0.33524322509765625
2025-08-04 04:25:41,402 - father_agent.py:386 - Step: 430, Training loss: 0.5357853770256042
2025-08-04 04:25:43,298 - father_agent.py:386 - Step: 435, Training loss: 0.5421910881996155
2025-08-04 04:25:45,177 - father_agent.py:386 - Step: 440, Training loss: 0.6023988723754883
2025-08-04 04:25:47,062 - father_agent.py:386 - Step: 445, Training loss: 0.05477021634578705
2025-08-04 04:25:48,941 - father_agent.py:386 - Step: 450, Training loss: 0.5307321548461914
2025-08-04 04:25:50,823 - father_agent.py:386 - Step: 455, Training loss: 0.5337475538253784
2025-08-04 04:25:52,681 - father_agent.py:386 - Step: 460, Training loss: 0.13073889911174774
2025-08-04 04:25:54,553 - father_agent.py:386 - Step: 465, Training loss: 0.7331029176712036
2025-08-04 04:25:56,424 - father_agent.py:386 - Step: 470, Training loss: 0.6696324348449707
2025-08-04 04:25:58,280 - father_agent.py:386 - Step: 475, Training loss: 0.24307094514369965
2025-08-04 04:26:00,154 - father_agent.py:386 - Step: 480, Training loss: 0.6551499962806702
2025-08-04 04:26:02,055 - father_agent.py:386 - Step: 485, Training loss: 0.7246841192245483
2025-08-04 04:26:03,976 - father_agent.py:386 - Step: 490, Training loss: 0.3259091079235077
2025-08-04 04:26:05,914 - father_agent.py:386 - Step: 495, Training loss: 0.4364100694656372
2025-08-04 04:26:07,843 - father_agent.py:386 - Step: 500, Training loss: 0.6466401815414429
2025-08-04 04:26:08,098 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:26:08,100 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:26:18,804 - evaluation_results_class.py:131 - Average Return = -24.026348114013672
2025-08-04 04:26:18,805 - evaluation_results_class.py:133 - Average Virtual Goal Value = 55.97365188598633
2025-08-04 04:26:18,805 - evaluation_results_class.py:135 - Average Discounted Reward = 29.11737632751465
2025-08-04 04:26:18,805 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:26:18,805 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:26:18,805 - evaluation_results_class.py:141 - Variance of Return = 610.4669799804688
2025-08-04 04:26:18,805 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:26:18,805 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:26:18,805 - evaluation_results_class.py:147 - Average Episode Length = 54.61847733105218
2025-08-04 04:26:18,805 - evaluation_results_class.py:149 - Counted Episodes = 5845
2025-08-04 04:26:19,214 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:26:19,232 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:26:19,370 - father_agent.py:547 - Training finished.
2025-08-04 04:26:19,571 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:26:19,575 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:26:19,578 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 04:26:30,428 - evaluation_results_class.py:131 - Average Return = -23.412227630615234
2025-08-04 04:26:30,428 - evaluation_results_class.py:133 - Average Virtual Goal Value = 56.587772369384766
2025-08-04 04:26:30,428 - evaluation_results_class.py:135 - Average Discounted Reward = 29.702119827270508
2025-08-04 04:26:30,428 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:26:30,428 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:26:30,428 - evaluation_results_class.py:141 - Variance of Return = 561.2835693359375
2025-08-04 04:26:30,428 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:26:30,428 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:26:30,428 - evaluation_results_class.py:147 - Average Episode Length = 54.44637978142077
2025-08-04 04:26:30,428 - evaluation_results_class.py:149 - Counted Episodes = 5856
2025-08-04 04:26:30,835 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:26:30,838 - self_interpretable_extractor.py:286 - True
2025-08-04 04:26:30,856 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:26:48,519 - evaluation_results_class.py:131 - Average Return = -24.20112419128418
2025-08-04 04:26:48,520 - evaluation_results_class.py:133 - Average Virtual Goal Value = 55.79887390136719
2025-08-04 04:26:48,520 - evaluation_results_class.py:135 - Average Discounted Reward = 28.908674240112305
2025-08-04 04:26:48,520 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:26:48,520 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:26:48,520 - evaluation_results_class.py:141 - Variance of Return = 626.6129760742188
2025-08-04 04:26:48,520 - evaluation_results_class.py:143 - Current Best Return = -24.20112419128418
2025-08-04 04:26:48,520 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:26:48,520 - evaluation_results_class.py:147 - Average Episode Length = 55.401568092722
2025-08-04 04:26:48,520 - evaluation_results_class.py:149 - Counted Episodes = 5867
2025-08-04 04:26:48,520 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 04:26:48,520 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 26799 trajectories
Learned trajectory lengths  {26, 29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125, 128, 131, 134, 137, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 311, 314, 320, 323, 329, 341, 353, 365, 401, 404, 410, 413}
Buffer 1
2025-08-04 04:27:59,807 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 53748 trajectories
Learned trajectory lengths  {26, 29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125, 128, 131, 134, 137, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 320, 323, 326, 329, 332, 338, 341, 350, 353, 365, 380, 401, 404, 410, 413, 422, 476}
Buffer 2
2025-08-04 04:29:11,068 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 80580 trajectories
Learned trajectory lengths  {26, 29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125, 128, 131, 134, 137, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 320, 323, 326, 329, 332, 335, 338, 341, 347, 350, 353, 365, 374, 380, 398, 401, 404, 410, 413, 422, 476}
All trajectories collected
2025-08-04 04:30:21,530 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 04:30:21,530 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 80580 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_5.dot.
Learned FSC of size 2
2025-08-04 04:30:24,014 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 04:30:40,047 - evaluation_results_class.py:131 - Average Return = -86.13758087158203
2025-08-04 04:30:40,047 - evaluation_results_class.py:133 - Average Virtual Goal Value = -6.137580394744873
2025-08-04 04:30:40,047 - evaluation_results_class.py:135 - Average Discounted Reward = -22.72224998474121
2025-08-04 04:30:40,047 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:30:40,047 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:30:40,047 - evaluation_results_class.py:141 - Variance of Return = 5185.90087890625
2025-08-04 04:30:40,047 - evaluation_results_class.py:143 - Current Best Return = -86.13758087158203
2025-08-04 04:30:40,047 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:30:40,047 - evaluation_results_class.py:147 - Average Episode Length = 55.420874279905114
2025-08-04 04:30:40,047 - evaluation_results_class.py:149 - Counted Episodes = 5902
FSC Result: {'best_episode_return': -6.1375804, 'best_return': -86.13758, 'goal_value': 0.0, 'returns_episodic': [-6.1375804], 'returns': [-86.13758], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [5185.901], 'each_episode_virtual_variance': [5185.901], 'combined_variance': [20743.604], 'num_episodes': [5902], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [55.420874279905114], 'counted_episodes': [5902], 'discounted_rewards': [-22.72225], 'new_pomdp_iteration_numbers': []}
[[5], [3, 4, 6, 7], [3, 4, 6, 7], [1], [1], [1], [1], [2], [2], [2], [2], [3, 4, 6, 7], [3, 4, 6, 7], [0]]
2025-08-04 04:30:40,161 - statistic.py:67 - synthesis initiated, design space: 81
2025-08-04 04:30:40,161 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 04:30:40,178 - synthesizer_ar.py:122 - value 65.391 achieved after 3259.43 seconds
2025-08-04 04:30:40,185 - synthesizer_ar.py:122 - value 67.5548 achieved after 3259.44 seconds
2025-08-04 04:30:40,201 - synthesizer_ar.py:122 - value 72.4715 achieved after 3259.46 seconds
2025-08-04 04:30:40,208 - synthesizer_ar.py:122 - value 73.8428 achieved after 3259.46 seconds
2025-08-04 04:30:40,223 - synthesizer_ar.py:122 - value 81.3011 achieved after 3259.48 seconds
2025-08-04 04:30:40,230 - synthesizer_ar.py:122 - value 82.074 achieved after 3259.49 seconds
2025-08-04 04:30:40,294 - synthesizer_ar.py:122 - value 84.7246 achieved after 3259.55 seconds
2025-08-04 04:30:40,348 - synthesizer_ar.py:122 - value 86.6761 achieved after 3259.6 seconds
2025-08-04 04:30:40,355 - synthesizer_ar.py:122 - value 86.7405 achieved after 3259.61 seconds
2025-08-04 04:30:40,356 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 04:30:40,356 - synthesizer.py:193 - o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 04:30:40,358 - synthesizer.py:198 - double-checking specification satisfiability:  : 86.74050108034173
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 10) & (y = 10))] 

method: AR, synthesis time: 0.2 s
number of holes: 4, family size: 81, quotient: 1063 states / 1215 actions
explored: 100 %
MDP stats: avg MDP size: 931, iterations: 109

optimum: 86.740501
--------------------
2025-08-04 04:30:40,358 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 04:30:40,361 - robust_rl_trainer.py:432 - Iteration 7 of pure RL loop
2025-08-04 04:30:40,402 - storm_vec_env.py:70 - Computing row map
2025-08-04 04:30:40,408 - storm_vec_env.py:97 - Computing transitions
2025-08-04 04:30:40,421 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 04:30:40,421 - storm_vec_env.py:114 - Computing sinks
2025-08-04 04:30:40,421 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 04:30:40,425 - storm_vec_env.py:143 - Computing labels
2025-08-04 04:30:40,425 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 04:30:40,425 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 04:30:40,425 - storm_vec_env.py:175 - Computing observations
2025-08-04 04:30:40,605 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 04:30:40,605 - father_agent.py:540 - Before training evaluation.
2025-08-04 04:30:40,774 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:30:40,777 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:30:51,916 - evaluation_results_class.py:131 - Average Return = -24.19508934020996
2025-08-04 04:30:51,916 - evaluation_results_class.py:133 - Average Virtual Goal Value = 55.80491256713867
2025-08-04 04:30:51,916 - evaluation_results_class.py:135 - Average Discounted Reward = 29.12892723083496
2025-08-04 04:30:51,916 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:30:51,916 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:30:51,916 - evaluation_results_class.py:141 - Variance of Return = 635.461181640625
2025-08-04 04:30:51,916 - evaluation_results_class.py:143 - Current Best Return = -23.18852424621582
2025-08-04 04:30:51,916 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:30:51,916 - evaluation_results_class.py:147 - Average Episode Length = 54.35948158253752
2025-08-04 04:30:51,916 - evaluation_results_class.py:149 - Counted Episodes = 5864
2025-08-04 04:30:52,164 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:30:52,177 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:30:52,294 - father_agent.py:436 - Training agent on-policy
2025-08-04 04:31:00,674 - father_agent.py:386 - Step: 0, Training loss: 0.5194985270500183
2025-08-04 04:31:02,576 - father_agent.py:386 - Step: 5, Training loss: 0.3871798813343048
2025-08-04 04:31:04,464 - father_agent.py:386 - Step: 10, Training loss: 0.42103782296180725
2025-08-04 04:31:06,381 - father_agent.py:386 - Step: 15, Training loss: 0.6251184344291687
2025-08-04 04:31:08,282 - father_agent.py:386 - Step: 20, Training loss: 0.7020203471183777
2025-08-04 04:31:10,189 - father_agent.py:386 - Step: 25, Training loss: 0.281599223613739
2025-08-04 04:31:12,091 - father_agent.py:386 - Step: 30, Training loss: 0.5796084403991699
2025-08-04 04:31:13,980 - father_agent.py:386 - Step: 35, Training loss: 0.2526552975177765
2025-08-04 04:31:15,882 - father_agent.py:386 - Step: 40, Training loss: 0.2504844069480896
2025-08-04 04:31:17,776 - father_agent.py:386 - Step: 45, Training loss: 0.6430612802505493
2025-08-04 04:31:19,702 - father_agent.py:386 - Step: 50, Training loss: 0.854871928691864
2025-08-04 04:31:21,617 - father_agent.py:386 - Step: 55, Training loss: 0.8546642065048218
2025-08-04 04:31:23,517 - father_agent.py:386 - Step: 60, Training loss: 0.16170062124729156
2025-08-04 04:31:25,436 - father_agent.py:386 - Step: 65, Training loss: 0.42923322319984436
2025-08-04 04:31:27,367 - father_agent.py:386 - Step: 70, Training loss: 0.1513710469007492
2025-08-04 04:31:29,287 - father_agent.py:386 - Step: 75, Training loss: 0.32416364550590515
2025-08-04 04:31:31,196 - father_agent.py:386 - Step: 80, Training loss: 0.6202526092529297
2025-08-04 04:31:33,101 - father_agent.py:386 - Step: 85, Training loss: 0.3120833933353424
2025-08-04 04:31:35,015 - father_agent.py:386 - Step: 90, Training loss: 0.23364904522895813
2025-08-04 04:31:36,904 - father_agent.py:386 - Step: 95, Training loss: 0.5142187476158142
2025-08-04 04:31:38,828 - father_agent.py:386 - Step: 100, Training loss: 0.4107387959957123
2025-08-04 04:31:39,090 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:31:39,093 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:31:49,847 - evaluation_results_class.py:131 - Average Return = -17.955677032470703
2025-08-04 04:31:49,847 - evaluation_results_class.py:133 - Average Virtual Goal Value = 62.0443229675293
2025-08-04 04:31:49,847 - evaluation_results_class.py:135 - Average Discounted Reward = 35.29066848754883
2025-08-04 04:31:49,847 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:31:49,847 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:31:49,847 - evaluation_results_class.py:141 - Variance of Return = 176.29151916503906
2025-08-04 04:31:49,847 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:31:49,847 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:31:49,847 - evaluation_results_class.py:147 - Average Episode Length = 47.79973494330732
2025-08-04 04:31:49,847 - evaluation_results_class.py:149 - Counted Episodes = 6791
2025-08-04 04:31:50,107 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:31:50,120 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:31:57,539 - father_agent.py:386 - Step: 105, Training loss: 0.8425909280776978
2025-08-04 04:31:59,465 - father_agent.py:386 - Step: 110, Training loss: 0.3322014808654785
2025-08-04 04:32:01,369 - father_agent.py:386 - Step: 115, Training loss: 0.4613076448440552
2025-08-04 04:32:03,277 - father_agent.py:386 - Step: 120, Training loss: 0.5836166739463806
2025-08-04 04:32:05,185 - father_agent.py:386 - Step: 125, Training loss: 0.3633110225200653
2025-08-04 04:32:07,085 - father_agent.py:386 - Step: 130, Training loss: 0.10147588700056076
2025-08-04 04:32:09,010 - father_agent.py:386 - Step: 135, Training loss: 0.531454861164093
2025-08-04 04:32:10,906 - father_agent.py:386 - Step: 140, Training loss: 0.618617594242096
2025-08-04 04:32:12,793 - father_agent.py:386 - Step: 145, Training loss: 0.30732449889183044
2025-08-04 04:32:14,716 - father_agent.py:386 - Step: 150, Training loss: 0.49191170930862427
2025-08-04 04:32:16,639 - father_agent.py:386 - Step: 155, Training loss: 0.299784779548645
2025-08-04 04:32:18,538 - father_agent.py:386 - Step: 160, Training loss: 0.21588927507400513
2025-08-04 04:32:20,472 - father_agent.py:386 - Step: 165, Training loss: 0.7377691268920898
2025-08-04 04:32:22,396 - father_agent.py:386 - Step: 170, Training loss: 0.6058563590049744
2025-08-04 04:32:24,301 - father_agent.py:386 - Step: 175, Training loss: 0.29666668176651
2025-08-04 04:32:26,202 - father_agent.py:386 - Step: 180, Training loss: 0.2773950695991516
2025-08-04 04:32:28,123 - father_agent.py:386 - Step: 185, Training loss: 0.48795202374458313
2025-08-04 04:32:30,039 - father_agent.py:386 - Step: 190, Training loss: 0.3797140121459961
2025-08-04 04:32:31,959 - father_agent.py:386 - Step: 195, Training loss: 0.5960575342178345
2025-08-04 04:32:33,871 - father_agent.py:386 - Step: 200, Training loss: 0.38870131969451904
2025-08-04 04:32:34,136 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:32:34,140 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:32:44,806 - evaluation_results_class.py:131 - Average Return = -25.70743751525879
2025-08-04 04:32:44,806 - evaluation_results_class.py:133 - Average Virtual Goal Value = 54.29256057739258
2025-08-04 04:32:44,806 - evaluation_results_class.py:135 - Average Discounted Reward = 26.652502059936523
2025-08-04 04:32:44,806 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:32:44,806 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:32:44,806 - evaluation_results_class.py:141 - Variance of Return = 649.123046875
2025-08-04 04:32:44,806 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:32:44,806 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:32:44,806 - evaluation_results_class.py:147 - Average Episode Length = 58.39742883379247
2025-08-04 04:32:44,806 - evaluation_results_class.py:149 - Counted Episodes = 5445
2025-08-04 04:32:45,072 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:32:45,085 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:32:49,562 - father_agent.py:386 - Step: 205, Training loss: 0.608639657497406
2025-08-04 04:32:51,442 - father_agent.py:386 - Step: 210, Training loss: 0.6722247004508972
2025-08-04 04:32:53,324 - father_agent.py:386 - Step: 215, Training loss: 0.3662383258342743
2025-08-04 04:32:55,204 - father_agent.py:386 - Step: 220, Training loss: 0.17753396928310394
2025-08-04 04:32:57,103 - father_agent.py:386 - Step: 225, Training loss: 0.6591836214065552
2025-08-04 04:32:59,014 - father_agent.py:386 - Step: 230, Training loss: 0.4865160882472992
2025-08-04 04:33:00,921 - father_agent.py:386 - Step: 235, Training loss: 0.25691550970077515
2025-08-04 04:33:02,862 - father_agent.py:386 - Step: 240, Training loss: 0.42394518852233887
2025-08-04 04:33:04,751 - father_agent.py:386 - Step: 245, Training loss: 0.4227641820907593
2025-08-04 04:33:06,636 - father_agent.py:386 - Step: 250, Training loss: 0.4641324579715729
2025-08-04 04:33:08,549 - father_agent.py:386 - Step: 255, Training loss: 0.5172598361968994
2025-08-04 04:33:10,463 - father_agent.py:386 - Step: 260, Training loss: 0.4256800413131714
2025-08-04 04:33:12,359 - father_agent.py:386 - Step: 265, Training loss: 0.9477304816246033
2025-08-04 04:33:14,261 - father_agent.py:386 - Step: 270, Training loss: 0.40690773725509644
2025-08-04 04:33:16,167 - father_agent.py:386 - Step: 275, Training loss: 0.27518230676651
2025-08-04 04:33:18,058 - father_agent.py:386 - Step: 280, Training loss: 0.40948915481567383
2025-08-04 04:33:19,967 - father_agent.py:386 - Step: 285, Training loss: 0.753642201423645
2025-08-04 04:33:21,859 - father_agent.py:386 - Step: 290, Training loss: 0.6238852739334106
2025-08-04 04:33:23,759 - father_agent.py:386 - Step: 295, Training loss: 0.47927325963974
2025-08-04 04:33:25,658 - father_agent.py:386 - Step: 300, Training loss: 0.32083478569984436
2025-08-04 04:33:25,923 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:33:25,926 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:33:36,626 - evaluation_results_class.py:131 - Average Return = -22.424955368041992
2025-08-04 04:33:36,626 - evaluation_results_class.py:133 - Average Virtual Goal Value = 57.575042724609375
2025-08-04 04:33:36,626 - evaluation_results_class.py:135 - Average Discounted Reward = 31.01957130432129
2025-08-04 04:33:36,627 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:33:36,627 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:33:36,627 - evaluation_results_class.py:141 - Variance of Return = 520.9837646484375
2025-08-04 04:33:36,627 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:33:36,627 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:33:36,627 - evaluation_results_class.py:147 - Average Episode Length = 51.105045360496575
2025-08-04 04:33:36,627 - evaluation_results_class.py:149 - Counted Episodes = 6283
2025-08-04 04:33:36,894 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:33:36,907 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:33:41,023 - father_agent.py:386 - Step: 305, Training loss: 0.49902114272117615
2025-08-04 04:33:42,924 - father_agent.py:386 - Step: 310, Training loss: 0.14324720203876495
2025-08-04 04:33:44,838 - father_agent.py:386 - Step: 315, Training loss: 0.8055615425109863
2025-08-04 04:33:46,753 - father_agent.py:386 - Step: 320, Training loss: 0.4473288953304291
2025-08-04 04:33:48,659 - father_agent.py:386 - Step: 325, Training loss: 0.4700636863708496
2025-08-04 04:33:50,565 - father_agent.py:386 - Step: 330, Training loss: 0.4367453455924988
2025-08-04 04:33:52,464 - father_agent.py:386 - Step: 335, Training loss: 0.37543177604675293
2025-08-04 04:33:54,361 - father_agent.py:386 - Step: 340, Training loss: 0.5779762268066406
2025-08-04 04:33:56,262 - father_agent.py:386 - Step: 345, Training loss: 0.43413734436035156
2025-08-04 04:33:58,157 - father_agent.py:386 - Step: 350, Training loss: 0.46381044387817383
2025-08-04 04:34:00,057 - father_agent.py:386 - Step: 355, Training loss: 0.6596553325653076
2025-08-04 04:34:01,934 - father_agent.py:386 - Step: 360, Training loss: 0.43422648310661316
2025-08-04 04:34:03,805 - father_agent.py:386 - Step: 365, Training loss: 0.6576793193817139
2025-08-04 04:34:05,699 - father_agent.py:386 - Step: 370, Training loss: 0.5155125856399536
2025-08-04 04:34:07,588 - father_agent.py:386 - Step: 375, Training loss: 0.34228360652923584
2025-08-04 04:34:09,475 - father_agent.py:386 - Step: 380, Training loss: 0.5264445543289185
2025-08-04 04:34:11,345 - father_agent.py:386 - Step: 385, Training loss: 0.46878933906555176
2025-08-04 04:34:13,249 - father_agent.py:386 - Step: 390, Training loss: 0.3430138826370239
2025-08-04 04:34:15,136 - father_agent.py:386 - Step: 395, Training loss: 0.46725744009017944
2025-08-04 04:34:17,015 - father_agent.py:386 - Step: 400, Training loss: 0.6914396286010742
2025-08-04 04:34:17,285 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:34:17,287 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:34:28,018 - evaluation_results_class.py:131 - Average Return = -24.03879737854004
2025-08-04 04:34:28,019 - evaluation_results_class.py:133 - Average Virtual Goal Value = 55.961204528808594
2025-08-04 04:34:28,019 - evaluation_results_class.py:135 - Average Discounted Reward = 29.55938720703125
2025-08-04 04:34:28,019 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:34:28,019 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:34:28,019 - evaluation_results_class.py:141 - Variance of Return = 658.0731201171875
2025-08-04 04:34:28,019 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:34:28,019 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:34:28,019 - evaluation_results_class.py:147 - Average Episode Length = 50.89279493269992
2025-08-04 04:34:28,019 - evaluation_results_class.py:149 - Counted Episodes = 6315
2025-08-04 04:34:28,290 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:34:28,303 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:34:32,819 - father_agent.py:386 - Step: 405, Training loss: 0.4401334524154663
2025-08-04 04:34:34,699 - father_agent.py:386 - Step: 410, Training loss: 0.3074192702770233
2025-08-04 04:34:36,597 - father_agent.py:386 - Step: 415, Training loss: 0.5554826855659485
2025-08-04 04:34:38,480 - father_agent.py:386 - Step: 420, Training loss: 0.3188134431838989
2025-08-04 04:34:40,410 - father_agent.py:386 - Step: 425, Training loss: 0.4261324107646942
2025-08-04 04:34:42,343 - father_agent.py:386 - Step: 430, Training loss: 0.5394675135612488
2025-08-04 04:34:44,267 - father_agent.py:386 - Step: 435, Training loss: 0.49806925654411316
2025-08-04 04:34:46,208 - father_agent.py:386 - Step: 440, Training loss: 0.3937535583972931
2025-08-04 04:34:48,133 - father_agent.py:386 - Step: 445, Training loss: 0.2588273286819458
2025-08-04 04:34:50,078 - father_agent.py:386 - Step: 450, Training loss: 0.4995356798171997
2025-08-04 04:34:52,059 - father_agent.py:386 - Step: 455, Training loss: 0.7494279146194458
2025-08-04 04:34:54,014 - father_agent.py:386 - Step: 460, Training loss: 0.3582499623298645
2025-08-04 04:34:55,961 - father_agent.py:386 - Step: 465, Training loss: 0.5394202470779419
2025-08-04 04:34:57,886 - father_agent.py:386 - Step: 470, Training loss: 0.40676259994506836
2025-08-04 04:34:59,817 - father_agent.py:386 - Step: 475, Training loss: 0.23516541719436646
2025-08-04 04:35:01,788 - father_agent.py:386 - Step: 480, Training loss: 0.3295222520828247
2025-08-04 04:35:03,666 - father_agent.py:386 - Step: 485, Training loss: 0.25354161858558655
2025-08-04 04:35:05,552 - father_agent.py:386 - Step: 490, Training loss: 0.27405935525894165
2025-08-04 04:35:07,429 - father_agent.py:386 - Step: 495, Training loss: 0.5148875117301941
2025-08-04 04:35:09,300 - father_agent.py:386 - Step: 500, Training loss: 0.8113702535629272
2025-08-04 04:35:09,573 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:35:09,575 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:35:20,166 - evaluation_results_class.py:131 - Average Return = -24.774503707885742
2025-08-04 04:35:20,167 - evaluation_results_class.py:133 - Average Virtual Goal Value = 55.22549819946289
2025-08-04 04:35:20,167 - evaluation_results_class.py:135 - Average Discounted Reward = 28.21880340576172
2025-08-04 04:35:20,167 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:35:20,167 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:35:20,167 - evaluation_results_class.py:141 - Variance of Return = 674.1179809570312
2025-08-04 04:35:20,167 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:35:20,167 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:35:20,167 - evaluation_results_class.py:147 - Average Episode Length = 53.222185430463576
2025-08-04 04:35:20,167 - evaluation_results_class.py:149 - Counted Episodes = 6040
2025-08-04 04:35:20,435 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:35:20,448 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:35:20,565 - father_agent.py:547 - Training finished.
2025-08-04 04:35:20,723 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:35:20,726 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:35:20,729 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 04:35:31,309 - evaluation_results_class.py:131 - Average Return = -24.73060417175293
2025-08-04 04:35:31,309 - evaluation_results_class.py:133 - Average Virtual Goal Value = 55.2693977355957
2025-08-04 04:35:31,309 - evaluation_results_class.py:135 - Average Discounted Reward = 28.36284828186035
2025-08-04 04:35:31,309 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:35:31,309 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:35:31,309 - evaluation_results_class.py:141 - Variance of Return = 673.664306640625
2025-08-04 04:35:31,309 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:35:31,309 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:35:31,309 - evaluation_results_class.py:147 - Average Episode Length = 52.95212941564873
2025-08-04 04:35:31,309 - evaluation_results_class.py:149 - Counted Episodes = 6058
2025-08-04 04:35:31,579 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:35:31,581 - self_interpretable_extractor.py:286 - True
2025-08-04 04:35:31,594 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:35:48,715 - evaluation_results_class.py:131 - Average Return = -24.60222053527832
2025-08-04 04:35:48,716 - evaluation_results_class.py:133 - Average Virtual Goal Value = 55.39777755737305
2025-08-04 04:35:48,716 - evaluation_results_class.py:135 - Average Discounted Reward = 28.263099670410156
2025-08-04 04:35:48,716 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:35:48,716 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:35:48,716 - evaluation_results_class.py:141 - Variance of Return = 640.5619506835938
2025-08-04 04:35:48,716 - evaluation_results_class.py:143 - Current Best Return = -24.60222053527832
2025-08-04 04:35:48,716 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:35:48,716 - evaluation_results_class.py:147 - Average Episode Length = 53.36577400391901
2025-08-04 04:35:48,716 - evaluation_results_class.py:149 - Counted Episodes = 6124
2025-08-04 04:35:48,716 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 04:35:48,716 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 28051 trajectories
Learned trajectory lengths  {128, 131, 134, 137, 140, 143, 146, 149, 152, 26, 155, 29, 158, 32, 161, 35, 164, 38, 167, 41, 170, 44, 173, 47, 50, 179, 53, 56, 59, 188, 62, 65, 68, 71, 200, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125}
Buffer 1
2025-08-04 04:36:59,234 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 56011 trajectories
Learned trajectory lengths  {128, 131, 134, 137, 140, 143, 146, 149, 152, 26, 155, 29, 158, 32, 161, 35, 164, 38, 167, 41, 170, 44, 173, 47, 176, 50, 179, 53, 182, 56, 59, 188, 62, 191, 65, 194, 68, 71, 200, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 242, 116, 119, 122, 125}
Buffer 2
2025-08-04 04:38:10,288 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 84046 trajectories
Learned trajectory lengths  {128, 131, 134, 137, 140, 143, 146, 149, 152, 26, 155, 29, 158, 32, 161, 35, 164, 38, 167, 41, 170, 44, 173, 47, 176, 50, 179, 53, 182, 56, 185, 59, 188, 62, 191, 65, 194, 68, 71, 200, 74, 77, 80, 83, 86, 215, 89, 92, 221, 95, 98, 101, 104, 107, 110, 113, 242, 116, 119, 248, 122, 125}
All trajectories collected
2025-08-04 04:39:21,903 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 04:39:21,903 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 84046 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_6.dot.
Learned FSC of size 2
2025-08-04 04:39:24,732 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 04:39:40,798 - evaluation_results_class.py:131 - Average Return = -88.63408660888672
2025-08-04 04:39:40,798 - evaluation_results_class.py:133 - Average Virtual Goal Value = -8.634085655212402
2025-08-04 04:39:40,798 - evaluation_results_class.py:135 - Average Discounted Reward = -24.49835777282715
2025-08-04 04:39:40,798 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:39:40,798 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:39:40,798 - evaluation_results_class.py:141 - Variance of Return = 5097.36865234375
2025-08-04 04:39:40,798 - evaluation_results_class.py:143 - Current Best Return = -88.63408660888672
2025-08-04 04:39:40,798 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:39:40,798 - evaluation_results_class.py:147 - Average Episode Length = 54.27231994689678
2025-08-04 04:39:40,798 - evaluation_results_class.py:149 - Counted Episodes = 6026
FSC Result: {'best_episode_return': -8.634086, 'best_return': -88.63409, 'goal_value': 0.0, 'returns_episodic': [-8.634086], 'returns': [-88.63409], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [5097.3687], 'each_episode_virtual_variance': [5097.3696], 'combined_variance': [20389.475], 'num_episodes': [6026], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [54.27231994689678], 'counted_episodes': [6026], 'discounted_rewards': [-24.498358], 'new_pomdp_iteration_numbers': []}
[[5], [3, 4, 6, 7], [3, 4, 6, 7], [1], [1], [1], [1], [2], [2], [2], [2], [3, 4, 6, 7], [3, 4, 6, 7], [0]]
2025-08-04 04:39:40,911 - statistic.py:67 - synthesis initiated, design space: 81
2025-08-04 04:39:40,911 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 04:39:40,927 - synthesizer_ar.py:122 - value 68.3428 achieved after 3800.18 seconds
2025-08-04 04:39:40,933 - synthesizer_ar.py:122 - value 70.3811 achieved after 3800.19 seconds
2025-08-04 04:39:40,948 - synthesizer_ar.py:122 - value 75.2622 achieved after 3800.2 seconds
2025-08-04 04:39:40,954 - synthesizer_ar.py:122 - value 76.6217 achieved after 3800.21 seconds
2025-08-04 04:39:40,968 - synthesizer_ar.py:122 - value 83.6595 achieved after 3800.22 seconds
2025-08-04 04:39:40,974 - synthesizer_ar.py:122 - value 84.4914 achieved after 3800.23 seconds
2025-08-04 04:39:41,032 - synthesizer_ar.py:122 - value 87.4379 achieved after 3800.29 seconds
2025-08-04 04:39:41,086 - synthesizer_ar.py:122 - value 89.5194 achieved after 3800.34 seconds
2025-08-04 04:39:41,092 - synthesizer_ar.py:122 - value 90.3678 achieved after 3800.35 seconds
2025-08-04 04:39:41,093 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 04:39:41,093 - synthesizer.py:193 - o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 04:39:41,094 - synthesizer.py:198 - double-checking specification satisfiability:  : 90.3678432977277
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 10) & (y = 10))] 

method: AR, synthesis time: 0.18 s
number of holes: 4, family size: 81, quotient: 1063 states / 1215 actions
explored: 100 %
MDP stats: avg MDP size: 931, iterations: 112

optimum: 90.367843
--------------------
2025-08-04 04:39:41,094 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 04:39:41,098 - robust_rl_trainer.py:432 - Iteration 8 of pure RL loop
2025-08-04 04:39:41,137 - storm_vec_env.py:70 - Computing row map
2025-08-04 04:39:41,142 - storm_vec_env.py:97 - Computing transitions
2025-08-04 04:39:41,155 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 04:39:41,155 - storm_vec_env.py:114 - Computing sinks
2025-08-04 04:39:41,155 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 04:39:41,159 - storm_vec_env.py:143 - Computing labels
2025-08-04 04:39:41,159 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 04:39:41,159 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 04:39:41,159 - storm_vec_env.py:175 - Computing observations
2025-08-04 04:39:41,331 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 04:39:41,332 - father_agent.py:540 - Before training evaluation.
2025-08-04 04:39:41,494 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:39:41,497 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:39:52,044 - evaluation_results_class.py:131 - Average Return = -24.864788055419922
2025-08-04 04:39:52,047 - evaluation_results_class.py:133 - Average Virtual Goal Value = 55.13521194458008
2025-08-04 04:39:52,047 - evaluation_results_class.py:135 - Average Discounted Reward = 28.004804611206055
2025-08-04 04:39:52,047 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:39:52,047 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:39:52,047 - evaluation_results_class.py:141 - Variance of Return = 657.04150390625
2025-08-04 04:39:52,047 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:39:52,047 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:39:52,047 - evaluation_results_class.py:147 - Average Episode Length = 53.552350783594534
2025-08-04 04:39:52,047 - evaluation_results_class.py:149 - Counted Episodes = 5998
2025-08-04 04:39:52,302 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:39:52,314 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:39:52,429 - father_agent.py:436 - Training agent on-policy
2025-08-04 04:40:00,717 - father_agent.py:386 - Step: 0, Training loss: 0.6096569895744324
2025-08-04 04:40:02,601 - father_agent.py:386 - Step: 5, Training loss: 0.34655848145484924
2025-08-04 04:40:04,483 - father_agent.py:386 - Step: 10, Training loss: 0.4753281772136688
2025-08-04 04:40:06,385 - father_agent.py:386 - Step: 15, Training loss: 0.45486682653427124
2025-08-04 04:40:08,287 - father_agent.py:386 - Step: 20, Training loss: 0.390168696641922
2025-08-04 04:40:10,256 - father_agent.py:386 - Step: 25, Training loss: 0.27078503370285034
2025-08-04 04:40:12,232 - father_agent.py:386 - Step: 30, Training loss: 0.2965264916419983
2025-08-04 04:40:14,235 - father_agent.py:386 - Step: 35, Training loss: 0.3552124500274658
2025-08-04 04:40:16,216 - father_agent.py:386 - Step: 40, Training loss: 0.384770005941391
2025-08-04 04:40:18,229 - father_agent.py:386 - Step: 45, Training loss: 0.26655998826026917
2025-08-04 04:40:20,183 - father_agent.py:386 - Step: 50, Training loss: 0.4195876717567444
2025-08-04 04:40:22,142 - father_agent.py:386 - Step: 55, Training loss: 0.18561220169067383
2025-08-04 04:40:24,108 - father_agent.py:386 - Step: 60, Training loss: 0.48543447256088257
2025-08-04 04:40:26,082 - father_agent.py:386 - Step: 65, Training loss: 0.30866262316703796
2025-08-04 04:40:28,092 - father_agent.py:386 - Step: 70, Training loss: 0.28329628705978394
2025-08-04 04:40:30,028 - father_agent.py:386 - Step: 75, Training loss: 0.26344379782676697
2025-08-04 04:40:31,959 - father_agent.py:386 - Step: 80, Training loss: 0.4802173674106598
2025-08-04 04:40:33,901 - father_agent.py:386 - Step: 85, Training loss: 0.40554559230804443
2025-08-04 04:40:35,850 - father_agent.py:386 - Step: 90, Training loss: 0.5715022087097168
2025-08-04 04:40:37,772 - father_agent.py:386 - Step: 95, Training loss: 0.4351026713848114
2025-08-04 04:40:39,649 - father_agent.py:386 - Step: 100, Training loss: 0.20590633153915405
2025-08-04 04:40:39,914 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:40:39,917 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:40:51,119 - evaluation_results_class.py:131 - Average Return = -22.997892379760742
2025-08-04 04:40:51,120 - evaluation_results_class.py:133 - Average Virtual Goal Value = 57.00210952758789
2025-08-04 04:40:51,120 - evaluation_results_class.py:135 - Average Discounted Reward = 29.983354568481445
2025-08-04 04:40:51,120 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:40:51,120 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:40:51,120 - evaluation_results_class.py:141 - Variance of Return = 535.3233642578125
2025-08-04 04:40:51,120 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:40:51,120 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:40:51,120 - evaluation_results_class.py:147 - Average Episode Length = 52.100697259607585
2025-08-04 04:40:51,120 - evaluation_results_class.py:149 - Counted Episodes = 6167
2025-08-04 04:40:51,387 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:40:51,399 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:40:58,844 - father_agent.py:386 - Step: 105, Training loss: 0.6210309267044067
2025-08-04 04:41:00,741 - father_agent.py:386 - Step: 110, Training loss: 0.5624222159385681
2025-08-04 04:41:02,610 - father_agent.py:386 - Step: 115, Training loss: 0.1175987720489502
2025-08-04 04:41:04,485 - father_agent.py:386 - Step: 120, Training loss: 0.30147746205329895
2025-08-04 04:41:06,370 - father_agent.py:386 - Step: 125, Training loss: 0.18747112154960632
2025-08-04 04:41:08,257 - father_agent.py:386 - Step: 130, Training loss: 0.170889750123024
2025-08-04 04:41:10,135 - father_agent.py:386 - Step: 135, Training loss: 0.5666530132293701
2025-08-04 04:41:11,986 - father_agent.py:386 - Step: 140, Training loss: 0.21499872207641602
2025-08-04 04:41:13,831 - father_agent.py:386 - Step: 145, Training loss: 0.35659950971603394
2025-08-04 04:41:15,671 - father_agent.py:386 - Step: 150, Training loss: 0.3263590633869171
2025-08-04 04:41:17,518 - father_agent.py:386 - Step: 155, Training loss: 0.20657578110694885
2025-08-04 04:41:19,395 - father_agent.py:386 - Step: 160, Training loss: 0.3552005887031555
2025-08-04 04:41:21,256 - father_agent.py:386 - Step: 165, Training loss: 0.20847120881080627
2025-08-04 04:41:23,115 - father_agent.py:386 - Step: 170, Training loss: 0.15158171951770782
2025-08-04 04:41:24,959 - father_agent.py:386 - Step: 175, Training loss: 0.282385915517807
2025-08-04 04:41:26,860 - father_agent.py:386 - Step: 180, Training loss: 0.7028045058250427
2025-08-04 04:41:28,772 - father_agent.py:386 - Step: 185, Training loss: 0.34484627842903137
2025-08-04 04:41:30,664 - father_agent.py:386 - Step: 190, Training loss: 0.4958941638469696
2025-08-04 04:41:32,535 - father_agent.py:386 - Step: 195, Training loss: 0.36792391538619995
2025-08-04 04:41:34,438 - father_agent.py:386 - Step: 200, Training loss: 0.363763689994812
2025-08-04 04:41:34,697 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:41:34,702 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:41:45,312 - evaluation_results_class.py:131 - Average Return = -23.516437530517578
2025-08-04 04:41:45,313 - evaluation_results_class.py:133 - Average Virtual Goal Value = 56.48356246948242
2025-08-04 04:41:45,313 - evaluation_results_class.py:135 - Average Discounted Reward = 29.300006866455078
2025-08-04 04:41:45,313 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:41:45,313 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:41:45,313 - evaluation_results_class.py:141 - Variance of Return = 560.7367553710938
2025-08-04 04:41:45,313 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:41:45,313 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:41:45,313 - evaluation_results_class.py:147 - Average Episode Length = 53.15876424913266
2025-08-04 04:41:45,313 - evaluation_results_class.py:149 - Counted Episodes = 6053
2025-08-04 04:41:45,582 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:41:45,594 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:41:49,655 - father_agent.py:386 - Step: 205, Training loss: 0.7534412741661072
2025-08-04 04:41:51,545 - father_agent.py:386 - Step: 210, Training loss: 0.4807230234146118
2025-08-04 04:41:53,444 - father_agent.py:386 - Step: 215, Training loss: 0.24659603834152222
2025-08-04 04:41:55,327 - father_agent.py:386 - Step: 220, Training loss: 0.1567602902650833
2025-08-04 04:41:57,202 - father_agent.py:386 - Step: 225, Training loss: 0.2369154989719391
2025-08-04 04:41:59,107 - father_agent.py:386 - Step: 230, Training loss: 0.8698930144309998
2025-08-04 04:42:01,001 - father_agent.py:386 - Step: 235, Training loss: 0.7579729557037354
2025-08-04 04:42:02,887 - father_agent.py:386 - Step: 240, Training loss: 0.19522517919540405
2025-08-04 04:42:04,771 - father_agent.py:386 - Step: 245, Training loss: 0.38626623153686523
2025-08-04 04:42:06,657 - father_agent.py:386 - Step: 250, Training loss: 0.19250725209712982
2025-08-04 04:42:08,529 - father_agent.py:386 - Step: 255, Training loss: 0.7128134965896606
2025-08-04 04:42:10,442 - father_agent.py:386 - Step: 260, Training loss: 0.46461933851242065
2025-08-04 04:42:12,318 - father_agent.py:386 - Step: 265, Training loss: 0.5331552624702454
2025-08-04 04:42:14,214 - father_agent.py:386 - Step: 270, Training loss: 0.7527962327003479
2025-08-04 04:42:16,094 - father_agent.py:386 - Step: 275, Training loss: 0.38985124230384827
2025-08-04 04:42:17,977 - father_agent.py:386 - Step: 280, Training loss: 0.2657632827758789
2025-08-04 04:42:19,873 - father_agent.py:386 - Step: 285, Training loss: 0.3696092963218689
2025-08-04 04:42:21,765 - father_agent.py:386 - Step: 290, Training loss: 0.2447998821735382
2025-08-04 04:42:23,649 - father_agent.py:386 - Step: 295, Training loss: 0.5243746042251587
2025-08-04 04:42:25,547 - father_agent.py:386 - Step: 300, Training loss: 0.31289640069007874
2025-08-04 04:42:25,816 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:42:25,818 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:42:36,762 - evaluation_results_class.py:131 - Average Return = -22.834447860717773
2025-08-04 04:42:36,762 - evaluation_results_class.py:133 - Average Virtual Goal Value = 57.16555404663086
2025-08-04 04:42:36,762 - evaluation_results_class.py:135 - Average Discounted Reward = 30.374279022216797
2025-08-04 04:42:36,762 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:42:36,762 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:42:36,762 - evaluation_results_class.py:141 - Variance of Return = 542.4730834960938
2025-08-04 04:42:36,762 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:42:36,762 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:42:36,762 - evaluation_results_class.py:147 - Average Episode Length = 51.3282394141993
2025-08-04 04:42:36,762 - evaluation_results_class.py:149 - Counted Episodes = 6282
2025-08-04 04:42:37,030 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:42:37,043 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:42:41,178 - father_agent.py:386 - Step: 305, Training loss: 0.7877419590950012
2025-08-04 04:42:43,084 - father_agent.py:386 - Step: 310, Training loss: 0.3986909091472626
2025-08-04 04:42:44,969 - father_agent.py:386 - Step: 315, Training loss: 0.4110330045223236
2025-08-04 04:42:46,839 - father_agent.py:386 - Step: 320, Training loss: 0.5059632062911987
2025-08-04 04:42:48,711 - father_agent.py:386 - Step: 325, Training loss: 0.2637940049171448
2025-08-04 04:42:50,590 - father_agent.py:386 - Step: 330, Training loss: 0.4510851800441742
2025-08-04 04:42:52,467 - father_agent.py:386 - Step: 335, Training loss: 0.46552345156669617
2025-08-04 04:42:54,349 - father_agent.py:386 - Step: 340, Training loss: 0.3794208765029907
2025-08-04 04:42:56,229 - father_agent.py:386 - Step: 345, Training loss: 0.33744561672210693
2025-08-04 04:42:58,112 - father_agent.py:386 - Step: 350, Training loss: 0.47450703382492065
2025-08-04 04:43:00,009 - father_agent.py:386 - Step: 355, Training loss: 0.1991000473499298
2025-08-04 04:43:01,902 - father_agent.py:386 - Step: 360, Training loss: 0.1902408003807068
2025-08-04 04:43:03,772 - father_agent.py:386 - Step: 365, Training loss: 0.28127986192703247
2025-08-04 04:43:05,644 - father_agent.py:386 - Step: 370, Training loss: 0.22847528755664825
2025-08-04 04:43:07,532 - father_agent.py:386 - Step: 375, Training loss: 0.41665950417518616
2025-08-04 04:43:09,413 - father_agent.py:386 - Step: 380, Training loss: 0.23990115523338318
2025-08-04 04:43:11,298 - father_agent.py:386 - Step: 385, Training loss: 0.17322930693626404
2025-08-04 04:43:13,195 - father_agent.py:386 - Step: 390, Training loss: 0.39159727096557617
2025-08-04 04:43:15,085 - father_agent.py:386 - Step: 395, Training loss: 0.4161544144153595
2025-08-04 04:43:16,984 - father_agent.py:386 - Step: 400, Training loss: 0.4281696677207947
2025-08-04 04:43:17,256 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:43:17,259 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:43:27,948 - evaluation_results_class.py:131 - Average Return = -21.258909225463867
2025-08-04 04:43:27,948 - evaluation_results_class.py:133 - Average Virtual Goal Value = 58.741092681884766
2025-08-04 04:43:27,948 - evaluation_results_class.py:135 - Average Discounted Reward = 31.74931526184082
2025-08-04 04:43:27,948 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:43:27,948 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:43:27,948 - evaluation_results_class.py:141 - Variance of Return = 416.0855712890625
2025-08-04 04:43:27,948 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:43:27,948 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:43:27,948 - evaluation_results_class.py:147 - Average Episode Length = 50.82276884263639
2025-08-04 04:43:27,948 - evaluation_results_class.py:149 - Counted Episodes = 6342
2025-08-04 04:43:28,218 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:43:28,230 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:43:32,374 - father_agent.py:386 - Step: 405, Training loss: 0.6434736251831055
2025-08-04 04:43:34,265 - father_agent.py:386 - Step: 410, Training loss: 0.46521252393722534
2025-08-04 04:43:36,151 - father_agent.py:386 - Step: 415, Training loss: 0.34329262375831604
2025-08-04 04:43:38,027 - father_agent.py:386 - Step: 420, Training loss: 0.3172435760498047
2025-08-04 04:43:39,962 - father_agent.py:386 - Step: 425, Training loss: 0.5360444188117981
2025-08-04 04:43:41,879 - father_agent.py:386 - Step: 430, Training loss: 0.171921506524086
2025-08-04 04:43:43,850 - father_agent.py:386 - Step: 435, Training loss: 0.5952736735343933
2025-08-04 04:43:45,784 - father_agent.py:386 - Step: 440, Training loss: 0.35241860151290894
2025-08-04 04:43:47,717 - father_agent.py:386 - Step: 445, Training loss: 0.32417792081832886
2025-08-04 04:43:49,658 - father_agent.py:386 - Step: 450, Training loss: 0.30680710077285767
2025-08-04 04:43:51,591 - father_agent.py:386 - Step: 455, Training loss: 0.5664690732955933
2025-08-04 04:43:53,560 - father_agent.py:386 - Step: 460, Training loss: 0.33167320489883423
2025-08-04 04:43:55,476 - father_agent.py:386 - Step: 465, Training loss: 0.41041597723960876
2025-08-04 04:43:57,358 - father_agent.py:386 - Step: 470, Training loss: 0.3133164644241333
2025-08-04 04:43:59,250 - father_agent.py:386 - Step: 475, Training loss: 0.47575607895851135
2025-08-04 04:44:01,142 - father_agent.py:386 - Step: 480, Training loss: 0.5133829116821289
2025-08-04 04:44:03,023 - father_agent.py:386 - Step: 485, Training loss: 0.3123949468135834
2025-08-04 04:44:04,900 - father_agent.py:386 - Step: 490, Training loss: 0.24632230401039124
2025-08-04 04:44:06,768 - father_agent.py:386 - Step: 495, Training loss: 0.7518904805183411
2025-08-04 04:44:08,643 - father_agent.py:386 - Step: 500, Training loss: 0.2120654135942459
2025-08-04 04:44:08,917 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:44:08,920 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:44:19,817 - evaluation_results_class.py:131 - Average Return = -21.494003295898438
2025-08-04 04:44:19,817 - evaluation_results_class.py:133 - Average Virtual Goal Value = 58.50599670410156
2025-08-04 04:44:19,817 - evaluation_results_class.py:135 - Average Discounted Reward = 31.536365509033203
2025-08-04 04:44:19,817 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:44:19,817 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:44:19,817 - evaluation_results_class.py:141 - Variance of Return = 430.4737243652344
2025-08-04 04:44:19,817 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:44:19,817 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:44:19,817 - evaluation_results_class.py:147 - Average Episode Length = 50.85385101010101
2025-08-04 04:44:19,818 - evaluation_results_class.py:149 - Counted Episodes = 6336
2025-08-04 04:44:20,087 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:44:20,099 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:44:20,217 - father_agent.py:547 - Training finished.
2025-08-04 04:44:20,376 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:44:20,379 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:44:20,382 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 04:44:31,013 - evaluation_results_class.py:131 - Average Return = -21.908475875854492
2025-08-04 04:44:31,013 - evaluation_results_class.py:133 - Average Virtual Goal Value = 58.09152603149414
2025-08-04 04:44:31,013 - evaluation_results_class.py:135 - Average Discounted Reward = 31.24922752380371
2025-08-04 04:44:31,014 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:44:31,014 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:44:31,014 - evaluation_results_class.py:141 - Variance of Return = 482.1930847167969
2025-08-04 04:44:31,014 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:44:31,014 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:44:31,014 - evaluation_results_class.py:147 - Average Episode Length = 50.7978890989288
2025-08-04 04:44:31,014 - evaluation_results_class.py:149 - Counted Episodes = 6348
2025-08-04 04:44:31,282 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:44:31,284 - self_interpretable_extractor.py:286 - True
2025-08-04 04:44:31,297 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:44:48,515 - evaluation_results_class.py:131 - Average Return = -21.615949630737305
2025-08-04 04:44:48,515 - evaluation_results_class.py:133 - Average Virtual Goal Value = 58.38404846191406
2025-08-04 04:44:48,515 - evaluation_results_class.py:135 - Average Discounted Reward = 31.337909698486328
2025-08-04 04:44:48,516 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:44:48,516 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:44:48,516 - evaluation_results_class.py:141 - Variance of Return = 449.7605285644531
2025-08-04 04:44:48,516 - evaluation_results_class.py:143 - Current Best Return = -21.615949630737305
2025-08-04 04:44:48,516 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:44:48,516 - evaluation_results_class.py:147 - Average Episode Length = 51.14167318217358
2025-08-04 04:44:48,516 - evaluation_results_class.py:149 - Counted Episodes = 6395
2025-08-04 04:44:48,516 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 04:44:48,516 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 29325 trajectories
Learned trajectory lengths  {128, 131, 134, 137, 140, 143, 146, 149, 23, 152, 26, 155, 29, 158, 32, 161, 35, 38, 41, 44, 173, 47, 50, 53, 182, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125}
Buffer 1
2025-08-04 04:45:58,659 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 58559 trajectories
Learned trajectory lengths  {128, 131, 134, 137, 140, 143, 146, 149, 23, 152, 26, 155, 29, 158, 32, 161, 35, 164, 38, 41, 44, 173, 47, 50, 53, 182, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125}
Buffer 2
2025-08-04 04:47:08,561 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 87729 trajectories
Learned trajectory lengths  {128, 131, 134, 137, 140, 143, 146, 149, 23, 152, 26, 155, 29, 158, 32, 161, 35, 164, 38, 41, 44, 173, 47, 176, 50, 53, 182, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125}
All trajectories collected
2025-08-04 04:48:18,716 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 04:48:18,716 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 87729 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_7.dot.
Learned FSC of size 2
2025-08-04 04:48:22,365 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 04:48:38,297 - evaluation_results_class.py:131 - Average Return = -81.79454803466797
2025-08-04 04:48:38,297 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1.7945469617843628
2025-08-04 04:48:38,297 - evaluation_results_class.py:135 - Average Discounted Reward = -18.412309646606445
2025-08-04 04:48:38,297 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:48:38,297 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:48:38,297 - evaluation_results_class.py:141 - Variance of Return = 5233.67236328125
2025-08-04 04:48:38,297 - evaluation_results_class.py:143 - Current Best Return = -81.79454803466797
2025-08-04 04:48:38,297 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:48:38,297 - evaluation_results_class.py:147 - Average Episode Length = 52.45805934242181
2025-08-04 04:48:38,297 - evaluation_results_class.py:149 - Counted Episodes = 6235
FSC Result: {'best_episode_return': -1.794547, 'best_return': -81.79455, 'goal_value': 0.0, 'returns_episodic': [-1.794547], 'returns': [-81.79455], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [5233.6724], 'each_episode_virtual_variance': [5233.673], 'combined_variance': [20934.69], 'num_episodes': [6235], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [52.45805934242181], 'counted_episodes': [6235], 'discounted_rewards': [-18.41231], 'new_pomdp_iteration_numbers': []}
[[5], [3, 4, 6, 7], [3, 4, 6, 7], [1], [1], [1], [1], [2], [2], [2], [2], [3, 4, 6, 7], [3, 4, 6, 7], [0]]
2025-08-04 04:48:38,408 - statistic.py:67 - synthesis initiated, design space: 81
2025-08-04 04:48:38,408 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 04:48:38,424 - synthesizer_ar.py:122 - value 62.5869 achieved after 4337.68 seconds
2025-08-04 04:48:38,445 - synthesizer_ar.py:122 - value 69.5245 achieved after 4337.7 seconds
2025-08-04 04:48:38,466 - synthesizer_ar.py:122 - value 78.5359 achieved after 4337.72 seconds
2025-08-04 04:48:38,528 - synthesizer_ar.py:122 - value 79.7223 achieved after 4337.78 seconds
2025-08-04 04:48:38,535 - synthesizer_ar.py:122 - value 80.7556 achieved after 4337.79 seconds
2025-08-04 04:48:38,597 - synthesizer_ar.py:122 - value 84.0401 achieved after 4337.85 seconds
2025-08-04 04:48:38,604 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 04:48:38,604 - synthesizer.py:193 - o1x=3, o1y=5, o2x=6, o2y=7
2025-08-04 04:48:38,605 - synthesizer.py:198 - double-checking specification satisfiability:  : 84.04011045951916
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 10) & (y = 10))] 

method: AR, synthesis time: 0.2 s
number of holes: 4, family size: 81, quotient: 1018 states / 1170 actions
explored: 100 %
MDP stats: avg MDP size: 909, iterations: 115

optimum: 84.04011
--------------------
2025-08-04 04:48:38,605 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=7
2025-08-04 04:48:38,609 - robust_rl_trainer.py:432 - Iteration 9 of pure RL loop
2025-08-04 04:48:38,647 - storm_vec_env.py:70 - Computing row map
2025-08-04 04:48:38,652 - storm_vec_env.py:97 - Computing transitions
2025-08-04 04:48:38,665 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 04:48:38,665 - storm_vec_env.py:114 - Computing sinks
2025-08-04 04:48:38,665 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 04:48:38,669 - storm_vec_env.py:143 - Computing labels
2025-08-04 04:48:38,669 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 04:48:38,669 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 04:48:38,669 - storm_vec_env.py:175 - Computing observations
2025-08-04 04:48:38,849 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 04:48:38,850 - father_agent.py:540 - Before training evaluation.
2025-08-04 04:48:39,016 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:48:39,019 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:48:49,525 - evaluation_results_class.py:131 - Average Return = -27.855148315429688
2025-08-04 04:48:49,525 - evaluation_results_class.py:133 - Average Virtual Goal Value = 52.14485168457031
2025-08-04 04:48:49,525 - evaluation_results_class.py:135 - Average Discounted Reward = 26.819866180419922
2025-08-04 04:48:49,525 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:48:49,525 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:48:49,525 - evaluation_results_class.py:141 - Variance of Return = 1017.1524658203125
2025-08-04 04:48:49,525 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:48:49,525 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:48:49,525 - evaluation_results_class.py:147 - Average Episode Length = 50.65834902699309
2025-08-04 04:48:49,525 - evaluation_results_class.py:149 - Counted Episodes = 6372
2025-08-04 04:48:49,788 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:48:49,800 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:48:49,918 - father_agent.py:436 - Training agent on-policy
2025-08-04 04:48:58,963 - father_agent.py:386 - Step: 0, Training loss: 0.8685671091079712
2025-08-04 04:49:00,827 - father_agent.py:386 - Step: 5, Training loss: 0.853984534740448
2025-08-04 04:49:02,655 - father_agent.py:386 - Step: 10, Training loss: 1.0356968641281128
2025-08-04 04:49:04,496 - father_agent.py:386 - Step: 15, Training loss: 0.722372829914093
2025-08-04 04:49:06,342 - father_agent.py:386 - Step: 20, Training loss: 0.6697667837142944
2025-08-04 04:49:08,179 - father_agent.py:386 - Step: 25, Training loss: 0.8553319573402405
2025-08-04 04:49:10,068 - father_agent.py:386 - Step: 30, Training loss: 0.7903429269790649
2025-08-04 04:49:11,948 - father_agent.py:386 - Step: 35, Training loss: 0.37745600938796997
2025-08-04 04:49:13,809 - father_agent.py:386 - Step: 40, Training loss: 0.36070534586906433
2025-08-04 04:49:15,668 - father_agent.py:386 - Step: 45, Training loss: 0.7886801958084106
2025-08-04 04:49:17,531 - father_agent.py:386 - Step: 50, Training loss: 1.2894611358642578
2025-08-04 04:49:19,403 - father_agent.py:386 - Step: 55, Training loss: 0.2703196108341217
2025-08-04 04:49:21,284 - father_agent.py:386 - Step: 60, Training loss: 0.19928160309791565
2025-08-04 04:49:23,181 - father_agent.py:386 - Step: 65, Training loss: 0.9447501301765442
2025-08-04 04:49:25,035 - father_agent.py:386 - Step: 70, Training loss: 0.2916259169578552
2025-08-04 04:49:26,888 - father_agent.py:386 - Step: 75, Training loss: 0.9536700248718262
2025-08-04 04:49:28,769 - father_agent.py:386 - Step: 80, Training loss: 0.7141861915588379
2025-08-04 04:49:30,685 - father_agent.py:386 - Step: 85, Training loss: 0.39787495136260986
2025-08-04 04:49:32,623 - father_agent.py:386 - Step: 90, Training loss: 0.47731056809425354
2025-08-04 04:49:34,552 - father_agent.py:386 - Step: 95, Training loss: 0.9097017049789429
2025-08-04 04:49:36,508 - father_agent.py:386 - Step: 100, Training loss: 0.7628884315490723
2025-08-04 04:49:36,775 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:49:36,777 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:49:47,809 - evaluation_results_class.py:131 - Average Return = -31.859596252441406
2025-08-04 04:49:47,809 - evaluation_results_class.py:133 - Average Virtual Goal Value = 48.140403747558594
2025-08-04 04:49:47,809 - evaluation_results_class.py:135 - Average Discounted Reward = 22.240140914916992
2025-08-04 04:49:47,809 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 04:49:47,809 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:49:47,809 - evaluation_results_class.py:141 - Variance of Return = 1128.5274658203125
2025-08-04 04:49:47,809 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:49:47,809 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:49:47,809 - evaluation_results_class.py:147 - Average Episode Length = 71.4574317968015
2025-08-04 04:49:47,809 - evaluation_results_class.py:149 - Counted Episodes = 4252
2025-08-04 04:49:48,189 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:49:48,208 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:49:55,759 - father_agent.py:386 - Step: 105, Training loss: 0.8377857208251953
2025-08-04 04:49:57,690 - father_agent.py:386 - Step: 110, Training loss: 0.4235383868217468
2025-08-04 04:49:59,592 - father_agent.py:386 - Step: 115, Training loss: 0.5089174509048462
2025-08-04 04:50:01,493 - father_agent.py:386 - Step: 120, Training loss: 0.3354003429412842
2025-08-04 04:50:03,374 - father_agent.py:386 - Step: 125, Training loss: 0.5931991338729858
2025-08-04 04:50:05,252 - father_agent.py:386 - Step: 130, Training loss: 0.3458895683288574
2025-08-04 04:50:07,157 - father_agent.py:386 - Step: 135, Training loss: 0.3605481684207916
2025-08-04 04:50:09,027 - father_agent.py:386 - Step: 140, Training loss: 0.5279893279075623
2025-08-04 04:50:10,916 - father_agent.py:386 - Step: 145, Training loss: 0.607090950012207
2025-08-04 04:50:12,814 - father_agent.py:386 - Step: 150, Training loss: 0.5368613600730896
2025-08-04 04:50:14,704 - father_agent.py:386 - Step: 155, Training loss: 0.32407456636428833
2025-08-04 04:50:16,597 - father_agent.py:386 - Step: 160, Training loss: 0.4835795760154724
2025-08-04 04:50:18,513 - father_agent.py:386 - Step: 165, Training loss: 0.6242630481719971
2025-08-04 04:50:20,398 - father_agent.py:386 - Step: 170, Training loss: 0.5112993121147156
2025-08-04 04:50:22,286 - father_agent.py:386 - Step: 175, Training loss: 0.6613482236862183
2025-08-04 04:50:24,180 - father_agent.py:386 - Step: 180, Training loss: 0.8534576296806335
2025-08-04 04:50:26,066 - father_agent.py:386 - Step: 185, Training loss: 0.7426025867462158
2025-08-04 04:50:27,971 - father_agent.py:386 - Step: 190, Training loss: 0.34047752618789673
2025-08-04 04:50:29,864 - father_agent.py:386 - Step: 195, Training loss: 0.4636762738227844
2025-08-04 04:50:31,758 - father_agent.py:386 - Step: 200, Training loss: 0.5666069984436035
2025-08-04 04:50:32,031 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:50:32,034 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:50:42,537 - evaluation_results_class.py:131 - Average Return = -49.448665618896484
2025-08-04 04:50:42,537 - evaluation_results_class.py:133 - Average Virtual Goal Value = 30.06376075744629
2025-08-04 04:50:42,537 - evaluation_results_class.py:135 - Average Discounted Reward = 7.987688064575195
2025-08-04 04:50:42,537 - evaluation_results_class.py:137 - Goal Reach Probability = 0.993905297702766
2025-08-04 04:50:42,537 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:50:42,537 - evaluation_results_class.py:141 - Variance of Return = 2208.082275390625
2025-08-04 04:50:42,538 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:50:42,538 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:50:42,538 - evaluation_results_class.py:147 - Average Episode Length = 124.60853258321613
2025-08-04 04:50:42,538 - evaluation_results_class.py:149 - Counted Episodes = 2133
2025-08-04 04:50:42,813 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:50:42,826 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:50:47,287 - father_agent.py:386 - Step: 205, Training loss: 0.6369727253913879
2025-08-04 04:50:49,164 - father_agent.py:386 - Step: 210, Training loss: 0.2844823896884918
2025-08-04 04:50:51,076 - father_agent.py:386 - Step: 215, Training loss: 0.5409882068634033
2025-08-04 04:50:52,957 - father_agent.py:386 - Step: 220, Training loss: 0.6188907027244568
2025-08-04 04:50:54,853 - father_agent.py:386 - Step: 225, Training loss: 0.5564525127410889
2025-08-04 04:50:56,743 - father_agent.py:386 - Step: 230, Training loss: 0.4655028283596039
2025-08-04 04:50:58,645 - father_agent.py:386 - Step: 235, Training loss: 0.5657761096954346
2025-08-04 04:51:00,541 - father_agent.py:386 - Step: 240, Training loss: 0.9552330374717712
2025-08-04 04:51:02,421 - father_agent.py:386 - Step: 245, Training loss: 0.5898420810699463
2025-08-04 04:51:04,302 - father_agent.py:386 - Step: 250, Training loss: 0.7686903476715088
2025-08-04 04:51:06,191 - father_agent.py:386 - Step: 255, Training loss: 0.5484464168548584
2025-08-04 04:51:08,051 - father_agent.py:386 - Step: 260, Training loss: 0.5746194124221802
2025-08-04 04:51:09,943 - father_agent.py:386 - Step: 265, Training loss: 0.6271452307701111
2025-08-04 04:51:11,846 - father_agent.py:386 - Step: 270, Training loss: 0.5237231850624084
2025-08-04 04:51:13,742 - father_agent.py:386 - Step: 275, Training loss: 0.8282596468925476
2025-08-04 04:51:15,624 - father_agent.py:386 - Step: 280, Training loss: 0.4083852767944336
2025-08-04 04:51:17,501 - father_agent.py:386 - Step: 285, Training loss: 0.7039956450462341
2025-08-04 04:51:19,373 - father_agent.py:386 - Step: 290, Training loss: 0.3334757685661316
2025-08-04 04:51:21,256 - father_agent.py:386 - Step: 295, Training loss: 0.5217507481575012
2025-08-04 04:51:23,148 - father_agent.py:386 - Step: 300, Training loss: 0.5605313777923584
2025-08-04 04:51:23,422 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:51:23,425 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:51:33,851 - evaluation_results_class.py:131 - Average Return = -55.34797286987305
2025-08-04 04:51:33,851 - evaluation_results_class.py:133 - Average Virtual Goal Value = 23.570945739746094
2025-08-04 04:51:33,851 - evaluation_results_class.py:135 - Average Discounted Reward = 5.645637512207031
2025-08-04 04:51:33,852 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9864864864864865
2025-08-04 04:51:33,852 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:51:33,852 - evaluation_results_class.py:141 - Variance of Return = 2851.830322265625
2025-08-04 04:51:33,852 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:51:33,852 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:51:33,852 - evaluation_results_class.py:147 - Average Episode Length = 142.13400900900902
2025-08-04 04:51:33,852 - evaluation_results_class.py:149 - Counted Episodes = 1776
2025-08-04 04:51:34,133 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:51:34,145 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:51:38,319 - father_agent.py:386 - Step: 305, Training loss: 0.9059865474700928
2025-08-04 04:51:40,201 - father_agent.py:386 - Step: 310, Training loss: 0.3931886851787567
2025-08-04 04:51:42,063 - father_agent.py:386 - Step: 315, Training loss: 0.5756059288978577
2025-08-04 04:51:43,934 - father_agent.py:386 - Step: 320, Training loss: 0.7197128534317017
2025-08-04 04:51:45,805 - father_agent.py:386 - Step: 325, Training loss: 0.9085149765014648
2025-08-04 04:51:47,685 - father_agent.py:386 - Step: 330, Training loss: 0.6632205843925476
2025-08-04 04:51:49,557 - father_agent.py:386 - Step: 335, Training loss: 0.42744630575180054
2025-08-04 04:51:51,428 - father_agent.py:386 - Step: 340, Training loss: 0.6104103922843933
2025-08-04 04:51:53,289 - father_agent.py:386 - Step: 345, Training loss: 0.6467171311378479
2025-08-04 04:51:55,153 - father_agent.py:386 - Step: 350, Training loss: 0.5229747891426086
2025-08-04 04:51:57,017 - father_agent.py:386 - Step: 355, Training loss: 0.4622815251350403
2025-08-04 04:51:58,898 - father_agent.py:386 - Step: 360, Training loss: 0.36089837551116943
2025-08-04 04:52:00,758 - father_agent.py:386 - Step: 365, Training loss: 0.5694745182991028
2025-08-04 04:52:02,618 - father_agent.py:386 - Step: 370, Training loss: 0.9824879765510559
2025-08-04 04:52:04,478 - father_agent.py:386 - Step: 375, Training loss: 0.2582134008407593
2025-08-04 04:52:06,355 - father_agent.py:386 - Step: 380, Training loss: 0.4770638942718506
2025-08-04 04:52:08,234 - father_agent.py:386 - Step: 385, Training loss: 0.5647194981575012
2025-08-04 04:52:10,127 - father_agent.py:386 - Step: 390, Training loss: 0.44460391998291016
2025-08-04 04:52:11,999 - father_agent.py:386 - Step: 395, Training loss: 0.6705648303031921
2025-08-04 04:52:13,878 - father_agent.py:386 - Step: 400, Training loss: 0.8008029460906982
2025-08-04 04:52:14,154 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:52:14,157 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:52:24,700 - evaluation_results_class.py:131 - Average Return = -64.68428802490234
2025-08-04 04:52:24,700 - evaluation_results_class.py:133 - Average Virtual Goal Value = 12.687143325805664
2025-08-04 04:52:24,700 - evaluation_results_class.py:135 - Average Discounted Reward = 3.4581298828125
2025-08-04 04:52:24,700 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9671428571428572
2025-08-04 04:52:24,700 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:52:24,700 - evaluation_results_class.py:141 - Variance of Return = 4182.1630859375
2025-08-04 04:52:24,700 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:52:24,700 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:52:24,700 - evaluation_results_class.py:147 - Average Episode Length = 165.50571428571428
2025-08-04 04:52:24,700 - evaluation_results_class.py:149 - Counted Episodes = 1400
2025-08-04 04:52:24,977 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:52:24,989 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:52:29,113 - father_agent.py:386 - Step: 405, Training loss: 1.004974126815796
2025-08-04 04:52:30,986 - father_agent.py:386 - Step: 410, Training loss: 0.3714277148246765
2025-08-04 04:52:32,892 - father_agent.py:386 - Step: 415, Training loss: 0.210093691945076
2025-08-04 04:52:34,781 - father_agent.py:386 - Step: 420, Training loss: 0.5694802403450012
2025-08-04 04:52:36,662 - father_agent.py:386 - Step: 425, Training loss: 0.7113798260688782
2025-08-04 04:52:38,529 - father_agent.py:386 - Step: 430, Training loss: 0.48036935925483704
2025-08-04 04:52:40,403 - father_agent.py:386 - Step: 435, Training loss: 0.3940610885620117
2025-08-04 04:52:42,315 - father_agent.py:386 - Step: 440, Training loss: 0.3340340852737427
2025-08-04 04:52:44,202 - father_agent.py:386 - Step: 445, Training loss: 0.5449065566062927
2025-08-04 04:52:46,076 - father_agent.py:386 - Step: 450, Training loss: 0.5079700946807861
2025-08-04 04:52:47,980 - father_agent.py:386 - Step: 455, Training loss: 0.36783868074417114
2025-08-04 04:52:49,860 - father_agent.py:386 - Step: 460, Training loss: 0.49519798159599304
2025-08-04 04:52:51,730 - father_agent.py:386 - Step: 465, Training loss: 0.39773693680763245
2025-08-04 04:52:53,606 - father_agent.py:386 - Step: 470, Training loss: 0.4526861906051636
2025-08-04 04:52:55,483 - father_agent.py:386 - Step: 475, Training loss: 0.5341922640800476
2025-08-04 04:52:57,356 - father_agent.py:386 - Step: 480, Training loss: 0.7232692837715149
2025-08-04 04:52:59,220 - father_agent.py:386 - Step: 485, Training loss: 0.21755382418632507
2025-08-04 04:53:01,112 - father_agent.py:386 - Step: 490, Training loss: 0.7927073836326599
2025-08-04 04:53:02,992 - father_agent.py:386 - Step: 495, Training loss: 0.3833763301372528
2025-08-04 04:53:04,867 - father_agent.py:386 - Step: 500, Training loss: 0.7897626161575317
2025-08-04 04:53:05,148 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:53:05,151 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:53:15,885 - evaluation_results_class.py:131 - Average Return = -59.06705856323242
2025-08-04 04:53:15,885 - evaluation_results_class.py:133 - Average Virtual Goal Value = 19.21419334411621
2025-08-04 04:53:15,886 - evaluation_results_class.py:135 - Average Discounted Reward = 6.09846830368042
2025-08-04 04:53:15,886 - evaluation_results_class.py:137 - Goal Reach Probability = 0.978515625
2025-08-04 04:53:15,886 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:53:15,886 - evaluation_results_class.py:141 - Variance of Return = 3642.798095703125
2025-08-04 04:53:15,886 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:53:15,886 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:53:15,886 - evaluation_results_class.py:147 - Average Episode Length = 151.67057291666666
2025-08-04 04:53:15,886 - evaluation_results_class.py:149 - Counted Episodes = 1536
2025-08-04 04:53:16,165 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:53:16,177 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:53:16,296 - father_agent.py:547 - Training finished.
2025-08-04 04:53:16,452 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:53:16,455 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:53:16,458 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 04:53:26,991 - evaluation_results_class.py:131 - Average Return = -61.6338996887207
2025-08-04 04:53:26,992 - evaluation_results_class.py:133 - Average Virtual Goal Value = 15.003389358520508
2025-08-04 04:53:26,992 - evaluation_results_class.py:135 - Average Discounted Reward = 6.02189302444458
2025-08-04 04:53:26,992 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9579661016949153
2025-08-04 04:53:26,992 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:53:26,992 - evaluation_results_class.py:141 - Variance of Return = 4093.19384765625
2025-08-04 04:53:26,992 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:53:26,992 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:53:26,992 - evaluation_results_class.py:147 - Average Episode Length = 160.81694915254238
2025-08-04 04:53:26,992 - evaluation_results_class.py:149 - Counted Episodes = 1475
2025-08-04 04:53:27,273 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:53:27,275 - self_interpretable_extractor.py:286 - True
2025-08-04 04:53:27,288 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:53:44,038 - evaluation_results_class.py:131 - Average Return = -72.04368591308594
2025-08-04 04:53:44,038 - evaluation_results_class.py:133 - Average Virtual Goal Value = 2.49556303024292
2025-08-04 04:53:44,038 - evaluation_results_class.py:135 - Average Discounted Reward = 0.9023160934448242
2025-08-04 04:53:44,039 - evaluation_results_class.py:137 - Goal Reach Probability = 0.931740614334471
2025-08-04 04:53:44,039 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:53:44,039 - evaluation_results_class.py:141 - Variance of Return = 4844.90869140625
2025-08-04 04:53:44,039 - evaluation_results_class.py:143 - Current Best Return = -72.04368591308594
2025-08-04 04:53:44,039 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.931740614334471
2025-08-04 04:53:44,039 - evaluation_results_class.py:147 - Average Episode Length = 190.65597269624573
2025-08-04 04:53:44,039 - evaluation_results_class.py:149 - Counted Episodes = 1465
2025-08-04 04:53:44,039 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 04:53:44,039 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 7112 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
Buffer 1
2025-08-04 04:54:54,531 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 14109 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 23, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
Buffer 2
2025-08-04 04:56:04,784 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 21282 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 23, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
All trajectories collected
2025-08-04 04:57:15,433 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 04:57:15,433 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 21282 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_8.dot.
Learned FSC of size 2
2025-08-04 04:57:21,409 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 04:57:37,195 - evaluation_results_class.py:131 - Average Return = -131.13095092773438
2025-08-04 04:57:37,195 - evaluation_results_class.py:133 - Average Virtual Goal Value = -51.183860778808594
2025-08-04 04:57:37,195 - evaluation_results_class.py:135 - Average Discounted Reward = -62.504207611083984
2025-08-04 04:57:37,195 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9993386243386243
2025-08-04 04:57:37,195 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:57:37,195 - evaluation_results_class.py:141 - Variance of Return = 5558.48388671875
2025-08-04 04:57:37,195 - evaluation_results_class.py:143 - Current Best Return = -131.13095092773438
2025-08-04 04:57:37,195 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9993386243386243
2025-08-04 04:57:37,195 - evaluation_results_class.py:147 - Average Episode Length = 195.00198412698413
2025-08-04 04:57:37,195 - evaluation_results_class.py:149 - Counted Episodes = 1512
FSC Result: {'best_episode_return': -51.18386, 'best_return': -131.13095, 'goal_value': 0.0, 'returns_episodic': [-51.18386], 'returns': [-131.13095], 'reach_probs': [0.9993386243386243], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.9993386243386243, 'losses': [], 'best_updated': True, 'each_episode_variance': [5558.484], 'each_episode_virtual_variance': [5582.383], 'combined_variance': [22277.502], 'num_episodes': [1512], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [195.00198412698413], 'counted_episodes': [1512], 'discounted_rewards': [-62.504208], 'new_pomdp_iteration_numbers': []}
[[5], [3, 4, 6, 7], [3, 4, 6, 7], [1], [1], [1], [1], [2], [2], [2], [2], [3, 4, 6, 7], [3, 4, 6, 7], [0]]
2025-08-04 04:57:37,288 - statistic.py:67 - synthesis initiated, design space: 81
2025-08-04 04:57:37,288 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 04:57:37,306 - synthesizer_ar.py:122 - value 110.5193 achieved after 4876.56 seconds
2025-08-04 04:57:37,314 - synthesizer_ar.py:122 - value 121.6176 achieved after 4876.57 seconds
2025-08-04 04:57:37,322 - synthesizer_ar.py:122 - value 137.9063 achieved after 4876.58 seconds
2025-08-04 04:57:37,337 - synthesizer_ar.py:122 - value 138.5994 achieved after 4876.59 seconds
2025-08-04 04:57:37,351 - synthesizer_ar.py:122 - value 139.3122 achieved after 4876.61 seconds
2025-08-04 04:57:37,369 - synthesizer_ar.py:122 - value 141.2336 achieved after 4876.63 seconds
2025-08-04 04:57:37,385 - synthesizer_ar.py:122 - value 141.797 achieved after 4876.64 seconds
2025-08-04 04:57:37,400 - synthesizer_ar.py:122 - value 142.3455 achieved after 4876.66 seconds
2025-08-04 04:57:37,417 - synthesizer_ar.py:122 - value 146.1554 achieved after 4876.67 seconds
2025-08-04 04:57:37,431 - synthesizer_ar.py:122 - value 146.6089 achieved after 4876.69 seconds
2025-08-04 04:57:37,445 - synthesizer_ar.py:122 - value 147.0438 achieved after 4876.7 seconds
2025-08-04 04:57:37,447 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 04:57:37,447 - synthesizer.py:193 - o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 04:57:37,448 - synthesizer.py:198 - double-checking specification satisfiability:  : 147.04380916088283
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 10) & (y = 10))] 

method: AR, synthesis time: 0.16 s
number of holes: 4, family size: 81, quotient: 1018 states / 1170 actions
explored: 100 %
MDP stats: avg MDP size: 914, iterations: 73

optimum: 147.043809
--------------------
2025-08-04 04:57:37,448 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 04:57:37,453 - robust_rl_trainer.py:432 - Iteration 10 of pure RL loop
2025-08-04 04:57:37,490 - storm_vec_env.py:70 - Computing row map
2025-08-04 04:57:37,496 - storm_vec_env.py:97 - Computing transitions
2025-08-04 04:57:37,508 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 04:57:37,509 - storm_vec_env.py:114 - Computing sinks
2025-08-04 04:57:37,509 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 04:57:37,512 - storm_vec_env.py:143 - Computing labels
2025-08-04 04:57:37,512 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 04:57:37,512 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 04:57:37,512 - storm_vec_env.py:175 - Computing observations
2025-08-04 04:57:37,691 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 04:57:37,692 - father_agent.py:540 - Before training evaluation.
2025-08-04 04:57:37,867 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:57:37,870 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:57:48,301 - evaluation_results_class.py:131 - Average Return = -59.06095886230469
2025-08-04 04:57:48,302 - evaluation_results_class.py:133 - Average Virtual Goal Value = 18.760051727294922
2025-08-04 04:57:48,302 - evaluation_results_class.py:135 - Average Discounted Reward = 6.266678333282471
2025-08-04 04:57:48,302 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9727626459143969
2025-08-04 04:57:48,302 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:57:48,302 - evaluation_results_class.py:141 - Variance of Return = 3748.228271484375
2025-08-04 04:57:48,302 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:57:48,302 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:57:48,302 - evaluation_results_class.py:147 - Average Episode Length = 152.92088197146563
2025-08-04 04:57:48,302 - evaluation_results_class.py:149 - Counted Episodes = 1542
2025-08-04 04:57:48,588 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:57:48,600 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:57:48,721 - father_agent.py:436 - Training agent on-policy
2025-08-04 04:57:57,276 - father_agent.py:386 - Step: 0, Training loss: 0.49124833941459656
2025-08-04 04:57:59,174 - father_agent.py:386 - Step: 5, Training loss: 0.4138186275959015
2025-08-04 04:58:01,076 - father_agent.py:386 - Step: 10, Training loss: 0.4705292880535126
2025-08-04 04:58:02,970 - father_agent.py:386 - Step: 15, Training loss: 0.6239826083183289
2025-08-04 04:58:04,888 - father_agent.py:386 - Step: 20, Training loss: 0.6524068117141724
2025-08-04 04:58:06,830 - father_agent.py:386 - Step: 25, Training loss: 0.2546350955963135
2025-08-04 04:58:08,743 - father_agent.py:386 - Step: 30, Training loss: 0.422323614358902
2025-08-04 04:58:10,655 - father_agent.py:386 - Step: 35, Training loss: 0.4080699682235718
2025-08-04 04:58:12,566 - father_agent.py:386 - Step: 40, Training loss: 0.49559152126312256
2025-08-04 04:58:14,452 - father_agent.py:386 - Step: 45, Training loss: 0.3681280016899109
2025-08-04 04:58:16,338 - father_agent.py:386 - Step: 50, Training loss: 0.6201196312904358
2025-08-04 04:58:18,217 - father_agent.py:386 - Step: 55, Training loss: 0.43765443563461304
2025-08-04 04:58:20,151 - father_agent.py:386 - Step: 60, Training loss: 0.516071617603302
2025-08-04 04:58:22,114 - father_agent.py:386 - Step: 65, Training loss: 0.5089523792266846
2025-08-04 04:58:24,024 - father_agent.py:386 - Step: 70, Training loss: 0.5466882586479187
2025-08-04 04:58:25,912 - father_agent.py:386 - Step: 75, Training loss: 0.6272088885307312
2025-08-04 04:58:27,808 - father_agent.py:386 - Step: 80, Training loss: 0.48994630575180054
2025-08-04 04:58:29,706 - father_agent.py:386 - Step: 85, Training loss: 0.3610404133796692
2025-08-04 04:58:31,614 - father_agent.py:386 - Step: 90, Training loss: 0.4707512855529785
2025-08-04 04:58:33,495 - father_agent.py:386 - Step: 95, Training loss: 0.5712419152259827
2025-08-04 04:58:35,383 - father_agent.py:386 - Step: 100, Training loss: 0.5885764956474304
2025-08-04 04:58:35,665 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:58:35,668 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:58:46,092 - evaluation_results_class.py:131 - Average Return = -62.50034713745117
2025-08-04 04:58:46,092 - evaluation_results_class.py:133 - Average Virtual Goal Value = 14.03838062286377
2025-08-04 04:58:46,092 - evaluation_results_class.py:135 - Average Discounted Reward = 5.289210796356201
2025-08-04 04:58:46,092 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9567341242149338
2025-08-04 04:58:46,092 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:58:46,092 - evaluation_results_class.py:141 - Variance of Return = 4220.88916015625
2025-08-04 04:58:46,093 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:58:46,093 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:58:46,093 - evaluation_results_class.py:147 - Average Episode Length = 162.1116538729937
2025-08-04 04:58:46,093 - evaluation_results_class.py:149 - Counted Episodes = 1433
2025-08-04 04:58:46,373 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:58:46,385 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:58:53,904 - father_agent.py:386 - Step: 105, Training loss: 0.978508710861206
2025-08-04 04:58:55,793 - father_agent.py:386 - Step: 110, Training loss: 0.5710152387619019
2025-08-04 04:58:57,671 - father_agent.py:386 - Step: 115, Training loss: 0.22537443041801453
2025-08-04 04:58:59,566 - father_agent.py:386 - Step: 120, Training loss: 0.9169811606407166
2025-08-04 04:59:01,521 - father_agent.py:386 - Step: 125, Training loss: 0.4064312279224396
2025-08-04 04:59:03,466 - father_agent.py:386 - Step: 130, Training loss: 0.1828550547361374
2025-08-04 04:59:05,348 - father_agent.py:386 - Step: 135, Training loss: 0.3020442724227905
2025-08-04 04:59:07,235 - father_agent.py:386 - Step: 140, Training loss: 0.2979385256767273
2025-08-04 04:59:09,148 - father_agent.py:386 - Step: 145, Training loss: 0.5946363806724548
2025-08-04 04:59:11,050 - father_agent.py:386 - Step: 150, Training loss: 0.646221935749054
2025-08-04 04:59:12,934 - father_agent.py:386 - Step: 155, Training loss: 0.3856329321861267
2025-08-04 04:59:14,816 - father_agent.py:386 - Step: 160, Training loss: 0.34396812319755554
2025-08-04 04:59:16,695 - father_agent.py:386 - Step: 165, Training loss: 0.3667205572128296
2025-08-04 04:59:18,571 - father_agent.py:386 - Step: 170, Training loss: 0.39294034242630005
2025-08-04 04:59:20,460 - father_agent.py:386 - Step: 175, Training loss: 0.18890711665153503
2025-08-04 04:59:22,350 - father_agent.py:386 - Step: 180, Training loss: 0.745223343372345
2025-08-04 04:59:24,237 - father_agent.py:386 - Step: 185, Training loss: 0.24677401781082153
2025-08-04 04:59:26,119 - father_agent.py:386 - Step: 190, Training loss: 0.26636308431625366
2025-08-04 04:59:28,006 - father_agent.py:386 - Step: 195, Training loss: 0.15310657024383545
2025-08-04 04:59:29,914 - father_agent.py:386 - Step: 200, Training loss: 0.330854207277298
2025-08-04 04:59:30,204 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:59:30,207 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:59:40,654 - evaluation_results_class.py:131 - Average Return = -41.06352615356445
2025-08-04 04:59:40,654 - evaluation_results_class.py:133 - Average Virtual Goal Value = 36.35158920288086
2025-08-04 04:59:40,654 - evaluation_results_class.py:135 - Average Discounted Reward = 21.805391311645508
2025-08-04 04:59:40,654 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9676889375684556
2025-08-04 04:59:40,654 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 04:59:40,654 - evaluation_results_class.py:141 - Variance of Return = 2748.201904296875
2025-08-04 04:59:40,654 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 04:59:40,654 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 04:59:40,655 - evaluation_results_class.py:147 - Average Episode Length = 116.84939759036145
2025-08-04 04:59:40,655 - evaluation_results_class.py:149 - Counted Episodes = 1826
2025-08-04 04:59:40,938 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:59:40,951 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 04:59:45,151 - father_agent.py:386 - Step: 205, Training loss: 0.6527538299560547
2025-08-04 04:59:47,050 - father_agent.py:386 - Step: 210, Training loss: 0.14516133069992065
2025-08-04 04:59:48,948 - father_agent.py:386 - Step: 215, Training loss: 0.3706185817718506
2025-08-04 04:59:50,852 - father_agent.py:386 - Step: 220, Training loss: 0.42279449105262756
2025-08-04 04:59:52,752 - father_agent.py:386 - Step: 225, Training loss: 0.2762296199798584
2025-08-04 04:59:54,645 - father_agent.py:386 - Step: 230, Training loss: 0.20431993901729584
2025-08-04 04:59:56,532 - father_agent.py:386 - Step: 235, Training loss: 0.370932012796402
2025-08-04 04:59:58,420 - father_agent.py:386 - Step: 240, Training loss: 0.5627450942993164
2025-08-04 05:00:00,320 - father_agent.py:386 - Step: 245, Training loss: 0.553859531879425
2025-08-04 05:00:02,195 - father_agent.py:386 - Step: 250, Training loss: 0.2685723900794983
2025-08-04 05:00:04,107 - father_agent.py:386 - Step: 255, Training loss: 0.267530232667923
2025-08-04 05:00:06,003 - father_agent.py:386 - Step: 260, Training loss: 0.5561544895172119
2025-08-04 05:00:07,884 - father_agent.py:386 - Step: 265, Training loss: 0.42956775426864624
2025-08-04 05:00:09,788 - father_agent.py:386 - Step: 270, Training loss: 0.602330207824707
2025-08-04 05:00:11,707 - father_agent.py:386 - Step: 275, Training loss: 0.635440468788147
2025-08-04 05:00:13,621 - father_agent.py:386 - Step: 280, Training loss: 0.5265429615974426
2025-08-04 05:00:15,509 - father_agent.py:386 - Step: 285, Training loss: 0.5122398734092712
2025-08-04 05:00:17,401 - father_agent.py:386 - Step: 290, Training loss: 0.5125501155853271
2025-08-04 05:00:19,377 - father_agent.py:386 - Step: 295, Training loss: 0.46380943059921265
2025-08-04 05:00:21,406 - father_agent.py:386 - Step: 300, Training loss: 0.9271906018257141
2025-08-04 05:00:21,688 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:00:21,691 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:00:32,613 - evaluation_results_class.py:131 - Average Return = -77.33364868164062
2025-08-04 05:00:32,613 - evaluation_results_class.py:133 - Average Virtual Goal Value = -4.927631378173828
2025-08-04 05:00:32,613 - evaluation_results_class.py:135 - Average Discounted Reward = 1.6054129600524902
2025-08-04 05:00:32,613 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9050751879699248
2025-08-04 05:00:32,613 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:00:32,613 - evaluation_results_class.py:141 - Variance of Return = 6147.39306640625
2025-08-04 05:00:32,613 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:00:32,613 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:00:32,613 - evaluation_results_class.py:147 - Average Episode Length = 204.1109022556391
2025-08-04 05:00:32,613 - evaluation_results_class.py:149 - Counted Episodes = 1064
2025-08-04 05:00:32,902 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:00:32,915 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:00:37,107 - father_agent.py:386 - Step: 305, Training loss: 0.6103899478912354
2025-08-04 05:00:39,063 - father_agent.py:386 - Step: 310, Training loss: 0.7349960207939148
2025-08-04 05:00:41,030 - father_agent.py:386 - Step: 315, Training loss: 0.11119259148836136
2025-08-04 05:00:43,011 - father_agent.py:386 - Step: 320, Training loss: 0.7148399949073792
2025-08-04 05:00:44,985 - father_agent.py:386 - Step: 325, Training loss: 0.5767900347709656
2025-08-04 05:00:46,967 - father_agent.py:386 - Step: 330, Training loss: 0.36242011189460754
2025-08-04 05:00:48,944 - father_agent.py:386 - Step: 335, Training loss: 0.16909034550189972
2025-08-04 05:00:50,998 - father_agent.py:386 - Step: 340, Training loss: 0.30396726727485657
2025-08-04 05:00:53,030 - father_agent.py:386 - Step: 345, Training loss: 0.5921100974082947
2025-08-04 05:00:55,016 - father_agent.py:386 - Step: 350, Training loss: 0.6334049105644226
2025-08-04 05:00:57,001 - father_agent.py:386 - Step: 355, Training loss: 0.5722838044166565
2025-08-04 05:00:58,940 - father_agent.py:386 - Step: 360, Training loss: 0.6259894967079163
2025-08-04 05:01:00,891 - father_agent.py:386 - Step: 365, Training loss: 0.49273788928985596
2025-08-04 05:01:02,816 - father_agent.py:386 - Step: 370, Training loss: 0.3052186071872711
2025-08-04 05:01:04,759 - father_agent.py:386 - Step: 375, Training loss: 0.156647190451622
2025-08-04 05:01:06,705 - father_agent.py:386 - Step: 380, Training loss: 0.34577253460884094
2025-08-04 05:01:08,650 - father_agent.py:386 - Step: 385, Training loss: 0.6747531294822693
2025-08-04 05:01:10,627 - father_agent.py:386 - Step: 390, Training loss: 0.5315317511558533
2025-08-04 05:01:12,657 - father_agent.py:386 - Step: 395, Training loss: 0.39642399549484253
2025-08-04 05:01:14,613 - father_agent.py:386 - Step: 400, Training loss: 0.5116114020347595
2025-08-04 05:01:14,899 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:01:14,903 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:01:26,380 - evaluation_results_class.py:131 - Average Return = -55.4417724609375
2025-08-04 05:01:26,380 - evaluation_results_class.py:133 - Average Virtual Goal Value = 21.672151565551758
2025-08-04 05:01:26,380 - evaluation_results_class.py:135 - Average Discounted Reward = 10.25296401977539
2025-08-04 05:01:26,380 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9639240506329114
2025-08-04 05:01:26,380 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:01:26,380 - evaluation_results_class.py:141 - Variance of Return = 3885.213623046875
2025-08-04 05:01:26,380 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:01:26,380 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:01:26,380 - evaluation_results_class.py:147 - Average Episode Length = 142.63227848101266
2025-08-04 05:01:26,380 - evaluation_results_class.py:149 - Counted Episodes = 1580
2025-08-04 05:01:26,649 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:01:26,661 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:01:30,831 - father_agent.py:386 - Step: 405, Training loss: 0.750568687915802
2025-08-04 05:01:32,740 - father_agent.py:386 - Step: 410, Training loss: 0.5009106993675232
2025-08-04 05:01:34,619 - father_agent.py:386 - Step: 415, Training loss: 0.5653281807899475
2025-08-04 05:01:36,505 - father_agent.py:386 - Step: 420, Training loss: 0.486512154340744
2025-08-04 05:01:38,379 - father_agent.py:386 - Step: 425, Training loss: 0.41157257556915283
2025-08-04 05:01:40,299 - father_agent.py:386 - Step: 430, Training loss: 0.5187481641769409
2025-08-04 05:01:42,203 - father_agent.py:386 - Step: 435, Training loss: 0.6586002707481384
2025-08-04 05:01:44,116 - father_agent.py:386 - Step: 440, Training loss: 0.5100909471511841
2025-08-04 05:01:46,009 - father_agent.py:386 - Step: 445, Training loss: 0.4163811504840851
2025-08-04 05:01:47,895 - father_agent.py:386 - Step: 450, Training loss: 0.7753787040710449
2025-08-04 05:01:49,792 - father_agent.py:386 - Step: 455, Training loss: 0.9315215349197388
2025-08-04 05:01:51,678 - father_agent.py:386 - Step: 460, Training loss: 0.21730390191078186
2025-08-04 05:01:53,546 - father_agent.py:386 - Step: 465, Training loss: 0.631260871887207
2025-08-04 05:01:55,428 - father_agent.py:386 - Step: 470, Training loss: 0.3082231283187866
2025-08-04 05:01:57,323 - father_agent.py:386 - Step: 475, Training loss: 0.33589109778404236
2025-08-04 05:01:59,213 - father_agent.py:386 - Step: 480, Training loss: 0.6065300107002258
2025-08-04 05:02:01,091 - father_agent.py:386 - Step: 485, Training loss: 0.578214168548584
2025-08-04 05:02:02,985 - father_agent.py:386 - Step: 490, Training loss: 0.2291945219039917
2025-08-04 05:02:04,859 - father_agent.py:386 - Step: 495, Training loss: 0.3404773771762848
2025-08-04 05:02:06,767 - father_agent.py:386 - Step: 500, Training loss: 0.4835985600948334
2025-08-04 05:02:07,053 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:02:07,055 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:02:17,428 - evaluation_results_class.py:131 - Average Return = -77.59667205810547
2025-08-04 05:02:17,428 - evaluation_results_class.py:133 - Average Virtual Goal Value = -7.217391490936279
2025-08-04 05:02:17,428 - evaluation_results_class.py:135 - Average Discounted Reward = 1.9838132858276367
2025-08-04 05:02:17,428 - evaluation_results_class.py:137 - Goal Reach Probability = 0.879740980573543
2025-08-04 05:02:17,429 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:02:17,429 - evaluation_results_class.py:141 - Variance of Return = 5783.26904296875
2025-08-04 05:02:17,429 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:02:17,429 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:02:17,429 - evaluation_results_class.py:147 - Average Episode Length = 211.45513413506012
2025-08-04 05:02:17,429 - evaluation_results_class.py:149 - Counted Episodes = 1081
2025-08-04 05:02:17,712 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:02:17,725 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:02:17,844 - father_agent.py:547 - Training finished.
2025-08-04 05:02:18,003 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:02:18,006 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:02:18,008 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 05:02:28,510 - evaluation_results_class.py:131 - Average Return = -78.79759979248047
2025-08-04 05:02:28,510 - evaluation_results_class.py:133 - Average Virtual Goal Value = -8.48336410522461
2025-08-04 05:02:28,510 - evaluation_results_class.py:135 - Average Discounted Reward = 1.8829758167266846
2025-08-04 05:02:28,510 - evaluation_results_class.py:137 - Goal Reach Probability = 0.8789279112754159
2025-08-04 05:02:28,510 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:02:28,510 - evaluation_results_class.py:141 - Variance of Return = 6195.7412109375
2025-08-04 05:02:28,510 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:02:28,510 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:02:28,510 - evaluation_results_class.py:147 - Average Episode Length = 213.41404805914974
2025-08-04 05:02:28,510 - evaluation_results_class.py:149 - Counted Episodes = 1082
2025-08-04 05:02:28,802 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:02:28,805 - self_interpretable_extractor.py:286 - True
2025-08-04 05:02:28,818 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:02:45,923 - evaluation_results_class.py:131 - Average Return = -92.06938934326172
2025-08-04 05:02:45,923 - evaluation_results_class.py:133 - Average Virtual Goal Value = -27.126426696777344
2025-08-04 05:02:45,923 - evaluation_results_class.py:135 - Average Discounted Reward = -1.4326567649841309
2025-08-04 05:02:45,923 - evaluation_results_class.py:137 - Goal Reach Probability = 0.811787072243346
2025-08-04 05:02:45,923 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:02:45,923 - evaluation_results_class.py:141 - Variance of Return = 7274.44091796875
2025-08-04 05:02:45,923 - evaluation_results_class.py:143 - Current Best Return = -92.06938934326172
2025-08-04 05:02:45,923 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.811787072243346
2025-08-04 05:02:45,923 - evaluation_results_class.py:147 - Average Episode Length = 252.9096958174905
2025-08-04 05:02:45,923 - evaluation_results_class.py:149 - Counted Episodes = 1052
2025-08-04 05:02:45,923 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 05:02:45,924 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 5192 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 23, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
Buffer 1
2025-08-04 05:03:56,218 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 10333 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 23, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
Buffer 2
2025-08-04 05:05:06,448 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 15553 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 23, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
All trajectories collected
2025-08-04 05:06:16,469 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 05:06:16,469 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 15553 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_9.dot.
Learned FSC of size 2
2025-08-04 05:06:22,163 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 05:06:38,046 - evaluation_results_class.py:131 - Average Return = -161.7573699951172
2025-08-04 05:06:38,046 - evaluation_results_class.py:133 - Average Virtual Goal Value = -83.50808715820312
2025-08-04 05:06:38,046 - evaluation_results_class.py:135 - Average Discounted Reward = -73.76199340820312
2025-08-04 05:06:38,046 - evaluation_results_class.py:137 - Goal Reach Probability = 0.978116079923882
2025-08-04 05:06:38,046 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:06:38,046 - evaluation_results_class.py:141 - Variance of Return = 7708.71630859375
2025-08-04 05:06:38,046 - evaluation_results_class.py:143 - Current Best Return = -161.7573699951172
2025-08-04 05:06:38,046 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.978116079923882
2025-08-04 05:06:38,046 - evaluation_results_class.py:147 - Average Episode Length = 275.10656517602285
2025-08-04 05:06:38,046 - evaluation_results_class.py:149 - Counted Episodes = 1051
FSC Result: {'best_episode_return': -83.50809, 'best_return': -161.75737, 'goal_value': 0.0, 'returns_episodic': [-83.50809], 'returns': [-161.75737], 'reach_probs': [0.978116079923882], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.978116079923882, 'losses': [], 'best_updated': True, 'each_episode_variance': [7708.7163], 'each_episode_virtual_variance': [8404.502], 'combined_variance': [32089.45], 'num_episodes': [1051], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [275.10656517602285], 'counted_episodes': [1051], 'discounted_rewards': [-73.76199], 'new_pomdp_iteration_numbers': []}
[[5], [3, 4, 6, 7], [3, 4, 6, 7], [1], [1], [1], [1], [2], [2], [2], [2], [3, 4, 6, 7], [3, 4, 6, 7], [0]]
2025-08-04 05:06:38,136 - statistic.py:67 - synthesis initiated, design space: 81
2025-08-04 05:06:38,136 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 05:06:38,155 - synthesizer_ar.py:122 - value 140.3565 achieved after 5417.41 seconds
2025-08-04 05:06:38,157 - synthesizer_ar.py:122 - value 151.1096 achieved after 5417.41 seconds
2025-08-04 05:06:38,160 - synthesizer_ar.py:122 - value 169.1481 achieved after 5417.42 seconds
2025-08-04 05:06:38,176 - synthesizer_ar.py:122 - value 169.5559 achieved after 5417.43 seconds
2025-08-04 05:06:38,191 - synthesizer_ar.py:122 - value 170.0089 achieved after 5417.45 seconds
2025-08-04 05:06:38,211 - synthesizer_ar.py:122 - value 171.8969 achieved after 5417.47 seconds
2025-08-04 05:06:38,227 - synthesizer_ar.py:122 - value 172.215 achieved after 5417.48 seconds
2025-08-04 05:06:38,245 - synthesizer_ar.py:122 - value 172.5472 achieved after 5417.5 seconds
2025-08-04 05:06:38,264 - synthesizer_ar.py:122 - value 176.2417 achieved after 5417.52 seconds
2025-08-04 05:06:38,279 - synthesizer_ar.py:122 - value 176.4829 achieved after 5417.54 seconds
2025-08-04 05:06:38,294 - synthesizer_ar.py:122 - value 176.7372 achieved after 5417.55 seconds
2025-08-04 05:06:38,297 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 05:06:38,297 - synthesizer.py:193 - o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 05:06:38,298 - synthesizer.py:198 - double-checking specification satisfiability:  : 176.73722339762782
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 10) & (y = 10))] 

method: AR, synthesis time: 0.16 s
number of holes: 4, family size: 81, quotient: 1018 states / 1170 actions
explored: 100 %
MDP stats: avg MDP size: 914, iterations: 67

optimum: 176.737223
--------------------
2025-08-04 05:06:38,298 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 05:06:38,303 - robust_rl_trainer.py:432 - Iteration 11 of pure RL loop
2025-08-04 05:06:38,343 - storm_vec_env.py:70 - Computing row map
2025-08-04 05:06:38,349 - storm_vec_env.py:97 - Computing transitions
2025-08-04 05:06:38,362 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 05:06:38,362 - storm_vec_env.py:114 - Computing sinks
2025-08-04 05:06:38,362 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 05:06:38,365 - storm_vec_env.py:143 - Computing labels
2025-08-04 05:06:38,365 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 05:06:38,365 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 05:06:38,366 - storm_vec_env.py:175 - Computing observations
2025-08-04 05:06:38,546 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 05:06:38,548 - father_agent.py:540 - Before training evaluation.
2025-08-04 05:06:38,725 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:06:38,728 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:06:49,386 - evaluation_results_class.py:131 - Average Return = -76.5086441040039
2025-08-04 05:06:49,387 - evaluation_results_class.py:133 - Average Virtual Goal Value = -4.515923500061035
2025-08-04 05:06:49,387 - evaluation_results_class.py:135 - Average Discounted Reward = 2.6517107486724854
2025-08-04 05:06:49,387 - evaluation_results_class.py:137 - Goal Reach Probability = 0.899909008189263
2025-08-04 05:06:49,387 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:06:49,387 - evaluation_results_class.py:141 - Variance of Return = 5933.33837890625
2025-08-04 05:06:49,387 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:06:49,387 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:06:49,387 - evaluation_results_class.py:147 - Average Episode Length = 205.77616014558689
2025-08-04 05:06:49,387 - evaluation_results_class.py:149 - Counted Episodes = 1099
2025-08-04 05:06:49,668 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:06:49,680 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:06:49,800 - father_agent.py:436 - Training agent on-policy
2025-08-04 05:06:59,408 - father_agent.py:386 - Step: 0, Training loss: 1.016289234161377
2025-08-04 05:07:01,391 - father_agent.py:386 - Step: 5, Training loss: 0.45302921533584595
2025-08-04 05:07:03,345 - father_agent.py:386 - Step: 10, Training loss: 0.30268943309783936
2025-08-04 05:07:05,294 - father_agent.py:386 - Step: 15, Training loss: 0.2793746292591095
2025-08-04 05:07:07,235 - father_agent.py:386 - Step: 20, Training loss: 0.4467650055885315
2025-08-04 05:07:09,239 - father_agent.py:386 - Step: 25, Training loss: 0.4877411723136902
2025-08-04 05:07:11,170 - father_agent.py:386 - Step: 30, Training loss: 0.5781570672988892
2025-08-04 05:07:13,119 - father_agent.py:386 - Step: 35, Training loss: 0.6070696711540222
2025-08-04 05:07:15,062 - father_agent.py:386 - Step: 40, Training loss: 0.15515969693660736
2025-08-04 05:07:16,993 - father_agent.py:386 - Step: 45, Training loss: 0.5491136312484741
2025-08-04 05:07:18,953 - father_agent.py:386 - Step: 50, Training loss: 0.42223912477493286
2025-08-04 05:07:20,980 - father_agent.py:386 - Step: 55, Training loss: 0.3188190758228302
2025-08-04 05:07:23,054 - father_agent.py:386 - Step: 60, Training loss: 0.5279055237770081
2025-08-04 05:07:25,157 - father_agent.py:386 - Step: 65, Training loss: 0.06307508051395416
2025-08-04 05:07:27,264 - father_agent.py:386 - Step: 70, Training loss: 0.30362993478775024
2025-08-04 05:07:29,344 - father_agent.py:386 - Step: 75, Training loss: 0.25452694296836853
2025-08-04 05:07:31,333 - father_agent.py:386 - Step: 80, Training loss: 0.4006323218345642
2025-08-04 05:07:33,307 - father_agent.py:386 - Step: 85, Training loss: 0.5682701468467712
2025-08-04 05:07:35,293 - father_agent.py:386 - Step: 90, Training loss: 0.4112493097782135
2025-08-04 05:07:37,255 - father_agent.py:386 - Step: 95, Training loss: 0.39992424845695496
2025-08-04 05:07:39,152 - father_agent.py:386 - Step: 100, Training loss: 0.3101545572280884
2025-08-04 05:07:39,439 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:07:39,442 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:07:50,013 - evaluation_results_class.py:131 - Average Return = -70.37511444091797
2025-08-04 05:07:50,013 - evaluation_results_class.py:133 - Average Virtual Goal Value = -0.25259190797805786
2025-08-04 05:07:50,013 - evaluation_results_class.py:135 - Average Discounted Reward = 9.462736129760742
2025-08-04 05:07:50,013 - evaluation_results_class.py:137 - Goal Reach Probability = 0.8765315739868049
2025-08-04 05:07:50,013 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:07:50,013 - evaluation_results_class.py:141 - Variance of Return = 6162.19091796875
2025-08-04 05:07:50,013 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:07:50,013 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:07:50,013 - evaluation_results_class.py:147 - Average Episode Length = 196.19415645617343
2025-08-04 05:07:50,014 - evaluation_results_class.py:149 - Counted Episodes = 1061
2025-08-04 05:07:50,309 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:07:50,322 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:07:58,059 - father_agent.py:386 - Step: 105, Training loss: 0.9851059317588806
2025-08-04 05:07:59,944 - father_agent.py:386 - Step: 110, Training loss: 0.20754212141036987
2025-08-04 05:08:01,829 - father_agent.py:386 - Step: 115, Training loss: 0.4514297842979431
2025-08-04 05:08:03,708 - father_agent.py:386 - Step: 120, Training loss: 0.672254204750061
2025-08-04 05:08:05,597 - father_agent.py:386 - Step: 125, Training loss: 0.31226781010627747
2025-08-04 05:08:07,505 - father_agent.py:386 - Step: 130, Training loss: 0.4365755319595337
2025-08-04 05:08:09,401 - father_agent.py:386 - Step: 135, Training loss: 0.472421258687973
2025-08-04 05:08:11,298 - father_agent.py:386 - Step: 140, Training loss: 0.1949264109134674
2025-08-04 05:08:13,175 - father_agent.py:386 - Step: 145, Training loss: 0.40777307748794556
2025-08-04 05:08:15,047 - father_agent.py:386 - Step: 150, Training loss: 0.4310775399208069
2025-08-04 05:08:16,939 - father_agent.py:386 - Step: 155, Training loss: 0.2320001870393753
2025-08-04 05:08:18,824 - father_agent.py:386 - Step: 160, Training loss: 0.07574372738599777
2025-08-04 05:08:20,735 - father_agent.py:386 - Step: 165, Training loss: 0.38049057126045227
2025-08-04 05:08:22,643 - father_agent.py:386 - Step: 170, Training loss: 0.3227653503417969
2025-08-04 05:08:24,524 - father_agent.py:386 - Step: 175, Training loss: 0.2842187285423279
2025-08-04 05:08:26,399 - father_agent.py:386 - Step: 180, Training loss: 0.3354175388813019
2025-08-04 05:08:28,296 - father_agent.py:386 - Step: 185, Training loss: 0.36048877239227295
2025-08-04 05:08:30,188 - father_agent.py:386 - Step: 190, Training loss: 0.3809216022491455
2025-08-04 05:08:32,083 - father_agent.py:386 - Step: 195, Training loss: 0.6498070359230042
2025-08-04 05:08:33,973 - father_agent.py:386 - Step: 200, Training loss: 0.5627548098564148
2025-08-04 05:08:34,271 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:08:34,274 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:08:45,067 - evaluation_results_class.py:131 - Average Return = -18.394737243652344
2025-08-04 05:08:45,067 - evaluation_results_class.py:133 - Average Virtual Goal Value = 61.592655181884766
2025-08-04 05:08:45,067 - evaluation_results_class.py:135 - Average Discounted Reward = 35.574806213378906
2025-08-04 05:08:45,067 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9998424204223133
2025-08-04 05:08:45,067 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:08:45,067 - evaluation_results_class.py:141 - Variance of Return = 273.5600280761719
2025-08-04 05:08:45,067 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:08:45,067 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:08:45,067 - evaluation_results_class.py:147 - Average Episode Length = 50.085408131106206
2025-08-04 05:08:45,067 - evaluation_results_class.py:149 - Counted Episodes = 6346
2025-08-04 05:08:45,366 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:08:45,379 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:08:49,888 - father_agent.py:386 - Step: 205, Training loss: 0.35900503396987915
2025-08-04 05:08:51,799 - father_agent.py:386 - Step: 210, Training loss: 0.3076908588409424
2025-08-04 05:08:53,722 - father_agent.py:386 - Step: 215, Training loss: 0.47868233919143677
2025-08-04 05:08:55,626 - father_agent.py:386 - Step: 220, Training loss: 0.38582301139831543
2025-08-04 05:08:57,543 - father_agent.py:386 - Step: 225, Training loss: 0.3760984539985657
2025-08-04 05:08:59,439 - father_agent.py:386 - Step: 230, Training loss: 0.6242892742156982
2025-08-04 05:09:01,338 - father_agent.py:386 - Step: 235, Training loss: 0.5895354151725769
2025-08-04 05:09:03,238 - father_agent.py:386 - Step: 240, Training loss: 0.5936279892921448
2025-08-04 05:09:05,134 - father_agent.py:386 - Step: 245, Training loss: 0.3178994655609131
2025-08-04 05:09:07,028 - father_agent.py:386 - Step: 250, Training loss: 0.3499578833580017
2025-08-04 05:09:08,922 - father_agent.py:386 - Step: 255, Training loss: 0.37059664726257324
2025-08-04 05:09:10,807 - father_agent.py:386 - Step: 260, Training loss: 0.5185961723327637
2025-08-04 05:09:12,708 - father_agent.py:386 - Step: 265, Training loss: 0.35127589106559753
2025-08-04 05:09:14,622 - father_agent.py:386 - Step: 270, Training loss: 0.44419920444488525
2025-08-04 05:09:16,540 - father_agent.py:386 - Step: 275, Training loss: 0.28875523805618286
2025-08-04 05:09:18,447 - father_agent.py:386 - Step: 280, Training loss: 0.6678634285926819
2025-08-04 05:09:20,368 - father_agent.py:386 - Step: 285, Training loss: 0.29614073038101196
2025-08-04 05:09:22,277 - father_agent.py:386 - Step: 290, Training loss: 0.30903804302215576
2025-08-04 05:09:24,190 - father_agent.py:386 - Step: 295, Training loss: 0.32114312052726746
2025-08-04 05:09:26,076 - father_agent.py:386 - Step: 300, Training loss: 0.4559297263622284
2025-08-04 05:09:26,366 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:09:26,369 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:09:36,915 - evaluation_results_class.py:131 - Average Return = -43.31595993041992
2025-08-04 05:09:36,915 - evaluation_results_class.py:133 - Average Virtual Goal Value = 36.13053512573242
2025-08-04 05:09:36,915 - evaluation_results_class.py:135 - Average Discounted Reward = 14.283509254455566
2025-08-04 05:09:36,915 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9930811808118081
2025-08-04 05:09:36,915 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:09:36,915 - evaluation_results_class.py:141 - Variance of Return = 1960.849853515625
2025-08-04 05:09:36,915 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:09:36,915 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:09:36,916 - evaluation_results_class.py:147 - Average Episode Length = 121.10793357933579
2025-08-04 05:09:36,916 - evaluation_results_class.py:149 - Counted Episodes = 2168
2025-08-04 05:09:37,211 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:09:37,224 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:09:41,381 - father_agent.py:386 - Step: 305, Training loss: 0.9282492995262146
2025-08-04 05:09:43,293 - father_agent.py:386 - Step: 310, Training loss: 0.23958373069763184
2025-08-04 05:09:45,196 - father_agent.py:386 - Step: 315, Training loss: 0.5964336395263672
2025-08-04 05:09:47,111 - father_agent.py:386 - Step: 320, Training loss: 0.4130253493785858
2025-08-04 05:09:49,062 - father_agent.py:386 - Step: 325, Training loss: 0.2054300308227539
2025-08-04 05:09:50,962 - father_agent.py:386 - Step: 330, Training loss: 0.36618658900260925
2025-08-04 05:09:52,875 - father_agent.py:386 - Step: 335, Training loss: 0.3461274206638336
2025-08-04 05:09:54,771 - father_agent.py:386 - Step: 340, Training loss: 0.22554366290569305
2025-08-04 05:09:56,700 - father_agent.py:386 - Step: 345, Training loss: 0.28870126605033875
2025-08-04 05:09:58,612 - father_agent.py:386 - Step: 350, Training loss: 0.19234015047550201
2025-08-04 05:10:00,526 - father_agent.py:386 - Step: 355, Training loss: 0.3134906589984894
2025-08-04 05:10:02,437 - father_agent.py:386 - Step: 360, Training loss: 0.43215370178222656
2025-08-04 05:10:04,378 - father_agent.py:386 - Step: 365, Training loss: 0.13954152166843414
2025-08-04 05:10:06,300 - father_agent.py:386 - Step: 370, Training loss: 0.48852023482322693
2025-08-04 05:10:08,203 - father_agent.py:386 - Step: 375, Training loss: 0.27208462357521057
2025-08-04 05:10:10,117 - father_agent.py:386 - Step: 380, Training loss: 0.24269914627075195
2025-08-04 05:10:12,035 - father_agent.py:386 - Step: 385, Training loss: 0.5401178002357483
2025-08-04 05:10:13,940 - father_agent.py:386 - Step: 390, Training loss: 0.33038368821144104
2025-08-04 05:10:15,861 - father_agent.py:386 - Step: 395, Training loss: 0.366160124540329
2025-08-04 05:10:17,773 - father_agent.py:386 - Step: 400, Training loss: 0.43456828594207764
2025-08-04 05:10:18,068 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:10:18,071 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:10:28,582 - evaluation_results_class.py:131 - Average Return = -58.89713668823242
2025-08-04 05:10:28,582 - evaluation_results_class.py:133 - Average Virtual Goal Value = 15.286930084228516
2025-08-04 05:10:28,582 - evaluation_results_class.py:135 - Average Discounted Reward = 12.240965843200684
2025-08-04 05:10:28,582 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9273008507347255
2025-08-04 05:10:28,582 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:10:28,582 - evaluation_results_class.py:141 - Variance of Return = 4451.341796875
2025-08-04 05:10:28,582 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:10:28,582 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:10:28,582 - evaluation_results_class.py:147 - Average Episode Length = 167.2474864655839
2025-08-04 05:10:28,582 - evaluation_results_class.py:149 - Counted Episodes = 1293
2025-08-04 05:10:28,879 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:10:28,891 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:10:33,064 - father_agent.py:386 - Step: 405, Training loss: 0.3563568890094757
2025-08-04 05:10:34,992 - father_agent.py:386 - Step: 410, Training loss: 0.3583585321903229
2025-08-04 05:10:36,905 - father_agent.py:386 - Step: 415, Training loss: 0.16671961545944214
2025-08-04 05:10:38,822 - father_agent.py:386 - Step: 420, Training loss: 0.4040392339229584
2025-08-04 05:10:40,739 - father_agent.py:386 - Step: 425, Training loss: 0.09168978035449982
2025-08-04 05:10:42,654 - father_agent.py:386 - Step: 430, Training loss: 0.3070811927318573
2025-08-04 05:10:44,566 - father_agent.py:386 - Step: 435, Training loss: 0.2968336343765259
2025-08-04 05:10:46,474 - father_agent.py:386 - Step: 440, Training loss: 0.41037580370903015
2025-08-04 05:10:48,382 - father_agent.py:386 - Step: 445, Training loss: 0.11642996221780777
2025-08-04 05:10:50,302 - father_agent.py:386 - Step: 450, Training loss: 0.3653903603553772
2025-08-04 05:10:52,215 - father_agent.py:386 - Step: 455, Training loss: 0.23808646202087402
2025-08-04 05:10:54,131 - father_agent.py:386 - Step: 460, Training loss: 0.4259876012802124
2025-08-04 05:10:56,038 - father_agent.py:386 - Step: 465, Training loss: 0.26932695508003235
2025-08-04 05:10:57,945 - father_agent.py:386 - Step: 470, Training loss: 0.1864374727010727
2025-08-04 05:10:59,856 - father_agent.py:386 - Step: 475, Training loss: 0.4616714417934418
2025-08-04 05:11:01,778 - father_agent.py:386 - Step: 480, Training loss: 0.46815401315689087
2025-08-04 05:11:03,686 - father_agent.py:386 - Step: 485, Training loss: 0.18464885652065277
2025-08-04 05:11:05,576 - father_agent.py:386 - Step: 490, Training loss: 0.7024409174919128
2025-08-04 05:11:07,506 - father_agent.py:386 - Step: 495, Training loss: 0.8771246671676636
2025-08-04 05:11:09,499 - father_agent.py:386 - Step: 500, Training loss: 0.27063626050949097
2025-08-04 05:11:09,793 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:11:09,796 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:11:21,348 - evaluation_results_class.py:131 - Average Return = -65.11052703857422
2025-08-04 05:11:21,349 - evaluation_results_class.py:133 - Average Virtual Goal Value = 10.799247741699219
2025-08-04 05:11:21,349 - evaluation_results_class.py:135 - Average Discounted Reward = 4.724502086639404
2025-08-04 05:11:21,349 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9488721804511279
2025-08-04 05:11:21,349 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:11:21,349 - evaluation_results_class.py:141 - Variance of Return = 4721.13427734375
2025-08-04 05:11:21,349 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:11:21,349 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:11:21,349 - evaluation_results_class.py:147 - Average Episode Length = 174.3736842105263
2025-08-04 05:11:21,349 - evaluation_results_class.py:149 - Counted Episodes = 1330
2025-08-04 05:11:21,745 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:11:21,758 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:11:21,883 - father_agent.py:547 - Training finished.
2025-08-04 05:11:22,049 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:11:22,052 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:11:22,054 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 05:11:33,128 - evaluation_results_class.py:131 - Average Return = -65.68081665039062
2025-08-04 05:11:33,128 - evaluation_results_class.py:133 - Average Virtual Goal Value = 10.231132507324219
2025-08-04 05:11:33,128 - evaluation_results_class.py:135 - Average Discounted Reward = 4.61916446685791
2025-08-04 05:11:33,128 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9488993710691824
2025-08-04 05:11:33,128 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:11:33,128 - evaluation_results_class.py:141 - Variance of Return = 4386.19677734375
2025-08-04 05:11:33,128 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:11:33,128 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:11:33,128 - evaluation_results_class.py:147 - Average Episode Length = 176.35141509433961
2025-08-04 05:11:33,128 - evaluation_results_class.py:149 - Counted Episodes = 1272
2025-08-04 05:11:33,535 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:11:33,538 - self_interpretable_extractor.py:286 - True
2025-08-04 05:11:33,558 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:11:50,531 - evaluation_results_class.py:131 - Average Return = -75.197509765625
2025-08-04 05:11:50,531 - evaluation_results_class.py:133 - Average Virtual Goal Value = -2.3957571983337402
2025-08-04 05:11:50,531 - evaluation_results_class.py:135 - Average Discounted Reward = 1.5538413524627686
2025-08-04 05:11:50,531 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9100219458668617
2025-08-04 05:11:50,531 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:11:50,531 - evaluation_results_class.py:141 - Variance of Return = 5465.873046875
2025-08-04 05:11:50,531 - evaluation_results_class.py:143 - Current Best Return = -75.197509765625
2025-08-04 05:11:50,531 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9100219458668617
2025-08-04 05:11:50,531 - evaluation_results_class.py:147 - Average Episode Length = 205.18288222384786
2025-08-04 05:11:50,531 - evaluation_results_class.py:149 - Counted Episodes = 1367
2025-08-04 05:11:50,531 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 05:11:50,531 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 6341 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
Buffer 1
2025-08-04 05:13:01,757 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 12847 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
Buffer 2
2025-08-04 05:14:13,018 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 19266 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
All trajectories collected
2025-08-04 05:15:23,291 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 05:15:23,291 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 19266 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_10.dot.
Learned FSC of size 2
2025-08-04 05:15:30,406 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 05:15:47,722 - evaluation_results_class.py:131 - Average Return = -140.30038452148438
2025-08-04 05:15:47,722 - evaluation_results_class.py:133 - Average Virtual Goal Value = -60.66263961791992
2025-08-04 05:15:47,722 - evaluation_results_class.py:135 - Average Discounted Reward = -64.9561538696289
2025-08-04 05:15:47,722 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9954716981132076
2025-08-04 05:15:47,722 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:15:47,722 - evaluation_results_class.py:141 - Variance of Return = 7495.4375
2025-08-04 05:15:47,722 - evaluation_results_class.py:143 - Current Best Return = -140.30038452148438
2025-08-04 05:15:47,722 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9954716981132076
2025-08-04 05:15:47,722 - evaluation_results_class.py:147 - Average Episode Length = 220.69358490566037
2025-08-04 05:15:47,722 - evaluation_results_class.py:149 - Counted Episodes = 1325
FSC Result: {'best_episode_return': -60.66264, 'best_return': -140.30038, 'goal_value': 0.0, 'returns_episodic': [-60.66264], 'returns': [-140.30038], 'reach_probs': [0.9954716981132076], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.9954716981132076, 'losses': [], 'best_updated': True, 'each_episode_variance': [7495.4375], 'each_episode_virtual_variance': [7652.311], 'combined_variance': [30266.648], 'num_episodes': [1325], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [220.69358490566037], 'counted_episodes': [1325], 'discounted_rewards': [-64.956154], 'new_pomdp_iteration_numbers': []}
[[5], [3, 4, 6, 7], [3, 4, 6, 7], [1], [1], [1], [1], [2], [2], [2], [2], [3, 4, 6, 7], [3, 4, 6, 7], [0]]
2025-08-04 05:15:47,850 - statistic.py:67 - synthesis initiated, design space: 81
2025-08-04 05:15:47,850 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 05:15:47,869 - synthesizer_ar.py:122 - value 116.1603 achieved after 5967.13 seconds
2025-08-04 05:15:47,878 - synthesizer_ar.py:122 - value 126.6941 achieved after 5967.13 seconds
2025-08-04 05:15:47,887 - synthesizer_ar.py:122 - value 141.0491 achieved after 5967.14 seconds
2025-08-04 05:15:47,908 - synthesizer_ar.py:122 - value 141.687 achieved after 5967.16 seconds
2025-08-04 05:15:47,929 - synthesizer_ar.py:122 - value 142.3652 achieved after 5967.19 seconds
2025-08-04 05:15:47,957 - synthesizer_ar.py:122 - value 143.9299 achieved after 5967.21 seconds
2025-08-04 05:15:47,978 - synthesizer_ar.py:122 - value 144.4418 achieved after 5967.23 seconds
2025-08-04 05:15:47,999 - synthesizer_ar.py:122 - value 144.961 achieved after 5967.26 seconds
2025-08-04 05:15:48,018 - synthesizer_ar.py:122 - value 149.0443 achieved after 5967.27 seconds
2025-08-04 05:15:48,033 - synthesizer_ar.py:122 - value 149.4537 achieved after 5967.29 seconds
2025-08-04 05:15:48,048 - synthesizer_ar.py:122 - value 149.8678 achieved after 5967.3 seconds
2025-08-04 05:15:48,051 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 05:15:48,051 - synthesizer.py:193 - o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 05:15:48,052 - synthesizer.py:198 - double-checking specification satisfiability:  : 149.86780192431883
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 10) & (y = 10))] 

method: AR, synthesis time: 0.2 s
number of holes: 4, family size: 81, quotient: 1018 states / 1170 actions
explored: 100 %
MDP stats: avg MDP size: 911, iterations: 88

optimum: 149.867802
--------------------
2025-08-04 05:15:48,056 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 05:15:48,061 - robust_rl_trainer.py:432 - Iteration 12 of pure RL loop
2025-08-04 05:15:48,100 - storm_vec_env.py:70 - Computing row map
2025-08-04 05:15:48,105 - storm_vec_env.py:97 - Computing transitions
2025-08-04 05:15:48,118 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 05:15:48,118 - storm_vec_env.py:114 - Computing sinks
2025-08-04 05:15:48,118 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 05:15:48,122 - storm_vec_env.py:143 - Computing labels
2025-08-04 05:15:48,122 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 05:15:48,122 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 05:15:48,122 - storm_vec_env.py:175 - Computing observations
2025-08-04 05:15:48,297 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 05:15:48,298 - father_agent.py:540 - Before training evaluation.
2025-08-04 05:15:48,479 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:15:48,482 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:15:58,953 - evaluation_results_class.py:131 - Average Return = -62.53333282470703
2025-08-04 05:15:58,953 - evaluation_results_class.py:133 - Average Virtual Goal Value = 13.631461143493652
2025-08-04 05:15:58,953 - evaluation_results_class.py:135 - Average Discounted Reward = 6.0978264808654785
2025-08-04 05:15:58,953 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9520599250936329
2025-08-04 05:15:58,953 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:15:58,953 - evaluation_results_class.py:141 - Variance of Return = 4132.60693359375
2025-08-04 05:15:58,953 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:15:58,953 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:15:58,953 - evaluation_results_class.py:147 - Average Episode Length = 170.53408239700374
2025-08-04 05:15:58,953 - evaluation_results_class.py:149 - Counted Episodes = 1335
2025-08-04 05:15:59,231 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:15:59,243 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:15:59,369 - father_agent.py:436 - Training agent on-policy
2025-08-04 05:16:08,035 - father_agent.py:386 - Step: 0, Training loss: 0.25406068563461304
2025-08-04 05:16:10,081 - father_agent.py:386 - Step: 5, Training loss: 0.41691020131111145
2025-08-04 05:16:12,094 - father_agent.py:386 - Step: 10, Training loss: 0.4056929349899292
2025-08-04 05:16:14,052 - father_agent.py:386 - Step: 15, Training loss: 0.4769570529460907
2025-08-04 05:16:16,006 - father_agent.py:386 - Step: 20, Training loss: 0.18089571595191956
2025-08-04 05:16:17,948 - father_agent.py:386 - Step: 25, Training loss: 0.11204899847507477
2025-08-04 05:16:19,894 - father_agent.py:386 - Step: 30, Training loss: 0.16051146388053894
2025-08-04 05:16:21,943 - father_agent.py:386 - Step: 35, Training loss: 0.2166140079498291
2025-08-04 05:16:23,949 - father_agent.py:386 - Step: 40, Training loss: 0.5734995007514954
2025-08-04 05:16:25,954 - father_agent.py:386 - Step: 45, Training loss: 0.2951965928077698
2025-08-04 05:16:27,961 - father_agent.py:386 - Step: 50, Training loss: 0.2698202133178711
2025-08-04 05:16:29,992 - father_agent.py:386 - Step: 55, Training loss: 0.5400598645210266
2025-08-04 05:16:32,049 - father_agent.py:386 - Step: 60, Training loss: 0.2996687889099121
2025-08-04 05:16:34,062 - father_agent.py:386 - Step: 65, Training loss: 0.3202284574508667
2025-08-04 05:16:36,014 - father_agent.py:386 - Step: 70, Training loss: 0.27544909715652466
2025-08-04 05:16:37,975 - father_agent.py:386 - Step: 75, Training loss: 0.35437870025634766
2025-08-04 05:16:39,965 - father_agent.py:386 - Step: 80, Training loss: 0.3439573645591736
2025-08-04 05:16:41,989 - father_agent.py:386 - Step: 85, Training loss: 0.9019111394882202
2025-08-04 05:16:43,942 - father_agent.py:386 - Step: 90, Training loss: 0.1741589605808258
2025-08-04 05:16:45,890 - father_agent.py:386 - Step: 95, Training loss: 0.39558619260787964
2025-08-04 05:16:47,808 - father_agent.py:386 - Step: 100, Training loss: 0.08401570469141006
2025-08-04 05:16:48,101 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:16:48,104 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:16:58,545 - evaluation_results_class.py:131 - Average Return = -56.24882125854492
2025-08-04 05:16:58,545 - evaluation_results_class.py:133 - Average Virtual Goal Value = 21.755226135253906
2025-08-04 05:16:58,545 - evaluation_results_class.py:135 - Average Discounted Reward = 7.830532550811768
2025-08-04 05:16:58,545 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9750505731625084
2025-08-04 05:16:58,546 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:16:58,546 - evaluation_results_class.py:141 - Variance of Return = 3153.14697265625
2025-08-04 05:16:58,546 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:16:58,546 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:16:58,546 - evaluation_results_class.py:147 - Average Episode Length = 160.54821308159137
2025-08-04 05:16:58,546 - evaluation_results_class.py:149 - Counted Episodes = 1483
2025-08-04 05:16:58,843 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:16:58,856 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:17:06,586 - father_agent.py:386 - Step: 105, Training loss: 0.4145243763923645
2025-08-04 05:17:08,480 - father_agent.py:386 - Step: 110, Training loss: 0.089744433760643
2025-08-04 05:17:10,379 - father_agent.py:386 - Step: 115, Training loss: 0.30193373560905457
2025-08-04 05:17:12,275 - father_agent.py:386 - Step: 120, Training loss: 0.56990647315979
2025-08-04 05:17:14,191 - father_agent.py:386 - Step: 125, Training loss: 0.27509427070617676
2025-08-04 05:17:16,077 - father_agent.py:386 - Step: 130, Training loss: 0.24369078874588013
2025-08-04 05:17:17,964 - father_agent.py:386 - Step: 135, Training loss: 0.2728383541107178
2025-08-04 05:17:19,898 - father_agent.py:386 - Step: 140, Training loss: 0.2648038864135742
2025-08-04 05:17:21,787 - father_agent.py:386 - Step: 145, Training loss: 0.15819916129112244
2025-08-04 05:17:23,687 - father_agent.py:386 - Step: 150, Training loss: 0.16658304631710052
2025-08-04 05:17:25,597 - father_agent.py:386 - Step: 155, Training loss: 0.4699200987815857
2025-08-04 05:17:27,523 - father_agent.py:386 - Step: 160, Training loss: 0.18832215666770935
2025-08-04 05:17:29,427 - father_agent.py:386 - Step: 165, Training loss: 0.43079668283462524
2025-08-04 05:17:31,350 - father_agent.py:386 - Step: 170, Training loss: 0.333462655544281
2025-08-04 05:17:33,253 - father_agent.py:386 - Step: 175, Training loss: 0.2269861102104187
2025-08-04 05:17:35,146 - father_agent.py:386 - Step: 180, Training loss: 0.3300793170928955
2025-08-04 05:17:37,042 - father_agent.py:386 - Step: 185, Training loss: 0.6769376397132874
2025-08-04 05:17:38,938 - father_agent.py:386 - Step: 190, Training loss: 0.18602512776851654
2025-08-04 05:17:40,823 - father_agent.py:386 - Step: 195, Training loss: 0.2714187502861023
2025-08-04 05:17:42,735 - father_agent.py:386 - Step: 200, Training loss: 0.12484877556562424
2025-08-04 05:17:43,035 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:17:43,037 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:17:53,948 - evaluation_results_class.py:131 - Average Return = -50.0839958190918
2025-08-04 05:17:53,948 - evaluation_results_class.py:133 - Average Virtual Goal Value = 26.482526779174805
2025-08-04 05:17:53,948 - evaluation_results_class.py:135 - Average Discounted Reward = 15.708271026611328
2025-08-04 05:17:53,948 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9570815450643777
2025-08-04 05:17:53,948 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:17:53,948 - evaluation_results_class.py:141 - Variance of Return = 3884.933837890625
2025-08-04 05:17:53,948 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:17:53,948 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:17:53,948 - evaluation_results_class.py:147 - Average Episode Length = 132.9785407725322
2025-08-04 05:17:53,948 - evaluation_results_class.py:149 - Counted Episodes = 1631
2025-08-04 05:17:54,242 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:17:54,255 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:17:58,415 - father_agent.py:386 - Step: 205, Training loss: 0.27104389667510986
2025-08-04 05:18:00,323 - father_agent.py:386 - Step: 210, Training loss: 0.2414717674255371
2025-08-04 05:18:02,231 - father_agent.py:386 - Step: 215, Training loss: 0.08841559290885925
2025-08-04 05:18:04,147 - father_agent.py:386 - Step: 220, Training loss: 0.40484195947647095
2025-08-04 05:18:06,068 - father_agent.py:386 - Step: 225, Training loss: 0.017967984080314636
2025-08-04 05:18:07,961 - father_agent.py:386 - Step: 230, Training loss: 0.16009259223937988
2025-08-04 05:18:09,858 - father_agent.py:386 - Step: 235, Training loss: 0.15436190366744995
2025-08-04 05:18:11,761 - father_agent.py:386 - Step: 240, Training loss: 0.20608070492744446
2025-08-04 05:18:13,673 - father_agent.py:386 - Step: 245, Training loss: 0.04715704917907715
2025-08-04 05:18:15,572 - father_agent.py:386 - Step: 250, Training loss: 0.4672325551509857
2025-08-04 05:18:17,476 - father_agent.py:386 - Step: 255, Training loss: 0.25975099205970764
2025-08-04 05:18:19,377 - father_agent.py:386 - Step: 260, Training loss: 0.15352407097816467
2025-08-04 05:18:21,269 - father_agent.py:386 - Step: 265, Training loss: 0.3250453472137451
2025-08-04 05:18:23,154 - father_agent.py:386 - Step: 270, Training loss: 0.22412243485450745
2025-08-04 05:18:25,036 - father_agent.py:386 - Step: 275, Training loss: 0.11959879100322723
2025-08-04 05:18:26,948 - father_agent.py:386 - Step: 280, Training loss: 0.4045226573944092
2025-08-04 05:18:28,849 - father_agent.py:386 - Step: 285, Training loss: 0.4427178204059601
2025-08-04 05:18:30,760 - father_agent.py:386 - Step: 290, Training loss: 0.22644953429698944
2025-08-04 05:18:32,647 - father_agent.py:386 - Step: 295, Training loss: 0.4529752731323242
2025-08-04 05:18:34,524 - father_agent.py:386 - Step: 300, Training loss: 0.5462599396705627
2025-08-04 05:18:34,815 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:18:34,818 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:18:45,388 - evaluation_results_class.py:131 - Average Return = -35.32785415649414
2025-08-04 05:18:45,388 - evaluation_results_class.py:133 - Average Virtual Goal Value = 43.71022033691406
2025-08-04 05:18:45,388 - evaluation_results_class.py:135 - Average Discounted Reward = 24.207250595092773
2025-08-04 05:18:45,388 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9879759519038076
2025-08-04 05:18:45,388 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:18:45,388 - evaluation_results_class.py:141 - Variance of Return = 2112.450439453125
2025-08-04 05:18:45,388 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:18:45,388 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:18:45,388 - evaluation_results_class.py:147 - Average Episode Length = 94.56993987975952
2025-08-04 05:18:45,388 - evaluation_results_class.py:149 - Counted Episodes = 2495
2025-08-04 05:18:45,682 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:18:45,694 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:18:49,916 - father_agent.py:386 - Step: 305, Training loss: 0.6385255455970764
2025-08-04 05:18:51,840 - father_agent.py:386 - Step: 310, Training loss: 0.19439265131950378
2025-08-04 05:18:53,754 - father_agent.py:386 - Step: 315, Training loss: 0.09255602955818176
2025-08-04 05:18:55,662 - father_agent.py:386 - Step: 320, Training loss: 0.34640178084373474
2025-08-04 05:18:57,555 - father_agent.py:386 - Step: 325, Training loss: 0.3417297899723053
2025-08-04 05:18:59,451 - father_agent.py:386 - Step: 330, Training loss: 0.20719656348228455
2025-08-04 05:19:01,363 - father_agent.py:386 - Step: 335, Training loss: 0.12533774971961975
2025-08-04 05:19:03,244 - father_agent.py:386 - Step: 340, Training loss: 0.39602017402648926
2025-08-04 05:19:05,130 - father_agent.py:386 - Step: 345, Training loss: 0.35724180936813354
2025-08-04 05:19:07,031 - father_agent.py:386 - Step: 350, Training loss: 0.5432264804840088
2025-08-04 05:19:08,935 - father_agent.py:386 - Step: 355, Training loss: 0.3276665508747101
2025-08-04 05:19:10,837 - father_agent.py:386 - Step: 360, Training loss: 0.02790336310863495
2025-08-04 05:19:12,737 - father_agent.py:386 - Step: 365, Training loss: 0.10016902536153793
2025-08-04 05:19:14,661 - father_agent.py:386 - Step: 370, Training loss: 0.5095372796058655
2025-08-04 05:19:16,552 - father_agent.py:386 - Step: 375, Training loss: 0.1340983808040619
2025-08-04 05:19:18,451 - father_agent.py:386 - Step: 380, Training loss: 0.19377660751342773
2025-08-04 05:19:20,345 - father_agent.py:386 - Step: 385, Training loss: 0.19073539972305298
2025-08-04 05:19:22,229 - father_agent.py:386 - Step: 390, Training loss: 0.3463900089263916
2025-08-04 05:19:24,107 - father_agent.py:386 - Step: 395, Training loss: 0.40597233176231384
2025-08-04 05:19:25,989 - father_agent.py:386 - Step: 400, Training loss: 0.10559093952178955
2025-08-04 05:19:26,281 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:19:26,283 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:19:37,193 - evaluation_results_class.py:131 - Average Return = -25.268566131591797
2025-08-04 05:19:37,193 - evaluation_results_class.py:133 - Average Virtual Goal Value = 54.316925048828125
2025-08-04 05:19:37,193 - evaluation_results_class.py:135 - Average Discounted Reward = 31.38654327392578
2025-08-04 05:19:37,193 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9948186528497409
2025-08-04 05:19:37,193 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:19:37,193 - evaluation_results_class.py:141 - Variance of Return = 1140.3857421875
2025-08-04 05:19:37,193 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:19:37,193 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:19:37,193 - evaluation_results_class.py:147 - Average Episode Length = 69.30944156591825
2025-08-04 05:19:37,193 - evaluation_results_class.py:149 - Counted Episodes = 3474
2025-08-04 05:19:37,486 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:19:37,499 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:19:41,648 - father_agent.py:386 - Step: 405, Training loss: 0.3565664291381836
2025-08-04 05:19:43,540 - father_agent.py:386 - Step: 410, Training loss: 0.23024585843086243
2025-08-04 05:19:45,422 - father_agent.py:386 - Step: 415, Training loss: 0.10953424870967865
2025-08-04 05:19:47,305 - father_agent.py:386 - Step: 420, Training loss: 0.1457536220550537
2025-08-04 05:19:49,196 - father_agent.py:386 - Step: 425, Training loss: 0.22902414202690125
2025-08-04 05:19:51,089 - father_agent.py:386 - Step: 430, Training loss: 0.20820605754852295
2025-08-04 05:19:52,978 - father_agent.py:386 - Step: 435, Training loss: 0.35759398341178894
2025-08-04 05:19:54,870 - father_agent.py:386 - Step: 440, Training loss: 0.31785717606544495
2025-08-04 05:19:56,772 - father_agent.py:386 - Step: 445, Training loss: 0.8093997836112976
2025-08-04 05:19:58,656 - father_agent.py:386 - Step: 450, Training loss: 0.37918585538864136
2025-08-04 05:20:00,537 - father_agent.py:386 - Step: 455, Training loss: 0.5320019721984863
2025-08-04 05:20:02,420 - father_agent.py:386 - Step: 460, Training loss: 0.09959297627210617
2025-08-04 05:20:04,313 - father_agent.py:386 - Step: 465, Training loss: 0.3632877469062805
2025-08-04 05:20:06,206 - father_agent.py:386 - Step: 470, Training loss: 0.32456207275390625
2025-08-04 05:20:08,102 - father_agent.py:386 - Step: 475, Training loss: 0.05036859214305878
2025-08-04 05:20:10,009 - father_agent.py:386 - Step: 480, Training loss: 0.3386675715446472
2025-08-04 05:20:11,904 - father_agent.py:386 - Step: 485, Training loss: 0.4067709743976593
2025-08-04 05:20:13,790 - father_agent.py:386 - Step: 490, Training loss: 0.2560296356678009
2025-08-04 05:20:15,699 - father_agent.py:386 - Step: 495, Training loss: 0.1850982904434204
2025-08-04 05:20:17,621 - father_agent.py:386 - Step: 500, Training loss: 0.34218698740005493
2025-08-04 05:20:17,911 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:20:17,913 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:20:28,416 - evaluation_results_class.py:131 - Average Return = -27.991390228271484
2025-08-04 05:20:28,416 - evaluation_results_class.py:133 - Average Virtual Goal Value = 51.84233856201172
2025-08-04 05:20:28,416 - evaluation_results_class.py:135 - Average Discounted Reward = 27.500221252441406
2025-08-04 05:20:28,416 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9979216152019003
2025-08-04 05:20:28,416 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:20:28,416 - evaluation_results_class.py:141 - Variance of Return = 1044.9930419921875
2025-08-04 05:20:28,416 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:20:28,416 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:20:28,416 - evaluation_results_class.py:147 - Average Episode Length = 80.68646080760095
2025-08-04 05:20:28,416 - evaluation_results_class.py:149 - Counted Episodes = 3368
2025-08-04 05:20:28,712 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:20:28,725 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:20:28,845 - father_agent.py:547 - Training finished.
2025-08-04 05:20:29,008 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:20:29,011 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:20:29,015 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 05:20:39,575 - evaluation_results_class.py:131 - Average Return = -29.68834114074707
2025-08-04 05:20:39,576 - evaluation_results_class.py:133 - Average Virtual Goal Value = 50.00416564941406
2025-08-04 05:20:39,576 - evaluation_results_class.py:135 - Average Discounted Reward = 26.111221313476562
2025-08-04 05:20:39,576 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9961563100576554
2025-08-04 05:20:39,576 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:20:39,576 - evaluation_results_class.py:141 - Variance of Return = 1152.2337646484375
2025-08-04 05:20:39,576 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:20:39,576 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:20:39,576 - evaluation_results_class.py:147 - Average Episode Length = 85.69474695707879
2025-08-04 05:20:39,576 - evaluation_results_class.py:149 - Counted Episodes = 3122
2025-08-04 05:20:39,871 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:20:39,874 - self_interpretable_extractor.py:286 - True
2025-08-04 05:20:39,887 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:20:57,331 - evaluation_results_class.py:131 - Average Return = -32.75495529174805
2025-08-04 05:20:57,332 - evaluation_results_class.py:133 - Average Virtual Goal Value = 46.3390998840332
2025-08-04 05:20:57,332 - evaluation_results_class.py:135 - Average Discounted Reward = 24.768800735473633
2025-08-04 05:20:57,332 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9886756841774142
2025-08-04 05:20:57,332 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:20:57,332 - evaluation_results_class.py:141 - Variance of Return = 1617.2725830078125
2025-08-04 05:20:57,332 - evaluation_results_class.py:143 - Current Best Return = -32.75495529174805
2025-08-04 05:20:57,332 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9886756841774142
2025-08-04 05:20:57,332 - evaluation_results_class.py:147 - Average Episode Length = 94.45894935514313
2025-08-04 05:20:57,332 - evaluation_results_class.py:149 - Counted Episodes = 3179
2025-08-04 05:20:57,332 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 05:20:57,332 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 14259 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 533, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
Buffer 1
2025-08-04 05:22:08,498 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 28692 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
Buffer 2
2025-08-04 05:23:19,425 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 42667 trajectories
Learned trajectory lengths  {512, 515, 518, 521, 524, 527, 530, 533, 536, 26, 539, 29, 542, 32, 545, 35, 548, 38, 551, 41, 554, 44, 557, 47, 560, 50, 563, 53, 566, 56, 569, 59, 572, 62, 575, 65, 578, 68, 581, 71, 584, 74, 587, 77, 590, 80, 593, 83, 596, 86, 599, 89, 602, 92, 605, 95, 608, 98, 611, 101, 614, 104, 617, 107, 620, 110, 623, 113, 626, 116, 629, 119, 632, 122, 635, 125, 638, 128, 641, 131, 644, 134, 647, 137, 650, 651, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299, 302, 305, 308, 311, 314, 317, 320, 323, 326, 329, 332, 335, 338, 341, 344, 347, 350, 353, 356, 359, 362, 365, 368, 371, 374, 377, 380, 383, 386, 389, 392, 395, 398, 401, 404, 407, 410, 413, 416, 419, 422, 425, 428, 431, 434, 437, 440, 443, 446, 449, 452, 455, 458, 461, 464, 467, 470, 473, 476, 479, 482, 485, 488, 491, 494, 497, 500, 503, 506, 509}
All trajectories collected
2025-08-04 05:24:30,402 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 05:24:30,403 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 42667 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_11.dot.
Learned FSC of size 2
2025-08-04 05:24:34,301 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 05:24:50,322 - evaluation_results_class.py:131 - Average Return = -88.1971664428711
2025-08-04 05:24:50,323 - evaluation_results_class.py:133 - Average Virtual Goal Value = -8.19717025756836
2025-08-04 05:24:50,323 - evaluation_results_class.py:135 - Average Discounted Reward = -30.565235137939453
2025-08-04 05:24:50,323 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:24:50,323 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:24:50,323 - evaluation_results_class.py:141 - Variance of Return = 5410.8818359375
2025-08-04 05:24:50,323 - evaluation_results_class.py:143 - Current Best Return = -88.1971664428711
2025-08-04 05:24:50,323 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:24:50,323 - evaluation_results_class.py:147 - Average Episode Length = 99.17138364779875
2025-08-04 05:24:50,323 - evaluation_results_class.py:149 - Counted Episodes = 3180
FSC Result: {'best_episode_return': -8.19717, 'best_return': -88.19717, 'goal_value': 0.0, 'returns_episodic': [-8.19717], 'returns': [-88.19717], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [5410.882], 'each_episode_virtual_variance': [5410.882], 'combined_variance': [21643.527], 'num_episodes': [3180], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [99.17138364779875], 'counted_episodes': [3180], 'discounted_rewards': [-30.565235], 'new_pomdp_iteration_numbers': []}
[[5], [3, 4, 6, 7], [3, 4, 6, 7], [1], [1], [1], [1], [2], [2], [2], [2], [3, 4, 6, 7], [3, 4, 6, 7], [0]]
2025-08-04 05:24:50,422 - statistic.py:67 - synthesis initiated, design space: 81
2025-08-04 05:24:50,422 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 05:24:50,439 - synthesizer_ar.py:122 - value 67.4454 achieved after 6509.7 seconds
2025-08-04 05:24:50,446 - synthesizer_ar.py:122 - value 74.319 achieved after 6509.7 seconds
2025-08-04 05:24:50,453 - synthesizer_ar.py:122 - value 77.3413 achieved after 6509.71 seconds
2025-08-04 05:24:50,476 - synthesizer_ar.py:122 - value 79.0259 achieved after 6509.73 seconds
2025-08-04 05:24:50,499 - synthesizer_ar.py:122 - value 80.803 achieved after 6509.76 seconds
2025-08-04 05:24:50,550 - synthesizer_ar.py:122 - value 81.9913 achieved after 6509.81 seconds
2025-08-04 05:24:50,573 - synthesizer_ar.py:122 - value 83.3885 achieved after 6509.83 seconds
2025-08-04 05:24:50,587 - synthesizer_ar.py:122 - value 84.3813 achieved after 6509.84 seconds
2025-08-04 05:24:50,597 - synthesizer_ar.py:122 - value 88.6518 achieved after 6509.85 seconds
2025-08-04 05:24:50,620 - synthesizer_ar.py:122 - value 88.8325 achieved after 6509.88 seconds
2025-08-04 05:24:50,627 - synthesizer_ar.py:122 - value 89.0312 achieved after 6509.88 seconds
2025-08-04 05:24:50,650 - synthesizer_ar.py:122 - value 90.183 achieved after 6509.91 seconds
2025-08-04 05:24:50,652 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 05:24:50,652 - synthesizer.py:193 - o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 05:24:50,653 - synthesizer.py:198 - double-checking specification satisfiability:  : 90.18298368592384
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 10) & (y = 10))] 

method: AR, synthesis time: 0.23 s
number of holes: 4, family size: 81, quotient: 1018 states / 1170 actions
explored: 100 %
MDP stats: avg MDP size: 908, iterations: 121

optimum: 90.182984
--------------------
2025-08-04 05:24:50,653 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 05:24:50,658 - robust_rl_trainer.py:432 - Iteration 13 of pure RL loop
2025-08-04 05:24:50,699 - storm_vec_env.py:70 - Computing row map
2025-08-04 05:24:50,704 - storm_vec_env.py:97 - Computing transitions
2025-08-04 05:24:50,717 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 05:24:50,717 - storm_vec_env.py:114 - Computing sinks
2025-08-04 05:24:50,717 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 05:24:50,721 - storm_vec_env.py:143 - Computing labels
2025-08-04 05:24:50,721 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 05:24:50,721 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 05:24:50,721 - storm_vec_env.py:175 - Computing observations
2025-08-04 05:24:50,911 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 05:24:50,912 - father_agent.py:540 - Before training evaluation.
2025-08-04 05:24:51,092 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:24:51,095 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:25:01,636 - evaluation_results_class.py:131 - Average Return = -28.149112701416016
2025-08-04 05:25:01,636 - evaluation_results_class.py:133 - Average Virtual Goal Value = 51.67942428588867
2025-08-04 05:25:01,636 - evaluation_results_class.py:135 - Average Discounted Reward = 27.071746826171875
2025-08-04 05:25:01,636 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9978567054500919
2025-08-04 05:25:01,636 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:25:01,636 - evaluation_results_class.py:141 - Variance of Return = 976.7821655273438
2025-08-04 05:25:01,636 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:25:01,636 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:25:01,636 - evaluation_results_class.py:147 - Average Episode Length = 81.91488058787507
2025-08-04 05:25:01,636 - evaluation_results_class.py:149 - Counted Episodes = 3266
2025-08-04 05:25:01,926 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:25:01,939 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:25:02,059 - father_agent.py:436 - Training agent on-policy
2025-08-04 05:25:10,806 - father_agent.py:386 - Step: 0, Training loss: 0.08742401003837585
2025-08-04 05:25:12,726 - father_agent.py:386 - Step: 5, Training loss: 0.21601882576942444
2025-08-04 05:25:14,628 - father_agent.py:386 - Step: 10, Training loss: 0.07750227302312851
2025-08-04 05:25:16,531 - father_agent.py:386 - Step: 15, Training loss: 0.4580036997795105
2025-08-04 05:25:18,424 - father_agent.py:386 - Step: 20, Training loss: 0.3768148422241211
2025-08-04 05:25:20,340 - father_agent.py:386 - Step: 25, Training loss: 0.3661530315876007
2025-08-04 05:25:22,221 - father_agent.py:386 - Step: 30, Training loss: 0.06883223354816437
2025-08-04 05:25:24,110 - father_agent.py:386 - Step: 35, Training loss: 0.18613217771053314
2025-08-04 05:25:25,993 - father_agent.py:386 - Step: 40, Training loss: 0.3807082176208496
2025-08-04 05:25:27,877 - father_agent.py:386 - Step: 45, Training loss: 0.2458268105983734
2025-08-04 05:25:29,766 - father_agent.py:386 - Step: 50, Training loss: 0.2006371021270752
2025-08-04 05:25:31,684 - father_agent.py:386 - Step: 55, Training loss: 0.20368117094039917
2025-08-04 05:25:33,590 - father_agent.py:386 - Step: 60, Training loss: 0.14224356412887573
2025-08-04 05:25:35,484 - father_agent.py:386 - Step: 65, Training loss: 0.15237963199615479
2025-08-04 05:25:37,382 - father_agent.py:386 - Step: 70, Training loss: 0.08254563808441162
2025-08-04 05:25:39,268 - father_agent.py:386 - Step: 75, Training loss: 0.27395254373550415
2025-08-04 05:25:41,155 - father_agent.py:386 - Step: 80, Training loss: 0.19797757267951965
2025-08-04 05:25:43,052 - father_agent.py:386 - Step: 85, Training loss: 0.16913564503192902
2025-08-04 05:25:44,938 - father_agent.py:386 - Step: 90, Training loss: 0.22027277946472168
2025-08-04 05:25:46,833 - father_agent.py:386 - Step: 95, Training loss: 0.5945037603378296
2025-08-04 05:25:48,705 - father_agent.py:386 - Step: 100, Training loss: 0.30153098702430725
2025-08-04 05:25:49,004 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:25:49,007 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:26:00,017 - evaluation_results_class.py:131 - Average Return = -20.74483871459961
2025-08-04 05:26:00,017 - evaluation_results_class.py:133 - Average Virtual Goal Value = 59.25516128540039
2025-08-04 05:26:00,017 - evaluation_results_class.py:135 - Average Discounted Reward = 32.54618453979492
2025-08-04 05:26:00,017 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:26:00,017 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:26:00,017 - evaluation_results_class.py:141 - Variance of Return = 398.153076171875
2025-08-04 05:26:00,017 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:26:00,017 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:26:00,017 - evaluation_results_class.py:147 - Average Episode Length = 50.02669563867764
2025-08-04 05:26:00,017 - evaluation_results_class.py:149 - Counted Episodes = 6443
2025-08-04 05:26:00,308 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:26:00,321 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:26:11,103 - father_agent.py:386 - Step: 105, Training loss: 0.22455498576164246
2025-08-04 05:26:13,060 - father_agent.py:386 - Step: 110, Training loss: 0.16672207415103912
2025-08-04 05:26:15,035 - father_agent.py:386 - Step: 115, Training loss: 0.38861531019210815
2025-08-04 05:26:17,021 - father_agent.py:386 - Step: 120, Training loss: 0.4140607714653015
2025-08-04 05:26:19,023 - father_agent.py:386 - Step: 125, Training loss: 0.500623345375061
2025-08-04 05:26:20,992 - father_agent.py:386 - Step: 130, Training loss: 0.2146543562412262
2025-08-04 05:26:22,937 - father_agent.py:386 - Step: 135, Training loss: 0.1598244160413742
2025-08-04 05:26:24,945 - father_agent.py:386 - Step: 140, Training loss: 0.3270717263221741
2025-08-04 05:26:26,926 - father_agent.py:386 - Step: 145, Training loss: 0.23941068351268768
2025-08-04 05:26:28,912 - father_agent.py:386 - Step: 150, Training loss: 0.20337893068790436
2025-08-04 05:26:30,899 - father_agent.py:386 - Step: 155, Training loss: 0.297587513923645
2025-08-04 05:26:32,881 - father_agent.py:386 - Step: 160, Training loss: 0.295918345451355
2025-08-04 05:26:34,858 - father_agent.py:386 - Step: 165, Training loss: 0.4779704511165619
2025-08-04 05:26:36,845 - father_agent.py:386 - Step: 170, Training loss: 0.17736226320266724
2025-08-04 05:26:38,839 - father_agent.py:386 - Step: 175, Training loss: 0.3771643340587616
2025-08-04 05:26:40,816 - father_agent.py:386 - Step: 180, Training loss: 0.4662937819957733
2025-08-04 05:26:42,766 - father_agent.py:386 - Step: 185, Training loss: 0.16312171518802643
2025-08-04 05:26:44,771 - father_agent.py:386 - Step: 190, Training loss: 0.40407872200012207
2025-08-04 05:26:46,778 - father_agent.py:386 - Step: 195, Training loss: 0.2807385325431824
2025-08-04 05:26:48,713 - father_agent.py:386 - Step: 200, Training loss: 0.3199310302734375
2025-08-04 05:26:48,993 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:26:48,996 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:26:59,887 - evaluation_results_class.py:131 - Average Return = -24.865583419799805
2025-08-04 05:26:59,887 - evaluation_results_class.py:133 - Average Virtual Goal Value = 55.13441467285156
2025-08-04 05:26:59,887 - evaluation_results_class.py:135 - Average Discounted Reward = 26.69507598876953
2025-08-04 05:26:59,887 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:26:59,887 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:26:59,887 - evaluation_results_class.py:141 - Variance of Return = 554.3411865234375
2025-08-04 05:26:59,887 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:26:59,887 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:26:59,887 - evaluation_results_class.py:147 - Average Episode Length = 58.93426883308715
2025-08-04 05:26:59,887 - evaluation_results_class.py:149 - Counted Episodes = 5416
2025-08-04 05:27:00,188 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:27:00,202 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:27:04,478 - father_agent.py:386 - Step: 205, Training loss: 0.44319459795951843
2025-08-04 05:27:06,441 - father_agent.py:386 - Step: 210, Training loss: 0.21864894032478333
2025-08-04 05:27:08,387 - father_agent.py:386 - Step: 215, Training loss: 0.15130390226840973
2025-08-04 05:27:10,323 - father_agent.py:386 - Step: 220, Training loss: 0.2958167493343353
2025-08-04 05:27:12,239 - father_agent.py:386 - Step: 225, Training loss: 0.2918700575828552
2025-08-04 05:27:14,156 - father_agent.py:386 - Step: 230, Training loss: 0.1757001280784607
2025-08-04 05:27:16,101 - father_agent.py:386 - Step: 235, Training loss: 0.3543436527252197
2025-08-04 05:27:18,025 - father_agent.py:386 - Step: 240, Training loss: 0.1710510551929474
2025-08-04 05:27:19,943 - father_agent.py:386 - Step: 245, Training loss: 0.27064648270606995
2025-08-04 05:27:21,845 - father_agent.py:386 - Step: 250, Training loss: 0.32682058215141296
2025-08-04 05:27:23,742 - father_agent.py:386 - Step: 255, Training loss: 0.24391672015190125
2025-08-04 05:27:25,661 - father_agent.py:386 - Step: 260, Training loss: 0.18343578279018402
2025-08-04 05:27:27,594 - father_agent.py:386 - Step: 265, Training loss: 0.37956148386001587
2025-08-04 05:27:29,521 - father_agent.py:386 - Step: 270, Training loss: 0.34080401062965393
2025-08-04 05:27:31,604 - father_agent.py:386 - Step: 275, Training loss: 0.3005216121673584
2025-08-04 05:27:33,589 - father_agent.py:386 - Step: 280, Training loss: 0.264693945646286
2025-08-04 05:27:35,557 - father_agent.py:386 - Step: 285, Training loss: 0.24575074017047882
2025-08-04 05:27:37,540 - father_agent.py:386 - Step: 290, Training loss: 0.4222494661808014
2025-08-04 05:27:39,531 - father_agent.py:386 - Step: 295, Training loss: 0.30427882075309753
2025-08-04 05:27:41,523 - father_agent.py:386 - Step: 300, Training loss: 0.1666589081287384
2025-08-04 05:27:41,927 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:27:41,931 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:27:53,008 - evaluation_results_class.py:131 - Average Return = -21.631696701049805
2025-08-04 05:27:53,008 - evaluation_results_class.py:133 - Average Virtual Goal Value = 58.36830139160156
2025-08-04 05:27:53,008 - evaluation_results_class.py:135 - Average Discounted Reward = 31.10721778869629
2025-08-04 05:27:53,008 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:27:53,008 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:27:53,009 - evaluation_results_class.py:141 - Variance of Return = 421.4117126464844
2025-08-04 05:27:53,009 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:27:53,009 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:27:53,009 - evaluation_results_class.py:147 - Average Episode Length = 51.93821399839099
2025-08-04 05:27:53,009 - evaluation_results_class.py:149 - Counted Episodes = 6215
2025-08-04 05:27:53,305 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:27:53,319 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:27:57,671 - father_agent.py:386 - Step: 305, Training loss: 0.24139729142189026
2025-08-04 05:27:59,707 - father_agent.py:386 - Step: 310, Training loss: 0.18897700309753418
2025-08-04 05:28:01,616 - father_agent.py:386 - Step: 315, Training loss: 0.3945583701133728
2025-08-04 05:28:03,528 - father_agent.py:386 - Step: 320, Training loss: 0.2618658244609833
2025-08-04 05:28:05,441 - father_agent.py:386 - Step: 325, Training loss: 0.15158537030220032
2025-08-04 05:28:07,349 - father_agent.py:386 - Step: 330, Training loss: 0.27625417709350586
2025-08-04 05:28:09,272 - father_agent.py:386 - Step: 335, Training loss: 0.4410367012023926
2025-08-04 05:28:11,191 - father_agent.py:386 - Step: 340, Training loss: 0.12613186240196228
2025-08-04 05:28:13,100 - father_agent.py:386 - Step: 345, Training loss: 0.21801526844501495
2025-08-04 05:28:15,026 - father_agent.py:386 - Step: 350, Training loss: 0.20460397005081177
2025-08-04 05:28:16,943 - father_agent.py:386 - Step: 355, Training loss: 0.39214688539505005
2025-08-04 05:28:18,872 - father_agent.py:386 - Step: 360, Training loss: 0.21405474841594696
2025-08-04 05:28:20,791 - father_agent.py:386 - Step: 365, Training loss: 0.5623673796653748
2025-08-04 05:28:22,677 - father_agent.py:386 - Step: 370, Training loss: 0.3180136978626251
2025-08-04 05:28:24,577 - father_agent.py:386 - Step: 375, Training loss: 0.3505070209503174
2025-08-04 05:28:26,474 - father_agent.py:386 - Step: 380, Training loss: 0.12853237986564636
2025-08-04 05:28:28,375 - father_agent.py:386 - Step: 385, Training loss: 0.3667486310005188
2025-08-04 05:28:30,282 - father_agent.py:386 - Step: 390, Training loss: 0.30849647521972656
2025-08-04 05:28:32,183 - father_agent.py:386 - Step: 395, Training loss: 0.3540806770324707
2025-08-04 05:28:34,084 - father_agent.py:386 - Step: 400, Training loss: 0.38448116183280945
2025-08-04 05:28:34,378 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:28:34,381 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:28:45,686 - evaluation_results_class.py:131 - Average Return = -22.058935165405273
2025-08-04 05:28:45,686 - evaluation_results_class.py:133 - Average Virtual Goal Value = 57.941062927246094
2025-08-04 05:28:45,686 - evaluation_results_class.py:135 - Average Discounted Reward = 31.081106185913086
2025-08-04 05:28:45,686 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:28:45,686 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:28:45,686 - evaluation_results_class.py:141 - Variance of Return = 480.135498046875
2025-08-04 05:28:45,686 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:28:45,686 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:28:45,687 - evaluation_results_class.py:147 - Average Episode Length = 51.08435266084194
2025-08-04 05:28:45,687 - evaluation_results_class.py:149 - Counted Episodes = 6295
2025-08-04 05:28:46,053 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:28:46,066 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:28:50,230 - father_agent.py:386 - Step: 405, Training loss: 0.42127126455307007
2025-08-04 05:28:52,139 - father_agent.py:386 - Step: 410, Training loss: 0.32741618156433105
2025-08-04 05:28:54,058 - father_agent.py:386 - Step: 415, Training loss: 0.4154708981513977
2025-08-04 05:28:55,997 - father_agent.py:386 - Step: 420, Training loss: 0.43878307938575745
2025-08-04 05:28:57,902 - father_agent.py:386 - Step: 425, Training loss: 0.2907680869102478
2025-08-04 05:28:59,838 - father_agent.py:386 - Step: 430, Training loss: 0.20816923677921295
2025-08-04 05:29:01,748 - father_agent.py:386 - Step: 435, Training loss: 0.5356432199478149
2025-08-04 05:29:03,662 - father_agent.py:386 - Step: 440, Training loss: 0.4226953685283661
2025-08-04 05:29:05,564 - father_agent.py:386 - Step: 445, Training loss: 0.29303500056266785
2025-08-04 05:29:07,454 - father_agent.py:386 - Step: 450, Training loss: 0.08326130360364914
2025-08-04 05:29:09,384 - father_agent.py:386 - Step: 455, Training loss: 0.33521437644958496
2025-08-04 05:29:11,295 - father_agent.py:386 - Step: 460, Training loss: 0.17015224695205688
2025-08-04 05:29:13,200 - father_agent.py:386 - Step: 465, Training loss: 0.21479524672031403
2025-08-04 05:29:15,125 - father_agent.py:386 - Step: 470, Training loss: 0.2736048102378845
2025-08-04 05:29:17,030 - father_agent.py:386 - Step: 475, Training loss: 0.1556100845336914
2025-08-04 05:29:18,924 - father_agent.py:386 - Step: 480, Training loss: 0.35997891426086426
2025-08-04 05:29:20,830 - father_agent.py:386 - Step: 485, Training loss: 0.2536797821521759
2025-08-04 05:29:22,730 - father_agent.py:386 - Step: 490, Training loss: 0.1359095275402069
2025-08-04 05:29:24,639 - father_agent.py:386 - Step: 495, Training loss: 0.3126305341720581
2025-08-04 05:29:26,543 - father_agent.py:386 - Step: 500, Training loss: 0.23424461483955383
2025-08-04 05:29:26,843 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:29:26,845 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:29:37,447 - evaluation_results_class.py:131 - Average Return = -25.18490982055664
2025-08-04 05:29:37,447 - evaluation_results_class.py:133 - Average Virtual Goal Value = 54.81509017944336
2025-08-04 05:29:37,447 - evaluation_results_class.py:135 - Average Discounted Reward = 26.56181526184082
2025-08-04 05:29:37,447 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:29:37,447 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:29:37,447 - evaluation_results_class.py:141 - Variance of Return = 593.9874877929688
2025-08-04 05:29:37,447 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:29:37,447 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:29:37,447 - evaluation_results_class.py:147 - Average Episode Length = 58.152545454545454
2025-08-04 05:29:37,447 - evaluation_results_class.py:149 - Counted Episodes = 5500
2025-08-04 05:29:37,748 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:29:37,762 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:29:37,882 - father_agent.py:547 - Training finished.
2025-08-04 05:29:38,059 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:29:38,063 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:29:38,066 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 05:29:48,700 - evaluation_results_class.py:131 - Average Return = -24.91020393371582
2025-08-04 05:29:48,701 - evaluation_results_class.py:133 - Average Virtual Goal Value = 55.08979415893555
2025-08-04 05:29:48,701 - evaluation_results_class.py:135 - Average Discounted Reward = 26.773372650146484
2025-08-04 05:29:48,701 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:29:48,701 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:29:48,701 - evaluation_results_class.py:141 - Variance of Return = 559.9508056640625
2025-08-04 05:29:48,701 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:29:48,701 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:29:48,701 - evaluation_results_class.py:147 - Average Episode Length = 58.329736649597656
2025-08-04 05:29:48,701 - evaluation_results_class.py:149 - Counted Episodes = 5468
2025-08-04 05:29:48,998 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:29:49,000 - self_interpretable_extractor.py:286 - True
2025-08-04 05:29:49,015 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:30:06,646 - evaluation_results_class.py:131 - Average Return = -26.38595199584961
2025-08-04 05:30:06,646 - evaluation_results_class.py:133 - Average Virtual Goal Value = 53.61404800415039
2025-08-04 05:30:06,646 - evaluation_results_class.py:135 - Average Discounted Reward = 25.488117218017578
2025-08-04 05:30:06,646 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:30:06,646 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:30:06,646 - evaluation_results_class.py:141 - Variance of Return = 686.5606079101562
2025-08-04 05:30:06,646 - evaluation_results_class.py:143 - Current Best Return = -26.38595199584961
2025-08-04 05:30:06,646 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:30:06,646 - evaluation_results_class.py:147 - Average Episode Length = 58.940622737146995
2025-08-04 05:30:06,646 - evaluation_results_class.py:149 - Counted Episodes = 5524
2025-08-04 05:30:06,646 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 05:30:06,647 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 25486 trajectories
Learned trajectory lengths  {128, 131, 134, 137, 140, 143, 146, 149, 23, 152, 26, 155, 29, 158, 32, 161, 35, 164, 38, 167, 41, 170, 44, 173, 47, 176, 50, 179, 53, 182, 56, 185, 59, 188, 62, 191, 65, 194, 68, 197, 71, 200, 74, 203, 77, 80, 83, 86, 215, 89, 218, 92, 95, 98, 101, 230, 104, 107, 110, 113, 116, 119, 122, 125}
Buffer 1
2025-08-04 05:31:17,158 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 50849 trajectories
Learned trajectory lengths  {128, 131, 134, 137, 140, 143, 146, 149, 23, 152, 26, 155, 29, 158, 32, 161, 35, 164, 38, 167, 41, 170, 44, 173, 47, 176, 50, 179, 53, 182, 56, 185, 59, 188, 62, 191, 65, 194, 68, 197, 71, 200, 74, 203, 77, 206, 80, 209, 83, 86, 215, 89, 218, 92, 95, 98, 101, 230, 104, 233, 107, 236, 110, 113, 116, 245, 119, 122, 125}
Buffer 2
2025-08-04 05:32:27,871 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 76203 trajectories
Learned trajectory lengths  {128, 131, 134, 137, 140, 143, 146, 275, 149, 23, 152, 281, 26, 155, 29, 158, 32, 161, 35, 164, 38, 167, 41, 170, 44, 173, 47, 176, 50, 179, 53, 182, 56, 185, 59, 188, 62, 191, 65, 194, 68, 197, 71, 200, 74, 203, 77, 206, 80, 209, 83, 212, 86, 215, 89, 218, 92, 95, 98, 101, 230, 104, 233, 107, 236, 110, 239, 113, 242, 116, 245, 119, 122, 125}
All trajectories collected
2025-08-04 05:33:38,278 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 05:33:38,278 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 76203 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 2
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_12.dot.
Learned FSC of size 2
2025-08-04 05:33:42,209 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 05:33:58,497 - evaluation_results_class.py:131 - Average Return = -89.5328369140625
2025-08-04 05:33:58,497 - evaluation_results_class.py:133 - Average Virtual Goal Value = -9.532837867736816
2025-08-04 05:33:58,497 - evaluation_results_class.py:135 - Average Discounted Reward = -26.60930061340332
2025-08-04 05:33:58,497 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:33:58,497 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:33:58,497 - evaluation_results_class.py:141 - Variance of Return = 5234.681640625
2025-08-04 05:33:58,497 - evaluation_results_class.py:143 - Current Best Return = -89.5328369140625
2025-08-04 05:33:58,497 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:33:58,497 - evaluation_results_class.py:147 - Average Episode Length = 58.988933236574745
2025-08-04 05:33:58,497 - evaluation_results_class.py:149 - Counted Episodes = 5512
FSC Result: {'best_episode_return': -9.532838, 'best_return': -89.53284, 'goal_value': 0.0, 'returns_episodic': [-9.532838], 'returns': [-89.53284], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [5234.6816], 'each_episode_virtual_variance': [5234.6816], 'combined_variance': [20938.727], 'num_episodes': [5512], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [58.988933236574745], 'counted_episodes': [5512], 'discounted_rewards': [-26.6093], 'new_pomdp_iteration_numbers': []}
[[5], [3, 4, 6, 7], [3, 4, 6, 7], [1], [1], [1], [1], [2], [2], [2], [2], [3, 4, 6, 7], [3, 4, 6, 7], [0]]
2025-08-04 05:33:58,609 - statistic.py:67 - synthesis initiated, design space: 81
2025-08-04 05:33:58,609 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 05:33:58,626 - synthesizer_ar.py:122 - value 67.5599 achieved after 7057.88 seconds
2025-08-04 05:33:58,633 - synthesizer_ar.py:122 - value 72.1744 achieved after 7057.89 seconds
2025-08-04 05:33:58,649 - synthesizer_ar.py:122 - value 74.7085 achieved after 7057.91 seconds
2025-08-04 05:33:58,656 - synthesizer_ar.py:122 - value 78.4525 achieved after 7057.91 seconds
2025-08-04 05:33:58,672 - synthesizer_ar.py:122 - value 83.3534 achieved after 7057.93 seconds
2025-08-04 05:33:58,679 - synthesizer_ar.py:122 - value 86.3842 achieved after 7057.94 seconds
2025-08-04 05:33:58,745 - synthesizer_ar.py:122 - value 88.0473 achieved after 7058.0 seconds
2025-08-04 05:33:58,800 - synthesizer_ar.py:122 - value 88.9424 achieved after 7058.06 seconds
2025-08-04 05:33:58,807 - synthesizer_ar.py:122 - value 91.4485 achieved after 7058.06 seconds
2025-08-04 05:33:58,809 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 05:33:58,809 - synthesizer.py:193 - o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 05:33:58,810 - synthesizer.py:198 - double-checking specification satisfiability:  : 91.44851266272805
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 10) & (y = 10))] 

method: AR, synthesis time: 0.2 s
number of holes: 4, family size: 81, quotient: 1018 states / 1170 actions
explored: 100 %
MDP stats: avg MDP size: 910, iterations: 109

optimum: 91.448513
--------------------
2025-08-04 05:33:58,810 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=3, o1y=5, o2x=6, o2y=6
2025-08-04 05:33:58,816 - robust_rl_trainer.py:432 - Iteration 14 of pure RL loop
2025-08-04 05:33:58,857 - storm_vec_env.py:70 - Computing row map
2025-08-04 05:33:58,862 - storm_vec_env.py:97 - Computing transitions
2025-08-04 05:33:58,875 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 05:33:58,875 - storm_vec_env.py:114 - Computing sinks
2025-08-04 05:33:58,876 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 05:33:58,879 - storm_vec_env.py:143 - Computing labels
2025-08-04 05:33:58,879 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 05:33:58,879 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 05:33:58,879 - storm_vec_env.py:175 - Computing observations
2025-08-04 05:33:59,079 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 05:33:59,081 - father_agent.py:540 - Before training evaluation.
2025-08-04 05:33:59,261 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:33:59,263 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:34:10,026 - evaluation_results_class.py:131 - Average Return = -25.312885284423828
2025-08-04 05:34:10,026 - evaluation_results_class.py:133 - Average Virtual Goal Value = 54.68711471557617
2025-08-04 05:34:10,026 - evaluation_results_class.py:135 - Average Discounted Reward = 26.488786697387695
2025-08-04 05:34:10,026 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:34:10,026 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:34:10,026 - evaluation_results_class.py:141 - Variance of Return = 616.4454956054688
2025-08-04 05:34:10,026 - evaluation_results_class.py:143 - Current Best Return = -17.955677032470703
2025-08-04 05:34:10,026 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:34:10,027 - evaluation_results_class.py:147 - Average Episode Length = 57.96733212341198
2025-08-04 05:34:10,027 - evaluation_results_class.py:149 - Counted Episodes = 5510
2025-08-04 05:34:10,317 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:34:10,331 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:34:10,457 - father_agent.py:436 - Training agent on-policy
2025-08-04 05:34:19,267 - father_agent.py:386 - Step: 0, Training loss: 0.3375914394855499
2025-08-04 05:34:21,183 - father_agent.py:386 - Step: 5, Training loss: 0.37175026535987854
2025-08-04 05:34:23,080 - father_agent.py:386 - Step: 10, Training loss: 0.21521756052970886
2025-08-04 05:34:25,028 - father_agent.py:386 - Step: 15, Training loss: 0.1592848002910614
2025-08-04 05:34:26,909 - father_agent.py:386 - Step: 20, Training loss: 0.2761290967464447
2025-08-04 05:34:28,777 - father_agent.py:386 - Step: 25, Training loss: 0.2479662448167801
2025-08-04 05:34:30,725 - father_agent.py:386 - Step: 30, Training loss: 0.6478481292724609
2025-08-04 05:34:32,651 - father_agent.py:386 - Step: 35, Training loss: 0.36154675483703613
2025-08-04 05:34:34,566 - father_agent.py:386 - Step: 40, Training loss: 0.47174882888793945
2025-08-04 05:34:36,469 - father_agent.py:386 - Step: 45, Training loss: 0.2247072160243988
2025-08-04 05:34:38,363 - father_agent.py:386 - Step: 50, Training loss: 0.23187020421028137
2025-08-04 05:34:40,268 - father_agent.py:386 - Step: 55, Training loss: 0.21728260815143585
2025-08-04 05:34:42,164 - father_agent.py:386 - Step: 60, Training loss: 0.2754516303539276
2025-08-04 05:34:44,087 - father_agent.py:386 - Step: 65, Training loss: 0.08390896767377853
2025-08-04 05:34:46,007 - father_agent.py:386 - Step: 70, Training loss: 0.1129654198884964
2025-08-04 05:34:47,925 - father_agent.py:386 - Step: 75, Training loss: 0.2840101718902588
2025-08-04 05:34:49,862 - father_agent.py:386 - Step: 80, Training loss: 0.2986670732498169
2025-08-04 05:34:51,804 - father_agent.py:386 - Step: 85, Training loss: 0.12231671065092087
2025-08-04 05:34:53,706 - father_agent.py:386 - Step: 90, Training loss: 0.27692317962646484
2025-08-04 05:34:55,619 - father_agent.py:386 - Step: 95, Training loss: 0.15871861577033997
2025-08-04 05:34:57,510 - father_agent.py:386 - Step: 100, Training loss: 0.20452606678009033
2025-08-04 05:34:57,807 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:34:57,811 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:35:08,482 - evaluation_results_class.py:131 - Average Return = -17.913331985473633
2025-08-04 05:35:08,482 - evaluation_results_class.py:133 - Average Virtual Goal Value = 62.086666107177734
2025-08-04 05:35:08,482 - evaluation_results_class.py:135 - Average Discounted Reward = 34.709259033203125
2025-08-04 05:35:08,482 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:35:08,482 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:35:08,483 - evaluation_results_class.py:141 - Variance of Return = 119.29991912841797
2025-08-04 05:35:08,483 - evaluation_results_class.py:143 - Current Best Return = -17.913331985473633
2025-08-04 05:35:08,483 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:35:08,483 - evaluation_results_class.py:147 - Average Episode Length = 49.83377104897266
2025-08-04 05:35:08,483 - evaluation_results_class.py:149 - Counted Episodes = 6473
2025-08-04 05:35:08,778 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:35:08,792 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:35:17,400 - father_agent.py:386 - Step: 105, Training loss: 0.3211766481399536
2025-08-04 05:35:19,426 - father_agent.py:386 - Step: 110, Training loss: 0.3743092715740204
2025-08-04 05:35:21,423 - father_agent.py:386 - Step: 115, Training loss: 0.11733052134513855
2025-08-04 05:35:23,353 - father_agent.py:386 - Step: 120, Training loss: 0.2965834140777588
2025-08-04 05:35:25,257 - father_agent.py:386 - Step: 125, Training loss: 0.3554607033729553
2025-08-04 05:35:27,153 - father_agent.py:386 - Step: 130, Training loss: 0.48003119230270386
2025-08-04 05:35:29,080 - father_agent.py:386 - Step: 135, Training loss: 0.5151112675666809
2025-08-04 05:35:30,980 - father_agent.py:386 - Step: 140, Training loss: 0.35064488649368286
2025-08-04 05:35:32,871 - father_agent.py:386 - Step: 145, Training loss: 0.19023814797401428
2025-08-04 05:35:34,754 - father_agent.py:386 - Step: 150, Training loss: 0.309037446975708
2025-08-04 05:35:36,643 - father_agent.py:386 - Step: 155, Training loss: 0.3187396824359894
2025-08-04 05:35:38,534 - father_agent.py:386 - Step: 160, Training loss: 0.16597144305706024
2025-08-04 05:35:40,475 - father_agent.py:386 - Step: 165, Training loss: 0.45222437381744385
2025-08-04 05:35:42,416 - father_agent.py:386 - Step: 170, Training loss: 0.19655928015708923
2025-08-04 05:35:44,342 - father_agent.py:386 - Step: 175, Training loss: 0.24875058233737946
2025-08-04 05:35:46,255 - father_agent.py:386 - Step: 180, Training loss: 0.21570374071598053
2025-08-04 05:35:48,176 - father_agent.py:386 - Step: 185, Training loss: 0.3704656958580017
2025-08-04 05:35:50,102 - father_agent.py:386 - Step: 190, Training loss: 0.498421311378479
2025-08-04 05:35:51,999 - father_agent.py:386 - Step: 195, Training loss: 0.28843414783477783
2025-08-04 05:35:53,902 - father_agent.py:386 - Step: 200, Training loss: 0.22596792876720428
2025-08-04 05:35:54,182 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:35:54,185 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:36:04,905 - evaluation_results_class.py:131 - Average Return = -22.06085777282715
2025-08-04 05:36:04,905 - evaluation_results_class.py:133 - Average Virtual Goal Value = 57.939144134521484
2025-08-04 05:36:04,905 - evaluation_results_class.py:135 - Average Discounted Reward = 30.056917190551758
2025-08-04 05:36:04,905 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:36:04,905 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:36:04,905 - evaluation_results_class.py:141 - Variance of Return = 394.9240417480469
2025-08-04 05:36:04,905 - evaluation_results_class.py:143 - Current Best Return = -17.913331985473633
2025-08-04 05:36:04,905 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:36:04,905 - evaluation_results_class.py:147 - Average Episode Length = 54.36514663502289
2025-08-04 05:36:04,905 - evaluation_results_class.py:149 - Counted Episodes = 5899
2025-08-04 05:36:05,208 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:36:05,222 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:36:09,471 - father_agent.py:386 - Step: 205, Training loss: 0.48658204078674316
2025-08-04 05:36:11,414 - father_agent.py:386 - Step: 210, Training loss: 0.17979197204113007
2025-08-04 05:36:13,365 - father_agent.py:386 - Step: 215, Training loss: 0.5581255555152893
2025-08-04 05:36:15,305 - father_agent.py:386 - Step: 220, Training loss: 0.7380433082580566
2025-08-04 05:36:17,270 - father_agent.py:386 - Step: 225, Training loss: 0.16070950031280518

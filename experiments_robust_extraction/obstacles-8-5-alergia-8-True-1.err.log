2025-08-04 05:36:18.771128: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-04 05:36:18.772997: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-04 05:36:18.803348: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-08-04 05:36:18.803393: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-08-04 05:36:18.804642: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-08-04 05:36:18.810443: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2025-08-04 05:36:18.810657: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-08-04 05:36:19.344921: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
INFO:paynt.parser.sketch:loading sketch from /home/ihudak/synthesis/models_robust_subset/obstacles-8-5/sketch.templ ...
INFO:paynt.parser.sketch:assuming sketch in PRISM format...
DEBUG:paynt.parser.prism_parser:PRISM model type: POMDP
INFO:paynt.parser.prism_parser:processing hole definitions...
INFO:paynt.parser.prism_parser:loading properties from /home/ihudak/synthesis/models_robust_subset/obstacles-8-5/sketch.props ...
INFO:paynt.parser.prism_parser:found the following specification: optimality: R{"penalty"}min=? [F ((x = 8) & (y = 8))] 
DEBUG:paynt.parser.jani:constructing JANI program...
DEBUG:paynt.parser.jani:constructing the quotient...
DEBUG:paynt.parser.jani:associating choices of the quotient with hole assignments...
INFO:paynt.parser.sketch:sketch parsing OK
WARNING:paynt.parser.sketch:WARNING: choice labeling for the quotient is not canonic
DEBUG:paynt.parser.sketch:constructed explicit quotient having 468 states and 975 choices
INFO:paynt.parser.sketch:found the following specification optimality: R{"rewardmodel_penalty"}min=? [F ((x = 8) & (y = 8))] 
INFO:environment.vectorized_sim_initializer:Compiling model obstacles-8-5...
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
ERROR:environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.
INFO:environment.environment_wrapper_vec:Vectorized simulator initialized with 256 environments.
INFO:root:Agent initialized
INFO:root:Replay buffer initialized
INFO:root:Collector driver initialized
INFO:robust_rl.robust_rl_trainer:Iteration 1 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1131.3702392578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1106.0267333984375
INFO:tools.evaluation_results_class:Average Discounted Reward = -307.549560546875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.31679389312977096
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 331056.4375
INFO:tools.evaluation_results_class:Current Best Return = -1131.3702392578125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.31679389312977096
INFO:tools.evaluation_results_class:Average Episode Length = 569.4885496183206
INFO:tools.evaluation_results_class:Counted Episodes = 524
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 202.79812622070312
INFO:agents.father_agent:Step: 5, Training loss: 6.999893665313721
INFO:agents.father_agent:Step: 10, Training loss: 5.069169044494629
INFO:agents.father_agent:Step: 15, Training loss: 14.463461875915527
INFO:agents.father_agent:Step: 20, Training loss: 6.45412015914917
INFO:agents.father_agent:Step: 25, Training loss: 8.677016258239746
INFO:agents.father_agent:Step: 30, Training loss: 5.415284633636475
INFO:agents.father_agent:Step: 35, Training loss: 8.642400741577148
INFO:agents.father_agent:Step: 40, Training loss: 8.257278442382812
INFO:agents.father_agent:Step: 45, Training loss: 5.306501388549805
INFO:agents.father_agent:Step: 50, Training loss: 4.200691223144531
INFO:agents.father_agent:Step: 55, Training loss: 5.827817440032959
INFO:agents.father_agent:Step: 60, Training loss: 5.185117721557617
INFO:agents.father_agent:Step: 65, Training loss: 5.170769214630127
INFO:agents.father_agent:Step: 70, Training loss: 6.65553617477417
INFO:agents.father_agent:Step: 75, Training loss: 6.737209320068359
INFO:agents.father_agent:Step: 80, Training loss: 5.66944694519043
INFO:agents.father_agent:Step: 85, Training loss: 5.367969989776611
INFO:agents.father_agent:Step: 90, Training loss: 5.3019633293151855
INFO:agents.father_agent:Step: 95, Training loss: 4.639986515045166
INFO:agents.father_agent:Step: 100, Training loss: 4.461611747741699
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -104.06402587890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -24.064027786254883
INFO:tools.evaluation_results_class:Average Discounted Reward = -44.182220458984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8855.87109375
INFO:tools.evaluation_results_class:Current Best Return = -104.06402587890625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 99.14221364221365
INFO:tools.evaluation_results_class:Counted Episodes = 3108
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 4.7826948165893555
INFO:agents.father_agent:Step: 110, Training loss: 4.820728302001953
INFO:agents.father_agent:Step: 115, Training loss: 4.450231075286865
INFO:agents.father_agent:Step: 120, Training loss: 4.246068000793457
INFO:agents.father_agent:Step: 125, Training loss: 4.086848258972168
INFO:agents.father_agent:Step: 130, Training loss: 5.521506309509277
INFO:agents.father_agent:Step: 135, Training loss: 4.129128456115723
INFO:agents.father_agent:Step: 140, Training loss: 3.590630054473877
INFO:agents.father_agent:Step: 145, Training loss: 7.104152202606201
INFO:agents.father_agent:Step: 150, Training loss: 3.585496425628662
INFO:agents.father_agent:Step: 155, Training loss: 2.912667751312256
INFO:agents.father_agent:Step: 160, Training loss: 3.958212375640869
INFO:agents.father_agent:Step: 165, Training loss: 3.66904878616333
INFO:agents.father_agent:Step: 170, Training loss: 2.850506544113159
INFO:agents.father_agent:Step: 175, Training loss: 4.469029426574707
INFO:agents.father_agent:Step: 180, Training loss: 4.687090873718262
INFO:agents.father_agent:Step: 185, Training loss: 4.311385154724121
INFO:agents.father_agent:Step: 190, Training loss: 2.434455633163452
INFO:agents.father_agent:Step: 195, Training loss: 4.992140293121338
INFO:agents.father_agent:Step: 200, Training loss: 3.598881483078003
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.621768951416016
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 26.378232955932617
INFO:tools.evaluation_results_class:Average Discounted Reward = -3.7443037033081055
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3973.780517578125
INFO:tools.evaluation_results_class:Current Best Return = -53.621768951416016
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.79312304631998
INFO:tools.evaluation_results_class:Counted Episodes = 3519
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 2.111912727355957
INFO:agents.father_agent:Step: 210, Training loss: 2.863858938217163
INFO:agents.father_agent:Step: 215, Training loss: 4.392488479614258
INFO:agents.father_agent:Step: 220, Training loss: 3.4214017391204834
INFO:agents.father_agent:Step: 225, Training loss: 2.158501625061035
INFO:agents.father_agent:Step: 230, Training loss: 4.990423202514648
INFO:agents.father_agent:Step: 235, Training loss: 3.6046438217163086
INFO:agents.father_agent:Step: 240, Training loss: 2.185356378555298
INFO:agents.father_agent:Step: 245, Training loss: 2.6509275436401367
INFO:agents.father_agent:Step: 250, Training loss: 3.034937620162964
INFO:agents.father_agent:Step: 255, Training loss: 2.966449737548828
INFO:agents.father_agent:Step: 260, Training loss: 2.1687850952148438
INFO:agents.father_agent:Step: 265, Training loss: 2.955864906311035
INFO:agents.father_agent:Step: 270, Training loss: 3.726423501968384
INFO:agents.father_agent:Step: 275, Training loss: 2.061990976333618
INFO:agents.father_agent:Step: 280, Training loss: 2.818622589111328
INFO:agents.father_agent:Step: 285, Training loss: 3.547217845916748
INFO:agents.father_agent:Step: 290, Training loss: 2.320222854614258
INFO:agents.father_agent:Step: 295, Training loss: 3.2942185401916504
INFO:agents.father_agent:Step: 300, Training loss: 3.8508734703063965
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -46.47800827026367
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 33.52199172973633
INFO:tools.evaluation_results_class:Average Discounted Reward = 2.532562017440796
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4072.94091796875
INFO:tools.evaluation_results_class:Current Best Return = -46.47800827026367
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.20670698185816
INFO:tools.evaluation_results_class:Counted Episodes = 3638
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.6907731294631958
INFO:agents.father_agent:Step: 310, Training loss: 3.5188841819763184
INFO:agents.father_agent:Step: 315, Training loss: 2.665771961212158
INFO:agents.father_agent:Step: 320, Training loss: 2.366917848587036
INFO:agents.father_agent:Step: 325, Training loss: 1.9432450532913208
INFO:agents.father_agent:Step: 330, Training loss: 3.1431779861450195
INFO:agents.father_agent:Step: 335, Training loss: 2.404710054397583
INFO:agents.father_agent:Step: 340, Training loss: 2.0356974601745605
INFO:agents.father_agent:Step: 345, Training loss: 3.496746778488159
INFO:agents.father_agent:Step: 350, Training loss: 1.9111958742141724
INFO:agents.father_agent:Step: 355, Training loss: 2.0083835124969482
INFO:agents.father_agent:Step: 360, Training loss: 4.124835014343262
INFO:agents.father_agent:Step: 365, Training loss: 2.479755401611328
INFO:agents.father_agent:Step: 370, Training loss: 3.2520220279693604
INFO:agents.father_agent:Step: 375, Training loss: 2.4341371059417725
INFO:agents.father_agent:Step: 380, Training loss: 2.9771125316619873
INFO:agents.father_agent:Step: 385, Training loss: 2.8375372886657715
INFO:agents.father_agent:Step: 390, Training loss: 1.8780596256256104
INFO:agents.father_agent:Step: 395, Training loss: 3.1848740577697754
INFO:agents.father_agent:Step: 400, Training loss: 2.8712966442108154
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -48.535072326660156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 31.464929580688477
INFO:tools.evaluation_results_class:Average Discounted Reward = 2.609086275100708
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7126.69873046875
INFO:tools.evaluation_results_class:Current Best Return = -46.47800827026367
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 87.12198502911006
INFO:tools.evaluation_results_class:Counted Episodes = 3607
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.6381094455718994
INFO:agents.father_agent:Step: 410, Training loss: 3.2741851806640625
INFO:agents.father_agent:Step: 415, Training loss: 2.5042622089385986
INFO:agents.father_agent:Step: 420, Training loss: 2.848496198654175
INFO:agents.father_agent:Step: 425, Training loss: 2.0684964656829834
INFO:agents.father_agent:Step: 430, Training loss: 3.1984190940856934
INFO:agents.father_agent:Step: 435, Training loss: 3.1292030811309814
INFO:agents.father_agent:Step: 440, Training loss: 2.061893939971924
INFO:agents.father_agent:Step: 445, Training loss: 4.316507339477539
INFO:agents.father_agent:Step: 450, Training loss: 1.9416661262512207
INFO:agents.father_agent:Step: 455, Training loss: 3.5378079414367676
INFO:agents.father_agent:Step: 460, Training loss: 4.147576808929443
INFO:agents.father_agent:Step: 465, Training loss: 2.842033863067627
INFO:agents.father_agent:Step: 470, Training loss: 2.257113218307495
INFO:agents.father_agent:Step: 475, Training loss: 2.4206690788269043
INFO:agents.father_agent:Step: 480, Training loss: 2.780488967895508
INFO:agents.father_agent:Step: 485, Training loss: 1.9088654518127441
INFO:agents.father_agent:Step: 490, Training loss: 3.4803359508514404
INFO:agents.father_agent:Step: 495, Training loss: 3.0160913467407227
INFO:agents.father_agent:Step: 500, Training loss: 2.9459540843963623
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -40.29243087768555
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.70756912231445
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.694381237030029
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3677.622802734375
INFO:tools.evaluation_results_class:Current Best Return = -40.29243087768555
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.15675675675676
INFO:tools.evaluation_results_class:Counted Episodes = 3700
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 505, Training loss: 1.319060206413269
INFO:agents.father_agent:Step: 510, Training loss: 4.082722187042236
INFO:agents.father_agent:Step: 515, Training loss: 2.6698789596557617
INFO:agents.father_agent:Step: 520, Training loss: 6.058686256408691
INFO:agents.father_agent:Step: 525, Training loss: 4.157296657562256
INFO:agents.father_agent:Step: 530, Training loss: 2.3212690353393555
INFO:agents.father_agent:Step: 535, Training loss: 2.8488521575927734
INFO:agents.father_agent:Step: 540, Training loss: 2.581211566925049
INFO:agents.father_agent:Step: 545, Training loss: 2.0836822986602783
INFO:agents.father_agent:Step: 550, Training loss: 3.606597900390625
INFO:agents.father_agent:Step: 555, Training loss: 2.8280348777770996
INFO:agents.father_agent:Step: 560, Training loss: 2.667409658432007
INFO:agents.father_agent:Step: 565, Training loss: 2.9936110973358154
INFO:agents.father_agent:Step: 570, Training loss: 2.112502098083496
INFO:agents.father_agent:Step: 575, Training loss: 3.9394619464874268
INFO:agents.father_agent:Step: 580, Training loss: 2.522407293319702
INFO:agents.father_agent:Step: 585, Training loss: 2.5754573345184326
INFO:agents.father_agent:Step: 590, Training loss: 3.4201252460479736
INFO:agents.father_agent:Step: 595, Training loss: 2.2535295486450195
INFO:agents.father_agent:Step: 600, Training loss: 2.241560220718384
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -43.08932113647461
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 36.91067886352539
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.485418319702148
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5632.166015625
INFO:tools.evaluation_results_class:Current Best Return = -40.29243087768555
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.90084676317946
INFO:tools.evaluation_results_class:Counted Episodes = 3661
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 605, Training loss: 2.2364842891693115
INFO:agents.father_agent:Step: 610, Training loss: 2.139314651489258
INFO:agents.father_agent:Step: 615, Training loss: 2.7886412143707275
INFO:agents.father_agent:Step: 620, Training loss: 5.377361297607422
INFO:agents.father_agent:Step: 625, Training loss: 2.373331308364868
INFO:agents.father_agent:Step: 630, Training loss: 1.895978331565857
INFO:agents.father_agent:Step: 635, Training loss: 2.438002824783325
INFO:agents.father_agent:Step: 640, Training loss: 2.844681978225708
INFO:agents.father_agent:Step: 645, Training loss: 2.5750606060028076
INFO:agents.father_agent:Step: 650, Training loss: 2.396559238433838
INFO:agents.father_agent:Step: 655, Training loss: 2.17929744720459
INFO:agents.father_agent:Step: 660, Training loss: 2.351282835006714
INFO:agents.father_agent:Step: 665, Training loss: 2.8935320377349854
INFO:agents.father_agent:Step: 670, Training loss: 2.6045219898223877
INFO:agents.father_agent:Step: 675, Training loss: 4.952303886413574
INFO:agents.father_agent:Step: 680, Training loss: 2.276221990585327
INFO:agents.father_agent:Step: 685, Training loss: 1.9891386032104492
INFO:agents.father_agent:Step: 690, Training loss: 2.608790159225464
INFO:agents.father_agent:Step: 695, Training loss: 3.2574775218963623
INFO:agents.father_agent:Step: 700, Training loss: 2.3441975116729736
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -38.798095703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 41.201904296875
INFO:tools.evaluation_results_class:Average Discounted Reward = 8.543291091918945
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3026.939208984375
INFO:tools.evaluation_results_class:Current Best Return = -38.798095703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.7717006802721
INFO:tools.evaluation_results_class:Counted Episodes = 3675
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 705, Training loss: 1.5177556276321411
INFO:agents.father_agent:Step: 710, Training loss: 4.806301116943359
INFO:agents.father_agent:Step: 715, Training loss: 3.6653454303741455
INFO:agents.father_agent:Step: 720, Training loss: 3.112765312194824
INFO:agents.father_agent:Step: 725, Training loss: 2.281139612197876
INFO:agents.father_agent:Step: 730, Training loss: 1.6716662645339966
INFO:agents.father_agent:Step: 735, Training loss: 2.9624385833740234
INFO:agents.father_agent:Step: 740, Training loss: 2.4199135303497314
INFO:agents.father_agent:Step: 745, Training loss: 2.0485517978668213
INFO:agents.father_agent:Step: 750, Training loss: 2.987490653991699
INFO:agents.father_agent:Step: 755, Training loss: 2.696596622467041
INFO:agents.father_agent:Step: 760, Training loss: 1.947655200958252
INFO:agents.father_agent:Step: 765, Training loss: 1.9553464651107788
INFO:agents.father_agent:Step: 770, Training loss: 2.1708898544311523
INFO:agents.father_agent:Step: 775, Training loss: 2.0600879192352295
INFO:agents.father_agent:Step: 780, Training loss: 2.1846468448638916
INFO:agents.father_agent:Step: 785, Training loss: 3.114286422729492
INFO:agents.father_agent:Step: 790, Training loss: 1.9156906604766846
INFO:agents.father_agent:Step: 795, Training loss: 2.990551233291626
INFO:agents.father_agent:Step: 800, Training loss: 3.1247425079345703
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -40.67179489135742
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 39.32820510864258
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.991236209869385
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4243.14599609375
INFO:tools.evaluation_results_class:Current Best Return = -38.798095703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.20836707152496
INFO:tools.evaluation_results_class:Counted Episodes = 3705
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 805, Training loss: 1.171358346939087
INFO:agents.father_agent:Step: 810, Training loss: 3.109173536300659
INFO:agents.father_agent:Step: 815, Training loss: 3.478032112121582
INFO:agents.father_agent:Step: 820, Training loss: 3.069079875946045
INFO:agents.father_agent:Step: 825, Training loss: 4.022217273712158
INFO:agents.father_agent:Step: 830, Training loss: 2.5967562198638916
INFO:agents.father_agent:Step: 835, Training loss: 2.0866105556488037
INFO:agents.father_agent:Step: 840, Training loss: 2.5078139305114746
INFO:agents.father_agent:Step: 845, Training loss: 3.081003427505493
INFO:agents.father_agent:Step: 850, Training loss: 3.9180028438568115
INFO:agents.father_agent:Step: 855, Training loss: 2.602386236190796
INFO:agents.father_agent:Step: 860, Training loss: 3.3358888626098633
INFO:agents.father_agent:Step: 865, Training loss: 2.529862642288208
INFO:agents.father_agent:Step: 870, Training loss: 4.590804100036621
INFO:agents.father_agent:Step: 875, Training loss: 2.253856658935547
INFO:agents.father_agent:Step: 880, Training loss: 3.277125835418701
INFO:agents.father_agent:Step: 885, Training loss: 2.8386268615722656
INFO:agents.father_agent:Step: 890, Training loss: 2.766277313232422
INFO:agents.father_agent:Step: 895, Training loss: 2.988996744155884
INFO:agents.father_agent:Step: 900, Training loss: 2.3880417346954346
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -41.46995162963867
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 38.53004837036133
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.295712947845459
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4161.0380859375
INFO:tools.evaluation_results_class:Current Best Return = -38.798095703125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.15376285868977
INFO:tools.evaluation_results_class:Counted Episodes = 3694
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 905, Training loss: 1.187775731086731
INFO:agents.father_agent:Step: 910, Training loss: 2.1523854732513428
INFO:agents.father_agent:Step: 915, Training loss: 4.159679889678955
INFO:agents.father_agent:Step: 920, Training loss: 2.611640214920044
INFO:agents.father_agent:Step: 925, Training loss: 3.870439052581787
INFO:agents.father_agent:Step: 930, Training loss: 3.5096302032470703
INFO:agents.father_agent:Step: 935, Training loss: 2.3471391201019287
INFO:agents.father_agent:Step: 940, Training loss: 2.5018699169158936
INFO:agents.father_agent:Step: 945, Training loss: 2.3347420692443848
INFO:agents.father_agent:Step: 950, Training loss: 3.2114267349243164
INFO:agents.father_agent:Step: 955, Training loss: 1.8055392503738403
INFO:agents.father_agent:Step: 960, Training loss: 3.0859904289245605
INFO:agents.father_agent:Step: 965, Training loss: 2.6267480850219727
INFO:agents.father_agent:Step: 970, Training loss: 2.764526844024658
INFO:agents.father_agent:Step: 975, Training loss: 2.4777169227600098
INFO:agents.father_agent:Step: 980, Training loss: 1.9448699951171875
INFO:agents.father_agent:Step: 985, Training loss: 3.57599139213562
INFO:agents.father_agent:Step: 990, Training loss: 2.1458115577697754
INFO:agents.father_agent:Step: 995, Training loss: 3.5216195583343506
INFO:agents.father_agent:Step: 1000, Training loss: 2.2167303562164307
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -37.25813674926758
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 42.74186325073242
INFO:tools.evaluation_results_class:Average Discounted Reward = 9.953786849975586
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3447.476318359375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.51298878862455
INFO:tools.evaluation_results_class:Counted Episodes = 3657
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1005, Training loss: 1.8986331224441528
INFO:agents.father_agent:Step: 1010, Training loss: 4.809661865234375
INFO:agents.father_agent:Step: 1015, Training loss: 4.318797588348389
INFO:agents.father_agent:Step: 1020, Training loss: 2.3428049087524414
INFO:agents.father_agent:Step: 1025, Training loss: 2.16300892829895
INFO:agents.father_agent:Step: 1030, Training loss: 3.582685947418213
INFO:agents.father_agent:Step: 1035, Training loss: 2.0211405754089355
INFO:agents.father_agent:Step: 1040, Training loss: 1.3131755590438843
INFO:agents.father_agent:Step: 1045, Training loss: 3.337932586669922
INFO:agents.father_agent:Step: 1050, Training loss: 1.9040210247039795
INFO:agents.father_agent:Step: 1055, Training loss: 3.547628164291382
INFO:agents.father_agent:Step: 1060, Training loss: 1.8861403465270996
INFO:agents.father_agent:Step: 1065, Training loss: 2.640810489654541
INFO:agents.father_agent:Step: 1070, Training loss: 1.9431666135787964
INFO:agents.father_agent:Step: 1075, Training loss: 4.448386192321777
INFO:agents.father_agent:Step: 1080, Training loss: 2.055588722229004
INFO:agents.father_agent:Step: 1085, Training loss: 3.314081907272339
INFO:agents.father_agent:Step: 1090, Training loss: 1.9916177988052368
INFO:agents.father_agent:Step: 1095, Training loss: 2.131088972091675
INFO:agents.father_agent:Step: 1100, Training loss: 2.785785436630249
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -45.296043395996094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 34.703956604003906
INFO:tools.evaluation_results_class:Average Discounted Reward = 5.478992938995361
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6182.9228515625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 86.0417811984607
INFO:tools.evaluation_results_class:Counted Episodes = 3638
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1105, Training loss: 1.331667184829712
INFO:agents.father_agent:Step: 1110, Training loss: 3.009411334991455
INFO:agents.father_agent:Step: 1115, Training loss: 3.718580484390259
INFO:agents.father_agent:Step: 1120, Training loss: 3.4601402282714844
INFO:agents.father_agent:Step: 1125, Training loss: 3.515718936920166
INFO:agents.father_agent:Step: 1130, Training loss: 2.1612207889556885
INFO:agents.father_agent:Step: 1135, Training loss: 1.6656666994094849
INFO:agents.father_agent:Step: 1140, Training loss: 3.12912917137146
INFO:agents.father_agent:Step: 1145, Training loss: 2.363582134246826
INFO:agents.father_agent:Step: 1150, Training loss: 3.836514472961426
INFO:agents.father_agent:Step: 1155, Training loss: 2.1491503715515137
INFO:agents.father_agent:Step: 1160, Training loss: 3.355630397796631
INFO:agents.father_agent:Step: 1165, Training loss: 2.1038577556610107
INFO:agents.father_agent:Step: 1170, Training loss: 1.9015058279037476
INFO:agents.father_agent:Step: 1175, Training loss: 1.8877638578414917
INFO:agents.father_agent:Step: 1180, Training loss: 2.1940183639526367
INFO:agents.father_agent:Step: 1185, Training loss: 2.754075765609741
INFO:agents.father_agent:Step: 1190, Training loss: 3.105623245239258
INFO:agents.father_agent:Step: 1195, Training loss: 2.979649782180786
INFO:agents.father_agent:Step: 1200, Training loss: 1.789167046546936
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -44.27142333984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 35.72857666015625
INFO:tools.evaluation_results_class:Average Discounted Reward = 6.3274102210998535
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 6401.84375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 84.82103271154367
INFO:tools.evaluation_results_class:Counted Episodes = 3699
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1205, Training loss: 1.390672206878662
INFO:agents.father_agent:Step: 1210, Training loss: 1.5436073541641235
INFO:agents.father_agent:Step: 1215, Training loss: 2.340737819671631
INFO:agents.father_agent:Step: 1220, Training loss: 2.067244052886963
INFO:agents.father_agent:Step: 1225, Training loss: 4.024461269378662
INFO:agents.father_agent:Step: 1230, Training loss: 2.3407862186431885
INFO:agents.father_agent:Step: 1235, Training loss: 1.7725588083267212
INFO:agents.father_agent:Step: 1240, Training loss: 2.795091390609741
INFO:agents.father_agent:Step: 1245, Training loss: 2.819239377975464
INFO:agents.father_agent:Step: 1250, Training loss: 2.529447317123413
INFO:agents.father_agent:Step: 1255, Training loss: 1.6910074949264526
INFO:agents.father_agent:Step: 1260, Training loss: 3.165703773498535
INFO:agents.father_agent:Step: 1265, Training loss: 2.600170612335205
INFO:agents.father_agent:Step: 1270, Training loss: 2.361227512359619
INFO:agents.father_agent:Step: 1275, Training loss: 2.0577030181884766
INFO:agents.father_agent:Step: 1280, Training loss: 2.3937315940856934
INFO:agents.father_agent:Step: 1285, Training loss: 2.4968817234039307
INFO:agents.father_agent:Step: 1290, Training loss: 2.069655656814575
INFO:agents.father_agent:Step: 1295, Training loss: 2.0247387886047363
INFO:agents.father_agent:Step: 1300, Training loss: 1.6355400085449219
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -42.07560348510742
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 37.92439651489258
INFO:tools.evaluation_results_class:Average Discounted Reward = 7.566549301147461
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5801.5927734375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 85.28011966276856
INFO:tools.evaluation_results_class:Counted Episodes = 3677
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 1305, Training loss: 0.8556026220321655
INFO:agents.father_agent:Step: 1310, Training loss: 3.711498498916626
INFO:agents.father_agent:Step: 1315, Training loss: 1.6551769971847534
INFO:agents.father_agent:Step: 1320, Training loss: 4.976264953613281
INFO:agents.father_agent:Step: 1325, Training loss: 2.6776609420776367
INFO:agents.father_agent:Step: 1330, Training loss: 1.3621234893798828
INFO:agents.father_agent:Step: 1335, Training loss: 1.9502745866775513
INFO:agents.father_agent:Step: 1340, Training loss: 1.899674415588379
INFO:agents.father_agent:Step: 1345, Training loss: 3.5342557430267334
INFO:agents.father_agent:Step: 1350, Training loss: 2.6720454692840576
INFO:agents.father_agent:Step: 1355, Training loss: 1.8608675003051758
INFO:agents.father_agent:Step: 1360, Training loss: 1.4956406354904175
INFO:agents.father_agent:Step: 1365, Training loss: 1.5212482213974
INFO:agents.father_agent:Step: 1370, Training loss: 3.492518186569214
INFO:agents.father_agent:Step: 1375, Training loss: 2.894822597503662
INFO:agents.father_agent:Step: 1380, Training loss: 2.9877288341522217
INFO:agents.father_agent:Step: 1385, Training loss: 2.0399856567382812
INFO:agents.father_agent:Step: 1390, Training loss: 2.038287401199341
INFO:agents.father_agent:Step: 1395, Training loss: 1.3017587661743164
INFO:agents.father_agent:Step: 1400, Training loss: 1.2475906610488892
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -53.756187438964844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 26.243810653686523
INFO:tools.evaluation_results_class:Average Discounted Reward = 1.04918372631073
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12063.03515625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.92861255037421
INFO:tools.evaluation_results_class:Counted Episodes = 3474
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -50.700347900390625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 29.299654006958008
INFO:tools.evaluation_results_class:Average Discounted Reward = 2.498847484588623
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9963.708984375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.16281755196304
INFO:tools.evaluation_results_class:Counted Episodes = 3464
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -54.73227310180664
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 25.267724990844727
INFO:tools.evaluation_results_class:Average Discounted Reward = 0.36806851625442505
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12854.1201171875
INFO:tools.evaluation_results_class:Current Best Return = -54.73227310180664
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.38684061259217
INFO:tools.evaluation_results_class:Counted Episodes = 3526
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -55.22407531738281
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 24.775924682617188
INFO:tools.evaluation_results_class:Average Discounted Reward = -0.027508901432156563
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 12572.48828125
INFO:tools.evaluation_results_class:Current Best Return = -55.22407531738281
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.9513987001978
INFO:tools.evaluation_results_class:Counted Episodes = 3539
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 210.4027 achieved after 803.38 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 210.6136 achieved after 803.39 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 220.1358 achieved after 803.39 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 220.7513 achieved after 803.4 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 229.6695 achieved after 803.41 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 230.285 achieved after 803.42 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 282.2688 achieved after 803.45 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 282.4797 achieved after 803.46 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 292.0019 achieved after 803.46 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 292.6174 achieved after 803.47 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 301.5356 achieved after 803.48 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 302.1511 achieved after 803.49 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 310.9187 achieved after 803.66 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 311.5343 achieved after 803.66 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 311.53428062683577
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
INFO:robust_rl.robust_rl_trainer:Iteration 2 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -214.66311645507812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -134.66311645507812
INFO:tools.evaluation_results_class:Average Discounted Reward = -119.50979614257812
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 35848.78125
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 90.08724445724158
INFO:tools.evaluation_results_class:Counted Episodes = 3473
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 9.227607727050781
INFO:agents.father_agent:Step: 5, Training loss: 8.85965633392334
INFO:agents.father_agent:Step: 10, Training loss: 6.909024715423584
INFO:agents.father_agent:Step: 15, Training loss: 5.340254306793213
INFO:agents.father_agent:Step: 20, Training loss: 6.601536750793457
INFO:agents.father_agent:Step: 25, Training loss: 7.405174732208252
INFO:agents.father_agent:Step: 30, Training loss: 8.455516815185547
INFO:agents.father_agent:Step: 35, Training loss: 8.254193305969238
INFO:agents.father_agent:Step: 40, Training loss: 6.49594259262085
INFO:agents.father_agent:Step: 45, Training loss: 7.148080348968506
INFO:agents.father_agent:Step: 50, Training loss: 5.713720321655273
INFO:agents.father_agent:Step: 55, Training loss: 4.099081993103027
INFO:agents.father_agent:Step: 60, Training loss: 1.6113688945770264
INFO:agents.father_agent:Step: 65, Training loss: 2.0006182193756104
INFO:agents.father_agent:Step: 70, Training loss: 2.597339153289795
INFO:agents.father_agent:Step: 75, Training loss: 0.7991290092468262
INFO:agents.father_agent:Step: 80, Training loss: 2.483786106109619
INFO:agents.father_agent:Step: 85, Training loss: 6.911748886108398
INFO:agents.father_agent:Step: 90, Training loss: 3.234292984008789
INFO:agents.father_agent:Step: 95, Training loss: 2.0368480682373047
INFO:agents.father_agent:Step: 100, Training loss: 1.4345146417617798
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -72.79572296142578
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 7.204274654388428
INFO:tools.evaluation_results_class:Average Discounted Reward = -22.347286224365234
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7431.51416015625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 107.41836019621584
INFO:tools.evaluation_results_class:Counted Episodes = 2854
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 2.347766160964966
INFO:agents.father_agent:Step: 110, Training loss: 3.2468278408050537
INFO:agents.father_agent:Step: 115, Training loss: 2.0616791248321533
INFO:agents.father_agent:Step: 120, Training loss: 2.661055564880371
INFO:agents.father_agent:Step: 125, Training loss: 2.936363697052002
INFO:agents.father_agent:Step: 130, Training loss: 1.9052964448928833
INFO:agents.father_agent:Step: 135, Training loss: 5.496336936950684
INFO:agents.father_agent:Step: 140, Training loss: 3.596569776535034
INFO:agents.father_agent:Step: 145, Training loss: 3.569089651107788
INFO:agents.father_agent:Step: 150, Training loss: 2.2786147594451904
INFO:agents.father_agent:Step: 155, Training loss: 2.5911917686462402
INFO:agents.father_agent:Step: 160, Training loss: 4.8130879402160645
INFO:agents.father_agent:Step: 165, Training loss: 2.3489322662353516
INFO:agents.father_agent:Step: 170, Training loss: 2.3778672218322754
INFO:agents.father_agent:Step: 175, Training loss: 5.008965015411377
INFO:agents.father_agent:Step: 180, Training loss: 3.1259806156158447
INFO:agents.father_agent:Step: 185, Training loss: 2.2067995071411133
INFO:agents.father_agent:Step: 190, Training loss: 1.8694449663162231
INFO:agents.father_agent:Step: 195, Training loss: 2.2195067405700684
INFO:agents.father_agent:Step: 200, Training loss: 4.006885528564453
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -52.579349517822266
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 27.420650482177734
INFO:tools.evaluation_results_class:Average Discounted Reward = -6.37814474105835
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3021.986328125
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 88.15304101838755
INFO:tools.evaluation_results_class:Counted Episodes = 3535
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 1.6211695671081543
INFO:agents.father_agent:Step: 210, Training loss: 4.2474684715271
INFO:agents.father_agent:Step: 215, Training loss: 6.476137638092041
INFO:agents.father_agent:Step: 220, Training loss: 3.3334085941314697
INFO:agents.father_agent:Step: 225, Training loss: 2.8438687324523926
INFO:agents.father_agent:Step: 230, Training loss: 4.728704929351807
INFO:agents.father_agent:Step: 235, Training loss: 3.0643434524536133
INFO:agents.father_agent:Step: 240, Training loss: 2.076953887939453
INFO:agents.father_agent:Step: 245, Training loss: 1.9744867086410522
INFO:agents.father_agent:Step: 250, Training loss: 4.551590442657471
INFO:agents.father_agent:Step: 255, Training loss: 2.3434741497039795
INFO:agents.father_agent:Step: 260, Training loss: 2.511094331741333
INFO:agents.father_agent:Step: 265, Training loss: 3.9261019229888916
INFO:agents.father_agent:Step: 270, Training loss: 4.87089729309082
INFO:agents.father_agent:Step: 275, Training loss: 3.761660099029541
INFO:agents.father_agent:Step: 280, Training loss: 3.883389472961426
INFO:agents.father_agent:Step: 285, Training loss: 5.16911506652832
INFO:agents.father_agent:Step: 290, Training loss: 4.430420875549316
INFO:agents.father_agent:Step: 295, Training loss: 5.6328301429748535
INFO:agents.father_agent:Step: 300, Training loss: 3.6952672004699707
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -62.15751647949219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = 17.842483520507812
INFO:tools.evaluation_results_class:Average Discounted Reward = -10.721968650817871
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4452.27587890625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 83.64019189765459
INFO:tools.evaluation_results_class:Counted Episodes = 3752
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 2.9440293312072754
INFO:agents.father_agent:Step: 310, Training loss: 2.909520387649536
INFO:agents.father_agent:Step: 315, Training loss: 2.9112682342529297
INFO:agents.father_agent:Step: 320, Training loss: 5.404932498931885
INFO:agents.father_agent:Step: 325, Training loss: 7.833566188812256
INFO:agents.father_agent:Step: 330, Training loss: 3.6618590354919434
INFO:agents.father_agent:Step: 335, Training loss: 3.263840436935425
INFO:agents.father_agent:Step: 340, Training loss: 3.117854356765747
INFO:agents.father_agent:Step: 345, Training loss: 5.971907615661621
INFO:agents.father_agent:Step: 350, Training loss: 5.598970890045166
INFO:agents.father_agent:Step: 355, Training loss: 4.043924808502197
INFO:agents.father_agent:Step: 360, Training loss: 4.690976142883301
INFO:agents.father_agent:Step: 365, Training loss: 5.033308982849121
INFO:agents.father_agent:Step: 370, Training loss: 4.832028865814209
INFO:agents.father_agent:Step: 375, Training loss: 4.527629375457764
INFO:agents.father_agent:Step: 380, Training loss: 5.518060684204102
INFO:agents.father_agent:Step: 385, Training loss: 4.141854763031006
INFO:agents.father_agent:Step: 390, Training loss: 2.8474831581115723
INFO:agents.father_agent:Step: 395, Training loss: 4.990787982940674
INFO:agents.father_agent:Step: 400, Training loss: 3.535196304321289
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -80.98155975341797
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -0.9815561771392822
INFO:tools.evaluation_results_class:Average Discounted Reward = -21.957489013671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 9120.3623046875
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 89.4449567723343
INFO:tools.evaluation_results_class:Counted Episodes = 3470
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 3.5782971382141113
INFO:agents.father_agent:Step: 410, Training loss: 2.5621705055236816
INFO:agents.father_agent:Step: 415, Training loss: 7.678393363952637
INFO:agents.father_agent:Step: 420, Training loss: 4.636951446533203
INFO:agents.father_agent:Step: 425, Training loss: 4.433067798614502
INFO:agents.father_agent:Step: 430, Training loss: 4.494677543640137
INFO:agents.father_agent:Step: 435, Training loss: 3.9783124923706055
INFO:agents.father_agent:Step: 440, Training loss: 3.5028140544891357
INFO:agents.father_agent:Step: 445, Training loss: 3.6341238021850586
INFO:agents.father_agent:Step: 450, Training loss: 4.7663044929504395
INFO:agents.father_agent:Step: 455, Training loss: 5.293647289276123
INFO:agents.father_agent:Step: 460, Training loss: 4.278449058532715
INFO:agents.father_agent:Step: 465, Training loss: 2.861179828643799
INFO:agents.father_agent:Step: 470, Training loss: 4.498473644256592
INFO:agents.father_agent:Step: 475, Training loss: 4.371001720428467
INFO:agents.father_agent:Step: 480, Training loss: 2.899043560028076
INFO:agents.father_agent:Step: 485, Training loss: 3.7102253437042236
INFO:agents.father_agent:Step: 490, Training loss: 3.0411436557769775
INFO:agents.father_agent:Step: 495, Training loss: 3.5894064903259277
INFO:agents.father_agent:Step: 500, Training loss: 2.9943931102752686
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -84.54862213134766
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -4.548625469207764
INFO:tools.evaluation_results_class:Average Discounted Reward = -24.53957748413086
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 7463.06005859375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.02601241501625
INFO:tools.evaluation_results_class:Counted Episodes = 3383
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -84.65292358398438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -4.652926921844482
INFO:tools.evaluation_results_class:Average Discounted Reward = -24.744394302368164
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8180.37890625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.83273596176822
INFO:tools.evaluation_results_class:Counted Episodes = 3348
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -89.06697845458984
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -9.066980361938477
INFO:tools.evaluation_results_class:Average Discounted Reward = -26.918237686157227
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 11160.0966796875
INFO:tools.evaluation_results_class:Current Best Return = -89.06697845458984
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 93.78025851938895
INFO:tools.evaluation_results_class:Counted Episodes = 3404
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -159.75291442871094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -79.75291442871094
INFO:tools.evaluation_results_class:Average Discounted Reward = -61.77693557739258
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 68636.15625
INFO:tools.evaluation_results_class:Current Best Return = -159.75291442871094
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 105.67998667998668
INFO:tools.evaluation_results_class:Counted Episodes = 3003
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 216.2383 achieved after 1257.96 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 216.2385 achieved after 1257.97 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 216.2635 achieved after 1257.97 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 218.1412 achieved after 1257.99 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 218.1414 achieved after 1258.0 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 218.1665 achieved after 1258.0 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 305.8728 achieved after 1258.04 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 305.8731 achieved after 1258.05 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 305.8981 achieved after 1258.05 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 307.7758 achieved after 1258.07 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 307.776 achieved after 1258.07 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 307.801 achieved after 1258.08 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 320.5655 achieved after 1258.2 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 320.5657 achieved after 1258.2 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 320.5907 achieved after 1258.21 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 322.4685 achieved after 1258.22 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 322.4687 achieved after 1258.23 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 322.4937 achieved after 1258.24 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 327.5424 achieved after 1258.38 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 327.5426 achieved after 1258.38 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 327.5676 achieved after 1258.39 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 355.5845 achieved after 1258.45 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 355.5847 achieved after 1258.46 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 355.6097 achieved after 1258.46 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 370.2772 achieved after 1258.54 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 370.2774 achieved after 1258.55 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 370.3024 achieved after 1258.55 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 370.3124 achieved after 1258.67 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=3, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 370.3124091748648
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=3, o4y=8, o5x=8, o5y=5
INFO:robust_rl.robust_rl_trainer:Iteration 3 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.85316467285156
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -98.85315704345703
INFO:tools.evaluation_results_class:Average Discounted Reward = -108.01094055175781
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 25852.4453125
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.87689113022842
INFO:tools.evaluation_results_class:Counted Episodes = 3371
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 8.62336540222168
INFO:agents.father_agent:Step: 5, Training loss: 8.838754653930664
INFO:agents.father_agent:Step: 10, Training loss: 8.750473976135254
INFO:agents.father_agent:Step: 15, Training loss: 5.979590892791748
INFO:agents.father_agent:Step: 20, Training loss: 4.713597774505615
INFO:agents.father_agent:Step: 25, Training loss: 4.064867973327637
INFO:agents.father_agent:Step: 30, Training loss: 6.287428379058838
INFO:agents.father_agent:Step: 35, Training loss: 4.548336982727051
INFO:agents.father_agent:Step: 40, Training loss: 4.927133083343506
INFO:agents.father_agent:Step: 45, Training loss: 5.8105597496032715
INFO:agents.father_agent:Step: 50, Training loss: 3.113497495651245
INFO:agents.father_agent:Step: 55, Training loss: 5.277595520019531
INFO:agents.father_agent:Step: 60, Training loss: 4.166426181793213
INFO:agents.father_agent:Step: 65, Training loss: 4.54103946685791
INFO:agents.father_agent:Step: 70, Training loss: 5.313162326812744
INFO:agents.father_agent:Step: 75, Training loss: 4.170312404632568
INFO:agents.father_agent:Step: 80, Training loss: 3.6266002655029297
INFO:agents.father_agent:Step: 85, Training loss: 4.790923118591309
INFO:agents.father_agent:Step: 90, Training loss: 4.857534885406494
INFO:agents.father_agent:Step: 95, Training loss: 4.848907947540283
INFO:agents.father_agent:Step: 100, Training loss: 3.704702615737915
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -202.01968383789062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -122.0196762084961
INFO:tools.evaluation_results_class:Average Discounted Reward = -115.35103607177734
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37158.16796875
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 91.54919236417034
INFO:tools.evaluation_results_class:Counted Episodes = 3405
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 3.2755186557769775
INFO:agents.father_agent:Step: 110, Training loss: 5.380256175994873
INFO:agents.father_agent:Step: 115, Training loss: 6.224152088165283
INFO:agents.father_agent:Step: 120, Training loss: 7.507749080657959
INFO:agents.father_agent:Step: 125, Training loss: 3.9173972606658936
INFO:agents.father_agent:Step: 130, Training loss: 5.083684921264648
INFO:agents.father_agent:Step: 135, Training loss: 3.7100422382354736
INFO:agents.father_agent:Step: 140, Training loss: 4.524599075317383
INFO:agents.father_agent:Step: 145, Training loss: 5.355444431304932
INFO:agents.father_agent:Step: 150, Training loss: 4.256614685058594
INFO:agents.father_agent:Step: 155, Training loss: 4.158761501312256
INFO:agents.father_agent:Step: 160, Training loss: 3.6860721111297607
INFO:agents.father_agent:Step: 165, Training loss: 4.861097812652588
INFO:agents.father_agent:Step: 170, Training loss: 3.4865610599517822
INFO:agents.father_agent:Step: 175, Training loss: 3.670814275741577
INFO:agents.father_agent:Step: 180, Training loss: 5.522972106933594
INFO:agents.father_agent:Step: 185, Training loss: 5.647066116333008
INFO:agents.father_agent:Step: 190, Training loss: 3.552525281906128
INFO:agents.father_agent:Step: 195, Training loss: 5.633849143981934
INFO:agents.father_agent:Step: 200, Training loss: 4.5618414878845215
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -232.12742614746094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -152.12742614746094
INFO:tools.evaluation_results_class:Average Discounted Reward = -98.90858459472656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 84177.3359375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 106.75347222222223
INFO:tools.evaluation_results_class:Counted Episodes = 2880
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 3.271096706390381
INFO:agents.father_agent:Step: 210, Training loss: 7.1179609298706055
INFO:agents.father_agent:Step: 215, Training loss: 4.791068077087402
INFO:agents.father_agent:Step: 220, Training loss: 3.71415638923645
INFO:agents.father_agent:Step: 225, Training loss: 3.6916561126708984
INFO:agents.father_agent:Step: 230, Training loss: 3.3706021308898926
INFO:agents.father_agent:Step: 235, Training loss: 4.204084873199463
INFO:agents.father_agent:Step: 240, Training loss: 7.175692081451416
INFO:agents.father_agent:Step: 245, Training loss: 4.104319095611572
INFO:agents.father_agent:Step: 250, Training loss: 6.478020668029785
INFO:agents.father_agent:Step: 255, Training loss: 4.650625228881836
INFO:agents.father_agent:Step: 260, Training loss: 3.2258493900299072
INFO:agents.father_agent:Step: 265, Training loss: 4.574619770050049
INFO:agents.father_agent:Step: 270, Training loss: 3.2013654708862305
INFO:agents.father_agent:Step: 275, Training loss: 5.078547477722168
INFO:agents.father_agent:Step: 280, Training loss: 3.599550485610962
INFO:agents.father_agent:Step: 285, Training loss: 4.692515850067139
INFO:agents.father_agent:Step: 290, Training loss: 3.9153337478637695
INFO:agents.father_agent:Step: 295, Training loss: 4.493076324462891
INFO:agents.father_agent:Step: 300, Training loss: 3.7260076999664307
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -479.8232727050781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -399.8232727050781
INFO:tools.evaluation_results_class:Average Discounted Reward = -180.94432067871094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 316247.5625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 162.7012542759407
INFO:tools.evaluation_results_class:Counted Episodes = 1754
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 2.9103400707244873
INFO:agents.father_agent:Step: 310, Training loss: 4.7332258224487305
INFO:agents.father_agent:Step: 315, Training loss: 4.421858787536621
INFO:agents.father_agent:Step: 320, Training loss: 4.686683654785156
INFO:agents.father_agent:Step: 325, Training loss: 4.06039571762085
INFO:agents.father_agent:Step: 330, Training loss: 4.094394683837891
INFO:agents.father_agent:Step: 335, Training loss: 2.6524908542633057
INFO:agents.father_agent:Step: 340, Training loss: 3.873464822769165
INFO:agents.father_agent:Step: 345, Training loss: 3.8177266120910645
INFO:agents.father_agent:Step: 350, Training loss: 3.6788487434387207
INFO:agents.father_agent:Step: 355, Training loss: 3.747152805328369
INFO:agents.father_agent:Step: 360, Training loss: 3.6874890327453613
INFO:agents.father_agent:Step: 365, Training loss: 3.291156053543091
INFO:agents.father_agent:Step: 370, Training loss: 3.2912437915802
INFO:agents.father_agent:Step: 375, Training loss: 3.50256609916687
INFO:agents.father_agent:Step: 380, Training loss: 3.6793935298919678
INFO:agents.father_agent:Step: 385, Training loss: 4.775205612182617
INFO:agents.father_agent:Step: 390, Training loss: 3.254103660583496
INFO:agents.father_agent:Step: 395, Training loss: 3.2284085750579834
INFO:agents.father_agent:Step: 400, Training loss: 3.174247980117798
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -501.8223876953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -421.8223876953125
INFO:tools.evaluation_results_class:Average Discounted Reward = -186.70431518554688
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 381087.375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 170.58637469586375
INFO:tools.evaluation_results_class:Counted Episodes = 1644
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 2.3050003051757812
INFO:agents.father_agent:Step: 410, Training loss: 3.372812271118164
INFO:agents.father_agent:Step: 415, Training loss: 4.812985897064209
INFO:agents.father_agent:Step: 420, Training loss: 4.359365463256836
INFO:agents.father_agent:Step: 425, Training loss: 3.9041004180908203
INFO:agents.father_agent:Step: 430, Training loss: 4.8554792404174805
INFO:agents.father_agent:Step: 435, Training loss: 3.243351936340332
INFO:agents.father_agent:Step: 440, Training loss: 2.590996026992798
INFO:agents.father_agent:Step: 445, Training loss: 3.1217129230499268
INFO:agents.father_agent:Step: 450, Training loss: 2.9792861938476562
INFO:agents.father_agent:Step: 455, Training loss: 3.9948296546936035
INFO:agents.father_agent:Step: 460, Training loss: 4.222839832305908
INFO:agents.father_agent:Step: 465, Training loss: 3.040709972381592
INFO:agents.father_agent:Step: 470, Training loss: 2.9886367321014404
INFO:agents.father_agent:Step: 475, Training loss: 2.446274757385254
INFO:agents.father_agent:Step: 480, Training loss: 3.435823917388916
INFO:agents.father_agent:Step: 485, Training loss: 3.9540131092071533
INFO:agents.father_agent:Step: 490, Training loss: 3.3023931980133057
INFO:agents.father_agent:Step: 495, Training loss: 2.9587795734405518
INFO:agents.father_agent:Step: 500, Training loss: 2.950031042098999
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -768.248046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -688.3868408203125
INFO:tools.evaluation_results_class:Average Discounted Reward = -244.29379272460938
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9982653946227233
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 817418.625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 226.07718993928881
INFO:tools.evaluation_results_class:Counted Episodes = 1153
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -782.3842163085938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -702.5947265625
INFO:tools.evaluation_results_class:Average Discounted Reward = -253.52508544921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9973684210526316
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 713941.1875
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 227.80877192982456
INFO:tools.evaluation_results_class:Counted Episodes = 1140
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -810.9202880859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -731.57763671875
INFO:tools.evaluation_results_class:Average Discounted Reward = -244.98402404785156
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9917830731306492
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 963124.0625
INFO:tools.evaluation_results_class:Current Best Return = -810.9202880859375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9917830731306492
INFO:tools.evaluation_results_class:Average Episode Length = 241.2645850451931
INFO:tools.evaluation_results_class:Counted Episodes = 1217
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -813.6554565429688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -733.91796875
INFO:tools.evaluation_results_class:Average Discounted Reward = -246.87728881835938
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9967186218211649
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 796102.0625
INFO:tools.evaluation_results_class:Current Best Return = -813.6554565429688
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9967186218211649
INFO:tools.evaluation_results_class:Average Episode Length = 239.1575061525841
INFO:tools.evaluation_results_class:Counted Episodes = 1219
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 966.3139 achieved after 1742.69 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 967.0494 achieved after 1742.69 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 970.7829 achieved after 1742.7 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 970.8429 achieved after 1742.72 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 975.5039 achieved after 1742.73 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 976.2394 achieved after 1742.73 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 979.9729 achieved after 1742.74 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 980.8327 achieved after 1742.75 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1039.0674 achieved after 1742.76 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1039.8029 achieved after 1742.77 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1043.5363 achieved after 1742.78 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1043.5963 achieved after 1742.79 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1048.2573 achieved after 1742.81 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1048.9928 achieved after 1742.81 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1052.7263 achieved after 1742.81 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1053.5862 achieved after 1742.82 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1057.3562 achieved after 1742.9 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1058.0917 achieved after 1742.9 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1061.8251 achieved after 1742.9 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1062.685 achieved after 1742.91 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 1062.6850205364217
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
INFO:robust_rl.robust_rl_trainer:Iteration 4 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -849.9361572265625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -770.07421875
INFO:tools.evaluation_results_class:Average Discounted Reward = -314.78973388671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.998274374460742
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 660218.5
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 224.29076790336498
INFO:tools.evaluation_results_class:Counted Episodes = 1159
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.979803085327148
INFO:agents.father_agent:Step: 5, Training loss: 5.626772880554199
INFO:agents.father_agent:Step: 10, Training loss: 4.702489376068115
INFO:agents.father_agent:Step: 15, Training loss: 4.26137113571167
INFO:agents.father_agent:Step: 20, Training loss: 5.042490005493164
INFO:agents.father_agent:Step: 25, Training loss: 5.255946636199951
INFO:agents.father_agent:Step: 30, Training loss: 4.928521156311035
INFO:agents.father_agent:Step: 35, Training loss: 5.775749683380127
INFO:agents.father_agent:Step: 40, Training loss: 5.280072212219238
INFO:agents.father_agent:Step: 45, Training loss: 5.098330497741699
INFO:agents.father_agent:Step: 50, Training loss: 4.589235305786133
INFO:agents.father_agent:Step: 55, Training loss: 4.178799152374268
INFO:agents.father_agent:Step: 60, Training loss: 2.8761136531829834
INFO:agents.father_agent:Step: 65, Training loss: 3.3276660442352295
INFO:agents.father_agent:Step: 70, Training loss: 3.7984657287597656
INFO:agents.father_agent:Step: 75, Training loss: 3.4717559814453125
INFO:agents.father_agent:Step: 80, Training loss: 2.8119091987609863
INFO:agents.father_agent:Step: 85, Training loss: 3.4732203483581543
INFO:agents.father_agent:Step: 90, Training loss: 2.5447914600372314
INFO:agents.father_agent:Step: 95, Training loss: 2.2644011974334717
INFO:agents.father_agent:Step: 100, Training loss: 2.9674112796783447
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -102.61614990234375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -22.909805297851562
INFO:tools.evaluation_results_class:Average Discounted Reward = -41.32459259033203
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9963293130571579
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 49212.62109375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 139.7488201363398
INFO:tools.evaluation_results_class:Counted Episodes = 1907
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 1.6244616508483887
INFO:agents.father_agent:Step: 110, Training loss: 3.2779808044433594
INFO:agents.father_agent:Step: 115, Training loss: 2.9493296146392822
INFO:agents.father_agent:Step: 120, Training loss: 4.061424732208252
INFO:agents.father_agent:Step: 125, Training loss: 2.8801190853118896
INFO:agents.father_agent:Step: 130, Training loss: 3.352107524871826
INFO:agents.father_agent:Step: 135, Training loss: 2.8451457023620605
INFO:agents.father_agent:Step: 140, Training loss: 2.745746612548828
INFO:agents.father_agent:Step: 145, Training loss: 2.5384130477905273
INFO:agents.father_agent:Step: 150, Training loss: 3.2709789276123047
INFO:agents.father_agent:Step: 155, Training loss: 2.8263847827911377
INFO:agents.father_agent:Step: 160, Training loss: 4.461747646331787
INFO:agents.father_agent:Step: 165, Training loss: 2.624347448348999
INFO:agents.father_agent:Step: 170, Training loss: 2.219985246658325
INFO:agents.father_agent:Step: 175, Training loss: 2.677510976791382
INFO:agents.father_agent:Step: 180, Training loss: 2.7708213329315186
INFO:agents.father_agent:Step: 185, Training loss: 2.735103130340576
INFO:agents.father_agent:Step: 190, Training loss: 2.6842503547668457
INFO:agents.father_agent:Step: 195, Training loss: 3.2438385486602783
INFO:agents.father_agent:Step: 200, Training loss: 2.283583402633667
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -330.0396423339844
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -252.0576629638672
INFO:tools.evaluation_results_class:Average Discounted Reward = -114.35951232910156
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9747747747747748
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 747197.75
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 222.93513513513514
INFO:tools.evaluation_results_class:Counted Episodes = 1110
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 1.995407223701477
INFO:agents.father_agent:Step: 210, Training loss: 3.466409683227539
INFO:agents.father_agent:Step: 215, Training loss: 3.093238115310669
INFO:agents.father_agent:Step: 220, Training loss: 3.502295732498169
INFO:agents.father_agent:Step: 225, Training loss: 3.0821123123168945
INFO:agents.father_agent:Step: 230, Training loss: 2.605328321456909
INFO:agents.father_agent:Step: 235, Training loss: 3.0439555644989014
INFO:agents.father_agent:Step: 240, Training loss: 2.0151944160461426
INFO:agents.father_agent:Step: 245, Training loss: 2.46587872505188
INFO:agents.father_agent:Step: 250, Training loss: 3.5561113357543945
INFO:agents.father_agent:Step: 255, Training loss: 3.559155225753784
INFO:agents.father_agent:Step: 260, Training loss: 3.668156623840332
INFO:agents.father_agent:Step: 265, Training loss: 2.7445764541625977
INFO:agents.father_agent:Step: 270, Training loss: 2.4543211460113525
INFO:agents.father_agent:Step: 275, Training loss: 3.165947198867798
INFO:agents.father_agent:Step: 280, Training loss: 5.684512615203857
INFO:agents.father_agent:Step: 285, Training loss: 3.69305157661438
INFO:agents.father_agent:Step: 290, Training loss: 4.228034973144531
INFO:agents.father_agent:Step: 295, Training loss: 2.4082603454589844
INFO:agents.father_agent:Step: 300, Training loss: 2.0661580562591553
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -143.5603485107422
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -63.69324493408203
INFO:tools.evaluation_results_class:Average Discounted Reward = -62.86170196533203
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9983388704318937
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 81272.6875
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 153.7563676633444
INFO:tools.evaluation_results_class:Counted Episodes = 1806
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.9601454734802246
INFO:agents.father_agent:Step: 310, Training loss: 2.0625083446502686
INFO:agents.father_agent:Step: 315, Training loss: 3.0753014087677
INFO:agents.father_agent:Step: 320, Training loss: 2.642235279083252
INFO:agents.father_agent:Step: 325, Training loss: 5.1354475021362305
INFO:agents.father_agent:Step: 330, Training loss: 2.6769466400146484
INFO:agents.father_agent:Step: 335, Training loss: 2.2548859119415283
INFO:agents.father_agent:Step: 340, Training loss: 2.526885509490967
INFO:agents.father_agent:Step: 345, Training loss: 3.2706658840179443
INFO:agents.father_agent:Step: 350, Training loss: 3.507500648498535
INFO:agents.father_agent:Step: 355, Training loss: 3.0802934169769287
INFO:agents.father_agent:Step: 360, Training loss: 2.944964647293091
INFO:agents.father_agent:Step: 365, Training loss: 3.192185401916504
INFO:agents.father_agent:Step: 370, Training loss: 2.4707086086273193
INFO:agents.father_agent:Step: 375, Training loss: 2.692929267883301
INFO:agents.father_agent:Step: 380, Training loss: 3.650289297103882
INFO:agents.father_agent:Step: 385, Training loss: 3.9417402744293213
INFO:agents.father_agent:Step: 390, Training loss: 3.277005434036255
INFO:agents.father_agent:Step: 395, Training loss: 3.638965368270874
INFO:agents.father_agent:Step: 400, Training loss: 3.335859537124634
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -288.2252502441406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -212.42727661132812
INFO:tools.evaluation_results_class:Average Discounted Reward = -98.08273315429688
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9474747474747475
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 792537.125
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 250.0
INFO:tools.evaluation_results_class:Counted Episodes = 990
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.8418408632278442
INFO:agents.father_agent:Step: 410, Training loss: 2.5660927295684814
INFO:agents.father_agent:Step: 415, Training loss: 3.106241226196289
INFO:agents.father_agent:Step: 420, Training loss: 3.7209112644195557
INFO:agents.father_agent:Step: 425, Training loss: 4.109180927276611
INFO:agents.father_agent:Step: 430, Training loss: 2.8827788829803467
INFO:agents.father_agent:Step: 435, Training loss: 2.4629619121551514
INFO:agents.father_agent:Step: 440, Training loss: 3.286004066467285
INFO:agents.father_agent:Step: 445, Training loss: 2.70992374420166
INFO:agents.father_agent:Step: 450, Training loss: 3.2200403213500977
INFO:agents.father_agent:Step: 455, Training loss: 4.472311496734619
INFO:agents.father_agent:Step: 460, Training loss: 3.459087610244751
INFO:agents.father_agent:Step: 465, Training loss: 2.6863465309143066
INFO:agents.father_agent:Step: 470, Training loss: 2.8806331157684326
INFO:agents.father_agent:Step: 475, Training loss: 2.210425615310669
INFO:agents.father_agent:Step: 480, Training loss: 2.924138069152832
INFO:agents.father_agent:Step: 485, Training loss: 3.5173912048339844
INFO:agents.father_agent:Step: 490, Training loss: 2.507143974304199
INFO:agents.father_agent:Step: 495, Training loss: 3.019514799118042
INFO:agents.father_agent:Step: 500, Training loss: 2.8290843963623047
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -279.21588134765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -208.4466552734375
INFO:tools.evaluation_results_class:Average Discounted Reward = -91.64124298095703
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8846153846153846
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 787383.125
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 296.44168734491313
INFO:tools.evaluation_results_class:Counted Episodes = 806
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -300.42303466796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -232.53817749023438
INFO:tools.evaluation_results_class:Average Discounted Reward = -94.29273223876953
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8485607008760951
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 867670.4375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 309.59073842302877
INFO:tools.evaluation_results_class:Counted Episodes = 799
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -329.7976379394531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -262.6917724609375
INFO:tools.evaluation_results_class:Average Discounted Reward = -96.43875122070312
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8388235294117647
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1112805.375
INFO:tools.evaluation_results_class:Current Best Return = -329.7976379394531
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8388235294117647
INFO:tools.evaluation_results_class:Average Episode Length = 322.0423529411765
INFO:tools.evaluation_results_class:Counted Episodes = 850
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1156.05029296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1093.9664306640625
INFO:tools.evaluation_results_class:Average Discounted Reward = -224.07852172851562
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7760479041916167
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4169563.75
INFO:tools.evaluation_results_class:Current Best Return = -1156.05029296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.7760479041916167
INFO:tools.evaluation_results_class:Average Episode Length = 327.8502994011976
INFO:tools.evaluation_results_class:Counted Episodes = 835
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 1620.841 achieved after 2236.33 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1620.8669 achieved after 2236.33 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1627.8783 achieved after 2236.37 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1627.9042 achieved after 2236.38 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1636.7052 achieved after 2236.46 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1636.731 achieved after 2236.46 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1643.7425 achieved after 2236.51 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1643.7683 achieved after 2236.51 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1710.4401 achieved after 2236.59 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1710.4659 achieved after 2236.59 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1717.4774 achieved after 2236.62 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1717.5032 achieved after 2236.62 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1726.3043 achieved after 2236.68 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1726.3301 achieved after 2236.68 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1733.3416 achieved after 2236.71 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1733.3674 achieved after 2236.71 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1760.4746 achieved after 2236.86 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1760.5004 achieved after 2236.87 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1776.3388 achieved after 2236.91 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1776.3646 achieved after 2236.91 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1776.4332 achieved after 2236.99 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1776.459 achieved after 2237.0 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=5, o3y=1, o4x=3, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 1776.4590246662028
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=5, o3y=1, o4x=3, o4y=8, o5x=8, o5y=5
INFO:robust_rl.robust_rl_trainer:Iteration 5 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -315.8122863769531
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -245.15042114257812
INFO:tools.evaluation_results_class:Average Discounted Reward = -147.35321044921875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8832731648616126
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 659293.5
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 293.8231046931408
INFO:tools.evaluation_results_class:Counted Episodes = 831
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.935407638549805
INFO:agents.father_agent:Step: 5, Training loss: 5.62178373336792
INFO:agents.father_agent:Step: 10, Training loss: 3.138824939727783
INFO:agents.father_agent:Step: 15, Training loss: 3.6879465579986572
INFO:agents.father_agent:Step: 20, Training loss: 3.1039397716522217
INFO:agents.father_agent:Step: 25, Training loss: 3.3152847290039062
INFO:agents.father_agent:Step: 30, Training loss: 3.1117539405822754
INFO:agents.father_agent:Step: 35, Training loss: 3.648709774017334
INFO:agents.father_agent:Step: 40, Training loss: 3.755953073501587
INFO:agents.father_agent:Step: 45, Training loss: 3.445655107498169
INFO:agents.father_agent:Step: 50, Training loss: 3.2186713218688965
INFO:agents.father_agent:Step: 55, Training loss: 2.650627374649048
INFO:agents.father_agent:Step: 60, Training loss: 3.389133930206299
INFO:agents.father_agent:Step: 65, Training loss: 3.2931876182556152
INFO:agents.father_agent:Step: 70, Training loss: 2.5977516174316406
INFO:agents.father_agent:Step: 75, Training loss: 3.238970994949341
INFO:agents.father_agent:Step: 80, Training loss: 3.1426942348480225
INFO:agents.father_agent:Step: 85, Training loss: 3.893059492111206
INFO:agents.father_agent:Step: 90, Training loss: 4.6341166496276855
INFO:agents.father_agent:Step: 95, Training loss: 2.740978240966797
INFO:agents.father_agent:Step: 100, Training loss: 3.5328168869018555
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1429.091064453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1356.264892578125
INFO:tools.evaluation_results_class:Average Discounted Reward = -362.5882873535156
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9103260869565217
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3478576.0
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 333.70380434782606
INFO:tools.evaluation_results_class:Counted Episodes = 736
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 1.5353156328201294
INFO:agents.father_agent:Step: 110, Training loss: 3.7082483768463135
INFO:agents.father_agent:Step: 115, Training loss: 4.248671531677246
INFO:agents.father_agent:Step: 120, Training loss: 3.8925602436065674
INFO:agents.father_agent:Step: 125, Training loss: 2.782625198364258
INFO:agents.father_agent:Step: 130, Training loss: 3.101855516433716
INFO:agents.father_agent:Step: 135, Training loss: 2.6878135204315186
INFO:agents.father_agent:Step: 140, Training loss: 4.936034679412842
INFO:agents.father_agent:Step: 145, Training loss: 6.06257963180542
INFO:agents.father_agent:Step: 150, Training loss: 2.7031025886535645
INFO:agents.father_agent:Step: 155, Training loss: 3.1238458156585693
INFO:agents.father_agent:Step: 160, Training loss: 2.9305992126464844
INFO:agents.father_agent:Step: 165, Training loss: 2.9724957942962646
INFO:agents.father_agent:Step: 170, Training loss: 2.9479308128356934
INFO:agents.father_agent:Step: 175, Training loss: 2.2303693294525146
INFO:agents.father_agent:Step: 180, Training loss: 3.77508282661438
INFO:agents.father_agent:Step: 185, Training loss: 2.4589474201202393
INFO:agents.father_agent:Step: 190, Training loss: 2.6657729148864746
INFO:agents.father_agent:Step: 195, Training loss: 2.61620831489563
INFO:agents.father_agent:Step: 200, Training loss: 2.8820064067840576
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1318.64599609375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1243.4005126953125
INFO:tools.evaluation_results_class:Average Discounted Reward = -326.7645263671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9405684754521964
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2626093.25
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 317.9612403100775
INFO:tools.evaluation_results_class:Counted Episodes = 774
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 1.6625112295150757
INFO:agents.father_agent:Step: 210, Training loss: 2.8754401206970215
INFO:agents.father_agent:Step: 215, Training loss: 2.359705924987793
INFO:agents.father_agent:Step: 220, Training loss: 3.258505344390869
INFO:agents.father_agent:Step: 225, Training loss: 3.078120231628418
INFO:agents.father_agent:Step: 230, Training loss: 3.159691095352173
INFO:agents.father_agent:Step: 235, Training loss: 2.7787721157073975
INFO:agents.father_agent:Step: 240, Training loss: 2.132963180541992
INFO:agents.father_agent:Step: 245, Training loss: 2.1977181434631348
INFO:agents.father_agent:Step: 250, Training loss: 2.9347598552703857
INFO:agents.father_agent:Step: 255, Training loss: 2.409191131591797
INFO:agents.father_agent:Step: 260, Training loss: 2.5581181049346924
INFO:agents.father_agent:Step: 265, Training loss: 2.051095962524414
INFO:agents.father_agent:Step: 270, Training loss: 2.785060405731201
INFO:agents.father_agent:Step: 275, Training loss: 3.436082363128662
INFO:agents.father_agent:Step: 280, Training loss: 2.3218278884887695
INFO:agents.father_agent:Step: 285, Training loss: 3.502927541732788
INFO:agents.father_agent:Step: 290, Training loss: 2.420987844467163
INFO:agents.father_agent:Step: 295, Training loss: 3.1073415279388428
INFO:agents.father_agent:Step: 300, Training loss: 3.447812080383301
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1253.1571044921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1177.6185302734375
INFO:tools.evaluation_results_class:Average Discounted Reward = -301.9449768066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.944233206590621
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1906021.25
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 314.1660329531052
INFO:tools.evaluation_results_class:Counted Episodes = 789
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 2.0175251960754395
INFO:agents.father_agent:Step: 310, Training loss: 3.025942802429199
INFO:agents.father_agent:Step: 315, Training loss: 3.778665065765381
INFO:agents.father_agent:Step: 320, Training loss: 2.9739160537719727
INFO:agents.father_agent:Step: 325, Training loss: 4.035419940948486
INFO:agents.father_agent:Step: 330, Training loss: 2.7931113243103027
INFO:agents.father_agent:Step: 335, Training loss: 2.8407742977142334
INFO:agents.father_agent:Step: 340, Training loss: 2.47819185256958
INFO:agents.father_agent:Step: 345, Training loss: 2.2204747200012207
INFO:agents.father_agent:Step: 350, Training loss: 2.5145657062530518
INFO:agents.father_agent:Step: 355, Training loss: 2.399441719055176
INFO:agents.father_agent:Step: 360, Training loss: 3.5645065307617188
INFO:agents.father_agent:Step: 365, Training loss: 3.821244478225708
INFO:agents.father_agent:Step: 370, Training loss: 2.8090550899505615
INFO:agents.father_agent:Step: 375, Training loss: 2.559983015060425
INFO:agents.father_agent:Step: 380, Training loss: 2.7818517684936523
INFO:agents.father_agent:Step: 385, Training loss: 2.8784046173095703
INFO:agents.father_agent:Step: 390, Training loss: 2.306758165359497
INFO:agents.father_agent:Step: 395, Training loss: 3.7405543327331543
INFO:agents.father_agent:Step: 400, Training loss: 2.654547691345215
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1492.7725830078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1421.3267822265625
INFO:tools.evaluation_results_class:Average Discounted Reward = -317.2361755371094
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8930722891566265
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2470041.5
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 369.45180722891564
INFO:tools.evaluation_results_class:Counted Episodes = 664
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.9963784217834473
INFO:agents.father_agent:Step: 410, Training loss: 2.847832441329956
INFO:agents.father_agent:Step: 415, Training loss: 3.919339895248413
INFO:agents.father_agent:Step: 420, Training loss: 2.4564428329467773
INFO:agents.father_agent:Step: 425, Training loss: 3.590078830718994
INFO:agents.father_agent:Step: 430, Training loss: 3.200840950012207
INFO:agents.father_agent:Step: 435, Training loss: 3.087482452392578
INFO:agents.father_agent:Step: 440, Training loss: 2.5922532081604004
INFO:agents.father_agent:Step: 445, Training loss: 2.435703992843628
INFO:agents.father_agent:Step: 450, Training loss: 2.534881353378296
INFO:agents.father_agent:Step: 455, Training loss: 2.910574436187744
INFO:agents.father_agent:Step: 460, Training loss: 2.521824836730957
INFO:agents.father_agent:Step: 465, Training loss: 2.6036345958709717
INFO:agents.father_agent:Step: 470, Training loss: 2.9495482444763184
INFO:agents.father_agent:Step: 475, Training loss: 2.854065179824829
INFO:agents.father_agent:Step: 480, Training loss: 2.763800859451294
INFO:agents.father_agent:Step: 485, Training loss: 2.865077257156372
INFO:agents.father_agent:Step: 490, Training loss: 2.9443538188934326
INFO:agents.father_agent:Step: 495, Training loss: 2.2621986865997314
INFO:agents.father_agent:Step: 500, Training loss: 2.8249704837799072
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1045.93359375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -967.7353515625
INFO:tools.evaluation_results_class:Average Discounted Reward = -281.0818786621094
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9774774774774775
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1236888.875
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 281.5563063063063
INFO:tools.evaluation_results_class:Counted Episodes = 888
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -1064.3348388671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -986.0484619140625
INFO:tools.evaluation_results_class:Average Discounted Reward = -275.7456970214844
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9785794813979707
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1329254.125
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 283.7440811724915
INFO:tools.evaluation_results_class:Counted Episodes = 887
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1141.009521484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1063.5357666015625
INFO:tools.evaluation_results_class:Average Discounted Reward = -279.6269226074219
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.968421052631579
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1541025.375
INFO:tools.evaluation_results_class:Current Best Return = -1141.009521484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.968421052631579
INFO:tools.evaluation_results_class:Average Episode Length = 300.85052631578947
INFO:tools.evaluation_results_class:Counted Episodes = 950
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1145.121826171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1067.268310546875
INFO:tools.evaluation_results_class:Average Discounted Reward = -290.8123779296875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9731682146542827
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1576458.25
INFO:tools.evaluation_results_class:Current Best Return = -1145.121826171875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9731682146542827
INFO:tools.evaluation_results_class:Average Episode Length = 297.71826625386996
INFO:tools.evaluation_results_class:Counted Episodes = 969
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 1278.1832 achieved after 2771.62 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1278.6498 achieved after 2771.63 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1281.415 achieved after 2771.64 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1281.4679 achieved after 2771.66 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1287.6073 achieved after 2771.67 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1288.074 achieved after 2771.67 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1290.8391 achieved after 2771.68 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1291.6107 achieved after 2771.69 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1350.6889 achieved after 2771.7 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1351.1555 achieved after 2771.71 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1353.9207 achieved after 2771.72 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1353.9736 achieved after 2771.74 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1360.113 achieved after 2771.75 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1360.5796 achieved after 2771.75 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1363.3448 achieved after 2771.76 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1364.1164 achieved after 2771.77 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1369.4139 achieved after 2771.83 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1369.8805 achieved after 2771.83 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1372.6457 achieved after 2771.84 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1373.4173 achieved after 2771.85 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 1373.417307233089
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
INFO:robust_rl.robust_rl_trainer:Iteration 6 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1083.1624755859375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1005.1900634765625
INFO:tools.evaluation_results_class:Average Discounted Reward = -339.1302795410156
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9746543778801844
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1091836.375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 283.9884792626728
INFO:tools.evaluation_results_class:Counted Episodes = 868
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.8709495067596436
INFO:agents.father_agent:Step: 5, Training loss: 5.899718761444092
INFO:agents.father_agent:Step: 10, Training loss: 4.154151439666748
INFO:agents.father_agent:Step: 15, Training loss: 4.392709255218506
INFO:agents.father_agent:Step: 20, Training loss: 4.2529826164245605
INFO:agents.father_agent:Step: 25, Training loss: 4.116654872894287
INFO:agents.father_agent:Step: 30, Training loss: 4.044458866119385
INFO:agents.father_agent:Step: 35, Training loss: 4.405791759490967
INFO:agents.father_agent:Step: 40, Training loss: 5.055491924285889
INFO:agents.father_agent:Step: 45, Training loss: 3.8019580841064453
INFO:agents.father_agent:Step: 50, Training loss: 4.488419055938721
INFO:agents.father_agent:Step: 55, Training loss: 3.9946389198303223
INFO:agents.father_agent:Step: 60, Training loss: 5.351456642150879
INFO:agents.father_agent:Step: 65, Training loss: 4.063603401184082
INFO:agents.father_agent:Step: 70, Training loss: 4.783492088317871
INFO:agents.father_agent:Step: 75, Training loss: 4.152681827545166
INFO:agents.father_agent:Step: 80, Training loss: 4.590413570404053
INFO:agents.father_agent:Step: 85, Training loss: 3.2934353351593018
INFO:agents.father_agent:Step: 90, Training loss: 2.389411211013794
INFO:agents.father_agent:Step: 95, Training loss: 1.9687105417251587
INFO:agents.father_agent:Step: 100, Training loss: 2.3753881454467773
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -171.04054260253906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -92.63036346435547
INFO:tools.evaluation_results_class:Average Discounted Reward = -73.52013397216797
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.980127186009539
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 171661.203125
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 200.5262321144674
INFO:tools.evaluation_results_class:Counted Episodes = 1258
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 2.0517807006835938
INFO:agents.father_agent:Step: 110, Training loss: 2.72794246673584
INFO:agents.father_agent:Step: 115, Training loss: 2.5894100666046143
INFO:agents.father_agent:Step: 120, Training loss: 2.783346176147461
INFO:agents.father_agent:Step: 125, Training loss: 3.123326301574707
INFO:agents.father_agent:Step: 130, Training loss: 2.4759321212768555
INFO:agents.father_agent:Step: 135, Training loss: 2.524918794631958
INFO:agents.father_agent:Step: 140, Training loss: 1.8409411907196045
INFO:agents.father_agent:Step: 145, Training loss: 2.1785848140716553
INFO:agents.father_agent:Step: 150, Training loss: 3.577758550643921
INFO:agents.father_agent:Step: 155, Training loss: 3.3375251293182373
INFO:agents.father_agent:Step: 160, Training loss: 2.974661350250244
INFO:agents.father_agent:Step: 165, Training loss: 3.100929021835327
INFO:agents.father_agent:Step: 170, Training loss: 2.552668333053589
INFO:agents.father_agent:Step: 175, Training loss: 2.533970832824707
INFO:agents.father_agent:Step: 180, Training loss: 3.460582733154297
INFO:agents.father_agent:Step: 185, Training loss: 3.55248761177063
INFO:agents.father_agent:Step: 190, Training loss: 2.8699440956115723
INFO:agents.father_agent:Step: 195, Training loss: 2.578594446182251
INFO:agents.father_agent:Step: 200, Training loss: 1.8970366716384888
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -545.1398315429688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -468.2816162109375
INFO:tools.evaluation_results_class:Average Discounted Reward = -172.39614868164062
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.960727969348659
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1339362.375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 236.1168582375479
INFO:tools.evaluation_results_class:Counted Episodes = 1044
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 1.4228445291519165
INFO:agents.father_agent:Step: 210, Training loss: 2.4523932933807373
INFO:agents.father_agent:Step: 215, Training loss: 2.854318380355835
INFO:agents.father_agent:Step: 220, Training loss: 3.4773924350738525
INFO:agents.father_agent:Step: 225, Training loss: 3.0458059310913086
INFO:agents.father_agent:Step: 230, Training loss: 2.7330234050750732
INFO:agents.father_agent:Step: 235, Training loss: 2.5603280067443848
INFO:agents.father_agent:Step: 240, Training loss: 2.394911766052246
INFO:agents.father_agent:Step: 245, Training loss: 2.9597177505493164
INFO:agents.father_agent:Step: 250, Training loss: 3.2526297569274902
INFO:agents.father_agent:Step: 255, Training loss: 2.7805933952331543
INFO:agents.father_agent:Step: 260, Training loss: 3.1824448108673096
INFO:agents.father_agent:Step: 265, Training loss: 2.2838754653930664
INFO:agents.father_agent:Step: 270, Training loss: 2.260918378829956
INFO:agents.father_agent:Step: 275, Training loss: 2.5654938220977783
INFO:agents.father_agent:Step: 280, Training loss: 3.0029184818267822
INFO:agents.father_agent:Step: 285, Training loss: 3.158234119415283
INFO:agents.father_agent:Step: 290, Training loss: 2.7632462978363037
INFO:agents.father_agent:Step: 295, Training loss: 3.5590291023254395
INFO:agents.father_agent:Step: 300, Training loss: 2.5413870811462402
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.0708465576172
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -98.56130981445312
INFO:tools.evaluation_results_class:Average Discounted Reward = -74.52425384521484
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9938692098092643
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 154078.625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 176.57629427792915
INFO:tools.evaluation_results_class:Counted Episodes = 1468
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.5591670274734497
INFO:agents.father_agent:Step: 310, Training loss: 2.5316567420959473
INFO:agents.father_agent:Step: 315, Training loss: 3.214090347290039
INFO:agents.father_agent:Step: 320, Training loss: 3.627784490585327
INFO:agents.father_agent:Step: 325, Training loss: 2.759213924407959
INFO:agents.father_agent:Step: 330, Training loss: 3.442984104156494
INFO:agents.father_agent:Step: 335, Training loss: 3.4324843883514404
INFO:agents.father_agent:Step: 340, Training loss: 2.8490467071533203
INFO:agents.father_agent:Step: 345, Training loss: 3.0354840755462646
INFO:agents.father_agent:Step: 350, Training loss: 3.459573268890381
INFO:agents.father_agent:Step: 355, Training loss: 3.195974349975586
INFO:agents.father_agent:Step: 360, Training loss: 2.51020884513855
INFO:agents.father_agent:Step: 365, Training loss: 2.7237179279327393
INFO:agents.father_agent:Step: 370, Training loss: 2.868081569671631
INFO:agents.father_agent:Step: 375, Training loss: 4.24500036239624
INFO:agents.father_agent:Step: 380, Training loss: 3.009331703186035
INFO:agents.father_agent:Step: 385, Training loss: 2.8028793334960938
INFO:agents.father_agent:Step: 390, Training loss: 2.778421640396118
INFO:agents.father_agent:Step: 395, Training loss: 3.012941360473633
INFO:agents.father_agent:Step: 400, Training loss: 2.989490509033203
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -178.00350952148438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -98.00350189208984
INFO:tools.evaluation_results_class:Average Discounted Reward = -74.7693862915039
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 102654.203125
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 115.52705332814325
INFO:tools.evaluation_results_class:Counted Episodes = 2569
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 2.0830078125
INFO:agents.father_agent:Step: 410, Training loss: 3.108144998550415
INFO:agents.father_agent:Step: 415, Training loss: 2.8911917209625244
INFO:agents.father_agent:Step: 420, Training loss: 3.0638082027435303
INFO:agents.father_agent:Step: 425, Training loss: 3.090127468109131
INFO:agents.father_agent:Step: 430, Training loss: 2.482969045639038
INFO:agents.father_agent:Step: 435, Training loss: 2.6336703300476074
INFO:agents.father_agent:Step: 440, Training loss: 2.0458273887634277
INFO:agents.father_agent:Step: 445, Training loss: 3.0258352756500244
INFO:agents.father_agent:Step: 450, Training loss: 3.5502004623413086
INFO:agents.father_agent:Step: 455, Training loss: 3.3962109088897705
INFO:agents.father_agent:Step: 460, Training loss: 3.627748727798462
INFO:agents.father_agent:Step: 465, Training loss: 2.7488465309143066
INFO:agents.father_agent:Step: 470, Training loss: 3.2191052436828613
INFO:agents.father_agent:Step: 475, Training loss: 3.0930025577545166
INFO:agents.father_agent:Step: 480, Training loss: 3.4826865196228027
INFO:agents.father_agent:Step: 485, Training loss: 4.603338241577148
INFO:agents.father_agent:Step: 490, Training loss: 2.357307195663452
INFO:agents.father_agent:Step: 495, Training loss: 2.766770362854004
INFO:agents.father_agent:Step: 500, Training loss: 2.310080051422119
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -341.0430603027344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -261.5127258300781
INFO:tools.evaluation_results_class:Average Discounted Reward = -119.1946029663086
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9941291585127201
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 637063.625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 172.0515329419439
INFO:tools.evaluation_results_class:Counted Episodes = 1533
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -307.4197998046875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -228.06671142578125
INFO:tools.evaluation_results_class:Average Discounted Reward = -109.2421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9919137466307277
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 538605.3125
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 173.82614555256066
INFO:tools.evaluation_results_class:Counted Episodes = 1484
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -409.79364013671875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -331.11639404296875
INFO:tools.evaluation_results_class:Average Discounted Reward = -131.2557830810547
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9834656084656085
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 793332.8125
INFO:tools.evaluation_results_class:Current Best Return = -409.79364013671875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9834656084656085
INFO:tools.evaluation_results_class:Average Episode Length = 195.29232804232805
INFO:tools.evaluation_results_class:Counted Episodes = 1512
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -616.4118041992188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -537.0706787109375
INFO:tools.evaluation_results_class:Average Discounted Reward = -203.97727966308594
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9917638984214139
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 956254.8125
INFO:tools.evaluation_results_class:Current Best Return = -616.4118041992188
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9917638984214139
INFO:tools.evaluation_results_class:Average Episode Length = 200.38091969800962
INFO:tools.evaluation_results_class:Counted Episodes = 1457
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 643.3054 achieved after 3322.54 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 643.3163 achieved after 3322.55 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 654.2942 achieved after 3322.58 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 654.3051 achieved after 3322.6 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 662.2745 achieved after 3322.66 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 662.2853 achieved after 3322.67 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 662.6997 achieved after 3322.7 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 662.7105 achieved after 3322.71 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 733.2462 achieved after 3322.75 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 733.257 achieved after 3322.76 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 744.235 achieved after 3322.77 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 744.2458 achieved after 3322.79 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 752.2152 achieved after 3322.84 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 752.2261 achieved after 3322.85 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 752.6404 achieved after 3322.86 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 752.6512 achieved after 3322.87 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 784.8384 achieved after 3322.96 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 784.8492 achieved after 3322.98 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 792.8186 achieved after 3323.0 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 792.8294 achieved after 3323.01 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 793.2438 achieved after 3323.02 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 793.2546 achieved after 3323.04 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=4, o3x=3, o3y=2, o4x=3, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 793.2546178429669
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=4, o3x=3, o3y=2, o4x=3, o4y=6, o5x=8, o5y=5
INFO:robust_rl.robust_rl_trainer:Iteration 7 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -368.1172790527344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -288.3694763183594
INFO:tools.evaluation_results_class:Average Discounted Reward = -180.93247985839844
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9968474148802018
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 411842.09375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 167.69987389659522
INFO:tools.evaluation_results_class:Counted Episodes = 1586
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 6.306576251983643
INFO:agents.father_agent:Step: 5, Training loss: 5.247657775878906
INFO:agents.father_agent:Step: 10, Training loss: 5.304285049438477
INFO:agents.father_agent:Step: 15, Training loss: 4.007203102111816
INFO:agents.father_agent:Step: 20, Training loss: 3.377537965774536
INFO:agents.father_agent:Step: 25, Training loss: 4.044365406036377
INFO:agents.father_agent:Step: 30, Training loss: 4.6132917404174805
INFO:agents.father_agent:Step: 35, Training loss: 5.5369791984558105
INFO:agents.father_agent:Step: 40, Training loss: 4.426538944244385
INFO:agents.father_agent:Step: 45, Training loss: 4.8888044357299805
INFO:agents.father_agent:Step: 50, Training loss: 3.9638566970825195
INFO:agents.father_agent:Step: 55, Training loss: 4.862960338592529
INFO:agents.father_agent:Step: 60, Training loss: 3.6655778884887695
INFO:agents.father_agent:Step: 65, Training loss: 4.331271171569824
INFO:agents.father_agent:Step: 70, Training loss: 4.103673934936523
INFO:agents.father_agent:Step: 75, Training loss: 3.40673565864563
INFO:agents.father_agent:Step: 80, Training loss: 2.5710062980651855
INFO:agents.father_agent:Step: 85, Training loss: 4.418276309967041
INFO:agents.father_agent:Step: 90, Training loss: 3.2360589504241943
INFO:agents.father_agent:Step: 95, Training loss: 3.3730404376983643
INFO:agents.father_agent:Step: 100, Training loss: 2.898056745529175
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -356.70489501953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -276.70489501953125
INFO:tools.evaluation_results_class:Average Discounted Reward = -151.0916748046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 174139.375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 128.8632626568585
INFO:tools.evaluation_results_class:Counted Episodes = 2311
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 2.3539822101593018
INFO:agents.father_agent:Step: 110, Training loss: 2.7999823093414307
INFO:agents.father_agent:Step: 115, Training loss: 5.366123199462891
INFO:agents.father_agent:Step: 120, Training loss: 4.115611553192139
INFO:agents.father_agent:Step: 125, Training loss: 2.9175260066986084
INFO:agents.father_agent:Step: 130, Training loss: 2.463230848312378
INFO:agents.father_agent:Step: 135, Training loss: 3.135735034942627
INFO:agents.father_agent:Step: 140, Training loss: 2.4800782203674316
INFO:agents.father_agent:Step: 145, Training loss: 3.4799249172210693
INFO:agents.father_agent:Step: 150, Training loss: 2.9929862022399902
INFO:agents.father_agent:Step: 155, Training loss: 2.9831297397613525
INFO:agents.father_agent:Step: 160, Training loss: 2.646099805831909
INFO:agents.father_agent:Step: 165, Training loss: 2.9890921115875244
INFO:agents.father_agent:Step: 170, Training loss: 3.3377060890197754
INFO:agents.father_agent:Step: 175, Training loss: 2.7951533794403076
INFO:agents.father_agent:Step: 180, Training loss: 2.36064076423645
INFO:agents.father_agent:Step: 185, Training loss: 2.3749358654022217
INFO:agents.father_agent:Step: 190, Training loss: 3.2385783195495605
INFO:agents.father_agent:Step: 195, Training loss: 3.257692337036133
INFO:agents.father_agent:Step: 200, Training loss: 2.800631046295166
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -696.9189453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -616.9189453125
INFO:tools.evaluation_results_class:Average Discounted Reward = -250.66876220703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 391703.6875
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 171.4410011918951
INFO:tools.evaluation_results_class:Counted Episodes = 1678
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 3.0209035873413086
INFO:agents.father_agent:Step: 210, Training loss: 3.2599070072174072
INFO:agents.father_agent:Step: 215, Training loss: 2.9842214584350586
INFO:agents.father_agent:Step: 220, Training loss: 3.5509119033813477
INFO:agents.father_agent:Step: 225, Training loss: 3.0329089164733887
INFO:agents.father_agent:Step: 230, Training loss: 2.7986209392547607
INFO:agents.father_agent:Step: 235, Training loss: 2.539236545562744
INFO:agents.father_agent:Step: 240, Training loss: 2.6268556118011475
INFO:agents.father_agent:Step: 245, Training loss: 3.6354923248291016
INFO:agents.father_agent:Step: 250, Training loss: 3.0585954189300537
INFO:agents.father_agent:Step: 255, Training loss: 4.043916702270508
INFO:agents.father_agent:Step: 260, Training loss: 3.2954587936401367
INFO:agents.father_agent:Step: 265, Training loss: 3.1200830936431885
INFO:agents.father_agent:Step: 270, Training loss: 3.458191394805908
INFO:agents.father_agent:Step: 275, Training loss: 2.4658071994781494
INFO:agents.father_agent:Step: 280, Training loss: 2.517910957336426
INFO:agents.father_agent:Step: 285, Training loss: 2.2603280544281006
INFO:agents.father_agent:Step: 290, Training loss: 2.944383144378662
INFO:agents.father_agent:Step: 295, Training loss: 2.442885160446167
INFO:agents.father_agent:Step: 300, Training loss: 2.4197466373443604
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -538.32080078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -458.3208312988281
INFO:tools.evaluation_results_class:Average Discounted Reward = -210.60096740722656
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 235295.03125
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 145.396879570941
INFO:tools.evaluation_results_class:Counted Episodes = 2051
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 2.399122714996338
INFO:agents.father_agent:Step: 310, Training loss: 2.9359054565429688
INFO:agents.father_agent:Step: 315, Training loss: 2.9338955879211426
INFO:agents.father_agent:Step: 320, Training loss: 2.8860442638397217
INFO:agents.father_agent:Step: 325, Training loss: 2.9393484592437744
INFO:agents.father_agent:Step: 330, Training loss: 2.3816936016082764
INFO:agents.father_agent:Step: 335, Training loss: 2.9392457008361816
INFO:agents.father_agent:Step: 340, Training loss: 2.3981449604034424
INFO:agents.father_agent:Step: 345, Training loss: 2.295818567276001
INFO:agents.father_agent:Step: 350, Training loss: 2.696608304977417
INFO:agents.father_agent:Step: 355, Training loss: 2.6601853370666504
INFO:agents.father_agent:Step: 360, Training loss: 3.2745189666748047
INFO:agents.father_agent:Step: 365, Training loss: 3.0317211151123047
INFO:agents.father_agent:Step: 370, Training loss: 2.240746021270752
INFO:agents.father_agent:Step: 375, Training loss: 2.7738397121429443
INFO:agents.father_agent:Step: 380, Training loss: 2.339210271835327
INFO:agents.father_agent:Step: 385, Training loss: 2.6740217208862305
INFO:agents.father_agent:Step: 390, Training loss: 3.339839458465576
INFO:agents.father_agent:Step: 395, Training loss: 2.8771820068359375
INFO:agents.father_agent:Step: 400, Training loss: 3.0200107097625732
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -928.3860473632812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -848.3860473632812
INFO:tools.evaluation_results_class:Average Discounted Reward = -279.72235107421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 743800.9375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 214.7196554424432
INFO:tools.evaluation_results_class:Counted Episodes = 1277
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.7761036157608032
INFO:agents.father_agent:Step: 410, Training loss: 3.0184144973754883
INFO:agents.father_agent:Step: 415, Training loss: 5.8965744972229
INFO:agents.father_agent:Step: 420, Training loss: 3.139925479888916
INFO:agents.father_agent:Step: 425, Training loss: 3.5868163108825684
INFO:agents.father_agent:Step: 430, Training loss: 3.0300076007843018
INFO:agents.father_agent:Step: 435, Training loss: 2.4940805435180664
INFO:agents.father_agent:Step: 440, Training loss: 2.2228281497955322
INFO:agents.father_agent:Step: 445, Training loss: 2.3068931102752686
INFO:agents.father_agent:Step: 450, Training loss: 2.3855607509613037
INFO:agents.father_agent:Step: 455, Training loss: 3.727864980697632
INFO:agents.father_agent:Step: 460, Training loss: 3.197092056274414
INFO:agents.father_agent:Step: 465, Training loss: 3.0024795532226562
INFO:agents.father_agent:Step: 470, Training loss: 2.799459934234619
INFO:agents.father_agent:Step: 475, Training loss: 2.5725021362304688
INFO:agents.father_agent:Step: 480, Training loss: 2.8211426734924316
INFO:agents.father_agent:Step: 485, Training loss: 2.421739339828491
INFO:agents.father_agent:Step: 490, Training loss: 2.955615758895874
INFO:agents.father_agent:Step: 495, Training loss: 2.8332011699676514
INFO:agents.father_agent:Step: 500, Training loss: 2.7973532676696777
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -637.735107421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -557.735107421875
INFO:tools.evaluation_results_class:Average Discounted Reward = -216.01837158203125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 474516.15625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 182.21891191709844
INFO:tools.evaluation_results_class:Counted Episodes = 1544
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -660.6353149414062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -580.6353149414062
INFO:tools.evaluation_results_class:Average Discounted Reward = -223.9340057373047
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 488365.4375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 182.489941596366
INFO:tools.evaluation_results_class:Counted Episodes = 1541
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -642.896728515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -562.896728515625
INFO:tools.evaluation_results_class:Average Discounted Reward = -218.75242614746094
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 451659.84375
INFO:tools.evaluation_results_class:Current Best Return = -642.896728515625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 182.42792281498296
INFO:tools.evaluation_results_class:Counted Episodes = 1762
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -614.1401977539062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -534.1401977539062
INFO:tools.evaluation_results_class:Average Discounted Reward = -226.98135375976562
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 383636.25
INFO:tools.evaluation_results_class:Current Best Return = -614.1401977539062
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 177.37543453070683
INFO:tools.evaluation_results_class:Counted Episodes = 1726
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 657.6374 achieved after 3876.78 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 658.8999 achieved after 3876.78 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 664.1769 achieved after 3876.79 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 664.2037 achieved after 3876.81 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 666.7627 achieved after 3876.82 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 668.0251 achieved after 3876.83 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 673.3021 achieved after 3876.83 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 674.1805 achieved after 3876.84 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 730.5728 achieved after 3876.85 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 731.8352 achieved after 3876.86 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 737.1122 achieved after 3876.87 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 737.139 achieved after 3876.88 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 739.698 achieved after 3876.9 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 740.9604 achieved after 3876.9 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 746.2374 achieved after 3876.9 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 747.1158 achieved after 3876.91 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 748.7524 achieved after 3876.98 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 750.0148 achieved after 3876.99 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 755.2918 achieved after 3876.99 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 756.1702 achieved after 3877.0 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 756.1701781831465
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
INFO:robust_rl.robust_rl_trainer:Iteration 8 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -739.1237182617188
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -659.1237182617188
INFO:tools.evaluation_results_class:Average Discounted Reward = -290.0628967285156
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 485758.59375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 181.61269430051814
INFO:tools.evaluation_results_class:Counted Episodes = 1544
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.6499714851379395
INFO:agents.father_agent:Step: 5, Training loss: 5.0809760093688965
INFO:agents.father_agent:Step: 10, Training loss: 4.372861862182617
INFO:agents.father_agent:Step: 15, Training loss: 4.455509662628174
INFO:agents.father_agent:Step: 20, Training loss: 3.628080368041992
INFO:agents.father_agent:Step: 25, Training loss: 4.1170830726623535
INFO:agents.father_agent:Step: 30, Training loss: 4.214115142822266
INFO:agents.father_agent:Step: 35, Training loss: 3.8861823081970215
INFO:agents.father_agent:Step: 40, Training loss: 4.554192066192627
INFO:agents.father_agent:Step: 45, Training loss: 4.162882328033447
INFO:agents.father_agent:Step: 50, Training loss: 3.6361823081970215
INFO:agents.father_agent:Step: 55, Training loss: 3.6061487197875977
INFO:agents.father_agent:Step: 60, Training loss: 3.4349067211151123
INFO:agents.father_agent:Step: 65, Training loss: 3.6819450855255127
INFO:agents.father_agent:Step: 70, Training loss: 4.269874095916748
INFO:agents.father_agent:Step: 75, Training loss: 3.3487963676452637
INFO:agents.father_agent:Step: 80, Training loss: 3.148735761642456
INFO:agents.father_agent:Step: 85, Training loss: 3.3536806106567383
INFO:agents.father_agent:Step: 90, Training loss: 3.6306586265563965
INFO:agents.father_agent:Step: 95, Training loss: 3.459899425506592
INFO:agents.father_agent:Step: 100, Training loss: 3.962453842163086
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1312.57080078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1245.8846435546875
INFO:tools.evaluation_results_class:Average Discounted Reward = -336.3995361328125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8335766423357664
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3736301.75
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 357.3094890510949
INFO:tools.evaluation_results_class:Counted Episodes = 685
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 3.8875207901000977
INFO:agents.father_agent:Step: 110, Training loss: 2.142296314239502
INFO:agents.father_agent:Step: 115, Training loss: 5.215323448181152
INFO:agents.father_agent:Step: 120, Training loss: 4.336369514465332
INFO:agents.father_agent:Step: 125, Training loss: 3.5687928199768066
INFO:agents.father_agent:Step: 130, Training loss: 3.1085152626037598
INFO:agents.father_agent:Step: 135, Training loss: 2.2228004932403564
INFO:agents.father_agent:Step: 140, Training loss: 3.109621047973633
INFO:agents.father_agent:Step: 145, Training loss: 2.7032017707824707
INFO:agents.father_agent:Step: 150, Training loss: 1.7573686838150024
INFO:agents.father_agent:Step: 155, Training loss: 1.616333246231079
INFO:agents.father_agent:Step: 160, Training loss: 2.61625599861145
INFO:agents.father_agent:Step: 165, Training loss: 2.6076390743255615
INFO:agents.father_agent:Step: 170, Training loss: 2.6039440631866455
INFO:agents.father_agent:Step: 175, Training loss: 3.972346544265747
INFO:agents.father_agent:Step: 180, Training loss: 2.695416212081909
INFO:agents.father_agent:Step: 185, Training loss: 2.9705464839935303
INFO:agents.father_agent:Step: 190, Training loss: 2.580756902694702
INFO:agents.father_agent:Step: 195, Training loss: 2.697824239730835
INFO:agents.father_agent:Step: 200, Training loss: 2.8245882987976074
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -123.15214538574219
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -43.54478454589844
INFO:tools.evaluation_results_class:Average Discounted Reward = -53.911258697509766
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9950920245398773
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 68752.1171875
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 156.06257668711658
INFO:tools.evaluation_results_class:Counted Episodes = 1630
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 1.4079102277755737
INFO:agents.father_agent:Step: 210, Training loss: 2.247417449951172
INFO:agents.father_agent:Step: 215, Training loss: 3.27091908454895
INFO:agents.father_agent:Step: 220, Training loss: 3.535022735595703
INFO:agents.father_agent:Step: 225, Training loss: 3.004117250442505
INFO:agents.father_agent:Step: 230, Training loss: 2.607685089111328
INFO:agents.father_agent:Step: 235, Training loss: 2.295959234237671
INFO:agents.father_agent:Step: 240, Training loss: 2.6591594219207764
INFO:agents.father_agent:Step: 245, Training loss: 2.301877498626709
INFO:agents.father_agent:Step: 250, Training loss: 3.175391674041748
INFO:agents.father_agent:Step: 255, Training loss: 3.143864631652832
INFO:agents.father_agent:Step: 260, Training loss: 3.000398635864258
INFO:agents.father_agent:Step: 265, Training loss: 3.1108951568603516
INFO:agents.father_agent:Step: 270, Training loss: 2.5374252796173096
INFO:agents.father_agent:Step: 275, Training loss: 2.9050285816192627
INFO:agents.father_agent:Step: 280, Training loss: 2.263007879257202
INFO:agents.father_agent:Step: 285, Training loss: 2.862431764602661
INFO:agents.father_agent:Step: 290, Training loss: 3.3505659103393555
INFO:agents.father_agent:Step: 295, Training loss: 3.4547977447509766
INFO:agents.father_agent:Step: 300, Training loss: 3.495133876800537
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -174.69529724121094
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -94.76838684082031
INFO:tools.evaluation_results_class:Average Discounted Reward = -76.9610595703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9990863407948835
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 92222.234375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 129.09776153494747
INFO:tools.evaluation_results_class:Counted Episodes = 2189
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 3.0464062690734863
INFO:agents.father_agent:Step: 310, Training loss: 3.469757556915283
INFO:agents.father_agent:Step: 315, Training loss: 3.8373641967773438
INFO:agents.father_agent:Step: 320, Training loss: 2.7125961780548096
INFO:agents.father_agent:Step: 325, Training loss: 2.7685840129852295
INFO:agents.father_agent:Step: 330, Training loss: 2.1922080516815186
INFO:agents.father_agent:Step: 335, Training loss: 2.562770366668701
INFO:agents.father_agent:Step: 340, Training loss: 3.232098340988159
INFO:agents.father_agent:Step: 345, Training loss: 3.0082786083221436
INFO:agents.father_agent:Step: 350, Training loss: 3.5401828289031982
INFO:agents.father_agent:Step: 355, Training loss: 2.925603151321411
INFO:agents.father_agent:Step: 360, Training loss: 3.536914587020874
INFO:agents.father_agent:Step: 365, Training loss: 2.8099873065948486
INFO:agents.father_agent:Step: 370, Training loss: 2.915147542953491
INFO:agents.father_agent:Step: 375, Training loss: 2.4067957401275635
INFO:agents.father_agent:Step: 380, Training loss: 3.4612741470336914
INFO:agents.father_agent:Step: 385, Training loss: 2.8734281063079834
INFO:agents.father_agent:Step: 390, Training loss: 2.637073516845703
INFO:agents.father_agent:Step: 395, Training loss: 2.687495231628418
INFO:agents.father_agent:Step: 400, Training loss: 2.5851263999938965
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -169.47735595703125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -89.51589965820312
INFO:tools.evaluation_results_class:Average Discounted Reward = -72.37262725830078
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9995183044315993
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 146845.078125
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 135.60693641618496
INFO:tools.evaluation_results_class:Counted Episodes = 2076
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.6561026573181152
INFO:agents.father_agent:Step: 410, Training loss: 2.579211711883545
INFO:agents.father_agent:Step: 415, Training loss: 2.398768424987793
INFO:agents.father_agent:Step: 420, Training loss: 3.7649612426757812
INFO:agents.father_agent:Step: 425, Training loss: 2.99684476852417
INFO:agents.father_agent:Step: 430, Training loss: 3.465611457824707
INFO:agents.father_agent:Step: 435, Training loss: 1.8534172773361206
INFO:agents.father_agent:Step: 440, Training loss: 2.8928656578063965
INFO:agents.father_agent:Step: 445, Training loss: 2.881655693054199
INFO:agents.father_agent:Step: 450, Training loss: 2.7552273273468018
INFO:agents.father_agent:Step: 455, Training loss: 2.786261796951294
INFO:agents.father_agent:Step: 460, Training loss: 2.946444034576416
INFO:agents.father_agent:Step: 465, Training loss: 2.8071682453155518
INFO:agents.father_agent:Step: 470, Training loss: 2.8243913650512695
INFO:agents.father_agent:Step: 475, Training loss: 2.772853374481201
INFO:agents.father_agent:Step: 480, Training loss: 2.4454615116119385
INFO:agents.father_agent:Step: 485, Training loss: 2.4930639266967773
INFO:agents.father_agent:Step: 490, Training loss: 3.669114828109741
INFO:agents.father_agent:Step: 495, Training loss: 1.9524846076965332
INFO:agents.father_agent:Step: 500, Training loss: 2.994488000869751
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -371.2537841796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -291.6219482421875
INFO:tools.evaluation_results_class:Average Discounted Reward = -137.4110107421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9953977646285339
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 559809.875
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 174.85798816568047
INFO:tools.evaluation_results_class:Counted Episodes = 1521
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -373.306884765625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -293.6163635253906
INFO:tools.evaluation_results_class:Average Discounted Reward = -138.32675170898438
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9961315280464217
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 555714.75
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 172.6234687298517
INFO:tools.evaluation_results_class:Counted Episodes = 1551
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -453.9092102050781
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -374.763671875
INFO:tools.evaluation_results_class:Average Discounted Reward = -151.6955108642578
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9893190921228304
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 873490.6875
INFO:tools.evaluation_results_class:Current Best Return = -453.9092102050781
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9893190921228304
INFO:tools.evaluation_results_class:Average Episode Length = 198.5460614152203
INFO:tools.evaluation_results_class:Counted Episodes = 1498
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -600.7933959960938
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -521.1572265625
INFO:tools.evaluation_results_class:Average Discounted Reward = -211.15049743652344
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.99545159194282
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 740900.9375
INFO:tools.evaluation_results_class:Current Best Return = -600.7933959960938
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.99545159194282
INFO:tools.evaluation_results_class:Average Episode Length = 193.75503573749188
INFO:tools.evaluation_results_class:Counted Episodes = 1539
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 637.7507 achieved after 4440.76 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 637.8018 achieved after 4440.78 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 640.6591 achieved after 4440.8 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 640.7102 achieved after 4440.82 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 652.3421 achieved after 4440.85 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 652.3932 achieved after 4440.86 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 655.2505 achieved after 4440.89 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 655.3016 achieved after 4440.9 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 656.8792 achieved after 4440.96 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 656.9304 achieved after 4440.98 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 662.613 achieved after 4441.0 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 662.6641 achieved after 4441.01 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 726.9666 achieved after 4441.05 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 727.0177 achieved after 4441.07 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 729.875 achieved after 4441.08 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 729.9261 achieved after 4441.09 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 741.558 achieved after 4441.1 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 741.6091 achieved after 4441.12 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 744.4664 achieved after 4441.13 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 744.5175 achieved after 4441.14 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 746.0951 achieved after 4441.18 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 746.1463 achieved after 4441.2 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 751.8289 achieved after 4441.21 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 751.88 achieved after 4441.22 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 778.6471 achieved after 4441.3 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 778.6983 achieved after 4441.32 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 781.5555 achieved after 4441.33 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 781.6067 achieved after 4441.35 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 783.1843 achieved after 4441.37 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 783.2355 achieved after 4441.38 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 788.9181 achieved after 4441.39 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 788.9692 achieved after 4441.41 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 788.9798 achieved after 4441.46 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 789.031 achieved after 4441.47 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=3, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 789.0309738108817
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=3, o4y=6, o5x=8, o5y=5
INFO:robust_rl.robust_rl_trainer:Iteration 9 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -455.83184814453125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -376.3240051269531
INFO:tools.evaluation_results_class:Average Discounted Reward = -208.5066680908203
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9938482570061518
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 677092.25
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 178.32057416267943
INFO:tools.evaluation_results_class:Counted Episodes = 1463
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.296750068664551
INFO:agents.father_agent:Step: 5, Training loss: 4.704479694366455
INFO:agents.father_agent:Step: 10, Training loss: 2.710440158843994
INFO:agents.father_agent:Step: 15, Training loss: 3.23624324798584
INFO:agents.father_agent:Step: 20, Training loss: 3.274709939956665
INFO:agents.father_agent:Step: 25, Training loss: 3.6064131259918213
INFO:agents.father_agent:Step: 30, Training loss: 3.306636095046997
INFO:agents.father_agent:Step: 35, Training loss: 3.8008084297180176
INFO:agents.father_agent:Step: 40, Training loss: 3.650980234146118
INFO:agents.father_agent:Step: 45, Training loss: 4.341536998748779
INFO:agents.father_agent:Step: 50, Training loss: 5.7018537521362305
INFO:agents.father_agent:Step: 55, Training loss: 4.180350303649902
INFO:agents.father_agent:Step: 60, Training loss: 5.247307777404785
INFO:agents.father_agent:Step: 65, Training loss: 4.552698612213135
INFO:agents.father_agent:Step: 70, Training loss: 3.556204080581665
INFO:agents.father_agent:Step: 75, Training loss: 4.004881381988525
INFO:agents.father_agent:Step: 80, Training loss: 2.9468607902526855
INFO:agents.father_agent:Step: 85, Training loss: 4.755509853363037
INFO:agents.father_agent:Step: 90, Training loss: 3.04976224899292
INFO:agents.father_agent:Step: 95, Training loss: 3.353724718093872
INFO:agents.father_agent:Step: 100, Training loss: 3.1216766834259033
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -476.6862487792969
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -396.6862487792969
INFO:tools.evaluation_results_class:Average Discounted Reward = -187.20327758789062
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 382910.8125
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 157.47362637362636
INFO:tools.evaluation_results_class:Counted Episodes = 1820
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 2.2109696865081787
INFO:agents.father_agent:Step: 110, Training loss: 2.3809316158294678
INFO:agents.father_agent:Step: 115, Training loss: 3.3684372901916504
INFO:agents.father_agent:Step: 120, Training loss: 3.0924887657165527
INFO:agents.father_agent:Step: 125, Training loss: 2.957622528076172
INFO:agents.father_agent:Step: 130, Training loss: 2.7294344902038574
INFO:agents.father_agent:Step: 135, Training loss: 2.3979427814483643
INFO:agents.father_agent:Step: 140, Training loss: 2.4721314907073975
INFO:agents.father_agent:Step: 145, Training loss: 2.3880271911621094
INFO:agents.father_agent:Step: 150, Training loss: 2.6478872299194336
INFO:agents.father_agent:Step: 155, Training loss: 2.9887475967407227
INFO:agents.father_agent:Step: 160, Training loss: 3.092938184738159
INFO:agents.father_agent:Step: 165, Training loss: 3.1115458011627197
INFO:agents.father_agent:Step: 170, Training loss: 2.489952564239502
INFO:agents.father_agent:Step: 175, Training loss: 2.3098621368408203
INFO:agents.father_agent:Step: 180, Training loss: 2.065952777862549
INFO:agents.father_agent:Step: 185, Training loss: 2.7420601844787598
INFO:agents.father_agent:Step: 190, Training loss: 2.661367654800415
INFO:agents.father_agent:Step: 195, Training loss: 2.731764554977417
INFO:agents.father_agent:Step: 200, Training loss: 3.163701057434082
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -635.5347290039062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -555.5347290039062
INFO:tools.evaluation_results_class:Average Discounted Reward = -243.6350860595703
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 325844.09375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 153.11925708699903
INFO:tools.evaluation_results_class:Counted Episodes = 2046
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 2.053725481033325
INFO:agents.father_agent:Step: 210, Training loss: 2.8253557682037354
INFO:agents.father_agent:Step: 215, Training loss: 3.4419617652893066
INFO:agents.father_agent:Step: 220, Training loss: 3.014920234680176
INFO:agents.father_agent:Step: 225, Training loss: 3.3662638664245605
INFO:agents.father_agent:Step: 230, Training loss: 2.693960189819336
INFO:agents.father_agent:Step: 235, Training loss: 2.6983439922332764
INFO:agents.father_agent:Step: 240, Training loss: 2.341911554336548
INFO:agents.father_agent:Step: 245, Training loss: 1.9832217693328857
INFO:agents.father_agent:Step: 250, Training loss: 2.4031307697296143
INFO:agents.father_agent:Step: 255, Training loss: 2.4226953983306885
INFO:agents.father_agent:Step: 260, Training loss: 3.25996732711792
INFO:agents.father_agent:Step: 265, Training loss: 2.5554585456848145
INFO:agents.father_agent:Step: 270, Training loss: 3.2609546184539795
INFO:agents.father_agent:Step: 275, Training loss: 2.179121255874634
INFO:agents.father_agent:Step: 280, Training loss: 2.7131824493408203
INFO:agents.father_agent:Step: 285, Training loss: 2.2770564556121826
INFO:agents.father_agent:Step: 290, Training loss: 2.4733786582946777
INFO:agents.father_agent:Step: 295, Training loss: 2.402221918106079
INFO:agents.father_agent:Step: 300, Training loss: 2.369391441345215
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1983.37890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1922.0767822265625
INFO:tools.evaluation_results_class:Average Discounted Reward = -352.2502136230469
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7662771285475793
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4356138.5
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 432.7829716193656
INFO:tools.evaluation_results_class:Counted Episodes = 599
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.676194667816162
INFO:agents.father_agent:Step: 310, Training loss: 2.512361764907837
INFO:agents.father_agent:Step: 315, Training loss: 2.582444667816162
INFO:agents.father_agent:Step: 320, Training loss: 2.8179430961608887
INFO:agents.father_agent:Step: 325, Training loss: 3.7992520332336426
INFO:agents.father_agent:Step: 330, Training loss: 2.645080804824829
INFO:agents.father_agent:Step: 335, Training loss: 2.790818929672241
INFO:agents.father_agent:Step: 340, Training loss: 2.696591377258301
INFO:agents.father_agent:Step: 345, Training loss: 2.7087056636810303
INFO:agents.father_agent:Step: 350, Training loss: 2.168999195098877
INFO:agents.father_agent:Step: 355, Training loss: 3.108738422393799
INFO:agents.father_agent:Step: 360, Training loss: 2.703523874282837
INFO:agents.father_agent:Step: 365, Training loss: 2.8918983936309814
INFO:agents.father_agent:Step: 370, Training loss: 2.1318836212158203
INFO:agents.father_agent:Step: 375, Training loss: 2.125915765762329
INFO:agents.father_agent:Step: 380, Training loss: 2.438403367996216
INFO:agents.father_agent:Step: 385, Training loss: 2.6347246170043945
INFO:agents.father_agent:Step: 390, Training loss: 2.6936981678009033
INFO:agents.father_agent:Step: 395, Training loss: 2.105302572250366
INFO:agents.father_agent:Step: 400, Training loss: 3.1996593475341797
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1399.263427734375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1326.7161865234375
INFO:tools.evaluation_results_class:Average Discounted Reward = -320.42816162109375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9068413391557496
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2044847.75
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 350.72052401746726
INFO:tools.evaluation_results_class:Counted Episodes = 687
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.7734880447387695
INFO:agents.father_agent:Step: 410, Training loss: 2.7137160301208496
INFO:agents.father_agent:Step: 415, Training loss: 2.5297293663024902
INFO:agents.father_agent:Step: 420, Training loss: 2.8095414638519287
INFO:agents.father_agent:Step: 425, Training loss: 3.3929758071899414
INFO:agents.father_agent:Step: 430, Training loss: 3.1768810749053955
INFO:agents.father_agent:Step: 435, Training loss: 2.695871591567993
INFO:agents.father_agent:Step: 440, Training loss: 2.457157850265503
INFO:agents.father_agent:Step: 445, Training loss: 2.3801462650299072
INFO:agents.father_agent:Step: 450, Training loss: 2.749817371368408
INFO:agents.father_agent:Step: 455, Training loss: 2.8766777515411377
INFO:agents.father_agent:Step: 460, Training loss: 3.5531508922576904
INFO:agents.father_agent:Step: 465, Training loss: 2.9261701107025146
INFO:agents.father_agent:Step: 470, Training loss: 3.0960354804992676
INFO:agents.father_agent:Step: 475, Training loss: 2.5525026321411133
INFO:agents.father_agent:Step: 480, Training loss: 2.4051859378814697
INFO:agents.father_agent:Step: 485, Training loss: 2.62202787399292
INFO:agents.father_agent:Step: 490, Training loss: 2.2966597080230713
INFO:agents.father_agent:Step: 495, Training loss: 2.2431211471557617
INFO:agents.father_agent:Step: 500, Training loss: 2.453308582305908
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1455.485107421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1382.8717041015625
INFO:tools.evaluation_results_class:Average Discounted Reward = -317.70672607421875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9076682316118936
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2372748.5
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 373.3035993740219
INFO:tools.evaluation_results_class:Counted Episodes = 639
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -1522.381591796875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1452.517333984375
INFO:tools.evaluation_results_class:Average Discounted Reward = -306.7095031738281
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8733031674208145
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2666512.75
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 368.5897435897436
INFO:tools.evaluation_results_class:Counted Episodes = 663
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1518.5189208984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1449.1781005859375
INFO:tools.evaluation_results_class:Average Discounted Reward = -320.3142395019531
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8667601683029453
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2461712.25
INFO:tools.evaluation_results_class:Current Best Return = -1518.5189208984375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8667601683029453
INFO:tools.evaluation_results_class:Average Episode Length = 384.56241234221596
INFO:tools.evaluation_results_class:Counted Episodes = 713
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1546.63916015625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1476.3734130859375
INFO:tools.evaluation_results_class:Average Discounted Reward = -320.1080017089844
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8783216783216783
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2374682.5
INFO:tools.evaluation_results_class:Current Best Return = -1546.63916015625
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8783216783216783
INFO:tools.evaluation_results_class:Average Episode Length = 387.16783216783216
INFO:tools.evaluation_results_class:Counted Episodes = 715
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 1742.6167 achieved after 5011.03 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1742.8859 achieved after 5011.04 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1744.8037 achieved after 5011.04 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1744.8124 achieved after 5011.06 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1751.6685 achieved after 5011.08 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1751.9376 achieved after 5011.08 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1753.8554 achieved after 5011.08 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1754.799 achieved after 5011.1 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1815.8626 achieved after 5011.11 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1816.1318 achieved after 5011.12 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1818.0496 achieved after 5011.13 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1818.0583 achieved after 5011.15 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1824.9144 achieved after 5011.16 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1825.1835 achieved after 5011.17 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1827.1013 achieved after 5011.17 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1828.0448 achieved after 5011.18 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1833.9406 achieved after 5011.25 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1834.2097 achieved after 5011.25 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1836.1275 achieved after 5011.25 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1837.071 achieved after 5011.27 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 1837.0710447767483
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
INFO:robust_rl.robust_rl_trainer:Iteration 10 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1590.2047119140625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1519.9837646484375
INFO:tools.evaluation_results_class:Average Discounted Reward = -379.6455383300781
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8777614138438881
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2699256.0
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 373.1266568483063
INFO:tools.evaluation_results_class:Counted Episodes = 679
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 3.103874921798706
INFO:agents.father_agent:Step: 5, Training loss: 4.754324436187744
INFO:agents.father_agent:Step: 10, Training loss: 4.364187240600586
INFO:agents.father_agent:Step: 15, Training loss: 4.085613250732422
INFO:agents.father_agent:Step: 20, Training loss: 3.7412631511688232
INFO:agents.father_agent:Step: 25, Training loss: 4.028572082519531
INFO:agents.father_agent:Step: 30, Training loss: 4.224595546722412
INFO:agents.father_agent:Step: 35, Training loss: 4.0806565284729
INFO:agents.father_agent:Step: 40, Training loss: 3.8014423847198486
INFO:agents.father_agent:Step: 45, Training loss: 3.162777900695801
INFO:agents.father_agent:Step: 50, Training loss: 2.91119384765625
INFO:agents.father_agent:Step: 55, Training loss: 2.2585601806640625
INFO:agents.father_agent:Step: 60, Training loss: 1.3198044300079346
INFO:agents.father_agent:Step: 65, Training loss: 0.8923126459121704
INFO:agents.father_agent:Step: 70, Training loss: 0.7427567839622498
INFO:agents.father_agent:Step: 75, Training loss: 0.10271242260932922
INFO:agents.father_agent:Step: 80, Training loss: 0.061182890087366104
INFO:agents.father_agent:Step: 85, Training loss: 1.2887672185897827
INFO:agents.father_agent:Step: 90, Training loss: 4.169142723083496
INFO:agents.father_agent:Step: 95, Training loss: 7.071200847625732
INFO:agents.father_agent:Step: 100, Training loss: 5.389435291290283
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -520.2759399414062
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -465.557861328125
INFO:tools.evaluation_results_class:Average Discounted Reward = -119.50330352783203
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6839762611275965
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 2617232.25
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 361.3353115727003
INFO:tools.evaluation_results_class:Counted Episodes = 674
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 3.4980356693267822
INFO:agents.father_agent:Step: 110, Training loss: 2.933694839477539
INFO:agents.father_agent:Step: 115, Training loss: 2.544525384902954
INFO:agents.father_agent:Step: 120, Training loss: 2.5784807205200195
INFO:agents.father_agent:Step: 125, Training loss: 3.6789968013763428
INFO:agents.father_agent:Step: 130, Training loss: 2.274683713912964
INFO:agents.father_agent:Step: 135, Training loss: 1.5101333856582642
INFO:agents.father_agent:Step: 140, Training loss: 1.5116440057754517
INFO:agents.father_agent:Step: 145, Training loss: 3.335744619369507
INFO:agents.father_agent:Step: 150, Training loss: 2.421180248260498
INFO:agents.father_agent:Step: 155, Training loss: 3.1813488006591797
INFO:agents.father_agent:Step: 160, Training loss: 3.3205442428588867
INFO:agents.father_agent:Step: 165, Training loss: 1.973427176475525
INFO:agents.father_agent:Step: 170, Training loss: 2.363349437713623
INFO:agents.father_agent:Step: 175, Training loss: 3.2426419258117676
INFO:agents.father_agent:Step: 180, Training loss: 3.023942470550537
INFO:agents.father_agent:Step: 185, Training loss: 3.1204721927642822
INFO:agents.father_agent:Step: 190, Training loss: 2.7092628479003906
INFO:agents.father_agent:Step: 195, Training loss: 2.9661149978637695
INFO:agents.father_agent:Step: 200, Training loss: 2.509674072265625
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -87.8469009399414
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -8.493800163269043
INFO:tools.evaluation_results_class:Average Discounted Reward = -36.11245346069336
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9919137466307277
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 27914.654296875
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 138.533153638814
INFO:tools.evaluation_results_class:Counted Episodes = 1855
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 0.8634767532348633
INFO:agents.father_agent:Step: 210, Training loss: 2.5360844135284424
INFO:agents.father_agent:Step: 215, Training loss: 2.5316104888916016
INFO:agents.father_agent:Step: 220, Training loss: 3.1630496978759766
INFO:agents.father_agent:Step: 225, Training loss: 3.854671001434326
INFO:agents.father_agent:Step: 230, Training loss: 3.499530792236328
INFO:agents.father_agent:Step: 235, Training loss: 2.618825912475586
INFO:agents.father_agent:Step: 240, Training loss: 2.4043948650360107
INFO:agents.father_agent:Step: 245, Training loss: 2.3117246627807617
INFO:agents.father_agent:Step: 250, Training loss: 2.765873908996582
INFO:agents.father_agent:Step: 255, Training loss: 2.2340447902679443
INFO:agents.father_agent:Step: 260, Training loss: 2.4930267333984375
INFO:agents.father_agent:Step: 265, Training loss: 2.488159656524658
INFO:agents.father_agent:Step: 270, Training loss: 2.3693344593048096
INFO:agents.father_agent:Step: 275, Training loss: 2.2359378337860107
INFO:agents.father_agent:Step: 280, Training loss: 2.144805908203125
INFO:agents.father_agent:Step: 285, Training loss: 1.9610942602157593
INFO:agents.father_agent:Step: 290, Training loss: 2.056533098220825
INFO:agents.father_agent:Step: 295, Training loss: 2.778982162475586
INFO:agents.father_agent:Step: 300, Training loss: 3.027907371520996
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -142.94308471679688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -65.0446548461914
INFO:tools.evaluation_results_class:Average Discounted Reward = -73.44417572021484
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9737302977232924
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 22850.0078125
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 213.38353765323993
INFO:tools.evaluation_results_class:Counted Episodes = 1142
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.3080759048461914
INFO:agents.father_agent:Step: 310, Training loss: 2.29099440574646
INFO:agents.father_agent:Step: 315, Training loss: 2.3333160877227783
INFO:agents.father_agent:Step: 320, Training loss: 3.2946910858154297
INFO:agents.father_agent:Step: 325, Training loss: 2.689316511154175
INFO:agents.father_agent:Step: 330, Training loss: 2.4307198524475098
INFO:agents.father_agent:Step: 335, Training loss: 2.9846854209899902
INFO:agents.father_agent:Step: 340, Training loss: 3.2187583446502686
INFO:agents.father_agent:Step: 345, Training loss: 2.8011324405670166
INFO:agents.father_agent:Step: 350, Training loss: 3.0165982246398926
INFO:agents.father_agent:Step: 355, Training loss: 2.5040786266326904
INFO:agents.father_agent:Step: 360, Training loss: 3.2390849590301514
INFO:agents.father_agent:Step: 365, Training loss: 3.090437173843384
INFO:agents.father_agent:Step: 370, Training loss: 2.352808952331543
INFO:agents.father_agent:Step: 375, Training loss: 3.0927157402038574
INFO:agents.father_agent:Step: 380, Training loss: 2.110685348510742
INFO:agents.father_agent:Step: 385, Training loss: 2.775390863418579
INFO:agents.father_agent:Step: 390, Training loss: 2.816016912460327
INFO:agents.father_agent:Step: 395, Training loss: 3.1077094078063965
INFO:agents.father_agent:Step: 400, Training loss: 1.8677700757980347
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -135.79708862304688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -55.797088623046875
INFO:tools.evaluation_results_class:Average Discounted Reward = -64.2376708984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 37907.41796875
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 138.75728155339806
INFO:tools.evaluation_results_class:Counted Episodes = 2060
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.5533994436264038
INFO:agents.father_agent:Step: 410, Training loss: 2.437347173690796
INFO:agents.father_agent:Step: 415, Training loss: 2.790024518966675
INFO:agents.father_agent:Step: 420, Training loss: 2.983447790145874
INFO:agents.father_agent:Step: 425, Training loss: 3.886817216873169
INFO:agents.father_agent:Step: 430, Training loss: 2.8926570415496826
INFO:agents.father_agent:Step: 435, Training loss: 2.5880510807037354
INFO:agents.father_agent:Step: 440, Training loss: 1.5721986293792725
INFO:agents.father_agent:Step: 445, Training loss: 2.001075267791748
INFO:agents.father_agent:Step: 450, Training loss: 2.9770925045013428
INFO:agents.father_agent:Step: 455, Training loss: 2.872246742248535
INFO:agents.father_agent:Step: 460, Training loss: 3.417011022567749
INFO:agents.father_agent:Step: 465, Training loss: 2.9542229175567627
INFO:agents.father_agent:Step: 470, Training loss: 2.0163748264312744
INFO:agents.father_agent:Step: 475, Training loss: 2.3175952434539795
INFO:agents.father_agent:Step: 480, Training loss: 3.2231316566467285
INFO:agents.father_agent:Step: 485, Training loss: 2.1598310470581055
INFO:agents.father_agent:Step: 490, Training loss: 3.2377519607543945
INFO:agents.father_agent:Step: 495, Training loss: 2.725789785385132
INFO:agents.father_agent:Step: 500, Training loss: 2.7520980834960938
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -192.35667419433594
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -112.3566665649414
INFO:tools.evaluation_results_class:Average Discounted Reward = -83.0365982055664
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 172984.140625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 127.29153743907843
INFO:tools.evaluation_results_class:Counted Episodes = 2257
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -183.63143920898438
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -103.66757202148438
INFO:tools.evaluation_results_class:Average Discounted Reward = -81.09632110595703
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9995483288166215
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 127122.140625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 129.34959349593495
INFO:tools.evaluation_results_class:Counted Episodes = 2214
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -210.47650146484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -130.511962890625
INFO:tools.evaluation_results_class:Average Discounted Reward = -88.57559204101562
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9995567375886525
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 187535.171875
INFO:tools.evaluation_results_class:Current Best Return = -210.47650146484375
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.9995567375886525
INFO:tools.evaluation_results_class:Average Episode Length = 137.5336879432624
INFO:tools.evaluation_results_class:Counted Episodes = 2256
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -409.1049499511719
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -329.1049499511719
INFO:tools.evaluation_results_class:Average Discounted Reward = -160.3438720703125
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 445298.75
INFO:tools.evaluation_results_class:Current Best Return = -409.1049499511719
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 148.92225461613216
INFO:tools.evaluation_results_class:Counted Episodes = 2058
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 407.8674 achieved after 5563.77 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 407.874 achieved after 5563.78 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 415.2156 achieved after 5563.81 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 415.2223 achieved after 5563.83 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 497.7854 achieved after 5563.86 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 497.7921 achieved after 5563.87 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 505.1337 achieved after 5563.89 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 505.1403 achieved after 5563.9 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 508.0112 achieved after 5563.96 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 508.0179 achieved after 5563.97 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 515.3595 achieved after 5563.99 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 515.3661 achieved after 5564.0 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 548.1717 achieved after 5564.1 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 548.1784 achieved after 5564.12 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 558.3975 achieved after 5564.16 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 558.4042 achieved after 5564.17 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 558.4092 achieved after 5564.24 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=3, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 558.4091997119147
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=3, o4y=8, o5x=8, o5y=5
INFO:robust_rl.robust_rl_trainer:Iteration 11 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -234.4906768798828
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -154.4906768798828
INFO:tools.evaluation_results_class:Average Discounted Reward = -137.6419677734375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 100613.140625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 130.49704411095954
INFO:tools.evaluation_results_class:Counted Episodes = 2199
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 4.628857612609863
INFO:agents.father_agent:Step: 5, Training loss: 4.373992443084717
INFO:agents.father_agent:Step: 10, Training loss: 2.5734314918518066
INFO:agents.father_agent:Step: 15, Training loss: 2.9826693534851074
INFO:agents.father_agent:Step: 20, Training loss: 2.4946017265319824
INFO:agents.father_agent:Step: 25, Training loss: 2.0261313915252686
INFO:agents.father_agent:Step: 30, Training loss: 2.4965412616729736
INFO:agents.father_agent:Step: 35, Training loss: 2.944085121154785
INFO:agents.father_agent:Step: 40, Training loss: 2.3592748641967773
INFO:agents.father_agent:Step: 45, Training loss: 3.190459966659546
INFO:agents.father_agent:Step: 50, Training loss: 2.7897520065307617
INFO:agents.father_agent:Step: 55, Training loss: 2.9182958602905273
INFO:agents.father_agent:Step: 60, Training loss: 2.512500047683716
INFO:agents.father_agent:Step: 65, Training loss: 2.4582183361053467
INFO:agents.father_agent:Step: 70, Training loss: 2.76880145072937
INFO:agents.father_agent:Step: 75, Training loss: 3.1402056217193604
INFO:agents.father_agent:Step: 80, Training loss: 2.2444393634796143
INFO:agents.father_agent:Step: 85, Training loss: 2.937828540802002
INFO:agents.father_agent:Step: 90, Training loss: 2.390202760696411
INFO:agents.father_agent:Step: 95, Training loss: 2.322545051574707
INFO:agents.father_agent:Step: 100, Training loss: 2.268785238265991
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -488.6340026855469
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -411.8340148925781
INFO:tools.evaluation_results_class:Average Discounted Reward = -197.3547821044922
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.96
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1203928.875
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 243.172
INFO:tools.evaluation_results_class:Counted Episodes = 1000
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 1.706482172012329
INFO:agents.father_agent:Step: 110, Training loss: 3.2218306064605713
INFO:agents.father_agent:Step: 115, Training loss: 3.9170939922332764
INFO:agents.father_agent:Step: 120, Training loss: 3.81876802444458
INFO:agents.father_agent:Step: 125, Training loss: 2.4002268314361572
INFO:agents.father_agent:Step: 130, Training loss: 3.6532864570617676
INFO:agents.father_agent:Step: 135, Training loss: 2.245469331741333
INFO:agents.father_agent:Step: 140, Training loss: 3.094856023788452
INFO:agents.father_agent:Step: 145, Training loss: 2.1567842960357666
INFO:agents.father_agent:Step: 150, Training loss: 4.5706706047058105
INFO:agents.father_agent:Step: 155, Training loss: 2.1005008220672607
INFO:agents.father_agent:Step: 160, Training loss: 2.2813358306884766
INFO:agents.father_agent:Step: 165, Training loss: 2.447580575942993
INFO:agents.father_agent:Step: 170, Training loss: 2.281078338623047
INFO:agents.father_agent:Step: 175, Training loss: 2.4349348545074463
INFO:agents.father_agent:Step: 180, Training loss: 2.2629284858703613
INFO:agents.father_agent:Step: 185, Training loss: 3.6684634685516357
INFO:agents.father_agent:Step: 190, Training loss: 2.546257734298706
INFO:agents.father_agent:Step: 195, Training loss: 2.5226168632507324
INFO:agents.father_agent:Step: 200, Training loss: 2.819476842880249
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -572.0852661132812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -492.0852355957031
INFO:tools.evaluation_results_class:Average Discounted Reward = -223.05714416503906
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 262018.5625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 146.40477350219192
INFO:tools.evaluation_results_class:Counted Episodes = 2053
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 1.6024750471115112
INFO:agents.father_agent:Step: 210, Training loss: 2.5505175590515137
INFO:agents.father_agent:Step: 215, Training loss: 3.7465381622314453
INFO:agents.father_agent:Step: 220, Training loss: 3.2094924449920654
INFO:agents.father_agent:Step: 225, Training loss: 2.4272541999816895
INFO:agents.father_agent:Step: 230, Training loss: 2.3247318267822266
INFO:agents.father_agent:Step: 235, Training loss: 2.4102041721343994
INFO:agents.father_agent:Step: 240, Training loss: 1.9482935667037964
INFO:agents.father_agent:Step: 245, Training loss: 2.4548943042755127
INFO:agents.father_agent:Step: 250, Training loss: 2.366692543029785
INFO:agents.father_agent:Step: 255, Training loss: 2.6848526000976562
INFO:agents.father_agent:Step: 260, Training loss: 2.196974277496338
INFO:agents.father_agent:Step: 265, Training loss: 2.1261422634124756
INFO:agents.father_agent:Step: 270, Training loss: 1.6752345561981201
INFO:agents.father_agent:Step: 275, Training loss: 2.2657339572906494
INFO:agents.father_agent:Step: 280, Training loss: 2.48687744140625
INFO:agents.father_agent:Step: 285, Training loss: 2.717219114303589
INFO:agents.father_agent:Step: 290, Training loss: 2.31575083732605
INFO:agents.father_agent:Step: 295, Training loss: 1.9774582386016846
INFO:agents.father_agent:Step: 300, Training loss: 2.3511836528778076
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -773.1558837890625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -693.1558837890625
INFO:tools.evaluation_results_class:Average Discounted Reward = -320.9879150390625
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 284953.1875
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 131.78760255241568
INFO:tools.evaluation_results_class:Counted Episodes = 2194
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.8553929328918457
INFO:agents.father_agent:Step: 310, Training loss: 2.130744695663452
INFO:agents.father_agent:Step: 315, Training loss: 2.2302520275115967
INFO:agents.father_agent:Step: 320, Training loss: 2.2861287593841553
INFO:agents.father_agent:Step: 325, Training loss: 2.6602344512939453
INFO:agents.father_agent:Step: 330, Training loss: 1.8312433958053589
INFO:agents.father_agent:Step: 335, Training loss: 1.8542648553848267
INFO:agents.father_agent:Step: 340, Training loss: 1.9015284776687622
INFO:agents.father_agent:Step: 345, Training loss: 2.0285630226135254
INFO:agents.father_agent:Step: 350, Training loss: 2.183744192123413
INFO:agents.father_agent:Step: 355, Training loss: 2.4957032203674316
INFO:agents.father_agent:Step: 360, Training loss: 2.53576397895813
INFO:agents.father_agent:Step: 365, Training loss: 2.3660166263580322
INFO:agents.father_agent:Step: 370, Training loss: 2.051372528076172
INFO:agents.father_agent:Step: 375, Training loss: 1.6588568687438965
INFO:agents.father_agent:Step: 380, Training loss: 1.5140588283538818
INFO:agents.father_agent:Step: 385, Training loss: 2.6274452209472656
INFO:agents.father_agent:Step: 390, Training loss: 1.9807969331741333
INFO:agents.father_agent:Step: 395, Training loss: 2.000060558319092
INFO:agents.father_agent:Step: 400, Training loss: 2.1309974193573
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -799.8875732421875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -719.8875732421875
INFO:tools.evaluation_results_class:Average Discounted Reward = -336.4628601074219
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 204644.625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 126.89908621374653
INFO:tools.evaluation_results_class:Counted Episodes = 2517
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.9242042303085327
INFO:agents.father_agent:Step: 410, Training loss: 2.2767369747161865
INFO:agents.father_agent:Step: 415, Training loss: 2.3022282123565674
INFO:agents.father_agent:Step: 420, Training loss: 2.4869446754455566
INFO:agents.father_agent:Step: 425, Training loss: 2.74678111076355
INFO:agents.father_agent:Step: 430, Training loss: 1.9588029384613037
INFO:agents.father_agent:Step: 435, Training loss: 2.0299782752990723
INFO:agents.father_agent:Step: 440, Training loss: 1.8938512802124023
INFO:agents.father_agent:Step: 445, Training loss: 2.073648691177368
INFO:agents.father_agent:Step: 450, Training loss: 2.59114146232605
INFO:agents.father_agent:Step: 455, Training loss: 2.39837908744812
INFO:agents.father_agent:Step: 460, Training loss: 2.6671316623687744
INFO:agents.father_agent:Step: 465, Training loss: 2.7244739532470703
INFO:agents.father_agent:Step: 470, Training loss: 2.161794662475586
INFO:agents.father_agent:Step: 475, Training loss: 2.3344902992248535
INFO:agents.father_agent:Step: 480, Training loss: 2.5682225227355957
INFO:agents.father_agent:Step: 485, Training loss: 2.104158639907837
INFO:agents.father_agent:Step: 490, Training loss: 2.6844847202301025
INFO:agents.father_agent:Step: 495, Training loss: 2.4670329093933105
INFO:agents.father_agent:Step: 500, Training loss: 2.1174745559692383
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -925.6671752929688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -845.6671752929688
INFO:tools.evaluation_results_class:Average Discounted Reward = -352.0105285644531
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 473909.5625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 154.911045943304
INFO:tools.evaluation_results_class:Counted Episodes = 2046
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -935.8694458007812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -855.8694458007812
INFO:tools.evaluation_results_class:Average Discounted Reward = -356.3600158691406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 467379.9375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 155.39413202933986
INFO:tools.evaluation_results_class:Counted Episodes = 2045
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -916.7811279296875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -836.7811279296875
INFO:tools.evaluation_results_class:Average Discounted Reward = -349.20111083984375
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 473139.15625
INFO:tools.evaluation_results_class:Current Best Return = -916.7811279296875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 155.6497313141182
INFO:tools.evaluation_results_class:Counted Episodes = 2047
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -438.93133544921875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -358.93133544921875
INFO:tools.evaluation_results_class:Average Discounted Reward = -173.00025939941406
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 282790.53125
INFO:tools.evaluation_results_class:Current Best Return = -438.93133544921875
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 155.46542150429076
INFO:tools.evaluation_results_class:Counted Episodes = 1981
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 526.0844 achieved after 6115.6 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 526.0979 achieved after 6115.61 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 535.0799 achieved after 6115.62 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 536.0295 achieved after 6115.62 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 536.9584 achieved after 6115.63 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 537.908 achieved after 6115.63 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 546.1478 achieved after 6115.64 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 547.0974 achieved after 6115.64 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 578.3263 achieved after 6115.69 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 578.3399 achieved after 6115.69 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 587.3219 achieved after 6115.7 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 588.2714 achieved after 6115.7 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 589.2004 achieved after 6115.71 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 590.1499 achieved after 6115.71 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 598.3897 achieved after 6115.72 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 599.3393 achieved after 6115.73 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 607.3606 achieved after 6115.86 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 608.3101 achieved after 6115.86 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 608.3101180938271
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
INFO:robust_rl.robust_rl_trainer:Iteration 12 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -992.356201171875
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -912.356201171875
INFO:tools.evaluation_results_class:Average Discounted Reward = -414.1955261230469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 464979.03125
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 155.25575698187163
INFO:tools.evaluation_results_class:Counted Episodes = 2041
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 2.8540005683898926
INFO:agents.father_agent:Step: 5, Training loss: 3.273068904876709
INFO:agents.father_agent:Step: 10, Training loss: 2.9192328453063965
INFO:agents.father_agent:Step: 15, Training loss: 3.1338932514190674
INFO:agents.father_agent:Step: 20, Training loss: 3.308938503265381
INFO:agents.father_agent:Step: 25, Training loss: 3.1802899837493896
INFO:agents.father_agent:Step: 30, Training loss: 3.3579940795898438
INFO:agents.father_agent:Step: 35, Training loss: 4.450927257537842
INFO:agents.father_agent:Step: 40, Training loss: 2.90470290184021
INFO:agents.father_agent:Step: 45, Training loss: 2.572655200958252
INFO:agents.father_agent:Step: 50, Training loss: 2.5231106281280518
INFO:agents.father_agent:Step: 55, Training loss: 1.7094212770462036
INFO:agents.father_agent:Step: 60, Training loss: 1.583722472190857
INFO:agents.father_agent:Step: 65, Training loss: 1.5489811897277832
INFO:agents.father_agent:Step: 70, Training loss: 1.5480133295059204
INFO:agents.father_agent:Step: 75, Training loss: 1.8118104934692383
INFO:agents.father_agent:Step: 80, Training loss: 1.2355890274047852
INFO:agents.father_agent:Step: 85, Training loss: 2.0601913928985596
INFO:agents.father_agent:Step: 90, Training loss: 2.067981719970703
INFO:agents.father_agent:Step: 95, Training loss: 2.3488028049468994
INFO:agents.father_agent:Step: 100, Training loss: 1.915320873260498
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -235.93263244628906
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -179.04640197753906
INFO:tools.evaluation_results_class:Average Discounted Reward = -87.4936294555664
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7110778443113772
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 462587.875
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 383.76047904191614
INFO:tools.evaluation_results_class:Counted Episodes = 668
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 1.301844596862793
INFO:agents.father_agent:Step: 110, Training loss: 2.252784490585327
INFO:agents.father_agent:Step: 115, Training loss: 2.2774481773376465
INFO:agents.father_agent:Step: 120, Training loss: 3.2994132041931152
INFO:agents.father_agent:Step: 125, Training loss: 2.314300775527954
INFO:agents.father_agent:Step: 130, Training loss: 2.073709011077881
INFO:agents.father_agent:Step: 135, Training loss: 2.3566434383392334
INFO:agents.father_agent:Step: 140, Training loss: 1.7875924110412598
INFO:agents.father_agent:Step: 145, Training loss: 2.661564826965332
INFO:agents.father_agent:Step: 150, Training loss: 2.2609384059906006
INFO:agents.father_agent:Step: 155, Training loss: 2.2521066665649414
INFO:agents.father_agent:Step: 160, Training loss: 2.789759635925293
INFO:agents.father_agent:Step: 165, Training loss: 2.3221800327301025
INFO:agents.father_agent:Step: 170, Training loss: 2.554605722427368
INFO:agents.father_agent:Step: 175, Training loss: 2.2148566246032715
INFO:agents.father_agent:Step: 180, Training loss: 2.56156849861145
INFO:agents.father_agent:Step: 185, Training loss: 2.796645164489746
INFO:agents.father_agent:Step: 190, Training loss: 2.479259967803955
INFO:agents.father_agent:Step: 195, Training loss: 2.002908945083618
INFO:agents.father_agent:Step: 200, Training loss: 1.9497573375701904
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -262.736083984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -206.1964874267578
INFO:tools.evaluation_results_class:Average Discounted Reward = -81.0447998046875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7067448680351907
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 777225.5
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 362.45454545454544
INFO:tools.evaluation_results_class:Counted Episodes = 682
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 1.3323534727096558
INFO:agents.father_agent:Step: 210, Training loss: 2.1157889366149902
INFO:agents.father_agent:Step: 215, Training loss: 2.2442288398742676
INFO:agents.father_agent:Step: 220, Training loss: 2.7994132041931152
INFO:agents.father_agent:Step: 225, Training loss: 2.4084532260894775
INFO:agents.father_agent:Step: 230, Training loss: 2.172910451889038
INFO:agents.father_agent:Step: 235, Training loss: 2.5274362564086914
INFO:agents.father_agent:Step: 240, Training loss: 2.203028678894043
INFO:agents.father_agent:Step: 245, Training loss: 1.8228917121887207
INFO:agents.father_agent:Step: 250, Training loss: 1.4472626447677612
INFO:agents.father_agent:Step: 255, Training loss: 2.138657808303833
INFO:agents.father_agent:Step: 260, Training loss: 2.643817901611328
INFO:agents.father_agent:Step: 265, Training loss: 2.340599298477173
INFO:agents.father_agent:Step: 270, Training loss: 2.6387012004852295
INFO:agents.father_agent:Step: 275, Training loss: 1.946242094039917
INFO:agents.father_agent:Step: 280, Training loss: 1.513604760169983
INFO:agents.father_agent:Step: 285, Training loss: 2.9083070755004883
INFO:agents.father_agent:Step: 290, Training loss: 3.3452465534210205
INFO:agents.father_agent:Step: 295, Training loss: 2.759890556335449
INFO:agents.father_agent:Step: 300, Training loss: 2.7517294883728027
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -303.1539611816406
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -246.75108337402344
INFO:tools.evaluation_results_class:Average Discounted Reward = -86.0145263671875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7050359712230215
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1141649.375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 358.23741007194246
INFO:tools.evaluation_results_class:Counted Episodes = 695
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.140076756477356
INFO:agents.father_agent:Step: 310, Training loss: 2.8611509799957275
INFO:agents.father_agent:Step: 315, Training loss: 2.3137624263763428
INFO:agents.father_agent:Step: 320, Training loss: 2.5941243171691895
INFO:agents.father_agent:Step: 325, Training loss: 2.1147077083587646
INFO:agents.father_agent:Step: 330, Training loss: 2.2482709884643555
INFO:agents.father_agent:Step: 335, Training loss: 1.889723300933838
INFO:agents.father_agent:Step: 340, Training loss: 2.737659215927124
INFO:agents.father_agent:Step: 345, Training loss: 1.9349840879440308
INFO:agents.father_agent:Step: 350, Training loss: 2.1299004554748535
INFO:agents.father_agent:Step: 355, Training loss: 2.3610739707946777
INFO:agents.father_agent:Step: 360, Training loss: 2.9162840843200684
INFO:agents.father_agent:Step: 365, Training loss: 3.0080955028533936
INFO:agents.father_agent:Step: 370, Training loss: 2.335777759552002
INFO:agents.father_agent:Step: 375, Training loss: 2.6159472465515137
INFO:agents.father_agent:Step: 380, Training loss: 2.3780405521392822
INFO:agents.father_agent:Step: 385, Training loss: 2.9552295207977295
INFO:agents.father_agent:Step: 390, Training loss: 2.942965507507324
INFO:agents.father_agent:Step: 395, Training loss: 2.8927299976348877
INFO:agents.father_agent:Step: 400, Training loss: 2.9442291259765625
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -113.09982299804688
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -33.47127151489258
INFO:tools.evaluation_results_class:Average Discounted Reward = -49.24325942993164
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9953569355774812
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 39606.32421875
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 147.8322692977365
INFO:tools.evaluation_results_class:Counted Episodes = 1723
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 1.172203779220581
INFO:agents.father_agent:Step: 410, Training loss: 3.2322118282318115
INFO:agents.father_agent:Step: 415, Training loss: 1.924172043800354
INFO:agents.father_agent:Step: 420, Training loss: 3.1926944255828857
INFO:agents.father_agent:Step: 425, Training loss: 2.8620388507843018
INFO:agents.father_agent:Step: 430, Training loss: 2.2486507892608643
INFO:agents.father_agent:Step: 435, Training loss: 1.9633307456970215
INFO:agents.father_agent:Step: 440, Training loss: 1.7905610799789429
INFO:agents.father_agent:Step: 445, Training loss: 2.8053719997406006
INFO:agents.father_agent:Step: 450, Training loss: 2.5859744548797607
INFO:agents.father_agent:Step: 455, Training loss: 2.1456151008605957
INFO:agents.father_agent:Step: 460, Training loss: 2.839177370071411
INFO:agents.father_agent:Step: 465, Training loss: 2.581141471862793
INFO:agents.father_agent:Step: 470, Training loss: 2.537383794784546
INFO:agents.father_agent:Step: 475, Training loss: 1.8085439205169678
INFO:agents.father_agent:Step: 480, Training loss: 2.4606049060821533
INFO:agents.father_agent:Step: 485, Training loss: 2.0086474418640137
INFO:agents.father_agent:Step: 490, Training loss: 2.7292561531066895
INFO:agents.father_agent:Step: 495, Training loss: 2.9008514881134033
INFO:agents.father_agent:Step: 500, Training loss: 2.342116117477417
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -282.046142578125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -209.6945037841797
INFO:tools.evaluation_results_class:Average Discounted Reward = -96.38691711425781
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9043956043956044
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 769430.5
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 263.389010989011
INFO:tools.evaluation_results_class:Counted Episodes = 910
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -315.0372009277344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -241.1641082763672
INFO:tools.evaluation_results_class:Average Discounted Reward = -98.44849395751953
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9234135667396062
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 839662.125
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 256.8074398249453
INFO:tools.evaluation_results_class:Counted Episodes = 914
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -481.8252868652344
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -410.37615966796875
INFO:tools.evaluation_results_class:Average Discounted Reward = -117.91446685791016
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8931140801644398
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 1472327.25
INFO:tools.evaluation_results_class:Current Best Return = -481.8252868652344
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8931140801644398
INFO:tools.evaluation_results_class:Average Episode Length = 287.34121274409046
INFO:tools.evaluation_results_class:Counted Episodes = 973
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Data sampled
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Learning FSC from original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1016.8010864257812
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -949.947021484375
INFO:tools.evaluation_results_class:Average Discounted Reward = -212.48699951171875
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8356756756756757
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3482184.0
INFO:tools.evaluation_results_class:Current Best Return = -1016.8010864257812
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8356756756756757
INFO:tools.evaluation_results_class:Average Episode Length = 294.34486486486486
INFO:tools.evaluation_results_class:Counted Episodes = 925
INFO:paynt.synthesizer.statistic:synthesis initiated, design space: 11664
INFO:paynt.synthesizer.synthesizer:Synthesizer does not support timeout limiter. Running without timeout.
INFO:paynt.synthesizer.synthesizer_ar:value 1338.585 achieved after 6676.49 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1338.6184 achieved after 6676.5 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1345.7213 achieved after 6676.54 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1345.7546 achieved after 6676.56 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1354.0974 achieved after 6676.63 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1354.1307 achieved after 6676.65 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1361.2336 achieved after 6676.68 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1361.267 achieved after 6676.7 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1428.1035 achieved after 6676.76 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1428.1368 achieved after 6676.77 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1435.2397 achieved after 6676.79 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1435.273 achieved after 6676.81 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1443.6158 achieved after 6676.85 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1443.6492 achieved after 6676.87 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1450.7521 achieved after 6676.89 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1450.7854 achieved after 6676.91 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1478.4442 achieved after 6677.04 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1478.4775 achieved after 6677.05 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1493.9566 achieved after 6677.08 seconds
INFO:paynt.synthesizer.synthesizer_ar:value 1493.9899 achieved after 6677.09 seconds
INFO:paynt.synthesizer.synthesizer:printing synthesized assignment below:
INFO:paynt.synthesizer.synthesizer:o1x=1, o1y=2, o2x=1, o2y=4, o3x=3, o3y=2, o4x=3, o4y=8, o5x=8, o5y=5
INFO:paynt.synthesizer.synthesizer:double-checking specification satisfiability:  : 1493.9899193337799
INFO:robust_rl.robust_rl_trainer:Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=4, o3x=3, o3y=2, o4x=3, o4y=8, o5x=8, o5y=5
INFO:robust_rl.robust_rl_trainer:Iteration 13 of pure RL loop
INFO:vec_storm.storm_vec_env:Computing row map
INFO:vec_storm.storm_vec_env:Computing transitions
INFO:vec_storm.storm_vec_env:Computing allowed actions
INFO:vec_storm.storm_vec_env:Computing sinks
INFO:vec_storm.storm_vec_env:Computing raw rewards
INFO:vec_storm.storm_vec_env:Computing labels
INFO:vec_storm.storm_vec_env:Computing scalarized rewards
INFO:vec_storm.storm_vec_env:Computing metalabels
INFO:vec_storm.storm_vec_env:Computing observations
INFO:agents.father_agent:Training agent with replay buffer option: 1
INFO:agents.father_agent:Before training evaluation.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -312.5384521484375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -239.03419494628906
INFO:tools.evaluation_results_class:Average Discounted Reward = -145.1992950439453
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9188034188034188
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 551723.375
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 257.6666666666667
INFO:tools.evaluation_results_class:Counted Episodes = 936
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training agent on-policy
INFO:agents.father_agent:Step: 0, Training loss: 5.24472188949585
INFO:agents.father_agent:Step: 5, Training loss: 3.06213116645813
INFO:agents.father_agent:Step: 10, Training loss: 3.203150510787964
INFO:agents.father_agent:Step: 15, Training loss: 2.714077949523926
INFO:agents.father_agent:Step: 20, Training loss: 2.8669588565826416
INFO:agents.father_agent:Step: 25, Training loss: 2.7670199871063232
INFO:agents.father_agent:Step: 30, Training loss: 2.77453875541687
INFO:agents.father_agent:Step: 35, Training loss: 3.4487476348876953
INFO:agents.father_agent:Step: 40, Training loss: 2.5016446113586426
INFO:agents.father_agent:Step: 45, Training loss: 3.5711894035339355
INFO:agents.father_agent:Step: 50, Training loss: 3.0190515518188477
INFO:agents.father_agent:Step: 55, Training loss: 2.4060347080230713
INFO:agents.father_agent:Step: 60, Training loss: 4.414455890655518
INFO:agents.father_agent:Step: 65, Training loss: 2.937159538269043
INFO:agents.father_agent:Step: 70, Training loss: 2.832460880279541
INFO:agents.father_agent:Step: 75, Training loss: 3.1430015563964844
INFO:agents.father_agent:Step: 80, Training loss: 2.954869031906128
INFO:agents.father_agent:Step: 85, Training loss: 2.2385215759277344
INFO:agents.father_agent:Step: 90, Training loss: 2.6723783016204834
INFO:agents.father_agent:Step: 95, Training loss: 2.427840232849121
INFO:agents.father_agent:Step: 100, Training loss: 2.1438794136047363
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -658.8428955078125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -578.8428955078125
INFO:tools.evaluation_results_class:Average Discounted Reward = -284.8888854980469
INFO:tools.evaluation_results_class:Goal Reach Probability = 1.0
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 176279.0625
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 119.08576998050683
INFO:tools.evaluation_results_class:Counted Episodes = 2565
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 105, Training loss: 2.04002046585083
INFO:agents.father_agent:Step: 110, Training loss: 2.7742438316345215
INFO:agents.father_agent:Step: 115, Training loss: 1.769264817237854
INFO:agents.father_agent:Step: 120, Training loss: 2.7577192783355713
INFO:agents.father_agent:Step: 125, Training loss: 2.2437832355499268
INFO:agents.father_agent:Step: 130, Training loss: 2.256110191345215
INFO:agents.father_agent:Step: 135, Training loss: 2.071157455444336
INFO:agents.father_agent:Step: 140, Training loss: 1.5163004398345947
INFO:agents.father_agent:Step: 145, Training loss: 2.0756661891937256
INFO:agents.father_agent:Step: 150, Training loss: 2.4981303215026855
INFO:agents.father_agent:Step: 155, Training loss: 2.218789577484131
INFO:agents.father_agent:Step: 160, Training loss: 2.452639102935791
INFO:agents.father_agent:Step: 165, Training loss: 2.234344244003296
INFO:agents.father_agent:Step: 170, Training loss: 2.0399317741394043
INFO:agents.father_agent:Step: 175, Training loss: 2.3312795162200928
INFO:agents.father_agent:Step: 180, Training loss: 2.1469666957855225
INFO:agents.father_agent:Step: 185, Training loss: 1.7176791429519653
INFO:agents.father_agent:Step: 190, Training loss: 2.2138876914978027
INFO:agents.father_agent:Step: 195, Training loss: 1.9576927423477173
INFO:agents.father_agent:Step: 200, Training loss: 2.1655654907226562
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -776.365478515625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -723.200927734375
INFO:tools.evaluation_results_class:Average Discounted Reward = -161.63746643066406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6645569620253164
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 4196229.5
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 416.02848101265823
INFO:tools.evaluation_results_class:Counted Episodes = 632
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 205, Training loss: 1.2985557317733765
INFO:agents.father_agent:Step: 210, Training loss: 2.565232992172241
INFO:agents.father_agent:Step: 215, Training loss: 1.6351982355117798
INFO:agents.father_agent:Step: 220, Training loss: 2.4737861156463623
INFO:agents.father_agent:Step: 225, Training loss: 2.84997296333313
INFO:agents.father_agent:Step: 230, Training loss: 1.835303544998169
INFO:agents.father_agent:Step: 235, Training loss: 2.1305549144744873
INFO:agents.father_agent:Step: 240, Training loss: 1.7489131689071655
INFO:agents.father_agent:Step: 245, Training loss: 1.8209271430969238
INFO:agents.father_agent:Step: 250, Training loss: 2.1822056770324707
INFO:agents.father_agent:Step: 255, Training loss: 1.8532583713531494
INFO:agents.father_agent:Step: 260, Training loss: 2.4217514991760254
INFO:agents.father_agent:Step: 265, Training loss: 2.3950464725494385
INFO:agents.father_agent:Step: 270, Training loss: 2.4708518981933594
INFO:agents.father_agent:Step: 275, Training loss: 2.0192625522613525
INFO:agents.father_agent:Step: 280, Training loss: 1.7745819091796875
INFO:agents.father_agent:Step: 285, Training loss: 1.9573029279708862
INFO:agents.father_agent:Step: 290, Training loss: 2.3510491847991943
INFO:agents.father_agent:Step: 295, Training loss: 2.3744094371795654
INFO:agents.father_agent:Step: 300, Training loss: 2.8193342685699463
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1091.628173828125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1018.7051391601562
INFO:tools.evaluation_results_class:Average Discounted Reward = -289.9871520996094
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.9115384615384615
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 3366422.25
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 312.75128205128203
INFO:tools.evaluation_results_class:Counted Episodes = 780
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 305, Training loss: 1.148435115814209
INFO:agents.father_agent:Step: 310, Training loss: 2.3566408157348633
INFO:agents.father_agent:Step: 315, Training loss: 1.5440651178359985
INFO:agents.father_agent:Step: 320, Training loss: 3.0002429485321045
INFO:agents.father_agent:Step: 325, Training loss: 2.6397433280944824
INFO:agents.father_agent:Step: 330, Training loss: 1.9128859043121338
INFO:agents.father_agent:Step: 335, Training loss: 2.307614803314209
INFO:agents.father_agent:Step: 340, Training loss: 1.6171352863311768
INFO:agents.father_agent:Step: 345, Training loss: 1.6679998636245728
INFO:agents.father_agent:Step: 350, Training loss: 2.4595041275024414
INFO:agents.father_agent:Step: 355, Training loss: 2.476320743560791
INFO:agents.father_agent:Step: 360, Training loss: 2.1521637439727783
INFO:agents.father_agent:Step: 365, Training loss: 2.905851364135742
INFO:agents.father_agent:Step: 370, Training loss: 2.21195125579834
INFO:agents.father_agent:Step: 375, Training loss: 1.6062288284301758
INFO:agents.father_agent:Step: 380, Training loss: 2.1678555011749268
INFO:agents.father_agent:Step: 385, Training loss: 1.5066999197006226
INFO:agents.father_agent:Step: 390, Training loss: 2.0650815963745117
INFO:agents.father_agent:Step: 395, Training loss: 2.230771064758301
INFO:agents.father_agent:Step: 400, Training loss: 2.2875115871429443
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -2417.282958984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -2367.005126953125
INFO:tools.evaluation_results_class:Average Discounted Reward = -483.7486877441406
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6284722222222222
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 8039379.5
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 457.9270833333333
INFO:tools.evaluation_results_class:Counted Episodes = 576
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Step: 405, Training loss: 0.9057937264442444
INFO:agents.father_agent:Step: 410, Training loss: 2.3794264793395996
INFO:agents.father_agent:Step: 415, Training loss: 1.8129385709762573
INFO:agents.father_agent:Step: 420, Training loss: 2.6531827449798584
INFO:agents.father_agent:Step: 425, Training loss: 2.97458815574646
INFO:agents.father_agent:Step: 430, Training loss: 2.23354434967041
INFO:agents.father_agent:Step: 435, Training loss: 2.064157247543335
INFO:agents.father_agent:Step: 440, Training loss: 1.8328850269317627
INFO:agents.father_agent:Step: 445, Training loss: 1.7604018449783325
INFO:agents.father_agent:Step: 450, Training loss: 1.9103772640228271
INFO:agents.father_agent:Step: 455, Training loss: 2.0610053539276123
INFO:agents.father_agent:Step: 460, Training loss: 2.1723451614379883
INFO:agents.father_agent:Step: 465, Training loss: 2.5399630069732666
INFO:agents.father_agent:Step: 470, Training loss: 2.7555932998657227
INFO:agents.father_agent:Step: 475, Training loss: 2.00750994682312
INFO:agents.father_agent:Step: 480, Training loss: 1.6455920934677124
INFO:agents.father_agent:Step: 485, Training loss: 1.8250787258148193
INFO:agents.father_agent:Step: 490, Training loss: 2.024397611618042
INFO:agents.father_agent:Step: 495, Training loss: 1.6505043506622314
INFO:agents.father_agent:Step: 500, Training loss: 2.1935770511627197
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1969.9017333984375
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1909.2254638671875
INFO:tools.evaluation_results_class:Average Discounted Reward = -442.781494140625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7584541062801933
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5930713.0
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 410.20450885668276
INFO:tools.evaluation_results_class:Counted Episodes = 621
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Training finished.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:agents.father_agent:Evaluating agent with greedy masked policy.
INFO:tools.evaluation_results_class:Average Return = -2018.8258056640625
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1958.1806640625
INFO:tools.evaluation_results_class:Average Discounted Reward = -469.3277587890625
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.7580645161290323
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5639898.0
INFO:tools.evaluation_results_class:Current Best Return = -37.25813674926758
INFO:tools.evaluation_results_class:Current Best Reach Probability = 1.0
INFO:tools.evaluation_results_class:Average Episode Length = 414.38709677419354
INFO:tools.evaluation_results_class:Counted Episodes = 620
INFO:environment.environment_wrapper_vec:Resetting the environment.
ERROR:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:True
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:tools.evaluation_results_class:Average Return = -1832.1114501953125
INFO:tools.evaluation_results_class:Average Virtual Goal Value = -1767.0087890625
INFO:tools.evaluation_results_class:Average Discounted Reward = -431.775146484375
INFO:tools.evaluation_results_class:Goal Reach Probability = 0.8137829912023461
INFO:tools.evaluation_results_class:Trap Reach Probability = 0.0
INFO:tools.evaluation_results_class:Variance of Return = 5078746.5
INFO:tools.evaluation_results_class:Current Best Return = -1832.1114501953125
INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.8137829912023461
INFO:tools.evaluation_results_class:Average Episode Length = 397.04985337243403
INFO:tools.evaluation_results_class:Counted Episodes = 682
INFO:paynt.rl_extension.self_interpretable_interface.self_interpretable_extractor:Sampling data with original policy
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.
INFO:environment.environment_wrapper_vec:Resetting the environment.

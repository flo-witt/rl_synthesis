2025-08-04 05:36:21,123 - sketch.py:80 - loading sketch from /home/ihudak/synthesis/models_robust_subset/obstacles-8-5/sketch.templ ...
2025-08-04 05:36:21,123 - sketch.py:84 - assuming sketch in PRISM format...
2025-08-04 05:36:21,127 - prism_parser.py:31 - PRISM model type: POMDP
2025-08-04 05:36:21,127 - prism_parser.py:40 - processing hole definitions...
2025-08-04 05:36:21,128 - prism_parser.py:220 - loading properties from /home/ihudak/synthesis/models_robust_subset/obstacles-8-5/sketch.props ...
2025-08-04 05:36:21,128 - prism_parser.py:236 - found the following specification: optimality: R{"penalty"}min=? [F ((x = 8) & (y = 8))] 
2025-08-04 05:36:21,129 - jani.py:41 - constructing JANI program...
2025-08-04 05:36:21,132 - jani.py:61 - constructing the quotient...
2025-08-04 05:36:21,157 - jani.py:66 - associating choices of the quotient with hole assignments...
2025-08-04 05:36:21,159 - sketch.py:135 - sketch parsing OK
2025-08-04 05:36:21,160 - sketch.py:144 - WARNING: choice labeling for the quotient is not canonic
2025-08-04 05:36:21,160 - sketch.py:148 - constructed explicit quotient having 468 states and 975 choices
2025-08-04 05:36:21,160 - sketch.py:154 - found the following specification optimality: R{"rewardmodel_penalty"}min=? [F ((x = 8) & (y = 8))] 
2025-08-04 05:36:21,165 - vectorized_sim_initializer.py:61 - Compiling model obstacles-8-5...
Hello <stormpy.storage.storage.StateValuation object at 0x7264974d5d70>
2025-08-04 05:36:21,397 - storm_vec_env.py:70 - Computing row map
2025-08-04 05:36:21,405 - storm_vec_env.py:97 - Computing transitions
2025-08-04 05:36:21,420 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 05:36:21,420 - storm_vec_env.py:114 - Computing sinks
2025-08-04 05:36:21,420 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 05:36:21,425 - storm_vec_env.py:143 - Computing labels
2025-08-04 05:36:21,425 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 05:36:21,426 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 05:36:21,426 - storm_vec_env.py:175 - Computing observations
2025-08-04 05:36:21,510 - environment_wrapper_vec.py:146 - Grid-like renderer not possible to initialize.
2025-08-04 05:36:21,623 - environment_wrapper_vec.py:153 - Vectorized simulator initialized with 256 environments.
2025-08-04 05:36:21,850 - recurrent_ppo_agent.py:98 - Agent initialized
2025-08-04 05:36:21,873 - recurrent_ppo_agent.py:100 - Replay buffer initialized
Using unmasked training with policy wrapper.
2025-08-04 05:36:21,882 - recurrent_ppo_agent.py:112 - Collector driver initialized
2025-08-04 05:36:21,884 - robust_rl_trainer.py:432 - Iteration 1 of pure RL loop
2025-08-04 05:36:21,921 - storm_vec_env.py:70 - Computing row map
2025-08-04 05:36:21,928 - storm_vec_env.py:97 - Computing transitions
2025-08-04 05:36:21,943 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 05:36:21,944 - storm_vec_env.py:114 - Computing sinks
2025-08-04 05:36:21,944 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 05:36:21,949 - storm_vec_env.py:143 - Computing labels
2025-08-04 05:36:21,949 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 05:36:21,949 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 05:36:21,949 - storm_vec_env.py:175 - Computing observations
2025-08-04 05:36:22,035 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 05:36:22,035 - father_agent.py:540 - Before training evaluation.
2025-08-04 05:36:22,156 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:36:22,165 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:36:29,700 - evaluation_results_class.py:131 - Average Return = -1131.3702392578125
2025-08-04 05:36:29,700 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1106.0267333984375
2025-08-04 05:36:29,700 - evaluation_results_class.py:135 - Average Discounted Reward = -307.549560546875
2025-08-04 05:36:29,700 - evaluation_results_class.py:137 - Goal Reach Probability = 0.31679389312977096
2025-08-04 05:36:29,700 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:36:29,700 - evaluation_results_class.py:141 - Variance of Return = 331056.4375
2025-08-04 05:36:29,700 - evaluation_results_class.py:143 - Current Best Return = -1131.3702392578125
2025-08-04 05:36:29,700 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.31679389312977096
2025-08-04 05:36:29,700 - evaluation_results_class.py:147 - Average Episode Length = 569.4885496183206
2025-08-04 05:36:29,700 - evaluation_results_class.py:149 - Counted Episodes = 524
2025-08-04 05:36:29,844 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:36:29,855 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:36:29,957 - father_agent.py:436 - Training agent on-policy
2025-08-04 05:36:44,260 - father_agent.py:386 - Step: 0, Training loss: 202.79812622070312
2025-08-04 05:36:45,893 - father_agent.py:386 - Step: 5, Training loss: 6.999893665313721
2025-08-04 05:36:47,548 - father_agent.py:386 - Step: 10, Training loss: 5.069169044494629
2025-08-04 05:36:49,190 - father_agent.py:386 - Step: 15, Training loss: 14.463461875915527
2025-08-04 05:36:50,792 - father_agent.py:386 - Step: 20, Training loss: 6.45412015914917
2025-08-04 05:36:52,460 - father_agent.py:386 - Step: 25, Training loss: 8.677016258239746
2025-08-04 05:36:54,142 - father_agent.py:386 - Step: 30, Training loss: 5.415284633636475
2025-08-04 05:36:55,780 - father_agent.py:386 - Step: 35, Training loss: 8.642400741577148
2025-08-04 05:36:57,412 - father_agent.py:386 - Step: 40, Training loss: 8.257278442382812
2025-08-04 05:36:59,047 - father_agent.py:386 - Step: 45, Training loss: 5.306501388549805
2025-08-04 05:37:00,686 - father_agent.py:386 - Step: 50, Training loss: 4.200691223144531
2025-08-04 05:37:02,356 - father_agent.py:386 - Step: 55, Training loss: 5.827817440032959
2025-08-04 05:37:04,018 - father_agent.py:386 - Step: 60, Training loss: 5.185117721557617
2025-08-04 05:37:05,685 - father_agent.py:386 - Step: 65, Training loss: 5.170769214630127
2025-08-04 05:37:07,356 - father_agent.py:386 - Step: 70, Training loss: 6.65553617477417
2025-08-04 05:37:09,019 - father_agent.py:386 - Step: 75, Training loss: 6.737209320068359
2025-08-04 05:37:10,678 - father_agent.py:386 - Step: 80, Training loss: 5.66944694519043
2025-08-04 05:37:12,342 - father_agent.py:386 - Step: 85, Training loss: 5.367969989776611
2025-08-04 05:37:14,048 - father_agent.py:386 - Step: 90, Training loss: 5.3019633293151855
2025-08-04 05:37:15,729 - father_agent.py:386 - Step: 95, Training loss: 4.639986515045166
2025-08-04 05:37:17,392 - father_agent.py:386 - Step: 100, Training loss: 4.461611747741699
2025-08-04 05:37:17,545 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:37:17,548 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:37:25,089 - evaluation_results_class.py:131 - Average Return = -104.06402587890625
2025-08-04 05:37:25,089 - evaluation_results_class.py:133 - Average Virtual Goal Value = -24.064027786254883
2025-08-04 05:37:25,089 - evaluation_results_class.py:135 - Average Discounted Reward = -44.182220458984375
2025-08-04 05:37:25,089 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:37:25,089 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:37:25,089 - evaluation_results_class.py:141 - Variance of Return = 8855.87109375
2025-08-04 05:37:25,089 - evaluation_results_class.py:143 - Current Best Return = -104.06402587890625
2025-08-04 05:37:25,089 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:37:25,089 - evaluation_results_class.py:147 - Average Episode Length = 99.14221364221365
2025-08-04 05:37:25,089 - evaluation_results_class.py:149 - Counted Episodes = 3108
2025-08-04 05:37:25,245 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:37:25,253 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:37:35,016 - father_agent.py:386 - Step: 105, Training loss: 4.7826948165893555
2025-08-04 05:37:36,660 - father_agent.py:386 - Step: 110, Training loss: 4.820728302001953
2025-08-04 05:37:38,328 - father_agent.py:386 - Step: 115, Training loss: 4.450231075286865
2025-08-04 05:37:39,980 - father_agent.py:386 - Step: 120, Training loss: 4.246068000793457
2025-08-04 05:37:41,637 - father_agent.py:386 - Step: 125, Training loss: 4.086848258972168
2025-08-04 05:37:43,260 - father_agent.py:386 - Step: 130, Training loss: 5.521506309509277
2025-08-04 05:37:44,893 - father_agent.py:386 - Step: 135, Training loss: 4.129128456115723
2025-08-04 05:37:46,504 - father_agent.py:386 - Step: 140, Training loss: 3.590630054473877
2025-08-04 05:37:48,142 - father_agent.py:386 - Step: 145, Training loss: 7.104152202606201
2025-08-04 05:37:49,823 - father_agent.py:386 - Step: 150, Training loss: 3.585496425628662
2025-08-04 05:37:51,455 - father_agent.py:386 - Step: 155, Training loss: 2.912667751312256
2025-08-04 05:37:53,123 - father_agent.py:386 - Step: 160, Training loss: 3.958212375640869
2025-08-04 05:37:54,813 - father_agent.py:386 - Step: 165, Training loss: 3.66904878616333
2025-08-04 05:37:56,535 - father_agent.py:386 - Step: 170, Training loss: 2.850506544113159
2025-08-04 05:37:58,252 - father_agent.py:386 - Step: 175, Training loss: 4.469029426574707
2025-08-04 05:37:59,989 - father_agent.py:386 - Step: 180, Training loss: 4.687090873718262
2025-08-04 05:38:01,728 - father_agent.py:386 - Step: 185, Training loss: 4.311385154724121
2025-08-04 05:38:03,470 - father_agent.py:386 - Step: 190, Training loss: 2.434455633163452
2025-08-04 05:38:05,210 - father_agent.py:386 - Step: 195, Training loss: 4.992140293121338
2025-08-04 05:38:06,939 - father_agent.py:386 - Step: 200, Training loss: 3.598881483078003
2025-08-04 05:38:07,105 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:38:07,108 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:38:14,656 - evaluation_results_class.py:131 - Average Return = -53.621768951416016
2025-08-04 05:38:14,656 - evaluation_results_class.py:133 - Average Virtual Goal Value = 26.378232955932617
2025-08-04 05:38:14,656 - evaluation_results_class.py:135 - Average Discounted Reward = -3.7443037033081055
2025-08-04 05:38:14,656 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:38:14,656 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:38:14,657 - evaluation_results_class.py:141 - Variance of Return = 3973.780517578125
2025-08-04 05:38:14,657 - evaluation_results_class.py:143 - Current Best Return = -53.621768951416016
2025-08-04 05:38:14,657 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:38:14,657 - evaluation_results_class.py:147 - Average Episode Length = 88.79312304631998
2025-08-04 05:38:14,657 - evaluation_results_class.py:149 - Counted Episodes = 3519
2025-08-04 05:38:14,855 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:38:14,868 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:38:17,500 - father_agent.py:386 - Step: 205, Training loss: 2.111912727355957
2025-08-04 05:38:19,245 - father_agent.py:386 - Step: 210, Training loss: 2.863858938217163
2025-08-04 05:38:20,979 - father_agent.py:386 - Step: 215, Training loss: 4.392488479614258
2025-08-04 05:38:22,698 - father_agent.py:386 - Step: 220, Training loss: 3.4214017391204834
2025-08-04 05:38:24,450 - father_agent.py:386 - Step: 225, Training loss: 2.158501625061035
2025-08-04 05:38:26,207 - father_agent.py:386 - Step: 230, Training loss: 4.990423202514648
2025-08-04 05:38:27,955 - father_agent.py:386 - Step: 235, Training loss: 3.6046438217163086
2025-08-04 05:38:29,629 - father_agent.py:386 - Step: 240, Training loss: 2.185356378555298
2025-08-04 05:38:31,302 - father_agent.py:386 - Step: 245, Training loss: 2.6509275436401367
2025-08-04 05:38:32,972 - father_agent.py:386 - Step: 250, Training loss: 3.034937620162964
2025-08-04 05:38:34,637 - father_agent.py:386 - Step: 255, Training loss: 2.966449737548828
2025-08-04 05:38:36,339 - father_agent.py:386 - Step: 260, Training loss: 2.1687850952148438
2025-08-04 05:38:38,053 - father_agent.py:386 - Step: 265, Training loss: 2.955864906311035
2025-08-04 05:38:39,696 - father_agent.py:386 - Step: 270, Training loss: 3.726423501968384
2025-08-04 05:38:41,319 - father_agent.py:386 - Step: 275, Training loss: 2.061990976333618
2025-08-04 05:38:43,026 - father_agent.py:386 - Step: 280, Training loss: 2.818622589111328
2025-08-04 05:38:44,734 - father_agent.py:386 - Step: 285, Training loss: 3.547217845916748
2025-08-04 05:38:46,394 - father_agent.py:386 - Step: 290, Training loss: 2.320222854614258
2025-08-04 05:38:48,106 - father_agent.py:386 - Step: 295, Training loss: 3.2942185401916504
2025-08-04 05:38:49,763 - father_agent.py:386 - Step: 300, Training loss: 3.8508734703063965
2025-08-04 05:38:49,930 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:38:49,932 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:38:56,953 - evaluation_results_class.py:131 - Average Return = -46.47800827026367
2025-08-04 05:38:56,953 - evaluation_results_class.py:133 - Average Virtual Goal Value = 33.52199172973633
2025-08-04 05:38:56,953 - evaluation_results_class.py:135 - Average Discounted Reward = 2.532562017440796
2025-08-04 05:38:56,953 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:38:56,953 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:38:56,953 - evaluation_results_class.py:141 - Variance of Return = 4072.94091796875
2025-08-04 05:38:56,953 - evaluation_results_class.py:143 - Current Best Return = -46.47800827026367
2025-08-04 05:38:56,953 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:38:56,953 - evaluation_results_class.py:147 - Average Episode Length = 86.20670698185816
2025-08-04 05:38:56,953 - evaluation_results_class.py:149 - Counted Episodes = 3638
2025-08-04 05:38:57,117 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:38:57,126 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:38:59,872 - father_agent.py:386 - Step: 305, Training loss: 1.6907731294631958
2025-08-04 05:39:01,501 - father_agent.py:386 - Step: 310, Training loss: 3.5188841819763184
2025-08-04 05:39:03,192 - father_agent.py:386 - Step: 315, Training loss: 2.665771961212158
2025-08-04 05:39:04,871 - father_agent.py:386 - Step: 320, Training loss: 2.366917848587036
2025-08-04 05:39:06,575 - father_agent.py:386 - Step: 325, Training loss: 1.9432450532913208
2025-08-04 05:39:08,199 - father_agent.py:386 - Step: 330, Training loss: 3.1431779861450195
2025-08-04 05:39:09,919 - father_agent.py:386 - Step: 335, Training loss: 2.404710054397583
2025-08-04 05:39:11,654 - father_agent.py:386 - Step: 340, Training loss: 2.0356974601745605
2025-08-04 05:39:13,389 - father_agent.py:386 - Step: 345, Training loss: 3.496746778488159
2025-08-04 05:39:15,141 - father_agent.py:386 - Step: 350, Training loss: 1.9111958742141724
2025-08-04 05:39:16,866 - father_agent.py:386 - Step: 355, Training loss: 2.0083835124969482
2025-08-04 05:39:18,581 - father_agent.py:386 - Step: 360, Training loss: 4.124835014343262
2025-08-04 05:39:20,359 - father_agent.py:386 - Step: 365, Training loss: 2.479755401611328
2025-08-04 05:39:22,113 - father_agent.py:386 - Step: 370, Training loss: 3.2520220279693604
2025-08-04 05:39:23,880 - father_agent.py:386 - Step: 375, Training loss: 2.4341371059417725
2025-08-04 05:39:25,639 - father_agent.py:386 - Step: 380, Training loss: 2.9771125316619873
2025-08-04 05:39:27,422 - father_agent.py:386 - Step: 385, Training loss: 2.8375372886657715
2025-08-04 05:39:29,188 - father_agent.py:386 - Step: 390, Training loss: 1.8780596256256104
2025-08-04 05:39:30,899 - father_agent.py:386 - Step: 395, Training loss: 3.1848740577697754
2025-08-04 05:39:32,583 - father_agent.py:386 - Step: 400, Training loss: 2.8712966442108154
2025-08-04 05:39:32,761 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:39:32,764 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:39:40,291 - evaluation_results_class.py:131 - Average Return = -48.535072326660156
2025-08-04 05:39:40,292 - evaluation_results_class.py:133 - Average Virtual Goal Value = 31.464929580688477
2025-08-04 05:39:40,292 - evaluation_results_class.py:135 - Average Discounted Reward = 2.609086275100708
2025-08-04 05:39:40,292 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:39:40,292 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:39:40,292 - evaluation_results_class.py:141 - Variance of Return = 7126.69873046875
2025-08-04 05:39:40,292 - evaluation_results_class.py:143 - Current Best Return = -46.47800827026367
2025-08-04 05:39:40,292 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:39:40,292 - evaluation_results_class.py:147 - Average Episode Length = 87.12198502911006
2025-08-04 05:39:40,292 - evaluation_results_class.py:149 - Counted Episodes = 3607
2025-08-04 05:39:40,454 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:39:40,464 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:39:42,982 - father_agent.py:386 - Step: 405, Training loss: 1.6381094455718994
2025-08-04 05:39:44,725 - father_agent.py:386 - Step: 410, Training loss: 3.2741851806640625
2025-08-04 05:39:46,455 - father_agent.py:386 - Step: 415, Training loss: 2.5042622089385986
2025-08-04 05:39:48,182 - father_agent.py:386 - Step: 420, Training loss: 2.848496198654175
2025-08-04 05:39:49,838 - father_agent.py:386 - Step: 425, Training loss: 2.0684964656829834
2025-08-04 05:39:51,490 - father_agent.py:386 - Step: 430, Training loss: 3.1984190940856934
2025-08-04 05:39:53,143 - father_agent.py:386 - Step: 435, Training loss: 3.1292030811309814
2025-08-04 05:39:54,776 - father_agent.py:386 - Step: 440, Training loss: 2.061893939971924
2025-08-04 05:39:56,367 - father_agent.py:386 - Step: 445, Training loss: 4.316507339477539
2025-08-04 05:39:57,948 - father_agent.py:386 - Step: 450, Training loss: 1.9416661262512207
2025-08-04 05:39:59,526 - father_agent.py:386 - Step: 455, Training loss: 3.5378079414367676
2025-08-04 05:40:01,127 - father_agent.py:386 - Step: 460, Training loss: 4.147576808929443
2025-08-04 05:40:02,719 - father_agent.py:386 - Step: 465, Training loss: 2.842033863067627
2025-08-04 05:40:04,291 - father_agent.py:386 - Step: 470, Training loss: 2.257113218307495
2025-08-04 05:40:05,885 - father_agent.py:386 - Step: 475, Training loss: 2.4206690788269043
2025-08-04 05:40:07,474 - father_agent.py:386 - Step: 480, Training loss: 2.780488967895508
2025-08-04 05:40:09,054 - father_agent.py:386 - Step: 485, Training loss: 1.9088654518127441
2025-08-04 05:40:10,640 - father_agent.py:386 - Step: 490, Training loss: 3.4803359508514404
2025-08-04 05:40:12,217 - father_agent.py:386 - Step: 495, Training loss: 3.0160913467407227
2025-08-04 05:40:13,790 - father_agent.py:386 - Step: 500, Training loss: 2.9459540843963623
2025-08-04 05:40:13,955 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:40:13,958 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:40:20,995 - evaluation_results_class.py:131 - Average Return = -40.29243087768555
2025-08-04 05:40:20,995 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.70756912231445
2025-08-04 05:40:20,995 - evaluation_results_class.py:135 - Average Discounted Reward = 7.694381237030029
2025-08-04 05:40:20,995 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:40:20,996 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:40:20,996 - evaluation_results_class.py:141 - Variance of Return = 3677.622802734375
2025-08-04 05:40:20,996 - evaluation_results_class.py:143 - Current Best Return = -40.29243087768555
2025-08-04 05:40:20,996 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:40:20,996 - evaluation_results_class.py:147 - Average Episode Length = 85.15675675675676
2025-08-04 05:40:20,996 - evaluation_results_class.py:149 - Counted Episodes = 3700
2025-08-04 05:40:21,156 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:40:21,165 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:40:23,552 - father_agent.py:386 - Step: 505, Training loss: 1.319060206413269
2025-08-04 05:40:25,116 - father_agent.py:386 - Step: 510, Training loss: 4.082722187042236
2025-08-04 05:40:26,681 - father_agent.py:386 - Step: 515, Training loss: 2.6698789596557617
2025-08-04 05:40:28,228 - father_agent.py:386 - Step: 520, Training loss: 6.058686256408691
2025-08-04 05:40:29,791 - father_agent.py:386 - Step: 525, Training loss: 4.157296657562256
2025-08-04 05:40:31,348 - father_agent.py:386 - Step: 530, Training loss: 2.3212690353393555
2025-08-04 05:40:32,913 - father_agent.py:386 - Step: 535, Training loss: 2.8488521575927734
2025-08-04 05:40:34,476 - father_agent.py:386 - Step: 540, Training loss: 2.581211566925049
2025-08-04 05:40:36,043 - father_agent.py:386 - Step: 545, Training loss: 2.0836822986602783
2025-08-04 05:40:37,600 - father_agent.py:386 - Step: 550, Training loss: 3.606597900390625
2025-08-04 05:40:39,174 - father_agent.py:386 - Step: 555, Training loss: 2.8280348777770996
2025-08-04 05:40:40,748 - father_agent.py:386 - Step: 560, Training loss: 2.667409658432007
2025-08-04 05:40:42,299 - father_agent.py:386 - Step: 565, Training loss: 2.9936110973358154
2025-08-04 05:40:43,860 - father_agent.py:386 - Step: 570, Training loss: 2.112502098083496
2025-08-04 05:40:45,431 - father_agent.py:386 - Step: 575, Training loss: 3.9394619464874268
2025-08-04 05:40:46,999 - father_agent.py:386 - Step: 580, Training loss: 2.522407293319702
2025-08-04 05:40:48,575 - father_agent.py:386 - Step: 585, Training loss: 2.5754573345184326
2025-08-04 05:40:50,155 - father_agent.py:386 - Step: 590, Training loss: 3.4201252460479736
2025-08-04 05:40:51,729 - father_agent.py:386 - Step: 595, Training loss: 2.2535295486450195
2025-08-04 05:40:53,295 - father_agent.py:386 - Step: 600, Training loss: 2.241560220718384
2025-08-04 05:40:53,461 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:40:53,464 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:41:00,462 - evaluation_results_class.py:131 - Average Return = -43.08932113647461
2025-08-04 05:41:00,462 - evaluation_results_class.py:133 - Average Virtual Goal Value = 36.91067886352539
2025-08-04 05:41:00,462 - evaluation_results_class.py:135 - Average Discounted Reward = 6.485418319702148
2025-08-04 05:41:00,462 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:41:00,462 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:41:00,462 - evaluation_results_class.py:141 - Variance of Return = 5632.166015625
2025-08-04 05:41:00,462 - evaluation_results_class.py:143 - Current Best Return = -40.29243087768555
2025-08-04 05:41:00,462 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:41:00,462 - evaluation_results_class.py:147 - Average Episode Length = 85.90084676317946
2025-08-04 05:41:00,462 - evaluation_results_class.py:149 - Counted Episodes = 3661
2025-08-04 05:41:00,626 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:41:00,635 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:41:03,035 - father_agent.py:386 - Step: 605, Training loss: 2.2364842891693115
2025-08-04 05:41:04,606 - father_agent.py:386 - Step: 610, Training loss: 2.139314651489258
2025-08-04 05:41:06,181 - father_agent.py:386 - Step: 615, Training loss: 2.7886412143707275
2025-08-04 05:41:07,757 - father_agent.py:386 - Step: 620, Training loss: 5.377361297607422
2025-08-04 05:41:09,326 - father_agent.py:386 - Step: 625, Training loss: 2.373331308364868
2025-08-04 05:41:10,909 - father_agent.py:386 - Step: 630, Training loss: 1.895978331565857
2025-08-04 05:41:12,501 - father_agent.py:386 - Step: 635, Training loss: 2.438002824783325
2025-08-04 05:41:14,072 - father_agent.py:386 - Step: 640, Training loss: 2.844681978225708
2025-08-04 05:41:15,667 - father_agent.py:386 - Step: 645, Training loss: 2.5750606060028076
2025-08-04 05:41:17,250 - father_agent.py:386 - Step: 650, Training loss: 2.396559238433838
2025-08-04 05:41:18,843 - father_agent.py:386 - Step: 655, Training loss: 2.17929744720459
2025-08-04 05:41:20,440 - father_agent.py:386 - Step: 660, Training loss: 2.351282835006714
2025-08-04 05:41:22,036 - father_agent.py:386 - Step: 665, Training loss: 2.8935320377349854
2025-08-04 05:41:23,629 - father_agent.py:386 - Step: 670, Training loss: 2.6045219898223877
2025-08-04 05:41:25,213 - father_agent.py:386 - Step: 675, Training loss: 4.952303886413574
2025-08-04 05:41:26,782 - father_agent.py:386 - Step: 680, Training loss: 2.276221990585327
2025-08-04 05:41:28,366 - father_agent.py:386 - Step: 685, Training loss: 1.9891386032104492
2025-08-04 05:41:29,960 - father_agent.py:386 - Step: 690, Training loss: 2.608790159225464
2025-08-04 05:41:31,519 - father_agent.py:386 - Step: 695, Training loss: 3.2574775218963623
2025-08-04 05:41:33,088 - father_agent.py:386 - Step: 700, Training loss: 2.3441975116729736
2025-08-04 05:41:33,256 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:41:33,259 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:41:40,159 - evaluation_results_class.py:131 - Average Return = -38.798095703125
2025-08-04 05:41:40,159 - evaluation_results_class.py:133 - Average Virtual Goal Value = 41.201904296875
2025-08-04 05:41:40,159 - evaluation_results_class.py:135 - Average Discounted Reward = 8.543291091918945
2025-08-04 05:41:40,159 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:41:40,159 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:41:40,159 - evaluation_results_class.py:141 - Variance of Return = 3026.939208984375
2025-08-04 05:41:40,159 - evaluation_results_class.py:143 - Current Best Return = -38.798095703125
2025-08-04 05:41:40,159 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:41:40,159 - evaluation_results_class.py:147 - Average Episode Length = 85.7717006802721
2025-08-04 05:41:40,159 - evaluation_results_class.py:149 - Counted Episodes = 3675
2025-08-04 05:41:40,321 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:41:40,330 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:41:42,686 - father_agent.py:386 - Step: 705, Training loss: 1.5177556276321411
2025-08-04 05:41:44,257 - father_agent.py:386 - Step: 710, Training loss: 4.806301116943359
2025-08-04 05:41:45,816 - father_agent.py:386 - Step: 715, Training loss: 3.6653454303741455
2025-08-04 05:41:47,367 - father_agent.py:386 - Step: 720, Training loss: 3.112765312194824
2025-08-04 05:41:48,908 - father_agent.py:386 - Step: 725, Training loss: 2.281139612197876
2025-08-04 05:41:50,462 - father_agent.py:386 - Step: 730, Training loss: 1.6716662645339966
2025-08-04 05:41:52,014 - father_agent.py:386 - Step: 735, Training loss: 2.9624385833740234
2025-08-04 05:41:53,565 - father_agent.py:386 - Step: 740, Training loss: 2.4199135303497314
2025-08-04 05:41:55,122 - father_agent.py:386 - Step: 745, Training loss: 2.0485517978668213
2025-08-04 05:41:56,696 - father_agent.py:386 - Step: 750, Training loss: 2.987490653991699
2025-08-04 05:41:58,264 - father_agent.py:386 - Step: 755, Training loss: 2.696596622467041
2025-08-04 05:41:59,840 - father_agent.py:386 - Step: 760, Training loss: 1.947655200958252
2025-08-04 05:42:01,427 - father_agent.py:386 - Step: 765, Training loss: 1.9553464651107788
2025-08-04 05:42:03,009 - father_agent.py:386 - Step: 770, Training loss: 2.1708898544311523
2025-08-04 05:42:04,586 - father_agent.py:386 - Step: 775, Training loss: 2.0600879192352295
2025-08-04 05:42:06,168 - father_agent.py:386 - Step: 780, Training loss: 2.1846468448638916
2025-08-04 05:42:07,754 - father_agent.py:386 - Step: 785, Training loss: 3.114286422729492
2025-08-04 05:42:09,349 - father_agent.py:386 - Step: 790, Training loss: 1.9156906604766846
2025-08-04 05:42:10,902 - father_agent.py:386 - Step: 795, Training loss: 2.990551233291626
2025-08-04 05:42:12,470 - father_agent.py:386 - Step: 800, Training loss: 3.1247425079345703
2025-08-04 05:42:12,636 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:42:12,639 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:42:19,683 - evaluation_results_class.py:131 - Average Return = -40.67179489135742
2025-08-04 05:42:19,683 - evaluation_results_class.py:133 - Average Virtual Goal Value = 39.32820510864258
2025-08-04 05:42:19,683 - evaluation_results_class.py:135 - Average Discounted Reward = 7.991236209869385
2025-08-04 05:42:19,683 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:42:19,683 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:42:19,684 - evaluation_results_class.py:141 - Variance of Return = 4243.14599609375
2025-08-04 05:42:19,684 - evaluation_results_class.py:143 - Current Best Return = -38.798095703125
2025-08-04 05:42:19,684 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:42:19,684 - evaluation_results_class.py:147 - Average Episode Length = 85.20836707152496
2025-08-04 05:42:19,684 - evaluation_results_class.py:149 - Counted Episodes = 3705
2025-08-04 05:42:19,844 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:42:19,853 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:42:22,561 - father_agent.py:386 - Step: 805, Training loss: 1.171358346939087
2025-08-04 05:42:24,127 - father_agent.py:386 - Step: 810, Training loss: 3.109173536300659
2025-08-04 05:42:25,707 - father_agent.py:386 - Step: 815, Training loss: 3.478032112121582
2025-08-04 05:42:27,308 - father_agent.py:386 - Step: 820, Training loss: 3.069079875946045
2025-08-04 05:42:28,891 - father_agent.py:386 - Step: 825, Training loss: 4.022217273712158
2025-08-04 05:42:30,478 - father_agent.py:386 - Step: 830, Training loss: 2.5967562198638916
2025-08-04 05:42:32,053 - father_agent.py:386 - Step: 835, Training loss: 2.0866105556488037
2025-08-04 05:42:33,617 - father_agent.py:386 - Step: 840, Training loss: 2.5078139305114746
2025-08-04 05:42:35,192 - father_agent.py:386 - Step: 845, Training loss: 3.081003427505493
2025-08-04 05:42:36,782 - father_agent.py:386 - Step: 850, Training loss: 3.9180028438568115
2025-08-04 05:42:38,356 - father_agent.py:386 - Step: 855, Training loss: 2.602386236190796
2025-08-04 05:42:39,934 - father_agent.py:386 - Step: 860, Training loss: 3.3358888626098633
2025-08-04 05:42:41,530 - father_agent.py:386 - Step: 865, Training loss: 2.529862642288208
2025-08-04 05:42:43,123 - father_agent.py:386 - Step: 870, Training loss: 4.590804100036621
2025-08-04 05:42:44,710 - father_agent.py:386 - Step: 875, Training loss: 2.253856658935547
2025-08-04 05:42:46,297 - father_agent.py:386 - Step: 880, Training loss: 3.277125835418701
2025-08-04 05:42:47,884 - father_agent.py:386 - Step: 885, Training loss: 2.8386268615722656
2025-08-04 05:42:49,457 - father_agent.py:386 - Step: 890, Training loss: 2.766277313232422
2025-08-04 05:42:51,039 - father_agent.py:386 - Step: 895, Training loss: 2.988996744155884
2025-08-04 05:42:52,616 - father_agent.py:386 - Step: 900, Training loss: 2.3880417346954346
2025-08-04 05:42:52,782 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:42:52,784 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:42:59,860 - evaluation_results_class.py:131 - Average Return = -41.46995162963867
2025-08-04 05:42:59,860 - evaluation_results_class.py:133 - Average Virtual Goal Value = 38.53004837036133
2025-08-04 05:42:59,860 - evaluation_results_class.py:135 - Average Discounted Reward = 7.295712947845459
2025-08-04 05:42:59,860 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:42:59,860 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:42:59,860 - evaluation_results_class.py:141 - Variance of Return = 4161.0380859375
2025-08-04 05:42:59,861 - evaluation_results_class.py:143 - Current Best Return = -38.798095703125
2025-08-04 05:42:59,861 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:42:59,861 - evaluation_results_class.py:147 - Average Episode Length = 85.15376285868977
2025-08-04 05:42:59,861 - evaluation_results_class.py:149 - Counted Episodes = 3694
2025-08-04 05:43:00,020 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:43:00,029 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:43:02,390 - father_agent.py:386 - Step: 905, Training loss: 1.187775731086731
2025-08-04 05:43:03,984 - father_agent.py:386 - Step: 910, Training loss: 2.1523854732513428
2025-08-04 05:43:05,581 - father_agent.py:386 - Step: 915, Training loss: 4.159679889678955
2025-08-04 05:43:07,158 - father_agent.py:386 - Step: 920, Training loss: 2.611640214920044
2025-08-04 05:43:08,760 - father_agent.py:386 - Step: 925, Training loss: 3.870439052581787
2025-08-04 05:43:10,344 - father_agent.py:386 - Step: 930, Training loss: 3.5096302032470703
2025-08-04 05:43:11,931 - father_agent.py:386 - Step: 935, Training loss: 2.3471391201019287
2025-08-04 05:43:13,527 - father_agent.py:386 - Step: 940, Training loss: 2.5018699169158936
2025-08-04 05:43:15,112 - father_agent.py:386 - Step: 945, Training loss: 2.3347420692443848
2025-08-04 05:43:16,681 - father_agent.py:386 - Step: 950, Training loss: 3.2114267349243164
2025-08-04 05:43:18,248 - father_agent.py:386 - Step: 955, Training loss: 1.8055392503738403
2025-08-04 05:43:19,834 - father_agent.py:386 - Step: 960, Training loss: 3.0859904289245605
2025-08-04 05:43:21,435 - father_agent.py:386 - Step: 965, Training loss: 2.6267480850219727
2025-08-04 05:43:23,006 - father_agent.py:386 - Step: 970, Training loss: 2.764526844024658
2025-08-04 05:43:24,592 - father_agent.py:386 - Step: 975, Training loss: 2.4777169227600098
2025-08-04 05:43:26,172 - father_agent.py:386 - Step: 980, Training loss: 1.9448699951171875
2025-08-04 05:43:27,748 - father_agent.py:386 - Step: 985, Training loss: 3.57599139213562
2025-08-04 05:43:29,326 - father_agent.py:386 - Step: 990, Training loss: 2.1458115577697754
2025-08-04 05:43:30,917 - father_agent.py:386 - Step: 995, Training loss: 3.5216195583343506
2025-08-04 05:43:32,495 - father_agent.py:386 - Step: 1000, Training loss: 2.2167303562164307
2025-08-04 05:43:32,657 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:43:32,660 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:43:39,726 - evaluation_results_class.py:131 - Average Return = -37.25813674926758
2025-08-04 05:43:39,727 - evaluation_results_class.py:133 - Average Virtual Goal Value = 42.74186325073242
2025-08-04 05:43:39,727 - evaluation_results_class.py:135 - Average Discounted Reward = 9.953786849975586
2025-08-04 05:43:39,727 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:43:39,727 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:43:39,727 - evaluation_results_class.py:141 - Variance of Return = 3447.476318359375
2025-08-04 05:43:39,727 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:43:39,727 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:43:39,727 - evaluation_results_class.py:147 - Average Episode Length = 85.51298878862455
2025-08-04 05:43:39,727 - evaluation_results_class.py:149 - Counted Episodes = 3657
2025-08-04 05:43:39,886 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:43:39,895 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:43:42,285 - father_agent.py:386 - Step: 1005, Training loss: 1.8986331224441528
2025-08-04 05:43:43,860 - father_agent.py:386 - Step: 1010, Training loss: 4.809661865234375
2025-08-04 05:43:45,431 - father_agent.py:386 - Step: 1015, Training loss: 4.318797588348389
2025-08-04 05:43:47,009 - father_agent.py:386 - Step: 1020, Training loss: 2.3428049087524414
2025-08-04 05:43:48,592 - father_agent.py:386 - Step: 1025, Training loss: 2.16300892829895
2025-08-04 05:43:50,162 - father_agent.py:386 - Step: 1030, Training loss: 3.582685947418213
2025-08-04 05:43:51,736 - father_agent.py:386 - Step: 1035, Training loss: 2.0211405754089355
2025-08-04 05:43:53,307 - father_agent.py:386 - Step: 1040, Training loss: 1.3131755590438843
2025-08-04 05:43:54,871 - father_agent.py:386 - Step: 1045, Training loss: 3.337932586669922
2025-08-04 05:43:56,434 - father_agent.py:386 - Step: 1050, Training loss: 1.9040210247039795
2025-08-04 05:43:57,998 - father_agent.py:386 - Step: 1055, Training loss: 3.547628164291382
2025-08-04 05:43:59,591 - father_agent.py:386 - Step: 1060, Training loss: 1.8861403465270996
2025-08-04 05:44:01,163 - father_agent.py:386 - Step: 1065, Training loss: 2.640810489654541
2025-08-04 05:44:02,730 - father_agent.py:386 - Step: 1070, Training loss: 1.9431666135787964
2025-08-04 05:44:04,282 - father_agent.py:386 - Step: 1075, Training loss: 4.448386192321777
2025-08-04 05:44:05,857 - father_agent.py:386 - Step: 1080, Training loss: 2.055588722229004
2025-08-04 05:44:07,425 - father_agent.py:386 - Step: 1085, Training loss: 3.314081907272339
2025-08-04 05:44:09,032 - father_agent.py:386 - Step: 1090, Training loss: 1.9916177988052368
2025-08-04 05:44:10,618 - father_agent.py:386 - Step: 1095, Training loss: 2.131088972091675
2025-08-04 05:44:12,186 - father_agent.py:386 - Step: 1100, Training loss: 2.785785436630249
2025-08-04 05:44:12,350 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:44:12,353 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:44:19,354 - evaluation_results_class.py:131 - Average Return = -45.296043395996094
2025-08-04 05:44:19,354 - evaluation_results_class.py:133 - Average Virtual Goal Value = 34.703956604003906
2025-08-04 05:44:19,354 - evaluation_results_class.py:135 - Average Discounted Reward = 5.478992938995361
2025-08-04 05:44:19,354 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:44:19,354 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:44:19,354 - evaluation_results_class.py:141 - Variance of Return = 6182.9228515625
2025-08-04 05:44:19,354 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:44:19,354 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:44:19,354 - evaluation_results_class.py:147 - Average Episode Length = 86.0417811984607
2025-08-04 05:44:19,354 - evaluation_results_class.py:149 - Counted Episodes = 3638
2025-08-04 05:44:19,516 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:44:19,525 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:44:21,902 - father_agent.py:386 - Step: 1105, Training loss: 1.331667184829712
2025-08-04 05:44:23,490 - father_agent.py:386 - Step: 1110, Training loss: 3.009411334991455
2025-08-04 05:44:25,063 - father_agent.py:386 - Step: 1115, Training loss: 3.718580484390259
2025-08-04 05:44:26,650 - father_agent.py:386 - Step: 1120, Training loss: 3.4601402282714844
2025-08-04 05:44:28,231 - father_agent.py:386 - Step: 1125, Training loss: 3.515718936920166
2025-08-04 05:44:29,803 - father_agent.py:386 - Step: 1130, Training loss: 2.1612207889556885
2025-08-04 05:44:31,370 - father_agent.py:386 - Step: 1135, Training loss: 1.6656666994094849
2025-08-04 05:44:32,937 - father_agent.py:386 - Step: 1140, Training loss: 3.12912917137146
2025-08-04 05:44:34,500 - father_agent.py:386 - Step: 1145, Training loss: 2.363582134246826
2025-08-04 05:44:36,069 - father_agent.py:386 - Step: 1150, Training loss: 3.836514472961426
2025-08-04 05:44:37,663 - father_agent.py:386 - Step: 1155, Training loss: 2.1491503715515137
2025-08-04 05:44:39,234 - father_agent.py:386 - Step: 1160, Training loss: 3.355630397796631
2025-08-04 05:44:40,799 - father_agent.py:386 - Step: 1165, Training loss: 2.1038577556610107
2025-08-04 05:44:42,377 - father_agent.py:386 - Step: 1170, Training loss: 1.9015058279037476
2025-08-04 05:44:43,963 - father_agent.py:386 - Step: 1175, Training loss: 1.8877638578414917
2025-08-04 05:44:45,547 - father_agent.py:386 - Step: 1180, Training loss: 2.1940183639526367
2025-08-04 05:44:47,134 - father_agent.py:386 - Step: 1185, Training loss: 2.754075765609741
2025-08-04 05:44:48,703 - father_agent.py:386 - Step: 1190, Training loss: 3.105623245239258
2025-08-04 05:44:50,276 - father_agent.py:386 - Step: 1195, Training loss: 2.979649782180786
2025-08-04 05:44:51,858 - father_agent.py:386 - Step: 1200, Training loss: 1.789167046546936
2025-08-04 05:44:52,023 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:44:52,026 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:44:59,072 - evaluation_results_class.py:131 - Average Return = -44.27142333984375
2025-08-04 05:44:59,072 - evaluation_results_class.py:133 - Average Virtual Goal Value = 35.72857666015625
2025-08-04 05:44:59,072 - evaluation_results_class.py:135 - Average Discounted Reward = 6.3274102210998535
2025-08-04 05:44:59,073 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:44:59,073 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:44:59,073 - evaluation_results_class.py:141 - Variance of Return = 6401.84375
2025-08-04 05:44:59,073 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:44:59,073 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:44:59,073 - evaluation_results_class.py:147 - Average Episode Length = 84.82103271154367
2025-08-04 05:44:59,073 - evaluation_results_class.py:149 - Counted Episodes = 3699
2025-08-04 05:44:59,231 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:44:59,241 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:45:01,584 - father_agent.py:386 - Step: 1205, Training loss: 1.390672206878662
2025-08-04 05:45:03,141 - father_agent.py:386 - Step: 1210, Training loss: 1.5436073541641235
2025-08-04 05:45:04,701 - father_agent.py:386 - Step: 1215, Training loss: 2.340737819671631
2025-08-04 05:45:06,252 - father_agent.py:386 - Step: 1220, Training loss: 2.067244052886963
2025-08-04 05:45:07,806 - father_agent.py:386 - Step: 1225, Training loss: 4.024461269378662
2025-08-04 05:45:09,371 - father_agent.py:386 - Step: 1230, Training loss: 2.3407862186431885
2025-08-04 05:45:10,928 - father_agent.py:386 - Step: 1235, Training loss: 1.7725588083267212
2025-08-04 05:45:12,506 - father_agent.py:386 - Step: 1240, Training loss: 2.795091390609741
2025-08-04 05:45:14,079 - father_agent.py:386 - Step: 1245, Training loss: 2.819239377975464
2025-08-04 05:45:15,654 - father_agent.py:386 - Step: 1250, Training loss: 2.529447317123413
2025-08-04 05:45:17,232 - father_agent.py:386 - Step: 1255, Training loss: 1.6910074949264526
2025-08-04 05:45:18,799 - father_agent.py:386 - Step: 1260, Training loss: 3.165703773498535
2025-08-04 05:45:20,378 - father_agent.py:386 - Step: 1265, Training loss: 2.600170612335205
2025-08-04 05:45:21,936 - father_agent.py:386 - Step: 1270, Training loss: 2.361227512359619
2025-08-04 05:45:23,502 - father_agent.py:386 - Step: 1275, Training loss: 2.0577030181884766
2025-08-04 05:45:25,069 - father_agent.py:386 - Step: 1280, Training loss: 2.3937315940856934
2025-08-04 05:45:26,630 - father_agent.py:386 - Step: 1285, Training loss: 2.4968817234039307
2025-08-04 05:45:28,189 - father_agent.py:386 - Step: 1290, Training loss: 2.069655656814575
2025-08-04 05:45:29,818 - father_agent.py:386 - Step: 1295, Training loss: 2.0247387886047363
2025-08-04 05:45:31,497 - father_agent.py:386 - Step: 1300, Training loss: 1.6355400085449219
2025-08-04 05:45:31,669 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:45:31,672 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:45:39,167 - evaluation_results_class.py:131 - Average Return = -42.07560348510742
2025-08-04 05:45:39,167 - evaluation_results_class.py:133 - Average Virtual Goal Value = 37.92439651489258
2025-08-04 05:45:39,167 - evaluation_results_class.py:135 - Average Discounted Reward = 7.566549301147461
2025-08-04 05:45:39,167 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:45:39,167 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:45:39,167 - evaluation_results_class.py:141 - Variance of Return = 5801.5927734375
2025-08-04 05:45:39,167 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:45:39,167 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:45:39,167 - evaluation_results_class.py:147 - Average Episode Length = 85.28011966276856
2025-08-04 05:45:39,167 - evaluation_results_class.py:149 - Counted Episodes = 3677
2025-08-04 05:45:39,328 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:45:39,338 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:45:41,991 - father_agent.py:386 - Step: 1305, Training loss: 0.8556026220321655
2025-08-04 05:45:43,590 - father_agent.py:386 - Step: 1310, Training loss: 3.711498498916626
2025-08-04 05:45:45,208 - father_agent.py:386 - Step: 1315, Training loss: 1.6551769971847534
2025-08-04 05:45:46,836 - father_agent.py:386 - Step: 1320, Training loss: 4.976264953613281
2025-08-04 05:45:48,454 - father_agent.py:386 - Step: 1325, Training loss: 2.6776609420776367
2025-08-04 05:45:50,077 - father_agent.py:386 - Step: 1330, Training loss: 1.3621234893798828
2025-08-04 05:45:51,658 - father_agent.py:386 - Step: 1335, Training loss: 1.9502745866775513
2025-08-04 05:45:53,209 - father_agent.py:386 - Step: 1340, Training loss: 1.899674415588379
2025-08-04 05:45:54,788 - father_agent.py:386 - Step: 1345, Training loss: 3.5342557430267334
2025-08-04 05:45:56,358 - father_agent.py:386 - Step: 1350, Training loss: 2.6720454692840576
2025-08-04 05:45:57,923 - father_agent.py:386 - Step: 1355, Training loss: 1.8608675003051758
2025-08-04 05:45:59,506 - father_agent.py:386 - Step: 1360, Training loss: 1.4956406354904175
2025-08-04 05:46:01,120 - father_agent.py:386 - Step: 1365, Training loss: 1.5212482213974
2025-08-04 05:46:02,696 - father_agent.py:386 - Step: 1370, Training loss: 3.492518186569214
2025-08-04 05:46:04,285 - father_agent.py:386 - Step: 1375, Training loss: 2.894822597503662
2025-08-04 05:46:05,882 - father_agent.py:386 - Step: 1380, Training loss: 2.9877288341522217
2025-08-04 05:46:07,468 - father_agent.py:386 - Step: 1385, Training loss: 2.0399856567382812
2025-08-04 05:46:09,047 - father_agent.py:386 - Step: 1390, Training loss: 2.038287401199341
2025-08-04 05:46:10,636 - father_agent.py:386 - Step: 1395, Training loss: 1.3017587661743164
2025-08-04 05:46:12,241 - father_agent.py:386 - Step: 1400, Training loss: 1.2475906610488892
2025-08-04 05:46:12,407 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:46:12,410 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:46:19,445 - evaluation_results_class.py:131 - Average Return = -53.756187438964844
2025-08-04 05:46:19,445 - evaluation_results_class.py:133 - Average Virtual Goal Value = 26.243810653686523
2025-08-04 05:46:19,445 - evaluation_results_class.py:135 - Average Discounted Reward = 1.04918372631073
2025-08-04 05:46:19,445 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:46:19,445 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:46:19,445 - evaluation_results_class.py:141 - Variance of Return = 12063.03515625
2025-08-04 05:46:19,445 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:46:19,445 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:46:19,445 - evaluation_results_class.py:147 - Average Episode Length = 89.92861255037421
2025-08-04 05:46:19,445 - evaluation_results_class.py:149 - Counted Episodes = 3474
2025-08-04 05:46:19,608 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:46:19,617 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:46:19,728 - father_agent.py:547 - Training finished.
2025-08-04 05:46:19,865 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:46:19,867 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:46:19,870 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 05:46:26,922 - evaluation_results_class.py:131 - Average Return = -50.700347900390625
2025-08-04 05:46:26,923 - evaluation_results_class.py:133 - Average Virtual Goal Value = 29.299654006958008
2025-08-04 05:46:26,923 - evaluation_results_class.py:135 - Average Discounted Reward = 2.498847484588623
2025-08-04 05:46:26,923 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:46:26,923 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:46:26,923 - evaluation_results_class.py:141 - Variance of Return = 9963.708984375
2025-08-04 05:46:26,923 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:46:26,923 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:46:26,923 - evaluation_results_class.py:147 - Average Episode Length = 90.16281755196304
2025-08-04 05:46:26,923 - evaluation_results_class.py:149 - Counted Episodes = 3464
2025-08-04 05:46:27,083 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:46:27,085 - self_interpretable_extractor.py:286 - True
2025-08-04 05:46:27,095 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:46:39,689 - evaluation_results_class.py:131 - Average Return = -54.73227310180664
2025-08-04 05:46:39,689 - evaluation_results_class.py:133 - Average Virtual Goal Value = 25.267724990844727
2025-08-04 05:46:39,689 - evaluation_results_class.py:135 - Average Discounted Reward = 0.36806851625442505
2025-08-04 05:46:39,689 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:46:39,689 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:46:39,690 - evaluation_results_class.py:141 - Variance of Return = 12854.1201171875
2025-08-04 05:46:39,690 - evaluation_results_class.py:143 - Current Best Return = -54.73227310180664
2025-08-04 05:46:39,690 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:46:39,690 - evaluation_results_class.py:147 - Average Episode Length = 91.38684061259217
2025-08-04 05:46:39,690 - evaluation_results_class.py:149 - Counted Episodes = 3526
2025-08-04 05:46:39,690 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 05:46:39,690 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 16556 trajectories
Learned trajectory lengths  {133, 139, 271, 145, 151, 157, 163, 169, 43, 175, 49, 181, 55, 187, 61, 193, 67, 199, 73, 205, 79, 211, 85, 217, 91, 223, 97, 103, 109, 115, 121, 127}
Buffer 1
2025-08-04 05:47:35,989 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 33126 trajectories
Learned trajectory lengths  {133, 139, 271, 145, 151, 157, 163, 169, 43, 175, 49, 181, 55, 187, 61, 193, 67, 199, 73, 205, 79, 211, 85, 217, 91, 223, 97, 103, 235, 109, 115, 121, 127}
Buffer 2
2025-08-04 05:48:32,302 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 49719 trajectories
Learned trajectory lengths  {133, 139, 271, 145, 151, 157, 163, 169, 43, 175, 49, 181, 55, 187, 61, 193, 67, 199, 73, 205, 79, 211, 85, 217, 91, 223, 97, 229, 103, 235, 109, 115, 121, 127}
All trajectories collected
2025-08-04 05:49:28,939 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 05:49:28,939 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 49719 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_0.dot.
Learned FSC of size 2
2025-08-04 05:49:32,473 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 05:49:44,373 - evaluation_results_class.py:131 - Average Return = -55.22407531738281
2025-08-04 05:49:44,373 - evaluation_results_class.py:133 - Average Virtual Goal Value = 24.775924682617188
2025-08-04 05:49:44,373 - evaluation_results_class.py:135 - Average Discounted Reward = -0.027508901432156563
2025-08-04 05:49:44,373 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:49:44,373 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:49:44,373 - evaluation_results_class.py:141 - Variance of Return = 12572.48828125
2025-08-04 05:49:44,373 - evaluation_results_class.py:143 - Current Best Return = -55.22407531738281
2025-08-04 05:49:44,373 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:49:44,373 - evaluation_results_class.py:147 - Average Episode Length = 90.9513987001978
2025-08-04 05:49:44,373 - evaluation_results_class.py:149 - Counted Episodes = 3539
FSC Result: {'best_episode_return': 24.775925, 'best_return': -55.224075, 'goal_value': 0.0, 'returns_episodic': [24.775925], 'returns': [-55.224075], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [12572.488], 'each_episode_virtual_variance': [12572.488], 'combined_variance': [50289.953], 'num_episodes': [3539], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [90.9513987001978], 'counted_episodes': [3539], 'discounted_rewards': [-0.027508901], 'new_pomdp_iteration_numbers': []}
[[6, 7, 8, 9], [1], [2], [3], [4], [5], [1], [1], [2], [2], [3], [3], [4], [4], [5], [5], [6, 7, 8, 9], [6, 7, 8, 9], [1], [2], [3], [4], [5], [6, 7, 8, 9], [0]]
2025-08-04 05:49:44,469 - statistic.py:67 - synthesis initiated, design space: 11664
2025-08-04 05:49:44,469 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 05:49:44,503 - synthesizer_ar.py:122 - value 210.4027 achieved after 803.38 seconds
2025-08-04 05:49:44,508 - synthesizer_ar.py:122 - value 210.6136 achieved after 803.39 seconds
2025-08-04 05:49:44,513 - synthesizer_ar.py:122 - value 220.1358 achieved after 803.39 seconds
2025-08-04 05:49:44,515 - synthesizer_ar.py:122 - value 220.7513 achieved after 803.4 seconds
2025-08-04 05:49:44,533 - synthesizer_ar.py:122 - value 229.6695 achieved after 803.41 seconds
2025-08-04 05:49:44,535 - synthesizer_ar.py:122 - value 230.285 achieved after 803.42 seconds
2025-08-04 05:49:44,574 - synthesizer_ar.py:122 - value 282.2688 achieved after 803.45 seconds
2025-08-04 05:49:44,579 - synthesizer_ar.py:122 - value 282.4797 achieved after 803.46 seconds
2025-08-04 05:49:44,584 - synthesizer_ar.py:122 - value 292.0019 achieved after 803.46 seconds
2025-08-04 05:49:44,586 - synthesizer_ar.py:122 - value 292.6174 achieved after 803.47 seconds
2025-08-04 05:49:44,605 - synthesizer_ar.py:122 - value 301.5356 achieved after 803.48 seconds
2025-08-04 05:49:44,606 - synthesizer_ar.py:122 - value 302.1511 achieved after 803.49 seconds
2025-08-04 05:49:44,781 - synthesizer_ar.py:122 - value 310.9187 achieved after 803.66 seconds
2025-08-04 05:49:44,783 - synthesizer_ar.py:122 - value 311.5343 achieved after 803.66 seconds
2025-08-04 05:49:44,939 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 05:49:44,939 - synthesizer.py:193 - o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
2025-08-04 05:49:44,940 - synthesizer.py:198 - double-checking specification satisfiability:  : 311.53428062683577
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 8) & (y = 8))] 

method: AR, synthesis time: 0.47 s
number of holes: 10, family size: 11664, quotient: 1232 states / 1448 actions
explored: 100 %
MDP stats: avg MDP size: 1025, iterations: 300

optimum: 311.534281
--------------------
2025-08-04 05:49:44,940 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
2025-08-04 05:49:44,947 - robust_rl_trainer.py:432 - Iteration 2 of pure RL loop
2025-08-04 05:49:44,989 - storm_vec_env.py:70 - Computing row map
2025-08-04 05:49:44,996 - storm_vec_env.py:97 - Computing transitions
2025-08-04 05:49:45,012 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 05:49:45,012 - storm_vec_env.py:114 - Computing sinks
2025-08-04 05:49:45,012 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 05:49:45,017 - storm_vec_env.py:143 - Computing labels
2025-08-04 05:49:45,017 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 05:49:45,017 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 05:49:45,017 - storm_vec_env.py:175 - Computing observations
2025-08-04 05:49:45,125 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 05:49:45,126 - father_agent.py:540 - Before training evaluation.
2025-08-04 05:49:45,261 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:49:45,264 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:49:53,575 - evaluation_results_class.py:131 - Average Return = -214.66311645507812
2025-08-04 05:49:53,575 - evaluation_results_class.py:133 - Average Virtual Goal Value = -134.66311645507812
2025-08-04 05:49:53,575 - evaluation_results_class.py:135 - Average Discounted Reward = -119.50979614257812
2025-08-04 05:49:53,575 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:49:53,575 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:49:53,576 - evaluation_results_class.py:141 - Variance of Return = 35848.78125
2025-08-04 05:49:53,576 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:49:53,576 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:49:53,576 - evaluation_results_class.py:147 - Average Episode Length = 90.08724445724158
2025-08-04 05:49:53,576 - evaluation_results_class.py:149 - Counted Episodes = 3473
2025-08-04 05:49:53,740 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:49:53,749 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:49:53,856 - father_agent.py:436 - Training agent on-policy
2025-08-04 05:50:00,217 - father_agent.py:386 - Step: 0, Training loss: 9.227607727050781
2025-08-04 05:50:01,856 - father_agent.py:386 - Step: 5, Training loss: 8.85965633392334
2025-08-04 05:50:03,498 - father_agent.py:386 - Step: 10, Training loss: 6.909024715423584
2025-08-04 05:50:05,210 - father_agent.py:386 - Step: 15, Training loss: 5.340254306793213
2025-08-04 05:50:06,914 - father_agent.py:386 - Step: 20, Training loss: 6.601536750793457
2025-08-04 05:50:08,584 - father_agent.py:386 - Step: 25, Training loss: 7.405174732208252
2025-08-04 05:50:10,237 - father_agent.py:386 - Step: 30, Training loss: 8.455516815185547
2025-08-04 05:50:11,877 - father_agent.py:386 - Step: 35, Training loss: 8.254193305969238
2025-08-04 05:50:13,521 - father_agent.py:386 - Step: 40, Training loss: 6.49594259262085
2025-08-04 05:50:15,175 - father_agent.py:386 - Step: 45, Training loss: 7.148080348968506
2025-08-04 05:50:16,815 - father_agent.py:386 - Step: 50, Training loss: 5.713720321655273
2025-08-04 05:50:18,448 - father_agent.py:386 - Step: 55, Training loss: 4.099081993103027
2025-08-04 05:50:20,131 - father_agent.py:386 - Step: 60, Training loss: 1.6113688945770264
2025-08-04 05:50:21,812 - father_agent.py:386 - Step: 65, Training loss: 2.0006182193756104
2025-08-04 05:50:23,540 - father_agent.py:386 - Step: 70, Training loss: 2.597339153289795
2025-08-04 05:50:25,257 - father_agent.py:386 - Step: 75, Training loss: 0.7991290092468262
2025-08-04 05:50:26,997 - father_agent.py:386 - Step: 80, Training loss: 2.483786106109619
2025-08-04 05:50:28,717 - father_agent.py:386 - Step: 85, Training loss: 6.911748886108398
2025-08-04 05:50:30,443 - father_agent.py:386 - Step: 90, Training loss: 3.234292984008789
2025-08-04 05:50:32,116 - father_agent.py:386 - Step: 95, Training loss: 2.0368480682373047
2025-08-04 05:50:33,843 - father_agent.py:386 - Step: 100, Training loss: 1.4345146417617798
2025-08-04 05:50:34,111 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:50:34,117 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:50:41,998 - evaluation_results_class.py:131 - Average Return = -72.79572296142578
2025-08-04 05:50:41,999 - evaluation_results_class.py:133 - Average Virtual Goal Value = 7.204274654388428
2025-08-04 05:50:41,999 - evaluation_results_class.py:135 - Average Discounted Reward = -22.347286224365234
2025-08-04 05:50:41,999 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:50:41,999 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:50:41,999 - evaluation_results_class.py:141 - Variance of Return = 7431.51416015625
2025-08-04 05:50:41,999 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:50:41,999 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:50:41,999 - evaluation_results_class.py:147 - Average Episode Length = 107.41836019621584
2025-08-04 05:50:41,999 - evaluation_results_class.py:149 - Counted Episodes = 2854
2025-08-04 05:50:42,172 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:50:42,182 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:50:47,883 - father_agent.py:386 - Step: 105, Training loss: 2.347766160964966
2025-08-04 05:50:49,700 - father_agent.py:386 - Step: 110, Training loss: 3.2468278408050537
2025-08-04 05:50:51,390 - father_agent.py:386 - Step: 115, Training loss: 2.0616791248321533
2025-08-04 05:50:53,079 - father_agent.py:386 - Step: 120, Training loss: 2.661055564880371
2025-08-04 05:50:54,783 - father_agent.py:386 - Step: 125, Training loss: 2.936363697052002
2025-08-04 05:50:56,495 - father_agent.py:386 - Step: 130, Training loss: 1.9052964448928833
2025-08-04 05:50:58,213 - father_agent.py:386 - Step: 135, Training loss: 5.496336936950684
2025-08-04 05:50:59,914 - father_agent.py:386 - Step: 140, Training loss: 3.596569776535034
2025-08-04 05:51:01,590 - father_agent.py:386 - Step: 145, Training loss: 3.569089651107788
2025-08-04 05:51:03,259 - father_agent.py:386 - Step: 150, Training loss: 2.2786147594451904
2025-08-04 05:51:04,940 - father_agent.py:386 - Step: 155, Training loss: 2.5911917686462402
2025-08-04 05:51:06,639 - father_agent.py:386 - Step: 160, Training loss: 4.8130879402160645
2025-08-04 05:51:08,296 - father_agent.py:386 - Step: 165, Training loss: 2.3489322662353516
2025-08-04 05:51:10,004 - father_agent.py:386 - Step: 170, Training loss: 2.3778672218322754
2025-08-04 05:51:11,737 - father_agent.py:386 - Step: 175, Training loss: 5.008965015411377
2025-08-04 05:51:13,427 - father_agent.py:386 - Step: 180, Training loss: 3.1259806156158447
2025-08-04 05:51:15,109 - father_agent.py:386 - Step: 185, Training loss: 2.2067995071411133
2025-08-04 05:51:16,863 - father_agent.py:386 - Step: 190, Training loss: 1.8694449663162231
2025-08-04 05:51:18,580 - father_agent.py:386 - Step: 195, Training loss: 2.2195067405700684
2025-08-04 05:51:20,338 - father_agent.py:386 - Step: 200, Training loss: 4.006885528564453
2025-08-04 05:51:20,527 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:51:20,529 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:51:28,422 - evaluation_results_class.py:131 - Average Return = -52.579349517822266
2025-08-04 05:51:28,422 - evaluation_results_class.py:133 - Average Virtual Goal Value = 27.420650482177734
2025-08-04 05:51:28,422 - evaluation_results_class.py:135 - Average Discounted Reward = -6.37814474105835
2025-08-04 05:51:28,422 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:51:28,422 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:51:28,422 - evaluation_results_class.py:141 - Variance of Return = 3021.986328125
2025-08-04 05:51:28,422 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:51:28,422 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:51:28,422 - evaluation_results_class.py:147 - Average Episode Length = 88.15304101838755
2025-08-04 05:51:28,422 - evaluation_results_class.py:149 - Counted Episodes = 3535
2025-08-04 05:51:28,604 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:51:28,614 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:51:31,398 - father_agent.py:386 - Step: 205, Training loss: 1.6211695671081543
2025-08-04 05:51:33,088 - father_agent.py:386 - Step: 210, Training loss: 4.2474684715271
2025-08-04 05:51:34,825 - father_agent.py:386 - Step: 215, Training loss: 6.476137638092041
2025-08-04 05:51:36,539 - father_agent.py:386 - Step: 220, Training loss: 3.3334085941314697
2025-08-04 05:51:38,261 - father_agent.py:386 - Step: 225, Training loss: 2.8438687324523926
2025-08-04 05:51:39,962 - father_agent.py:386 - Step: 230, Training loss: 4.728704929351807
2025-08-04 05:51:41,661 - father_agent.py:386 - Step: 235, Training loss: 3.0643434524536133
2025-08-04 05:51:43,324 - father_agent.py:386 - Step: 240, Training loss: 2.076953887939453
2025-08-04 05:51:44,985 - father_agent.py:386 - Step: 245, Training loss: 1.9744867086410522
2025-08-04 05:51:46,642 - father_agent.py:386 - Step: 250, Training loss: 4.551590442657471
2025-08-04 05:51:48,285 - father_agent.py:386 - Step: 255, Training loss: 2.3434741497039795
2025-08-04 05:51:49,954 - father_agent.py:386 - Step: 260, Training loss: 2.511094331741333
2025-08-04 05:51:51,614 - father_agent.py:386 - Step: 265, Training loss: 3.9261019229888916
2025-08-04 05:51:53,276 - father_agent.py:386 - Step: 270, Training loss: 4.87089729309082
2025-08-04 05:51:54,929 - father_agent.py:386 - Step: 275, Training loss: 3.761660099029541
2025-08-04 05:51:56,589 - father_agent.py:386 - Step: 280, Training loss: 3.883389472961426
2025-08-04 05:51:58,281 - father_agent.py:386 - Step: 285, Training loss: 5.16911506652832
2025-08-04 05:51:59,942 - father_agent.py:386 - Step: 290, Training loss: 4.430420875549316
2025-08-04 05:52:01,585 - father_agent.py:386 - Step: 295, Training loss: 5.6328301429748535
2025-08-04 05:52:03,235 - father_agent.py:386 - Step: 300, Training loss: 3.6952672004699707
2025-08-04 05:52:03,420 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:52:03,423 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:52:11,134 - evaluation_results_class.py:131 - Average Return = -62.15751647949219
2025-08-04 05:52:11,134 - evaluation_results_class.py:133 - Average Virtual Goal Value = 17.842483520507812
2025-08-04 05:52:11,134 - evaluation_results_class.py:135 - Average Discounted Reward = -10.721968650817871
2025-08-04 05:52:11,134 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:52:11,134 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:52:11,134 - evaluation_results_class.py:141 - Variance of Return = 4452.27587890625
2025-08-04 05:52:11,134 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:52:11,134 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:52:11,134 - evaluation_results_class.py:147 - Average Episode Length = 83.64019189765459
2025-08-04 05:52:11,135 - evaluation_results_class.py:149 - Counted Episodes = 3752
2025-08-04 05:52:11,314 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:52:11,324 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:52:14,125 - father_agent.py:386 - Step: 305, Training loss: 2.9440293312072754
2025-08-04 05:52:15,820 - father_agent.py:386 - Step: 310, Training loss: 2.909520387649536
2025-08-04 05:52:17,575 - father_agent.py:386 - Step: 315, Training loss: 2.9112682342529297
2025-08-04 05:52:19,302 - father_agent.py:386 - Step: 320, Training loss: 5.404932498931885
2025-08-04 05:52:21,014 - father_agent.py:386 - Step: 325, Training loss: 7.833566188812256
2025-08-04 05:52:22,713 - father_agent.py:386 - Step: 330, Training loss: 3.6618590354919434
2025-08-04 05:52:24,392 - father_agent.py:386 - Step: 335, Training loss: 3.263840436935425
2025-08-04 05:52:26,105 - father_agent.py:386 - Step: 340, Training loss: 3.117854356765747
2025-08-04 05:52:27,837 - father_agent.py:386 - Step: 345, Training loss: 5.971907615661621
2025-08-04 05:52:29,559 - father_agent.py:386 - Step: 350, Training loss: 5.598970890045166
2025-08-04 05:52:31,254 - father_agent.py:386 - Step: 355, Training loss: 4.043924808502197
2025-08-04 05:52:32,915 - father_agent.py:386 - Step: 360, Training loss: 4.690976142883301
2025-08-04 05:52:34,599 - father_agent.py:386 - Step: 365, Training loss: 5.033308982849121
2025-08-04 05:52:36,278 - father_agent.py:386 - Step: 370, Training loss: 4.832028865814209
2025-08-04 05:52:38,002 - father_agent.py:386 - Step: 375, Training loss: 4.527629375457764
2025-08-04 05:52:39,739 - father_agent.py:386 - Step: 380, Training loss: 5.518060684204102
2025-08-04 05:52:41,431 - father_agent.py:386 - Step: 385, Training loss: 4.141854763031006
2025-08-04 05:52:43,117 - father_agent.py:386 - Step: 390, Training loss: 2.8474831581115723
2025-08-04 05:52:44,848 - father_agent.py:386 - Step: 395, Training loss: 4.990787982940674
2025-08-04 05:52:46,586 - father_agent.py:386 - Step: 400, Training loss: 3.535196304321289
2025-08-04 05:52:46,778 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:52:47,104 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:52:54,878 - evaluation_results_class.py:131 - Average Return = -80.98155975341797
2025-08-04 05:52:54,878 - evaluation_results_class.py:133 - Average Virtual Goal Value = -0.9815561771392822
2025-08-04 05:52:54,878 - evaluation_results_class.py:135 - Average Discounted Reward = -21.957489013671875
2025-08-04 05:52:54,878 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:52:54,878 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:52:54,878 - evaluation_results_class.py:141 - Variance of Return = 9120.3623046875
2025-08-04 05:52:54,878 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:52:54,878 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:52:54,879 - evaluation_results_class.py:147 - Average Episode Length = 89.4449567723343
2025-08-04 05:52:54,879 - evaluation_results_class.py:149 - Counted Episodes = 3470
2025-08-04 05:52:55,071 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:52:55,082 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:52:57,882 - father_agent.py:386 - Step: 405, Training loss: 3.5782971382141113
2025-08-04 05:52:59,640 - father_agent.py:386 - Step: 410, Training loss: 2.5621705055236816
2025-08-04 05:53:01,372 - father_agent.py:386 - Step: 415, Training loss: 7.678393363952637
2025-08-04 05:53:03,084 - father_agent.py:386 - Step: 420, Training loss: 4.636951446533203
2025-08-04 05:53:04,789 - father_agent.py:386 - Step: 425, Training loss: 4.433067798614502
2025-08-04 05:53:06,464 - father_agent.py:386 - Step: 430, Training loss: 4.494677543640137
2025-08-04 05:53:08,162 - father_agent.py:386 - Step: 435, Training loss: 3.9783124923706055
2025-08-04 05:53:09,904 - father_agent.py:386 - Step: 440, Training loss: 3.5028140544891357
2025-08-04 05:53:11,676 - father_agent.py:386 - Step: 445, Training loss: 3.6341238021850586
2025-08-04 05:53:13,409 - father_agent.py:386 - Step: 450, Training loss: 4.7663044929504395
2025-08-04 05:53:15,127 - father_agent.py:386 - Step: 455, Training loss: 5.293647289276123
2025-08-04 05:53:16,845 - father_agent.py:386 - Step: 460, Training loss: 4.278449058532715
2025-08-04 05:53:18,565 - father_agent.py:386 - Step: 465, Training loss: 2.861179828643799
2025-08-04 05:53:20,300 - father_agent.py:386 - Step: 470, Training loss: 4.498473644256592
2025-08-04 05:53:22,095 - father_agent.py:386 - Step: 475, Training loss: 4.371001720428467
2025-08-04 05:53:23,805 - father_agent.py:386 - Step: 480, Training loss: 2.899043560028076
2025-08-04 05:53:25,501 - father_agent.py:386 - Step: 485, Training loss: 3.7102253437042236
2025-08-04 05:53:27,236 - father_agent.py:386 - Step: 490, Training loss: 3.0411436557769775
2025-08-04 05:53:28,963 - father_agent.py:386 - Step: 495, Training loss: 3.5894064903259277
2025-08-04 05:53:30,760 - father_agent.py:386 - Step: 500, Training loss: 2.9943931102752686
2025-08-04 05:53:30,958 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:53:30,960 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:53:39,199 - evaluation_results_class.py:131 - Average Return = -84.54862213134766
2025-08-04 05:53:39,199 - evaluation_results_class.py:133 - Average Virtual Goal Value = -4.548625469207764
2025-08-04 05:53:39,200 - evaluation_results_class.py:135 - Average Discounted Reward = -24.53957748413086
2025-08-04 05:53:39,200 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:53:39,200 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:53:39,200 - evaluation_results_class.py:141 - Variance of Return = 7463.06005859375
2025-08-04 05:53:39,200 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:53:39,200 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:53:39,200 - evaluation_results_class.py:147 - Average Episode Length = 91.02601241501625
2025-08-04 05:53:39,200 - evaluation_results_class.py:149 - Counted Episodes = 3383
2025-08-04 05:53:39,457 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:53:39,473 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:53:39,599 - father_agent.py:547 - Training finished.
2025-08-04 05:53:39,764 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:53:39,768 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:53:39,770 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 05:53:47,829 - evaluation_results_class.py:131 - Average Return = -84.65292358398438
2025-08-04 05:53:47,830 - evaluation_results_class.py:133 - Average Virtual Goal Value = -4.652926921844482
2025-08-04 05:53:47,830 - evaluation_results_class.py:135 - Average Discounted Reward = -24.744394302368164
2025-08-04 05:53:47,830 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:53:47,830 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:53:47,830 - evaluation_results_class.py:141 - Variance of Return = 8180.37890625
2025-08-04 05:53:47,830 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:53:47,830 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:53:47,830 - evaluation_results_class.py:147 - Average Episode Length = 91.83273596176822
2025-08-04 05:53:47,830 - evaluation_results_class.py:149 - Counted Episodes = 3348
2025-08-04 05:53:48,018 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:53:48,020 - self_interpretable_extractor.py:286 - True
2025-08-04 05:53:48,031 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:54:01,706 - evaluation_results_class.py:131 - Average Return = -89.06697845458984
2025-08-04 05:54:01,706 - evaluation_results_class.py:133 - Average Virtual Goal Value = -9.066980361938477
2025-08-04 05:54:01,706 - evaluation_results_class.py:135 - Average Discounted Reward = -26.918237686157227
2025-08-04 05:54:01,706 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:54:01,706 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:54:01,706 - evaluation_results_class.py:141 - Variance of Return = 11160.0966796875
2025-08-04 05:54:01,706 - evaluation_results_class.py:143 - Current Best Return = -89.06697845458984
2025-08-04 05:54:01,706 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:54:01,706 - evaluation_results_class.py:147 - Average Episode Length = 93.78025851938895
2025-08-04 05:54:01,706 - evaluation_results_class.py:149 - Counted Episodes = 3404
2025-08-04 05:54:01,707 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 05:54:01,707 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 16030 trajectories
Learned trajectory lengths  {259, 133, 265, 139, 397, 271, 145, 277, 151, 283, 157, 289, 163, 295, 169, 301, 175, 49, 307, 181, 55, 313, 187, 61, 193, 67, 325, 199, 73, 205, 79, 337, 211, 85, 343, 217, 91, 223, 97, 229, 103, 235, 109, 241, 115, 373, 247, 121, 253, 127}
Buffer 1
2025-08-04 05:55:01,653 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 32085 trajectories
Learned trajectory lengths  {259, 133, 391, 265, 139, 397, 271, 145, 277, 151, 283, 157, 289, 163, 295, 169, 301, 175, 49, 307, 181, 55, 313, 187, 61, 319, 193, 67, 325, 199, 73, 331, 205, 79, 337, 211, 85, 343, 217, 91, 223, 97, 229, 103, 361, 235, 109, 241, 115, 373, 247, 121, 253, 127}
Buffer 2
2025-08-04 05:56:02,344 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 48241 trajectories
Learned trajectory lengths  {259, 133, 391, 265, 139, 397, 271, 145, 277, 151, 409, 283, 157, 289, 163, 295, 169, 301, 175, 49, 307, 181, 55, 313, 187, 61, 319, 193, 67, 325, 199, 73, 331, 205, 79, 337, 211, 85, 343, 217, 91, 223, 97, 355, 229, 103, 361, 235, 109, 241, 115, 373, 247, 121, 253, 127}
All trajectories collected
2025-08-04 05:57:02,817 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 05:57:02,817 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 48241 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_1.dot.
Learned FSC of size 2
2025-08-04 05:57:06,242 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 05:57:18,938 - evaluation_results_class.py:131 - Average Return = -159.75291442871094
2025-08-04 05:57:18,938 - evaluation_results_class.py:133 - Average Virtual Goal Value = -79.75291442871094
2025-08-04 05:57:18,938 - evaluation_results_class.py:135 - Average Discounted Reward = -61.77693557739258
2025-08-04 05:57:18,938 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:57:18,938 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:57:18,938 - evaluation_results_class.py:141 - Variance of Return = 68636.15625
2025-08-04 05:57:18,938 - evaluation_results_class.py:143 - Current Best Return = -159.75291442871094
2025-08-04 05:57:18,939 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:57:18,939 - evaluation_results_class.py:147 - Average Episode Length = 105.67998667998668
2025-08-04 05:57:18,939 - evaluation_results_class.py:149 - Counted Episodes = 3003
FSC Result: {'best_episode_return': -79.752914, 'best_return': -159.75291, 'goal_value': 0.0, 'returns_episodic': [-79.752914], 'returns': [-159.75291], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [68636.16], 'each_episode_virtual_variance': [68636.16], 'combined_variance': [274544.62], 'num_episodes': [3003], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [105.67998667998668], 'counted_episodes': [3003], 'discounted_rewards': [-61.776936], 'new_pomdp_iteration_numbers': []}
[[6, 7, 8, 9], [1], [2], [3], [4], [5], [1], [1], [2], [2], [3], [3], [4], [4], [5], [5], [6, 7, 8, 9], [6, 7, 8, 9], [1], [2], [3], [4], [5], [6, 7, 8, 9], [0]]
2025-08-04 05:57:19,045 - statistic.py:67 - synthesis initiated, design space: 11664
2025-08-04 05:57:19,046 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 05:57:19,079 - synthesizer_ar.py:122 - value 216.2383 achieved after 1257.96 seconds
2025-08-04 05:57:19,086 - synthesizer_ar.py:122 - value 216.2385 achieved after 1257.97 seconds
2025-08-04 05:57:19,092 - synthesizer_ar.py:122 - value 216.2635 achieved after 1257.97 seconds
2025-08-04 05:57:19,109 - synthesizer_ar.py:122 - value 218.1412 achieved after 1257.99 seconds
2025-08-04 05:57:19,116 - synthesizer_ar.py:122 - value 218.1414 achieved after 1258.0 seconds
2025-08-04 05:57:19,124 - synthesizer_ar.py:122 - value 218.1665 achieved after 1258.0 seconds
2025-08-04 05:57:19,159 - synthesizer_ar.py:122 - value 305.8728 achieved after 1258.04 seconds
2025-08-04 05:57:19,165 - synthesizer_ar.py:122 - value 305.8731 achieved after 1258.05 seconds
2025-08-04 05:57:19,171 - synthesizer_ar.py:122 - value 305.8981 achieved after 1258.05 seconds
2025-08-04 05:57:19,186 - synthesizer_ar.py:122 - value 307.7758 achieved after 1258.07 seconds
2025-08-04 05:57:19,193 - synthesizer_ar.py:122 - value 307.776 achieved after 1258.07 seconds
2025-08-04 05:57:19,199 - synthesizer_ar.py:122 - value 307.801 achieved after 1258.08 seconds
2025-08-04 05:57:19,317 - synthesizer_ar.py:122 - value 320.5655 achieved after 1258.2 seconds
2025-08-04 05:57:19,324 - synthesizer_ar.py:122 - value 320.5657 achieved after 1258.2 seconds
2025-08-04 05:57:19,330 - synthesizer_ar.py:122 - value 320.5907 achieved after 1258.21 seconds
2025-08-04 05:57:19,344 - synthesizer_ar.py:122 - value 322.4685 achieved after 1258.22 seconds
2025-08-04 05:57:19,350 - synthesizer_ar.py:122 - value 322.4687 achieved after 1258.23 seconds
2025-08-04 05:57:19,356 - synthesizer_ar.py:122 - value 322.4937 achieved after 1258.24 seconds
2025-08-04 05:57:19,498 - synthesizer_ar.py:122 - value 327.5424 achieved after 1258.38 seconds
2025-08-04 05:57:19,505 - synthesizer_ar.py:122 - value 327.5426 achieved after 1258.38 seconds
2025-08-04 05:57:19,511 - synthesizer_ar.py:122 - value 327.5676 achieved after 1258.39 seconds
2025-08-04 05:57:19,570 - synthesizer_ar.py:122 - value 355.5845 achieved after 1258.45 seconds
2025-08-04 05:57:19,576 - synthesizer_ar.py:122 - value 355.5847 achieved after 1258.46 seconds
2025-08-04 05:57:19,582 - synthesizer_ar.py:122 - value 355.6097 achieved after 1258.46 seconds
2025-08-04 05:57:19,661 - synthesizer_ar.py:122 - value 370.2772 achieved after 1258.54 seconds
2025-08-04 05:57:19,667 - synthesizer_ar.py:122 - value 370.2774 achieved after 1258.55 seconds
2025-08-04 05:57:19,673 - synthesizer_ar.py:122 - value 370.3024 achieved after 1258.55 seconds
2025-08-04 05:57:19,793 - synthesizer_ar.py:122 - value 370.3124 achieved after 1258.67 seconds
2025-08-04 05:57:19,809 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 05:57:19,809 - synthesizer.py:193 - o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=3, o4y=8, o5x=8, o5y=5
2025-08-04 05:57:19,810 - synthesizer.py:198 - double-checking specification satisfiability:  : 370.3124091748648
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 8) & (y = 8))] 

method: AR, synthesis time: 0.76 s
number of holes: 10, family size: 11664, quotient: 1232 states / 1448 actions
explored: 100 %
MDP stats: avg MDP size: 1035, iterations: 429

optimum: 370.312409
--------------------
2025-08-04 05:57:19,810 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=3, o4y=8, o5x=8, o5y=5
2025-08-04 05:57:19,817 - robust_rl_trainer.py:432 - Iteration 3 of pure RL loop
2025-08-04 05:57:19,857 - storm_vec_env.py:70 - Computing row map
2025-08-04 05:57:19,864 - storm_vec_env.py:97 - Computing transitions
2025-08-04 05:57:19,879 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 05:57:19,880 - storm_vec_env.py:114 - Computing sinks
2025-08-04 05:57:19,880 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 05:57:19,884 - storm_vec_env.py:143 - Computing labels
2025-08-04 05:57:19,885 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 05:57:19,885 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 05:57:19,885 - storm_vec_env.py:175 - Computing observations
2025-08-04 05:57:20,009 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 05:57:20,010 - father_agent.py:540 - Before training evaluation.
2025-08-04 05:57:20,153 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:57:20,156 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:57:28,514 - evaluation_results_class.py:131 - Average Return = -178.85316467285156
2025-08-04 05:57:28,514 - evaluation_results_class.py:133 - Average Virtual Goal Value = -98.85315704345703
2025-08-04 05:57:28,514 - evaluation_results_class.py:135 - Average Discounted Reward = -108.01094055175781
2025-08-04 05:57:28,514 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:57:28,514 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:57:28,514 - evaluation_results_class.py:141 - Variance of Return = 25852.4453125
2025-08-04 05:57:28,514 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:57:28,514 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:57:28,514 - evaluation_results_class.py:147 - Average Episode Length = 91.87689113022842
2025-08-04 05:57:28,514 - evaluation_results_class.py:149 - Counted Episodes = 3371
2025-08-04 05:57:28,700 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:57:28,710 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:57:28,820 - father_agent.py:436 - Training agent on-policy
2025-08-04 05:57:36,347 - father_agent.py:386 - Step: 0, Training loss: 8.62336540222168
2025-08-04 05:57:38,110 - father_agent.py:386 - Step: 5, Training loss: 8.838754653930664
2025-08-04 05:57:39,826 - father_agent.py:386 - Step: 10, Training loss: 8.750473976135254
2025-08-04 05:57:41,536 - father_agent.py:386 - Step: 15, Training loss: 5.979590892791748
2025-08-04 05:57:43,264 - father_agent.py:386 - Step: 20, Training loss: 4.713597774505615
2025-08-04 05:57:44,968 - father_agent.py:386 - Step: 25, Training loss: 4.064867973327637
2025-08-04 05:57:46,676 - father_agent.py:386 - Step: 30, Training loss: 6.287428379058838
2025-08-04 05:57:48,378 - father_agent.py:386 - Step: 35, Training loss: 4.548336982727051
2025-08-04 05:57:50,108 - father_agent.py:386 - Step: 40, Training loss: 4.927133083343506
2025-08-04 05:57:51,819 - father_agent.py:386 - Step: 45, Training loss: 5.8105597496032715
2025-08-04 05:57:53,509 - father_agent.py:386 - Step: 50, Training loss: 3.113497495651245
2025-08-04 05:57:55,203 - father_agent.py:386 - Step: 55, Training loss: 5.277595520019531
2025-08-04 05:57:56,897 - father_agent.py:386 - Step: 60, Training loss: 4.166426181793213
2025-08-04 05:57:58,598 - father_agent.py:386 - Step: 65, Training loss: 4.54103946685791
2025-08-04 05:58:00,324 - father_agent.py:386 - Step: 70, Training loss: 5.313162326812744
2025-08-04 05:58:02,063 - father_agent.py:386 - Step: 75, Training loss: 4.170312404632568
2025-08-04 05:58:03,776 - father_agent.py:386 - Step: 80, Training loss: 3.6266002655029297
2025-08-04 05:58:05,509 - father_agent.py:386 - Step: 85, Training loss: 4.790923118591309
2025-08-04 05:58:07,234 - father_agent.py:386 - Step: 90, Training loss: 4.857534885406494
2025-08-04 05:58:08,954 - father_agent.py:386 - Step: 95, Training loss: 4.848907947540283
2025-08-04 05:58:10,671 - father_agent.py:386 - Step: 100, Training loss: 3.704702615737915
2025-08-04 05:58:10,862 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:58:10,865 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:58:19,106 - evaluation_results_class.py:131 - Average Return = -202.01968383789062
2025-08-04 05:58:19,106 - evaluation_results_class.py:133 - Average Virtual Goal Value = -122.0196762084961
2025-08-04 05:58:19,106 - evaluation_results_class.py:135 - Average Discounted Reward = -115.35103607177734
2025-08-04 05:58:19,106 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:58:19,106 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:58:19,106 - evaluation_results_class.py:141 - Variance of Return = 37158.16796875
2025-08-04 05:58:19,106 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:58:19,106 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:58:19,106 - evaluation_results_class.py:147 - Average Episode Length = 91.54919236417034
2025-08-04 05:58:19,106 - evaluation_results_class.py:149 - Counted Episodes = 3405
2025-08-04 05:58:19,296 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:58:19,307 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:58:25,426 - father_agent.py:386 - Step: 105, Training loss: 3.2755186557769775
2025-08-04 05:58:27,136 - father_agent.py:386 - Step: 110, Training loss: 5.380256175994873
2025-08-04 05:58:28,851 - father_agent.py:386 - Step: 115, Training loss: 6.224152088165283
2025-08-04 05:58:30,622 - father_agent.py:386 - Step: 120, Training loss: 7.507749080657959
2025-08-04 05:58:32,376 - father_agent.py:386 - Step: 125, Training loss: 3.9173972606658936
2025-08-04 05:58:34,117 - father_agent.py:386 - Step: 130, Training loss: 5.083684921264648
2025-08-04 05:58:35,892 - father_agent.py:386 - Step: 135, Training loss: 3.7100422382354736
2025-08-04 05:58:37,653 - father_agent.py:386 - Step: 140, Training loss: 4.524599075317383
2025-08-04 05:58:39,411 - father_agent.py:386 - Step: 145, Training loss: 5.355444431304932
2025-08-04 05:58:41,165 - father_agent.py:386 - Step: 150, Training loss: 4.256614685058594
2025-08-04 05:58:42,932 - father_agent.py:386 - Step: 155, Training loss: 4.158761501312256
2025-08-04 05:58:44,693 - father_agent.py:386 - Step: 160, Training loss: 3.6860721111297607
2025-08-04 05:58:46,454 - father_agent.py:386 - Step: 165, Training loss: 4.861097812652588
2025-08-04 05:58:48,223 - father_agent.py:386 - Step: 170, Training loss: 3.4865610599517822
2025-08-04 05:58:49,971 - father_agent.py:386 - Step: 175, Training loss: 3.670814275741577
2025-08-04 05:58:51,781 - father_agent.py:386 - Step: 180, Training loss: 5.522972106933594
2025-08-04 05:58:53,597 - father_agent.py:386 - Step: 185, Training loss: 5.647066116333008
2025-08-04 05:58:55,359 - father_agent.py:386 - Step: 190, Training loss: 3.552525281906128
2025-08-04 05:58:57,133 - father_agent.py:386 - Step: 195, Training loss: 5.633849143981934
2025-08-04 05:58:58,920 - father_agent.py:386 - Step: 200, Training loss: 4.5618414878845215
2025-08-04 05:58:59,127 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:58:59,130 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:59:07,508 - evaluation_results_class.py:131 - Average Return = -232.12742614746094
2025-08-04 05:59:07,508 - evaluation_results_class.py:133 - Average Virtual Goal Value = -152.12742614746094
2025-08-04 05:59:07,508 - evaluation_results_class.py:135 - Average Discounted Reward = -98.90858459472656
2025-08-04 05:59:07,508 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:59:07,508 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:59:07,509 - evaluation_results_class.py:141 - Variance of Return = 84177.3359375
2025-08-04 05:59:07,509 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:59:07,509 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:59:07,509 - evaluation_results_class.py:147 - Average Episode Length = 106.75347222222223
2025-08-04 05:59:07,509 - evaluation_results_class.py:149 - Counted Episodes = 2880
2025-08-04 05:59:07,710 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:59:07,720 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:59:10,929 - father_agent.py:386 - Step: 205, Training loss: 3.271096706390381
2025-08-04 05:59:12,687 - father_agent.py:386 - Step: 210, Training loss: 7.1179609298706055
2025-08-04 05:59:14,426 - father_agent.py:386 - Step: 215, Training loss: 4.791068077087402
2025-08-04 05:59:16,184 - father_agent.py:386 - Step: 220, Training loss: 3.71415638923645
2025-08-04 05:59:17,947 - father_agent.py:386 - Step: 225, Training loss: 3.6916561126708984
2025-08-04 05:59:19,716 - father_agent.py:386 - Step: 230, Training loss: 3.3706021308898926
2025-08-04 05:59:21,519 - father_agent.py:386 - Step: 235, Training loss: 4.204084873199463
2025-08-04 05:59:23,286 - father_agent.py:386 - Step: 240, Training loss: 7.175692081451416
2025-08-04 05:59:25,059 - father_agent.py:386 - Step: 245, Training loss: 4.104319095611572
2025-08-04 05:59:26,863 - father_agent.py:386 - Step: 250, Training loss: 6.478020668029785
2025-08-04 05:59:28,663 - father_agent.py:386 - Step: 255, Training loss: 4.650625228881836
2025-08-04 05:59:30,535 - father_agent.py:386 - Step: 260, Training loss: 3.2258493900299072
2025-08-04 05:59:32,415 - father_agent.py:386 - Step: 265, Training loss: 4.574619770050049
2025-08-04 05:59:34,260 - father_agent.py:386 - Step: 270, Training loss: 3.2013654708862305
2025-08-04 05:59:36,119 - father_agent.py:386 - Step: 275, Training loss: 5.078547477722168
2025-08-04 05:59:37,934 - father_agent.py:386 - Step: 280, Training loss: 3.599550485610962
2025-08-04 05:59:39,744 - father_agent.py:386 - Step: 285, Training loss: 4.692515850067139
2025-08-04 05:59:41,601 - father_agent.py:386 - Step: 290, Training loss: 3.9153337478637695
2025-08-04 05:59:43,447 - father_agent.py:386 - Step: 295, Training loss: 4.493076324462891
2025-08-04 05:59:45,263 - father_agent.py:386 - Step: 300, Training loss: 3.7260076999664307
2025-08-04 05:59:45,467 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:59:45,470 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:59:54,036 - evaluation_results_class.py:131 - Average Return = -479.8232727050781
2025-08-04 05:59:54,036 - evaluation_results_class.py:133 - Average Virtual Goal Value = -399.8232727050781
2025-08-04 05:59:54,036 - evaluation_results_class.py:135 - Average Discounted Reward = -180.94432067871094
2025-08-04 05:59:54,036 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 05:59:54,036 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 05:59:54,036 - evaluation_results_class.py:141 - Variance of Return = 316247.5625
2025-08-04 05:59:54,036 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 05:59:54,036 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 05:59:54,036 - evaluation_results_class.py:147 - Average Episode Length = 162.7012542759407
2025-08-04 05:59:54,036 - evaluation_results_class.py:149 - Counted Episodes = 1754
2025-08-04 05:59:54,237 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:59:54,248 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 05:59:57,820 - father_agent.py:386 - Step: 305, Training loss: 2.9103400707244873
2025-08-04 05:59:59,580 - father_agent.py:386 - Step: 310, Training loss: 4.7332258224487305
2025-08-04 06:00:01,395 - father_agent.py:386 - Step: 315, Training loss: 4.421858787536621
2025-08-04 06:00:03,218 - father_agent.py:386 - Step: 320, Training loss: 4.686683654785156
2025-08-04 06:00:04,999 - father_agent.py:386 - Step: 325, Training loss: 4.06039571762085
2025-08-04 06:00:06,785 - father_agent.py:386 - Step: 330, Training loss: 4.094394683837891
2025-08-04 06:00:08,594 - father_agent.py:386 - Step: 335, Training loss: 2.6524908542633057
2025-08-04 06:00:10,406 - father_agent.py:386 - Step: 340, Training loss: 3.873464822769165
2025-08-04 06:00:12,159 - father_agent.py:386 - Step: 345, Training loss: 3.8177266120910645
2025-08-04 06:00:13,898 - father_agent.py:386 - Step: 350, Training loss: 3.6788487434387207
2025-08-04 06:00:15,646 - father_agent.py:386 - Step: 355, Training loss: 3.747152805328369
2025-08-04 06:00:17,379 - father_agent.py:386 - Step: 360, Training loss: 3.6874890327453613
2025-08-04 06:00:19,152 - father_agent.py:386 - Step: 365, Training loss: 3.291156053543091
2025-08-04 06:00:20,998 - father_agent.py:386 - Step: 370, Training loss: 3.2912437915802
2025-08-04 06:00:22,794 - father_agent.py:386 - Step: 375, Training loss: 3.50256609916687
2025-08-04 06:00:24,617 - father_agent.py:386 - Step: 380, Training loss: 3.6793935298919678
2025-08-04 06:00:26,445 - father_agent.py:386 - Step: 385, Training loss: 4.775205612182617
2025-08-04 06:00:28,229 - father_agent.py:386 - Step: 390, Training loss: 3.254103660583496
2025-08-04 06:00:29,988 - father_agent.py:386 - Step: 395, Training loss: 3.2284085750579834
2025-08-04 06:00:31,696 - father_agent.py:386 - Step: 400, Training loss: 3.174247980117798
2025-08-04 06:00:31,900 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:00:31,903 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:00:40,271 - evaluation_results_class.py:131 - Average Return = -501.8223876953125
2025-08-04 06:00:40,271 - evaluation_results_class.py:133 - Average Virtual Goal Value = -421.8223876953125
2025-08-04 06:00:40,271 - evaluation_results_class.py:135 - Average Discounted Reward = -186.70431518554688
2025-08-04 06:00:40,271 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 06:00:40,271 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:00:40,271 - evaluation_results_class.py:141 - Variance of Return = 381087.375
2025-08-04 06:00:40,271 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:00:40,271 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:00:40,271 - evaluation_results_class.py:147 - Average Episode Length = 170.58637469586375
2025-08-04 06:00:40,271 - evaluation_results_class.py:149 - Counted Episodes = 1644
2025-08-04 06:00:40,470 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:00:40,480 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:00:43,652 - father_agent.py:386 - Step: 405, Training loss: 2.3050003051757812
2025-08-04 06:00:45,372 - father_agent.py:386 - Step: 410, Training loss: 3.372812271118164
2025-08-04 06:00:47,097 - father_agent.py:386 - Step: 415, Training loss: 4.812985897064209
2025-08-04 06:00:48,846 - father_agent.py:386 - Step: 420, Training loss: 4.359365463256836
2025-08-04 06:00:50,560 - father_agent.py:386 - Step: 425, Training loss: 3.9041004180908203
2025-08-04 06:00:52,270 - father_agent.py:386 - Step: 430, Training loss: 4.8554792404174805
2025-08-04 06:00:53,993 - father_agent.py:386 - Step: 435, Training loss: 3.243351936340332
2025-08-04 06:00:55,724 - father_agent.py:386 - Step: 440, Training loss: 2.590996026992798
2025-08-04 06:00:57,448 - father_agent.py:386 - Step: 445, Training loss: 3.1217129230499268
2025-08-04 06:00:59,181 - father_agent.py:386 - Step: 450, Training loss: 2.9792861938476562
2025-08-04 06:01:00,927 - father_agent.py:386 - Step: 455, Training loss: 3.9948296546936035
2025-08-04 06:01:02,647 - father_agent.py:386 - Step: 460, Training loss: 4.222839832305908
2025-08-04 06:01:04,372 - father_agent.py:386 - Step: 465, Training loss: 3.040709972381592
2025-08-04 06:01:06,077 - father_agent.py:386 - Step: 470, Training loss: 2.9886367321014404
2025-08-04 06:01:07,782 - father_agent.py:386 - Step: 475, Training loss: 2.446274757385254
2025-08-04 06:01:09,499 - father_agent.py:386 - Step: 480, Training loss: 3.435823917388916
2025-08-04 06:01:11,223 - father_agent.py:386 - Step: 485, Training loss: 3.9540131092071533
2025-08-04 06:01:12,954 - father_agent.py:386 - Step: 490, Training loss: 3.3023931980133057
2025-08-04 06:01:14,673 - father_agent.py:386 - Step: 495, Training loss: 2.9587795734405518
2025-08-04 06:01:16,381 - father_agent.py:386 - Step: 500, Training loss: 2.950031042098999
2025-08-04 06:01:16,588 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:01:16,591 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:01:25,023 - evaluation_results_class.py:131 - Average Return = -768.248046875
2025-08-04 06:01:25,023 - evaluation_results_class.py:133 - Average Virtual Goal Value = -688.3868408203125
2025-08-04 06:01:25,023 - evaluation_results_class.py:135 - Average Discounted Reward = -244.29379272460938
2025-08-04 06:01:25,023 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9982653946227233
2025-08-04 06:01:25,023 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:01:25,023 - evaluation_results_class.py:141 - Variance of Return = 817418.625
2025-08-04 06:01:25,024 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:01:25,024 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:01:25,024 - evaluation_results_class.py:147 - Average Episode Length = 226.07718993928881
2025-08-04 06:01:25,024 - evaluation_results_class.py:149 - Counted Episodes = 1153
2025-08-04 06:01:25,226 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:01:25,237 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:01:25,347 - father_agent.py:547 - Training finished.
2025-08-04 06:01:25,497 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:01:25,500 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:01:25,503 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 06:01:34,098 - evaluation_results_class.py:131 - Average Return = -782.3842163085938
2025-08-04 06:01:34,098 - evaluation_results_class.py:133 - Average Virtual Goal Value = -702.5947265625
2025-08-04 06:01:34,098 - evaluation_results_class.py:135 - Average Discounted Reward = -253.52508544921875
2025-08-04 06:01:34,098 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9973684210526316
2025-08-04 06:01:34,098 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:01:34,098 - evaluation_results_class.py:141 - Variance of Return = 713941.1875
2025-08-04 06:01:34,098 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:01:34,098 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:01:34,098 - evaluation_results_class.py:147 - Average Episode Length = 227.80877192982456
2025-08-04 06:01:34,098 - evaluation_results_class.py:149 - Counted Episodes = 1140
2025-08-04 06:01:34,355 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:01:34,359 - self_interpretable_extractor.py:286 - True
2025-08-04 06:01:34,374 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:01:49,294 - evaluation_results_class.py:131 - Average Return = -810.9202880859375
2025-08-04 06:01:49,294 - evaluation_results_class.py:133 - Average Virtual Goal Value = -731.57763671875
2025-08-04 06:01:49,294 - evaluation_results_class.py:135 - Average Discounted Reward = -244.98402404785156
2025-08-04 06:01:49,294 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9917830731306492
2025-08-04 06:01:49,294 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:01:49,294 - evaluation_results_class.py:141 - Variance of Return = 963124.0625
2025-08-04 06:01:49,294 - evaluation_results_class.py:143 - Current Best Return = -810.9202880859375
2025-08-04 06:01:49,294 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9917830731306492
2025-08-04 06:01:49,294 - evaluation_results_class.py:147 - Average Episode Length = 241.2645850451931
2025-08-04 06:01:49,294 - evaluation_results_class.py:149 - Counted Episodes = 1217
2025-08-04 06:01:49,294 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 06:01:49,295 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 6008 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 139, 651, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
Buffer 1
2025-08-04 06:02:52,365 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 12010 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 139, 651, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
Buffer 2
2025-08-04 06:03:55,383 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 17982 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 139, 651, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
All trajectories collected
2025-08-04 06:04:57,840 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 06:04:57,840 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 17982 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_2.dot.
Learned FSC of size 2
2025-08-04 06:05:10,513 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 06:05:23,692 - evaluation_results_class.py:131 - Average Return = -813.6554565429688
2025-08-04 06:05:23,693 - evaluation_results_class.py:133 - Average Virtual Goal Value = -733.91796875
2025-08-04 06:05:23,693 - evaluation_results_class.py:135 - Average Discounted Reward = -246.87728881835938
2025-08-04 06:05:23,693 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9967186218211649
2025-08-04 06:05:23,693 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:05:23,693 - evaluation_results_class.py:141 - Variance of Return = 796102.0625
2025-08-04 06:05:23,693 - evaluation_results_class.py:143 - Current Best Return = -813.6554565429688
2025-08-04 06:05:23,693 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9967186218211649
2025-08-04 06:05:23,693 - evaluation_results_class.py:147 - Average Episode Length = 239.1575061525841
2025-08-04 06:05:23,693 - evaluation_results_class.py:149 - Counted Episodes = 1219
FSC Result: {'best_episode_return': -733.91797, 'best_return': -813.65546, 'goal_value': 0.0, 'returns_episodic': [-733.91797], 'returns': [-813.65546], 'reach_probs': [0.9967186218211649], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.9967186218211649, 'losses': [], 'best_updated': True, 'each_episode_variance': [796102.06], 'each_episode_virtual_variance': [797052.56], 'combined_variance': [3186288.5], 'num_episodes': [1219], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [239.1575061525841], 'counted_episodes': [1219], 'discounted_rewards': [-246.87729], 'new_pomdp_iteration_numbers': []}
[[6, 7, 8, 9], [1], [2], [3], [4], [5], [1], [1], [2], [2], [3], [3], [4], [4], [5], [5], [6, 7, 8, 9], [6, 7, 8, 9], [1], [2], [3], [4], [5], [6, 7, 8, 9], [0]]
2025-08-04 06:05:23,771 - statistic.py:67 - synthesis initiated, design space: 11664
2025-08-04 06:05:23,771 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 06:05:23,808 - synthesizer_ar.py:122 - value 966.3139 achieved after 1742.69 seconds
2025-08-04 06:05:23,814 - synthesizer_ar.py:122 - value 967.0494 achieved after 1742.69 seconds
2025-08-04 06:05:23,820 - synthesizer_ar.py:122 - value 970.7829 achieved after 1742.7 seconds
2025-08-04 06:05:23,836 - synthesizer_ar.py:122 - value 970.8429 achieved after 1742.72 seconds
2025-08-04 06:05:23,852 - synthesizer_ar.py:122 - value 975.5039 achieved after 1742.73 seconds
2025-08-04 06:05:23,854 - synthesizer_ar.py:122 - value 976.2394 achieved after 1742.73 seconds
2025-08-04 06:05:23,856 - synthesizer_ar.py:122 - value 979.9729 achieved after 1742.74 seconds
2025-08-04 06:05:23,867 - synthesizer_ar.py:122 - value 980.8327 achieved after 1742.75 seconds
2025-08-04 06:05:23,884 - synthesizer_ar.py:122 - value 1039.0674 achieved after 1742.76 seconds
2025-08-04 06:05:23,890 - synthesizer_ar.py:122 - value 1039.8029 achieved after 1742.77 seconds
2025-08-04 06:05:23,896 - synthesizer_ar.py:122 - value 1043.5363 achieved after 1742.78 seconds
2025-08-04 06:05:23,912 - synthesizer_ar.py:122 - value 1043.5963 achieved after 1742.79 seconds
2025-08-04 06:05:23,928 - synthesizer_ar.py:122 - value 1048.2573 achieved after 1742.81 seconds
2025-08-04 06:05:23,930 - synthesizer_ar.py:122 - value 1048.9928 achieved after 1742.81 seconds
2025-08-04 06:05:23,932 - synthesizer_ar.py:122 - value 1052.7263 achieved after 1742.81 seconds
2025-08-04 06:05:23,943 - synthesizer_ar.py:122 - value 1053.5862 achieved after 1742.82 seconds
2025-08-04 06:05:24,017 - synthesizer_ar.py:122 - value 1057.3562 achieved after 1742.9 seconds
2025-08-04 06:05:24,019 - synthesizer_ar.py:122 - value 1058.0917 achieved after 1742.9 seconds
2025-08-04 06:05:24,020 - synthesizer_ar.py:122 - value 1061.8251 achieved after 1742.9 seconds
2025-08-04 06:05:24,032 - synthesizer_ar.py:122 - value 1062.685 achieved after 1742.91 seconds
2025-08-04 06:05:24,076 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 06:05:24,076 - synthesizer.py:193 - o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
2025-08-04 06:05:24,077 - synthesizer.py:198 - double-checking specification satisfiability:  : 1062.6850205364217
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 8) & (y = 8))] 

method: AR, synthesis time: 0.3 s
number of holes: 10, family size: 11664, quotient: 1232 states / 1448 actions
explored: 100 %
MDP stats: avg MDP size: 1004, iterations: 157

optimum: 1062.685021
--------------------
2025-08-04 06:05:24,077 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
2025-08-04 06:05:24,084 - robust_rl_trainer.py:432 - Iteration 4 of pure RL loop
2025-08-04 06:05:24,122 - storm_vec_env.py:70 - Computing row map
2025-08-04 06:05:24,130 - storm_vec_env.py:97 - Computing transitions
2025-08-04 06:05:24,145 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 06:05:24,145 - storm_vec_env.py:114 - Computing sinks
2025-08-04 06:05:24,145 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 06:05:24,150 - storm_vec_env.py:143 - Computing labels
2025-08-04 06:05:24,150 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 06:05:24,150 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 06:05:24,150 - storm_vec_env.py:175 - Computing observations
2025-08-04 06:05:24,289 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 06:05:24,290 - father_agent.py:540 - Before training evaluation.
2025-08-04 06:05:24,437 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:05:24,440 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:05:33,147 - evaluation_results_class.py:131 - Average Return = -849.9361572265625
2025-08-04 06:05:33,148 - evaluation_results_class.py:133 - Average Virtual Goal Value = -770.07421875
2025-08-04 06:05:33,148 - evaluation_results_class.py:135 - Average Discounted Reward = -314.78973388671875
2025-08-04 06:05:33,148 - evaluation_results_class.py:137 - Goal Reach Probability = 0.998274374460742
2025-08-04 06:05:33,148 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:05:33,148 - evaluation_results_class.py:141 - Variance of Return = 660218.5
2025-08-04 06:05:33,148 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:05:33,148 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:05:33,148 - evaluation_results_class.py:147 - Average Episode Length = 224.29076790336498
2025-08-04 06:05:33,148 - evaluation_results_class.py:149 - Counted Episodes = 1159
2025-08-04 06:05:33,360 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:05:33,372 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:05:33,483 - father_agent.py:436 - Training agent on-policy
2025-08-04 06:05:40,755 - father_agent.py:386 - Step: 0, Training loss: 4.979803085327148
2025-08-04 06:05:42,527 - father_agent.py:386 - Step: 5, Training loss: 5.626772880554199
2025-08-04 06:05:44,278 - father_agent.py:386 - Step: 10, Training loss: 4.702489376068115
2025-08-04 06:05:46,041 - father_agent.py:386 - Step: 15, Training loss: 4.26137113571167
2025-08-04 06:05:47,801 - father_agent.py:386 - Step: 20, Training loss: 5.042490005493164
2025-08-04 06:05:49,565 - father_agent.py:386 - Step: 25, Training loss: 5.255946636199951
2025-08-04 06:05:51,327 - father_agent.py:386 - Step: 30, Training loss: 4.928521156311035
2025-08-04 06:05:53,086 - father_agent.py:386 - Step: 35, Training loss: 5.775749683380127
2025-08-04 06:05:54,842 - father_agent.py:386 - Step: 40, Training loss: 5.280072212219238
2025-08-04 06:05:56,613 - father_agent.py:386 - Step: 45, Training loss: 5.098330497741699
2025-08-04 06:05:58,382 - father_agent.py:386 - Step: 50, Training loss: 4.589235305786133
2025-08-04 06:06:00,153 - father_agent.py:386 - Step: 55, Training loss: 4.178799152374268
2025-08-04 06:06:01,899 - father_agent.py:386 - Step: 60, Training loss: 2.8761136531829834
2025-08-04 06:06:03,642 - father_agent.py:386 - Step: 65, Training loss: 3.3276660442352295
2025-08-04 06:06:05,390 - father_agent.py:386 - Step: 70, Training loss: 3.7984657287597656
2025-08-04 06:06:07,127 - father_agent.py:386 - Step: 75, Training loss: 3.4717559814453125
2025-08-04 06:06:08,896 - father_agent.py:386 - Step: 80, Training loss: 2.8119091987609863
2025-08-04 06:06:10,663 - father_agent.py:386 - Step: 85, Training loss: 3.4732203483581543
2025-08-04 06:06:12,459 - father_agent.py:386 - Step: 90, Training loss: 2.5447914600372314
2025-08-04 06:06:14,230 - father_agent.py:386 - Step: 95, Training loss: 2.2644011974334717
2025-08-04 06:06:15,993 - father_agent.py:386 - Step: 100, Training loss: 2.9674112796783447
2025-08-04 06:06:16,209 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:06:16,213 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:06:25,304 - evaluation_results_class.py:131 - Average Return = -102.61614990234375
2025-08-04 06:06:25,305 - evaluation_results_class.py:133 - Average Virtual Goal Value = -22.909805297851562
2025-08-04 06:06:25,305 - evaluation_results_class.py:135 - Average Discounted Reward = -41.32459259033203
2025-08-04 06:06:25,305 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9963293130571579
2025-08-04 06:06:25,305 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:06:25,305 - evaluation_results_class.py:141 - Variance of Return = 49212.62109375
2025-08-04 06:06:25,305 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:06:25,305 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:06:25,305 - evaluation_results_class.py:147 - Average Episode Length = 139.7488201363398
2025-08-04 06:06:25,305 - evaluation_results_class.py:149 - Counted Episodes = 1907
2025-08-04 06:06:25,518 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:06:25,529 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:06:32,025 - father_agent.py:386 - Step: 105, Training loss: 1.6244616508483887
2025-08-04 06:06:33,869 - father_agent.py:386 - Step: 110, Training loss: 3.2779808044433594
2025-08-04 06:06:35,736 - father_agent.py:386 - Step: 115, Training loss: 2.9493296146392822
2025-08-04 06:06:37,647 - father_agent.py:386 - Step: 120, Training loss: 4.061424732208252
2025-08-04 06:06:39,606 - father_agent.py:386 - Step: 125, Training loss: 2.8801190853118896
2025-08-04 06:06:41,545 - father_agent.py:386 - Step: 130, Training loss: 3.352107524871826
2025-08-04 06:06:43,531 - father_agent.py:386 - Step: 135, Training loss: 2.8451457023620605
2025-08-04 06:06:45,467 - father_agent.py:386 - Step: 140, Training loss: 2.745746612548828
2025-08-04 06:06:47,405 - father_agent.py:386 - Step: 145, Training loss: 2.5384130477905273
2025-08-04 06:06:49,372 - father_agent.py:386 - Step: 150, Training loss: 3.2709789276123047
2025-08-04 06:06:51,202 - father_agent.py:386 - Step: 155, Training loss: 2.8263847827911377
2025-08-04 06:06:52,960 - father_agent.py:386 - Step: 160, Training loss: 4.461747646331787
2025-08-04 06:06:54,719 - father_agent.py:386 - Step: 165, Training loss: 2.624347448348999
2025-08-04 06:06:56,481 - father_agent.py:386 - Step: 170, Training loss: 2.219985246658325
2025-08-04 06:06:58,249 - father_agent.py:386 - Step: 175, Training loss: 2.677510976791382
2025-08-04 06:07:00,011 - father_agent.py:386 - Step: 180, Training loss: 2.7708213329315186
2025-08-04 06:07:01,795 - father_agent.py:386 - Step: 185, Training loss: 2.735103130340576
2025-08-04 06:07:03,566 - father_agent.py:386 - Step: 190, Training loss: 2.6842503547668457
2025-08-04 06:07:05,325 - father_agent.py:386 - Step: 195, Training loss: 3.2438385486602783
2025-08-04 06:07:07,090 - father_agent.py:386 - Step: 200, Training loss: 2.283583402633667
2025-08-04 06:07:07,313 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:07:07,315 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:07:16,086 - evaluation_results_class.py:131 - Average Return = -330.0396423339844
2025-08-04 06:07:16,086 - evaluation_results_class.py:133 - Average Virtual Goal Value = -252.0576629638672
2025-08-04 06:07:16,086 - evaluation_results_class.py:135 - Average Discounted Reward = -114.35951232910156
2025-08-04 06:07:16,086 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9747747747747748
2025-08-04 06:07:16,086 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:07:16,086 - evaluation_results_class.py:141 - Variance of Return = 747197.75
2025-08-04 06:07:16,086 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:07:16,086 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:07:16,086 - evaluation_results_class.py:147 - Average Episode Length = 222.93513513513514
2025-08-04 06:07:16,087 - evaluation_results_class.py:149 - Counted Episodes = 1110
2025-08-04 06:07:16,301 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:07:16,312 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:07:19,746 - father_agent.py:386 - Step: 205, Training loss: 1.995407223701477
2025-08-04 06:07:21,516 - father_agent.py:386 - Step: 210, Training loss: 3.466409683227539
2025-08-04 06:07:23,278 - father_agent.py:386 - Step: 215, Training loss: 3.093238115310669
2025-08-04 06:07:25,045 - father_agent.py:386 - Step: 220, Training loss: 3.502295732498169
2025-08-04 06:07:26,807 - father_agent.py:386 - Step: 225, Training loss: 3.0821123123168945
2025-08-04 06:07:28,557 - father_agent.py:386 - Step: 230, Training loss: 2.605328321456909
2025-08-04 06:07:30,324 - father_agent.py:386 - Step: 235, Training loss: 3.0439555644989014
2025-08-04 06:07:32,080 - father_agent.py:386 - Step: 240, Training loss: 2.0151944160461426
2025-08-04 06:07:33,839 - father_agent.py:386 - Step: 245, Training loss: 2.46587872505188
2025-08-04 06:07:35,599 - father_agent.py:386 - Step: 250, Training loss: 3.5561113357543945
2025-08-04 06:07:37,370 - father_agent.py:386 - Step: 255, Training loss: 3.559155225753784
2025-08-04 06:07:39,130 - father_agent.py:386 - Step: 260, Training loss: 3.668156623840332
2025-08-04 06:07:40,896 - father_agent.py:386 - Step: 265, Training loss: 2.7445764541625977
2025-08-04 06:07:42,648 - father_agent.py:386 - Step: 270, Training loss: 2.4543211460113525
2025-08-04 06:07:44,396 - father_agent.py:386 - Step: 275, Training loss: 3.165947198867798
2025-08-04 06:07:46,152 - father_agent.py:386 - Step: 280, Training loss: 5.684512615203857
2025-08-04 06:07:47,912 - father_agent.py:386 - Step: 285, Training loss: 3.69305157661438
2025-08-04 06:07:49,685 - father_agent.py:386 - Step: 290, Training loss: 4.228034973144531
2025-08-04 06:07:51,449 - father_agent.py:386 - Step: 295, Training loss: 2.4082603454589844
2025-08-04 06:07:53,181 - father_agent.py:386 - Step: 300, Training loss: 2.0661580562591553
2025-08-04 06:07:53,406 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:07:53,409 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:08:02,627 - evaluation_results_class.py:131 - Average Return = -143.5603485107422
2025-08-04 06:08:02,627 - evaluation_results_class.py:133 - Average Virtual Goal Value = -63.69324493408203
2025-08-04 06:08:02,627 - evaluation_results_class.py:135 - Average Discounted Reward = -62.86170196533203
2025-08-04 06:08:02,628 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9983388704318937
2025-08-04 06:08:02,628 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:08:02,628 - evaluation_results_class.py:141 - Variance of Return = 81272.6875
2025-08-04 06:08:02,628 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:08:02,628 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:08:02,628 - evaluation_results_class.py:147 - Average Episode Length = 153.7563676633444
2025-08-04 06:08:02,628 - evaluation_results_class.py:149 - Counted Episodes = 1806
2025-08-04 06:08:02,846 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:08:02,858 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:08:06,346 - father_agent.py:386 - Step: 305, Training loss: 1.9601454734802246
2025-08-04 06:08:08,164 - father_agent.py:386 - Step: 310, Training loss: 2.0625083446502686
2025-08-04 06:08:10,014 - father_agent.py:386 - Step: 315, Training loss: 3.0753014087677
2025-08-04 06:08:11,960 - father_agent.py:386 - Step: 320, Training loss: 2.642235279083252
2025-08-04 06:08:13,922 - father_agent.py:386 - Step: 325, Training loss: 5.1354475021362305
2025-08-04 06:08:15,903 - father_agent.py:386 - Step: 330, Training loss: 2.6769466400146484
2025-08-04 06:08:17,852 - father_agent.py:386 - Step: 335, Training loss: 2.2548859119415283
2025-08-04 06:08:19,774 - father_agent.py:386 - Step: 340, Training loss: 2.526885509490967
2025-08-04 06:08:21,694 - father_agent.py:386 - Step: 345, Training loss: 3.2706658840179443
2025-08-04 06:08:23,553 - father_agent.py:386 - Step: 350, Training loss: 3.507500648498535
2025-08-04 06:08:25,461 - father_agent.py:386 - Step: 355, Training loss: 3.0802934169769287
2025-08-04 06:08:27,399 - father_agent.py:386 - Step: 360, Training loss: 2.944964647293091
2025-08-04 06:08:29,315 - father_agent.py:386 - Step: 365, Training loss: 3.192185401916504
2025-08-04 06:08:31,138 - father_agent.py:386 - Step: 370, Training loss: 2.4707086086273193
2025-08-04 06:08:32,918 - father_agent.py:386 - Step: 375, Training loss: 2.692929267883301
2025-08-04 06:08:34,662 - father_agent.py:386 - Step: 380, Training loss: 3.650289297103882
2025-08-04 06:08:36,405 - father_agent.py:386 - Step: 385, Training loss: 3.9417402744293213
2025-08-04 06:08:38,187 - father_agent.py:386 - Step: 390, Training loss: 3.277005434036255
2025-08-04 06:08:39,949 - father_agent.py:386 - Step: 395, Training loss: 3.638965368270874
2025-08-04 06:08:41,754 - father_agent.py:386 - Step: 400, Training loss: 3.335859537124634
2025-08-04 06:08:41,982 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:08:41,984 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:08:50,829 - evaluation_results_class.py:131 - Average Return = -288.2252502441406
2025-08-04 06:08:50,829 - evaluation_results_class.py:133 - Average Virtual Goal Value = -212.42727661132812
2025-08-04 06:08:50,829 - evaluation_results_class.py:135 - Average Discounted Reward = -98.08273315429688
2025-08-04 06:08:50,829 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9474747474747475
2025-08-04 06:08:50,829 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:08:50,829 - evaluation_results_class.py:141 - Variance of Return = 792537.125
2025-08-04 06:08:50,829 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:08:50,829 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:08:50,829 - evaluation_results_class.py:147 - Average Episode Length = 250.0
2025-08-04 06:08:50,830 - evaluation_results_class.py:149 - Counted Episodes = 990
2025-08-04 06:08:51,046 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:08:51,058 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:08:54,473 - father_agent.py:386 - Step: 405, Training loss: 1.8418408632278442
2025-08-04 06:08:56,214 - father_agent.py:386 - Step: 410, Training loss: 2.5660927295684814
2025-08-04 06:08:57,982 - father_agent.py:386 - Step: 415, Training loss: 3.106241226196289
2025-08-04 06:08:59,756 - father_agent.py:386 - Step: 420, Training loss: 3.7209112644195557
2025-08-04 06:09:01,536 - father_agent.py:386 - Step: 425, Training loss: 4.109180927276611
2025-08-04 06:09:03,315 - father_agent.py:386 - Step: 430, Training loss: 2.8827788829803467
2025-08-04 06:09:05,070 - father_agent.py:386 - Step: 435, Training loss: 2.4629619121551514
2025-08-04 06:09:06,841 - father_agent.py:386 - Step: 440, Training loss: 3.286004066467285
2025-08-04 06:09:08,617 - father_agent.py:386 - Step: 445, Training loss: 2.70992374420166
2025-08-04 06:09:10,390 - father_agent.py:386 - Step: 450, Training loss: 3.2200403213500977
2025-08-04 06:09:12,157 - father_agent.py:386 - Step: 455, Training loss: 4.472311496734619
2025-08-04 06:09:13,898 - father_agent.py:386 - Step: 460, Training loss: 3.459087610244751
2025-08-04 06:09:15,666 - father_agent.py:386 - Step: 465, Training loss: 2.6863465309143066
2025-08-04 06:09:17,457 - father_agent.py:386 - Step: 470, Training loss: 2.8806331157684326
2025-08-04 06:09:19,233 - father_agent.py:386 - Step: 475, Training loss: 2.210425615310669
2025-08-04 06:09:21,032 - father_agent.py:386 - Step: 480, Training loss: 2.924138069152832
2025-08-04 06:09:22,816 - father_agent.py:386 - Step: 485, Training loss: 3.5173912048339844
2025-08-04 06:09:24,595 - father_agent.py:386 - Step: 490, Training loss: 2.507143974304199
2025-08-04 06:09:26,371 - father_agent.py:386 - Step: 495, Training loss: 3.019514799118042
2025-08-04 06:09:28,136 - father_agent.py:386 - Step: 500, Training loss: 2.8290843963623047
2025-08-04 06:09:28,356 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:09:28,358 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:09:37,187 - evaluation_results_class.py:131 - Average Return = -279.21588134765625
2025-08-04 06:09:37,187 - evaluation_results_class.py:133 - Average Virtual Goal Value = -208.4466552734375
2025-08-04 06:09:37,187 - evaluation_results_class.py:135 - Average Discounted Reward = -91.64124298095703
2025-08-04 06:09:37,187 - evaluation_results_class.py:137 - Goal Reach Probability = 0.8846153846153846
2025-08-04 06:09:37,187 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:09:37,187 - evaluation_results_class.py:141 - Variance of Return = 787383.125
2025-08-04 06:09:37,187 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:09:37,187 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:09:37,187 - evaluation_results_class.py:147 - Average Episode Length = 296.44168734491313
2025-08-04 06:09:37,187 - evaluation_results_class.py:149 - Counted Episodes = 806
2025-08-04 06:09:37,403 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:09:37,414 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:09:37,525 - father_agent.py:547 - Training finished.
2025-08-04 06:09:37,675 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:09:37,677 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:09:37,680 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 06:09:46,429 - evaluation_results_class.py:131 - Average Return = -300.42303466796875
2025-08-04 06:09:46,429 - evaluation_results_class.py:133 - Average Virtual Goal Value = -232.53817749023438
2025-08-04 06:09:46,429 - evaluation_results_class.py:135 - Average Discounted Reward = -94.29273223876953
2025-08-04 06:09:46,429 - evaluation_results_class.py:137 - Goal Reach Probability = 0.8485607008760951
2025-08-04 06:09:46,429 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:09:46,429 - evaluation_results_class.py:141 - Variance of Return = 867670.4375
2025-08-04 06:09:46,429 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:09:46,429 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:09:46,429 - evaluation_results_class.py:147 - Average Episode Length = 309.59073842302877
2025-08-04 06:09:46,429 - evaluation_results_class.py:149 - Counted Episodes = 799
2025-08-04 06:09:46,646 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:09:46,649 - self_interpretable_extractor.py:286 - True
2025-08-04 06:09:46,661 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:10:01,880 - evaluation_results_class.py:131 - Average Return = -329.7976379394531
2025-08-04 06:10:01,880 - evaluation_results_class.py:133 - Average Virtual Goal Value = -262.6917724609375
2025-08-04 06:10:01,880 - evaluation_results_class.py:135 - Average Discounted Reward = -96.43875122070312
2025-08-04 06:10:01,880 - evaluation_results_class.py:137 - Goal Reach Probability = 0.8388235294117647
2025-08-04 06:10:01,880 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:10:01,880 - evaluation_results_class.py:141 - Variance of Return = 1112805.375
2025-08-04 06:10:01,880 - evaluation_results_class.py:143 - Current Best Return = -329.7976379394531
2025-08-04 06:10:01,880 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.8388235294117647
2025-08-04 06:10:01,880 - evaluation_results_class.py:147 - Average Episode Length = 322.0423529411765
2025-08-04 06:10:01,880 - evaluation_results_class.py:149 - Counted Episodes = 850
2025-08-04 06:10:01,880 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 06:10:01,880 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 4241 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 651, 139, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
Buffer 1
2025-08-04 06:11:06,097 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 8424 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 651, 139, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
Buffer 2
2025-08-04 06:12:10,880 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 12653 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 651, 139, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
All trajectories collected
2025-08-04 06:13:15,707 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 06:13:15,707 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 12653 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_3.dot.
Learned FSC of size 2
2025-08-04 06:13:23,231 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 06:13:37,306 - evaluation_results_class.py:131 - Average Return = -1156.05029296875
2025-08-04 06:13:37,306 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1093.9664306640625
2025-08-04 06:13:37,306 - evaluation_results_class.py:135 - Average Discounted Reward = -224.07852172851562
2025-08-04 06:13:37,306 - evaluation_results_class.py:137 - Goal Reach Probability = 0.7760479041916167
2025-08-04 06:13:37,306 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:13:37,306 - evaluation_results_class.py:141 - Variance of Return = 4169563.75
2025-08-04 06:13:37,306 - evaluation_results_class.py:143 - Current Best Return = -1156.05029296875
2025-08-04 06:13:37,306 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.7760479041916167
2025-08-04 06:13:37,306 - evaluation_results_class.py:147 - Average Episode Length = 327.8502994011976
2025-08-04 06:13:37,306 - evaluation_results_class.py:149 - Counted Episodes = 835
FSC Result: {'best_episode_return': -1093.9664, 'best_return': -1156.0503, 'goal_value': 0.0, 'returns_episodic': [-1093.9664], 'returns': [-1156.0503], 'reach_probs': [0.7760479041916167], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.7760479041916167, 'losses': [], 'best_updated': True, 'each_episode_variance': [4169563.8], 'each_episode_virtual_variance': [4240138.0], 'combined_variance': [16818290.0], 'num_episodes': [835], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [327.8502994011976], 'counted_episodes': [835], 'discounted_rewards': [-224.07852], 'new_pomdp_iteration_numbers': []}
[[6, 7, 8, 9], [1], [2], [3], [4], [5], [1], [1], [2], [2], [3], [3], [4], [4], [5], [5], [6, 7, 8, 9], [6, 7, 8, 9], [1], [2], [3], [4], [5], [6, 7, 8, 9], [0]]
2025-08-04 06:13:37,397 - statistic.py:67 - synthesis initiated, design space: 11664
2025-08-04 06:13:37,398 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 06:13:37,445 - synthesizer_ar.py:122 - value 1620.841 achieved after 2236.33 seconds
2025-08-04 06:13:37,448 - synthesizer_ar.py:122 - value 1620.8669 achieved after 2236.33 seconds
2025-08-04 06:13:37,492 - synthesizer_ar.py:122 - value 1627.8783 achieved after 2236.37 seconds
2025-08-04 06:13:37,495 - synthesizer_ar.py:122 - value 1627.9042 achieved after 2236.38 seconds
2025-08-04 06:13:37,581 - synthesizer_ar.py:122 - value 1636.7052 achieved after 2236.46 seconds
2025-08-04 06:13:37,584 - synthesizer_ar.py:122 - value 1636.731 achieved after 2236.46 seconds
2025-08-04 06:13:37,630 - synthesizer_ar.py:122 - value 1643.7425 achieved after 2236.51 seconds
2025-08-04 06:13:37,633 - synthesizer_ar.py:122 - value 1643.7683 achieved after 2236.51 seconds
2025-08-04 06:13:37,707 - synthesizer_ar.py:122 - value 1710.4401 achieved after 2236.59 seconds
2025-08-04 06:13:37,710 - synthesizer_ar.py:122 - value 1710.4659 achieved after 2236.59 seconds
2025-08-04 06:13:37,741 - synthesizer_ar.py:122 - value 1717.4774 achieved after 2236.62 seconds
2025-08-04 06:13:37,743 - synthesizer_ar.py:122 - value 1717.5032 achieved after 2236.62 seconds
2025-08-04 06:13:37,799 - synthesizer_ar.py:122 - value 1726.3043 achieved after 2236.68 seconds
2025-08-04 06:13:37,802 - synthesizer_ar.py:122 - value 1726.3301 achieved after 2236.68 seconds
2025-08-04 06:13:37,825 - synthesizer_ar.py:122 - value 1733.3416 achieved after 2236.71 seconds
2025-08-04 06:13:37,828 - synthesizer_ar.py:122 - value 1733.3674 achieved after 2236.71 seconds
2025-08-04 06:13:37,983 - synthesizer_ar.py:122 - value 1760.4746 achieved after 2236.86 seconds
2025-08-04 06:13:37,986 - synthesizer_ar.py:122 - value 1760.5004 achieved after 2236.87 seconds
2025-08-04 06:13:38,025 - synthesizer_ar.py:122 - value 1776.3388 achieved after 2236.91 seconds
2025-08-04 06:13:38,028 - synthesizer_ar.py:122 - value 1776.3646 achieved after 2236.91 seconds
2025-08-04 06:13:38,112 - synthesizer_ar.py:122 - value 1776.4332 achieved after 2236.99 seconds
2025-08-04 06:13:38,115 - synthesizer_ar.py:122 - value 1776.459 achieved after 2237.0 seconds
2025-08-04 06:13:38,146 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 06:13:38,146 - synthesizer.py:193 - o1x=1, o1y=2, o2x=1, o2y=3, o3x=5, o3y=1, o4x=3, o4y=8, o5x=8, o5y=5
2025-08-04 06:13:38,147 - synthesizer.py:198 - double-checking specification satisfiability:  : 1776.4590246662028
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 8) & (y = 8))] 

method: AR, synthesis time: 0.75 s
number of holes: 10, family size: 11664, quotient: 1232 states / 1448 actions
explored: 100 %
MDP stats: avg MDP size: 1014, iterations: 254

optimum: 1776.459025
--------------------
2025-08-04 06:13:38,147 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=5, o3y=1, o4x=3, o4y=8, o5x=8, o5y=5
2025-08-04 06:13:38,156 - robust_rl_trainer.py:432 - Iteration 5 of pure RL loop
2025-08-04 06:13:38,193 - storm_vec_env.py:70 - Computing row map
2025-08-04 06:13:38,200 - storm_vec_env.py:97 - Computing transitions
2025-08-04 06:13:38,216 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 06:13:38,216 - storm_vec_env.py:114 - Computing sinks
2025-08-04 06:13:38,216 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 06:13:38,221 - storm_vec_env.py:143 - Computing labels
2025-08-04 06:13:38,221 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 06:13:38,221 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 06:13:38,221 - storm_vec_env.py:175 - Computing observations
2025-08-04 06:13:38,361 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 06:13:38,361 - father_agent.py:540 - Before training evaluation.
2025-08-04 06:13:38,519 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:13:38,522 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:13:47,832 - evaluation_results_class.py:131 - Average Return = -315.8122863769531
2025-08-04 06:13:47,832 - evaluation_results_class.py:133 - Average Virtual Goal Value = -245.15042114257812
2025-08-04 06:13:47,832 - evaluation_results_class.py:135 - Average Discounted Reward = -147.35321044921875
2025-08-04 06:13:47,832 - evaluation_results_class.py:137 - Goal Reach Probability = 0.8832731648616126
2025-08-04 06:13:47,832 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:13:47,832 - evaluation_results_class.py:141 - Variance of Return = 659293.5
2025-08-04 06:13:47,832 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:13:47,832 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:13:47,832 - evaluation_results_class.py:147 - Average Episode Length = 293.8231046931408
2025-08-04 06:13:47,832 - evaluation_results_class.py:149 - Counted Episodes = 831
2025-08-04 06:13:48,053 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:13:48,065 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:13:48,176 - father_agent.py:436 - Training agent on-policy
2025-08-04 06:13:55,925 - father_agent.py:386 - Step: 0, Training loss: 4.935407638549805
2025-08-04 06:13:57,807 - father_agent.py:386 - Step: 5, Training loss: 5.62178373336792
2025-08-04 06:13:59,641 - father_agent.py:386 - Step: 10, Training loss: 3.138824939727783
2025-08-04 06:14:01,495 - father_agent.py:386 - Step: 15, Training loss: 3.6879465579986572
2025-08-04 06:14:03,354 - father_agent.py:386 - Step: 20, Training loss: 3.1039397716522217
2025-08-04 06:14:05,200 - father_agent.py:386 - Step: 25, Training loss: 3.3152847290039062
2025-08-04 06:14:07,084 - father_agent.py:386 - Step: 30, Training loss: 3.1117539405822754
2025-08-04 06:14:08,945 - father_agent.py:386 - Step: 35, Training loss: 3.648709774017334
2025-08-04 06:14:10,798 - father_agent.py:386 - Step: 40, Training loss: 3.755953073501587
2025-08-04 06:14:12,667 - father_agent.py:386 - Step: 45, Training loss: 3.445655107498169
2025-08-04 06:14:14,526 - father_agent.py:386 - Step: 50, Training loss: 3.2186713218688965
2025-08-04 06:14:16,393 - father_agent.py:386 - Step: 55, Training loss: 2.650627374649048
2025-08-04 06:14:18,238 - father_agent.py:386 - Step: 60, Training loss: 3.389133930206299
2025-08-04 06:14:20,089 - father_agent.py:386 - Step: 65, Training loss: 3.2931876182556152
2025-08-04 06:14:21,950 - father_agent.py:386 - Step: 70, Training loss: 2.5977516174316406
2025-08-04 06:14:23,795 - father_agent.py:386 - Step: 75, Training loss: 3.238970994949341
2025-08-04 06:14:25,629 - father_agent.py:386 - Step: 80, Training loss: 3.1426942348480225
2025-08-04 06:14:27,465 - father_agent.py:386 - Step: 85, Training loss: 3.893059492111206
2025-08-04 06:14:29,301 - father_agent.py:386 - Step: 90, Training loss: 4.6341166496276855
2025-08-04 06:14:31,137 - father_agent.py:386 - Step: 95, Training loss: 2.740978240966797
2025-08-04 06:14:33,002 - father_agent.py:386 - Step: 100, Training loss: 3.5328168869018555
2025-08-04 06:14:33,234 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:14:33,238 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:14:42,596 - evaluation_results_class.py:131 - Average Return = -1429.091064453125
2025-08-04 06:14:42,596 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1356.264892578125
2025-08-04 06:14:42,596 - evaluation_results_class.py:135 - Average Discounted Reward = -362.5882873535156
2025-08-04 06:14:42,597 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9103260869565217
2025-08-04 06:14:42,597 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:14:42,597 - evaluation_results_class.py:141 - Variance of Return = 3478576.0
2025-08-04 06:14:42,597 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:14:42,597 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:14:42,597 - evaluation_results_class.py:147 - Average Episode Length = 333.70380434782606
2025-08-04 06:14:42,597 - evaluation_results_class.py:149 - Counted Episodes = 736
2025-08-04 06:14:42,830 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:14:42,841 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:14:49,635 - father_agent.py:386 - Step: 105, Training loss: 1.5353156328201294
2025-08-04 06:14:51,496 - father_agent.py:386 - Step: 110, Training loss: 3.7082483768463135
2025-08-04 06:14:53,333 - father_agent.py:386 - Step: 115, Training loss: 4.248671531677246
2025-08-04 06:14:55,161 - father_agent.py:386 - Step: 120, Training loss: 3.8925602436065674
2025-08-04 06:14:57,015 - father_agent.py:386 - Step: 125, Training loss: 2.782625198364258
2025-08-04 06:14:58,893 - father_agent.py:386 - Step: 130, Training loss: 3.101855516433716
2025-08-04 06:15:00,756 - father_agent.py:386 - Step: 135, Training loss: 2.6878135204315186
2025-08-04 06:15:02,601 - father_agent.py:386 - Step: 140, Training loss: 4.936034679412842
2025-08-04 06:15:04,444 - father_agent.py:386 - Step: 145, Training loss: 6.06257963180542
2025-08-04 06:15:06,278 - father_agent.py:386 - Step: 150, Training loss: 2.7031025886535645
2025-08-04 06:15:08,135 - father_agent.py:386 - Step: 155, Training loss: 3.1238458156585693
2025-08-04 06:15:09,997 - father_agent.py:386 - Step: 160, Training loss: 2.9305992126464844
2025-08-04 06:15:11,840 - father_agent.py:386 - Step: 165, Training loss: 2.9724957942962646
2025-08-04 06:15:13,690 - father_agent.py:386 - Step: 170, Training loss: 2.9479308128356934
2025-08-04 06:15:15,526 - father_agent.py:386 - Step: 175, Training loss: 2.2303693294525146
2025-08-04 06:15:17,365 - father_agent.py:386 - Step: 180, Training loss: 3.77508282661438
2025-08-04 06:15:19,215 - father_agent.py:386 - Step: 185, Training loss: 2.4589474201202393
2025-08-04 06:15:21,055 - father_agent.py:386 - Step: 190, Training loss: 2.6657729148864746
2025-08-04 06:15:22,928 - father_agent.py:386 - Step: 195, Training loss: 2.61620831489563
2025-08-04 06:15:24,792 - father_agent.py:386 - Step: 200, Training loss: 2.8820064067840576
2025-08-04 06:15:25,029 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:15:25,032 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:15:35,674 - evaluation_results_class.py:131 - Average Return = -1318.64599609375
2025-08-04 06:15:35,675 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1243.4005126953125
2025-08-04 06:15:35,675 - evaluation_results_class.py:135 - Average Discounted Reward = -326.7645263671875
2025-08-04 06:15:35,675 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9405684754521964
2025-08-04 06:15:35,675 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:15:35,675 - evaluation_results_class.py:141 - Variance of Return = 2626093.25
2025-08-04 06:15:35,675 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:15:35,675 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:15:35,675 - evaluation_results_class.py:147 - Average Episode Length = 317.9612403100775
2025-08-04 06:15:35,675 - evaluation_results_class.py:149 - Counted Episodes = 774
2025-08-04 06:15:35,895 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:15:35,906 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:15:39,699 - father_agent.py:386 - Step: 205, Training loss: 1.6625112295150757
2025-08-04 06:15:41,670 - father_agent.py:386 - Step: 210, Training loss: 2.8754401206970215
2025-08-04 06:15:43,684 - father_agent.py:386 - Step: 215, Training loss: 2.359705924987793
2025-08-04 06:15:45,633 - father_agent.py:386 - Step: 220, Training loss: 3.258505344390869
2025-08-04 06:15:47,561 - father_agent.py:386 - Step: 225, Training loss: 3.078120231628418
2025-08-04 06:15:49,526 - father_agent.py:386 - Step: 230, Training loss: 3.159691095352173
2025-08-04 06:15:51,503 - father_agent.py:386 - Step: 235, Training loss: 2.7787721157073975
2025-08-04 06:15:53,433 - father_agent.py:386 - Step: 240, Training loss: 2.132963180541992
2025-08-04 06:15:55,420 - father_agent.py:386 - Step: 245, Training loss: 2.1977181434631348
2025-08-04 06:15:57,428 - father_agent.py:386 - Step: 250, Training loss: 2.9347598552703857
2025-08-04 06:15:59,417 - father_agent.py:386 - Step: 255, Training loss: 2.409191131591797
2025-08-04 06:16:01,348 - father_agent.py:386 - Step: 260, Training loss: 2.5581181049346924
2025-08-04 06:16:03,317 - father_agent.py:386 - Step: 265, Training loss: 2.051095962524414
2025-08-04 06:16:05,225 - father_agent.py:386 - Step: 270, Training loss: 2.785060405731201
2025-08-04 06:16:07,148 - father_agent.py:386 - Step: 275, Training loss: 3.436082363128662
2025-08-04 06:16:09,072 - father_agent.py:386 - Step: 280, Training loss: 2.3218278884887695
2025-08-04 06:16:11,077 - father_agent.py:386 - Step: 285, Training loss: 3.502927541732788
2025-08-04 06:16:13,101 - father_agent.py:386 - Step: 290, Training loss: 2.420987844467163
2025-08-04 06:16:15,119 - father_agent.py:386 - Step: 295, Training loss: 3.1073415279388428
2025-08-04 06:16:17,099 - father_agent.py:386 - Step: 300, Training loss: 3.447812080383301
2025-08-04 06:16:17,467 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:16:17,471 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:16:27,457 - evaluation_results_class.py:131 - Average Return = -1253.1571044921875
2025-08-04 06:16:27,457 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1177.6185302734375
2025-08-04 06:16:27,457 - evaluation_results_class.py:135 - Average Discounted Reward = -301.9449768066406
2025-08-04 06:16:27,458 - evaluation_results_class.py:137 - Goal Reach Probability = 0.944233206590621
2025-08-04 06:16:27,458 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:16:27,458 - evaluation_results_class.py:141 - Variance of Return = 1906021.25
2025-08-04 06:16:27,458 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:16:27,458 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:16:27,458 - evaluation_results_class.py:147 - Average Episode Length = 314.1660329531052
2025-08-04 06:16:27,458 - evaluation_results_class.py:149 - Counted Episodes = 789
2025-08-04 06:16:27,768 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:16:27,784 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:16:31,713 - father_agent.py:386 - Step: 305, Training loss: 2.0175251960754395
2025-08-04 06:16:33,646 - father_agent.py:386 - Step: 310, Training loss: 3.025942802429199
2025-08-04 06:16:35,582 - father_agent.py:386 - Step: 315, Training loss: 3.778665065765381
2025-08-04 06:16:37,561 - father_agent.py:386 - Step: 320, Training loss: 2.9739160537719727
2025-08-04 06:16:39,603 - father_agent.py:386 - Step: 325, Training loss: 4.035419940948486
2025-08-04 06:16:41,673 - father_agent.py:386 - Step: 330, Training loss: 2.7931113243103027
2025-08-04 06:16:43,725 - father_agent.py:386 - Step: 335, Training loss: 2.8407742977142334
2025-08-04 06:16:45,714 - father_agent.py:386 - Step: 340, Training loss: 2.47819185256958
2025-08-04 06:16:47,682 - father_agent.py:386 - Step: 345, Training loss: 2.2204747200012207
2025-08-04 06:16:49,695 - father_agent.py:386 - Step: 350, Training loss: 2.5145657062530518
2025-08-04 06:16:51,715 - father_agent.py:386 - Step: 355, Training loss: 2.399441719055176
2025-08-04 06:16:53,636 - father_agent.py:386 - Step: 360, Training loss: 3.5645065307617188
2025-08-04 06:16:55,574 - father_agent.py:386 - Step: 365, Training loss: 3.821244478225708
2025-08-04 06:16:57,516 - father_agent.py:386 - Step: 370, Training loss: 2.8090550899505615
2025-08-04 06:16:59,438 - father_agent.py:386 - Step: 375, Training loss: 2.559983015060425
2025-08-04 06:17:01,370 - father_agent.py:386 - Step: 380, Training loss: 2.7818517684936523
2025-08-04 06:17:03,375 - father_agent.py:386 - Step: 385, Training loss: 2.8784046173095703
2025-08-04 06:17:05,287 - father_agent.py:386 - Step: 390, Training loss: 2.306758165359497
2025-08-04 06:17:07,219 - father_agent.py:386 - Step: 395, Training loss: 3.7405543327331543
2025-08-04 06:17:09,142 - father_agent.py:386 - Step: 400, Training loss: 2.654547691345215
2025-08-04 06:17:09,380 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:17:09,383 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:17:18,994 - evaluation_results_class.py:131 - Average Return = -1492.7725830078125
2025-08-04 06:17:18,995 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1421.3267822265625
2025-08-04 06:17:18,995 - evaluation_results_class.py:135 - Average Discounted Reward = -317.2361755371094
2025-08-04 06:17:18,995 - evaluation_results_class.py:137 - Goal Reach Probability = 0.8930722891566265
2025-08-04 06:17:18,995 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:17:18,995 - evaluation_results_class.py:141 - Variance of Return = 2470041.5
2025-08-04 06:17:18,995 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:17:18,995 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:17:18,995 - evaluation_results_class.py:147 - Average Episode Length = 369.45180722891564
2025-08-04 06:17:18,995 - evaluation_results_class.py:149 - Counted Episodes = 664
2025-08-04 06:17:19,229 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:17:19,241 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:17:23,200 - father_agent.py:386 - Step: 405, Training loss: 1.9963784217834473
2025-08-04 06:17:25,157 - father_agent.py:386 - Step: 410, Training loss: 2.847832441329956
2025-08-04 06:17:27,194 - father_agent.py:386 - Step: 415, Training loss: 3.919339895248413
2025-08-04 06:17:29,147 - father_agent.py:386 - Step: 420, Training loss: 2.4564428329467773
2025-08-04 06:17:31,017 - father_agent.py:386 - Step: 425, Training loss: 3.590078830718994
2025-08-04 06:17:32,837 - father_agent.py:386 - Step: 430, Training loss: 3.200840950012207
2025-08-04 06:17:34,658 - father_agent.py:386 - Step: 435, Training loss: 3.087482452392578
2025-08-04 06:17:36,482 - father_agent.py:386 - Step: 440, Training loss: 2.5922532081604004
2025-08-04 06:17:38,469 - father_agent.py:386 - Step: 445, Training loss: 2.435703992843628
2025-08-04 06:17:40,330 - father_agent.py:386 - Step: 450, Training loss: 2.534881353378296
2025-08-04 06:17:42,166 - father_agent.py:386 - Step: 455, Training loss: 2.910574436187744
2025-08-04 06:17:44,007 - father_agent.py:386 - Step: 460, Training loss: 2.521824836730957
2025-08-04 06:17:45,849 - father_agent.py:386 - Step: 465, Training loss: 2.6036345958709717
2025-08-04 06:17:47,700 - father_agent.py:386 - Step: 470, Training loss: 2.9495482444763184
2025-08-04 06:17:49,535 - father_agent.py:386 - Step: 475, Training loss: 2.854065179824829
2025-08-04 06:17:51,386 - father_agent.py:386 - Step: 480, Training loss: 2.763800859451294
2025-08-04 06:17:53,254 - father_agent.py:386 - Step: 485, Training loss: 2.865077257156372
2025-08-04 06:17:55,165 - father_agent.py:386 - Step: 490, Training loss: 2.9443538188934326
2025-08-04 06:17:57,011 - father_agent.py:386 - Step: 495, Training loss: 2.2621986865997314
2025-08-04 06:17:58,981 - father_agent.py:386 - Step: 500, Training loss: 2.8249704837799072
2025-08-04 06:17:59,299 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:17:59,303 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:18:09,361 - evaluation_results_class.py:131 - Average Return = -1045.93359375
2025-08-04 06:18:09,361 - evaluation_results_class.py:133 - Average Virtual Goal Value = -967.7353515625
2025-08-04 06:18:09,361 - evaluation_results_class.py:135 - Average Discounted Reward = -281.0818786621094
2025-08-04 06:18:09,361 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9774774774774775
2025-08-04 06:18:09,361 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:18:09,362 - evaluation_results_class.py:141 - Variance of Return = 1236888.875
2025-08-04 06:18:09,362 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:18:09,362 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:18:09,362 - evaluation_results_class.py:147 - Average Episode Length = 281.5563063063063
2025-08-04 06:18:09,362 - evaluation_results_class.py:149 - Counted Episodes = 888
2025-08-04 06:18:09,591 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:18:09,603 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:18:09,717 - father_agent.py:547 - Training finished.
2025-08-04 06:18:09,873 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:18:09,876 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:18:09,879 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 06:18:19,373 - evaluation_results_class.py:131 - Average Return = -1064.3348388671875
2025-08-04 06:18:19,373 - evaluation_results_class.py:133 - Average Virtual Goal Value = -986.0484619140625
2025-08-04 06:18:19,373 - evaluation_results_class.py:135 - Average Discounted Reward = -275.7456970214844
2025-08-04 06:18:19,373 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9785794813979707
2025-08-04 06:18:19,373 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:18:19,373 - evaluation_results_class.py:141 - Variance of Return = 1329254.125
2025-08-04 06:18:19,373 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:18:19,373 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:18:19,373 - evaluation_results_class.py:147 - Average Episode Length = 283.7440811724915
2025-08-04 06:18:19,373 - evaluation_results_class.py:149 - Counted Episodes = 887
2025-08-04 06:18:19,603 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:18:19,605 - self_interpretable_extractor.py:286 - True
2025-08-04 06:18:19,618 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:18:36,034 - evaluation_results_class.py:131 - Average Return = -1141.009521484375
2025-08-04 06:18:36,034 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1063.5357666015625
2025-08-04 06:18:36,034 - evaluation_results_class.py:135 - Average Discounted Reward = -279.6269226074219
2025-08-04 06:18:36,034 - evaluation_results_class.py:137 - Goal Reach Probability = 0.968421052631579
2025-08-04 06:18:36,034 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:18:36,034 - evaluation_results_class.py:141 - Variance of Return = 1541025.375
2025-08-04 06:18:36,034 - evaluation_results_class.py:143 - Current Best Return = -1141.009521484375
2025-08-04 06:18:36,034 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.968421052631579
2025-08-04 06:18:36,034 - evaluation_results_class.py:147 - Average Episode Length = 300.85052631578947
2025-08-04 06:18:36,034 - evaluation_results_class.py:149 - Counted Episodes = 950
2025-08-04 06:18:36,034 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 06:18:36,034 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 4733 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 651, 139, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
Buffer 1
2025-08-04 06:19:44,547 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 9398 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 651, 139, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
Buffer 2
2025-08-04 06:20:52,983 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 14145 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 651, 139, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
All trajectories collected
2025-08-04 06:22:01,200 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 06:22:01,200 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 14145 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_4.dot.
Learned FSC of size 2
2025-08-04 06:22:17,348 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 06:22:32,634 - evaluation_results_class.py:131 - Average Return = -1145.121826171875
2025-08-04 06:22:32,634 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1067.268310546875
2025-08-04 06:22:32,634 - evaluation_results_class.py:135 - Average Discounted Reward = -290.8123779296875
2025-08-04 06:22:32,634 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9731682146542827
2025-08-04 06:22:32,634 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:22:32,634 - evaluation_results_class.py:141 - Variance of Return = 1576458.25
2025-08-04 06:22:32,634 - evaluation_results_class.py:143 - Current Best Return = -1145.121826171875
2025-08-04 06:22:32,634 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9731682146542827
2025-08-04 06:22:32,634 - evaluation_results_class.py:147 - Average Episode Length = 297.71826625386996
2025-08-04 06:22:32,634 - evaluation_results_class.py:149 - Counted Episodes = 969
FSC Result: {'best_episode_return': -1067.2683, 'best_return': -1145.1218, 'goal_value': 0.0, 'returns_episodic': [-1067.2683], 'returns': [-1145.1218], 'reach_probs': [0.9731682146542827], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.9731682146542827, 'losses': [], 'best_updated': True, 'each_episode_variance': [1576458.2], 'each_episode_virtual_variance': [1585337.4], 'combined_variance': [6323424.0], 'num_episodes': [969], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [297.71826625386996], 'counted_episodes': [969], 'discounted_rewards': [-290.81238], 'new_pomdp_iteration_numbers': []}
[[6, 7, 8, 9], [1], [2], [3], [4], [5], [1], [1], [2], [2], [3], [3], [4], [4], [5], [5], [6, 7, 8, 9], [6, 7, 8, 9], [1], [2], [3], [4], [5], [6, 7, 8, 9], [0]]
2025-08-04 06:22:32,707 - statistic.py:67 - synthesis initiated, design space: 11664
2025-08-04 06:22:32,707 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 06:22:32,744 - synthesizer_ar.py:122 - value 1278.1832 achieved after 2771.62 seconds
2025-08-04 06:22:32,751 - synthesizer_ar.py:122 - value 1278.6498 achieved after 2771.63 seconds
2025-08-04 06:22:32,758 - synthesizer_ar.py:122 - value 1281.415 achieved after 2771.64 seconds
2025-08-04 06:22:32,775 - synthesizer_ar.py:122 - value 1281.4679 achieved after 2771.66 seconds
2025-08-04 06:22:32,792 - synthesizer_ar.py:122 - value 1287.6073 achieved after 2771.67 seconds
2025-08-04 06:22:32,794 - synthesizer_ar.py:122 - value 1288.074 achieved after 2771.67 seconds
2025-08-04 06:22:32,796 - synthesizer_ar.py:122 - value 1290.8391 achieved after 2771.68 seconds
2025-08-04 06:22:32,808 - synthesizer_ar.py:122 - value 1291.6107 achieved after 2771.69 seconds
2025-08-04 06:22:32,824 - synthesizer_ar.py:122 - value 1350.6889 achieved after 2771.7 seconds
2025-08-04 06:22:32,831 - synthesizer_ar.py:122 - value 1351.1555 achieved after 2771.71 seconds
2025-08-04 06:22:32,838 - synthesizer_ar.py:122 - value 1353.9207 achieved after 2771.72 seconds
2025-08-04 06:22:32,855 - synthesizer_ar.py:122 - value 1353.9736 achieved after 2771.74 seconds
2025-08-04 06:22:32,872 - synthesizer_ar.py:122 - value 1360.113 achieved after 2771.75 seconds
2025-08-04 06:22:32,874 - synthesizer_ar.py:122 - value 1360.5796 achieved after 2771.75 seconds
2025-08-04 06:22:32,876 - synthesizer_ar.py:122 - value 1363.3448 achieved after 2771.76 seconds
2025-08-04 06:22:32,888 - synthesizer_ar.py:122 - value 1364.1164 achieved after 2771.77 seconds
2025-08-04 06:22:32,952 - synthesizer_ar.py:122 - value 1369.4139 achieved after 2771.83 seconds
2025-08-04 06:22:32,954 - synthesizer_ar.py:122 - value 1369.8805 achieved after 2771.83 seconds
2025-08-04 06:22:32,956 - synthesizer_ar.py:122 - value 1372.6457 achieved after 2771.84 seconds
2025-08-04 06:22:32,968 - synthesizer_ar.py:122 - value 1373.4173 achieved after 2771.85 seconds
2025-08-04 06:22:33,012 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 06:22:33,012 - synthesizer.py:193 - o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
2025-08-04 06:22:33,013 - synthesizer.py:198 - double-checking specification satisfiability:  : 1373.417307233089
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 8) & (y = 8))] 

method: AR, synthesis time: 0.31 s
number of holes: 10, family size: 11664, quotient: 1232 states / 1448 actions
explored: 100 %
MDP stats: avg MDP size: 1005, iterations: 151

optimum: 1373.417307
--------------------
2025-08-04 06:22:33,014 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
2025-08-04 06:22:33,019 - robust_rl_trainer.py:432 - Iteration 6 of pure RL loop
2025-08-04 06:22:33,057 - storm_vec_env.py:70 - Computing row map
2025-08-04 06:22:33,064 - storm_vec_env.py:97 - Computing transitions
2025-08-04 06:22:33,079 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 06:22:33,079 - storm_vec_env.py:114 - Computing sinks
2025-08-04 06:22:33,079 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 06:22:33,084 - storm_vec_env.py:143 - Computing labels
2025-08-04 06:22:33,085 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 06:22:33,085 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 06:22:33,085 - storm_vec_env.py:175 - Computing observations
2025-08-04 06:22:33,248 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 06:22:33,248 - father_agent.py:540 - Before training evaluation.
2025-08-04 06:22:33,409 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:22:33,411 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:22:43,546 - evaluation_results_class.py:131 - Average Return = -1083.1624755859375
2025-08-04 06:22:43,546 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1005.1900634765625
2025-08-04 06:22:43,546 - evaluation_results_class.py:135 - Average Discounted Reward = -339.1302795410156
2025-08-04 06:22:43,546 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9746543778801844
2025-08-04 06:22:43,546 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:22:43,546 - evaluation_results_class.py:141 - Variance of Return = 1091836.375
2025-08-04 06:22:43,546 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:22:43,546 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:22:43,546 - evaluation_results_class.py:147 - Average Episode Length = 283.9884792626728
2025-08-04 06:22:43,546 - evaluation_results_class.py:149 - Counted Episodes = 868
2025-08-04 06:22:43,785 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:22:43,798 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:22:43,912 - father_agent.py:436 - Training agent on-policy
2025-08-04 06:22:52,073 - father_agent.py:386 - Step: 0, Training loss: 3.8709495067596436
2025-08-04 06:22:53,966 - father_agent.py:386 - Step: 5, Training loss: 5.899718761444092
2025-08-04 06:22:55,878 - father_agent.py:386 - Step: 10, Training loss: 4.154151439666748
2025-08-04 06:22:57,786 - father_agent.py:386 - Step: 15, Training loss: 4.392709255218506
2025-08-04 06:22:59,709 - father_agent.py:386 - Step: 20, Training loss: 4.2529826164245605
2025-08-04 06:23:01,714 - father_agent.py:386 - Step: 25, Training loss: 4.116654872894287
2025-08-04 06:23:03,726 - father_agent.py:386 - Step: 30, Training loss: 4.044458866119385
2025-08-04 06:23:05,731 - father_agent.py:386 - Step: 35, Training loss: 4.405791759490967
2025-08-04 06:23:07,813 - father_agent.py:386 - Step: 40, Training loss: 5.055491924285889
2025-08-04 06:23:09,828 - father_agent.py:386 - Step: 45, Training loss: 3.8019580841064453
2025-08-04 06:23:11,802 - father_agent.py:386 - Step: 50, Training loss: 4.488419055938721
2025-08-04 06:23:13,716 - father_agent.py:386 - Step: 55, Training loss: 3.9946389198303223
2025-08-04 06:23:15,621 - father_agent.py:386 - Step: 60, Training loss: 5.351456642150879
2025-08-04 06:23:17,563 - father_agent.py:386 - Step: 65, Training loss: 4.063603401184082
2025-08-04 06:23:19,469 - father_agent.py:386 - Step: 70, Training loss: 4.783492088317871
2025-08-04 06:23:21,402 - father_agent.py:386 - Step: 75, Training loss: 4.152681827545166
2025-08-04 06:23:23,335 - father_agent.py:386 - Step: 80, Training loss: 4.590413570404053
2025-08-04 06:23:25,247 - father_agent.py:386 - Step: 85, Training loss: 3.2934353351593018
2025-08-04 06:23:27,189 - father_agent.py:386 - Step: 90, Training loss: 2.389411211013794
2025-08-04 06:23:29,115 - father_agent.py:386 - Step: 95, Training loss: 1.9687105417251587
2025-08-04 06:23:31,031 - father_agent.py:386 - Step: 100, Training loss: 2.3753881454467773
2025-08-04 06:23:31,283 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:23:31,286 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:23:41,329 - evaluation_results_class.py:131 - Average Return = -171.04054260253906
2025-08-04 06:23:41,329 - evaluation_results_class.py:133 - Average Virtual Goal Value = -92.63036346435547
2025-08-04 06:23:41,329 - evaluation_results_class.py:135 - Average Discounted Reward = -73.52013397216797
2025-08-04 06:23:41,329 - evaluation_results_class.py:137 - Goal Reach Probability = 0.980127186009539
2025-08-04 06:23:41,329 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:23:41,329 - evaluation_results_class.py:141 - Variance of Return = 171661.203125
2025-08-04 06:23:41,329 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:23:41,329 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:23:41,329 - evaluation_results_class.py:147 - Average Episode Length = 200.5262321144674
2025-08-04 06:23:41,329 - evaluation_results_class.py:149 - Counted Episodes = 1258
2025-08-04 06:23:41,571 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:23:41,583 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:23:48,774 - father_agent.py:386 - Step: 105, Training loss: 2.0517807006835938
2025-08-04 06:23:50,710 - father_agent.py:386 - Step: 110, Training loss: 2.72794246673584
2025-08-04 06:23:52,624 - father_agent.py:386 - Step: 115, Training loss: 2.5894100666046143
2025-08-04 06:23:54,534 - father_agent.py:386 - Step: 120, Training loss: 2.783346176147461
2025-08-04 06:23:56,481 - father_agent.py:386 - Step: 125, Training loss: 3.123326301574707
2025-08-04 06:23:58,420 - father_agent.py:386 - Step: 130, Training loss: 2.4759321212768555
2025-08-04 06:24:00,357 - father_agent.py:386 - Step: 135, Training loss: 2.524918794631958
2025-08-04 06:24:02,283 - father_agent.py:386 - Step: 140, Training loss: 1.8409411907196045
2025-08-04 06:24:04,195 - father_agent.py:386 - Step: 145, Training loss: 2.1785848140716553
2025-08-04 06:24:06,112 - father_agent.py:386 - Step: 150, Training loss: 3.577758550643921
2025-08-04 06:24:08,031 - father_agent.py:386 - Step: 155, Training loss: 3.3375251293182373
2025-08-04 06:24:09,970 - father_agent.py:386 - Step: 160, Training loss: 2.974661350250244
2025-08-04 06:24:11,900 - father_agent.py:386 - Step: 165, Training loss: 3.100929021835327
2025-08-04 06:24:13,821 - father_agent.py:386 - Step: 170, Training loss: 2.552668333053589
2025-08-04 06:24:15,737 - father_agent.py:386 - Step: 175, Training loss: 2.533970832824707
2025-08-04 06:24:17,645 - father_agent.py:386 - Step: 180, Training loss: 3.460582733154297
2025-08-04 06:24:19,550 - father_agent.py:386 - Step: 185, Training loss: 3.55248761177063
2025-08-04 06:24:21,499 - father_agent.py:386 - Step: 190, Training loss: 2.8699440956115723
2025-08-04 06:24:23,466 - father_agent.py:386 - Step: 195, Training loss: 2.578594446182251
2025-08-04 06:24:25,397 - father_agent.py:386 - Step: 200, Training loss: 1.8970366716384888
2025-08-04 06:24:25,654 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:24:25,657 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:24:35,684 - evaluation_results_class.py:131 - Average Return = -545.1398315429688
2025-08-04 06:24:35,685 - evaluation_results_class.py:133 - Average Virtual Goal Value = -468.2816162109375
2025-08-04 06:24:35,685 - evaluation_results_class.py:135 - Average Discounted Reward = -172.39614868164062
2025-08-04 06:24:35,685 - evaluation_results_class.py:137 - Goal Reach Probability = 0.960727969348659
2025-08-04 06:24:35,685 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:24:35,685 - evaluation_results_class.py:141 - Variance of Return = 1339362.375
2025-08-04 06:24:35,685 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:24:35,685 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:24:35,685 - evaluation_results_class.py:147 - Average Episode Length = 236.1168582375479
2025-08-04 06:24:35,685 - evaluation_results_class.py:149 - Counted Episodes = 1044
2025-08-04 06:24:35,931 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:24:35,943 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:24:40,111 - father_agent.py:386 - Step: 205, Training loss: 1.4228445291519165
2025-08-04 06:24:42,071 - father_agent.py:386 - Step: 210, Training loss: 2.4523932933807373
2025-08-04 06:24:43,981 - father_agent.py:386 - Step: 215, Training loss: 2.854318380355835
2025-08-04 06:24:45,919 - father_agent.py:386 - Step: 220, Training loss: 3.4773924350738525
2025-08-04 06:24:47,810 - father_agent.py:386 - Step: 225, Training loss: 3.0458059310913086
2025-08-04 06:24:49,716 - father_agent.py:386 - Step: 230, Training loss: 2.7330234050750732
2025-08-04 06:24:51,607 - father_agent.py:386 - Step: 235, Training loss: 2.5603280067443848
2025-08-04 06:24:53,497 - father_agent.py:386 - Step: 240, Training loss: 2.394911766052246
2025-08-04 06:24:55,419 - father_agent.py:386 - Step: 245, Training loss: 2.9597177505493164
2025-08-04 06:24:57,324 - father_agent.py:386 - Step: 250, Training loss: 3.2526297569274902
2025-08-04 06:24:59,228 - father_agent.py:386 - Step: 255, Training loss: 2.7805933952331543
2025-08-04 06:25:01,235 - father_agent.py:386 - Step: 260, Training loss: 3.1824448108673096
2025-08-04 06:25:03,172 - father_agent.py:386 - Step: 265, Training loss: 2.2838754653930664
2025-08-04 06:25:05,078 - father_agent.py:386 - Step: 270, Training loss: 2.260918378829956
2025-08-04 06:25:06,992 - father_agent.py:386 - Step: 275, Training loss: 2.5654938220977783
2025-08-04 06:25:08,903 - father_agent.py:386 - Step: 280, Training loss: 3.0029184818267822
2025-08-04 06:25:10,828 - father_agent.py:386 - Step: 285, Training loss: 3.158234119415283
2025-08-04 06:25:12,742 - father_agent.py:386 - Step: 290, Training loss: 2.7632462978363037
2025-08-04 06:25:14,654 - father_agent.py:386 - Step: 295, Training loss: 3.5590291023254395
2025-08-04 06:25:16,564 - father_agent.py:386 - Step: 300, Training loss: 2.5413870811462402
2025-08-04 06:25:16,821 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:25:16,824 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:25:26,672 - evaluation_results_class.py:131 - Average Return = -178.0708465576172
2025-08-04 06:25:26,672 - evaluation_results_class.py:133 - Average Virtual Goal Value = -98.56130981445312
2025-08-04 06:25:26,672 - evaluation_results_class.py:135 - Average Discounted Reward = -74.52425384521484
2025-08-04 06:25:26,673 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9938692098092643
2025-08-04 06:25:26,673 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:25:26,673 - evaluation_results_class.py:141 - Variance of Return = 154078.625
2025-08-04 06:25:26,673 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:25:26,673 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:25:26,673 - evaluation_results_class.py:147 - Average Episode Length = 176.57629427792915
2025-08-04 06:25:26,673 - evaluation_results_class.py:149 - Counted Episodes = 1468
2025-08-04 06:25:26,924 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:25:26,936 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:25:30,980 - father_agent.py:386 - Step: 305, Training loss: 1.5591670274734497
2025-08-04 06:25:32,872 - father_agent.py:386 - Step: 310, Training loss: 2.5316567420959473
2025-08-04 06:25:34,796 - father_agent.py:386 - Step: 315, Training loss: 3.214090347290039
2025-08-04 06:25:36,701 - father_agent.py:386 - Step: 320, Training loss: 3.627784490585327
2025-08-04 06:25:38,594 - father_agent.py:386 - Step: 325, Training loss: 2.759213924407959
2025-08-04 06:25:40,509 - father_agent.py:386 - Step: 330, Training loss: 3.442984104156494
2025-08-04 06:25:42,399 - father_agent.py:386 - Step: 335, Training loss: 3.4324843883514404
2025-08-04 06:25:44,320 - father_agent.py:386 - Step: 340, Training loss: 2.8490467071533203
2025-08-04 06:25:46,228 - father_agent.py:386 - Step: 345, Training loss: 3.0354840755462646
2025-08-04 06:25:48,139 - father_agent.py:386 - Step: 350, Training loss: 3.459573268890381
2025-08-04 06:25:50,044 - father_agent.py:386 - Step: 355, Training loss: 3.195974349975586
2025-08-04 06:25:51,941 - father_agent.py:386 - Step: 360, Training loss: 2.51020884513855
2025-08-04 06:25:53,828 - father_agent.py:386 - Step: 365, Training loss: 2.7237179279327393
2025-08-04 06:25:55,712 - father_agent.py:386 - Step: 370, Training loss: 2.868081569671631
2025-08-04 06:25:57,599 - father_agent.py:386 - Step: 375, Training loss: 4.24500036239624
2025-08-04 06:25:59,498 - father_agent.py:386 - Step: 380, Training loss: 3.009331703186035
2025-08-04 06:26:01,409 - father_agent.py:386 - Step: 385, Training loss: 2.8028793334960938
2025-08-04 06:26:03,312 - father_agent.py:386 - Step: 390, Training loss: 2.778421640396118
2025-08-04 06:26:05,225 - father_agent.py:386 - Step: 395, Training loss: 3.012941360473633
2025-08-04 06:26:07,138 - father_agent.py:386 - Step: 400, Training loss: 2.989490509033203
2025-08-04 06:26:07,398 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:26:07,402 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:26:17,439 - evaluation_results_class.py:131 - Average Return = -178.00350952148438
2025-08-04 06:26:17,439 - evaluation_results_class.py:133 - Average Virtual Goal Value = -98.00350189208984
2025-08-04 06:26:17,439 - evaluation_results_class.py:135 - Average Discounted Reward = -74.7693862915039
2025-08-04 06:26:17,439 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 06:26:17,439 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:26:17,439 - evaluation_results_class.py:141 - Variance of Return = 102654.203125
2025-08-04 06:26:17,439 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:26:17,439 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:26:17,439 - evaluation_results_class.py:147 - Average Episode Length = 115.52705332814325
2025-08-04 06:26:17,439 - evaluation_results_class.py:149 - Counted Episodes = 2569
2025-08-04 06:26:17,693 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:26:17,706 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:26:24,702 - father_agent.py:386 - Step: 405, Training loss: 2.0830078125
2025-08-04 06:26:26,618 - father_agent.py:386 - Step: 410, Training loss: 3.108144998550415
2025-08-04 06:26:28,518 - father_agent.py:386 - Step: 415, Training loss: 2.8911917209625244
2025-08-04 06:26:30,433 - father_agent.py:386 - Step: 420, Training loss: 3.0638082027435303
2025-08-04 06:26:32,330 - father_agent.py:386 - Step: 425, Training loss: 3.090127468109131
2025-08-04 06:26:34,230 - father_agent.py:386 - Step: 430, Training loss: 2.482969045639038
2025-08-04 06:26:36,140 - father_agent.py:386 - Step: 435, Training loss: 2.6336703300476074
2025-08-04 06:26:38,043 - father_agent.py:386 - Step: 440, Training loss: 2.0458273887634277
2025-08-04 06:26:39,927 - father_agent.py:386 - Step: 445, Training loss: 3.0258352756500244
2025-08-04 06:26:41,843 - father_agent.py:386 - Step: 450, Training loss: 3.5502004623413086
2025-08-04 06:26:43,730 - father_agent.py:386 - Step: 455, Training loss: 3.3962109088897705
2025-08-04 06:26:45,599 - father_agent.py:386 - Step: 460, Training loss: 3.627748727798462
2025-08-04 06:26:47,472 - father_agent.py:386 - Step: 465, Training loss: 2.7488465309143066
2025-08-04 06:26:49,347 - father_agent.py:386 - Step: 470, Training loss: 3.2191052436828613
2025-08-04 06:26:51,279 - father_agent.py:386 - Step: 475, Training loss: 3.0930025577545166
2025-08-04 06:26:53,169 - father_agent.py:386 - Step: 480, Training loss: 3.4826865196228027
2025-08-04 06:26:55,088 - father_agent.py:386 - Step: 485, Training loss: 4.603338241577148
2025-08-04 06:26:57,012 - father_agent.py:386 - Step: 490, Training loss: 2.357307195663452
2025-08-04 06:26:58,942 - father_agent.py:386 - Step: 495, Training loss: 2.766770362854004
2025-08-04 06:27:00,893 - father_agent.py:386 - Step: 500, Training loss: 2.310080051422119
2025-08-04 06:27:01,137 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:27:01,140 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:27:11,259 - evaluation_results_class.py:131 - Average Return = -341.0430603027344
2025-08-04 06:27:11,259 - evaluation_results_class.py:133 - Average Virtual Goal Value = -261.5127258300781
2025-08-04 06:27:11,259 - evaluation_results_class.py:135 - Average Discounted Reward = -119.1946029663086
2025-08-04 06:27:11,259 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9941291585127201
2025-08-04 06:27:11,259 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:27:11,259 - evaluation_results_class.py:141 - Variance of Return = 637063.625
2025-08-04 06:27:11,260 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:27:11,260 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:27:11,260 - evaluation_results_class.py:147 - Average Episode Length = 172.0515329419439
2025-08-04 06:27:11,260 - evaluation_results_class.py:149 - Counted Episodes = 1533
2025-08-04 06:27:11,505 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:27:11,516 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:27:11,634 - father_agent.py:547 - Training finished.
2025-08-04 06:27:11,788 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:27:11,790 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:27:11,793 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 06:27:22,435 - evaluation_results_class.py:131 - Average Return = -307.4197998046875
2025-08-04 06:27:22,435 - evaluation_results_class.py:133 - Average Virtual Goal Value = -228.06671142578125
2025-08-04 06:27:22,435 - evaluation_results_class.py:135 - Average Discounted Reward = -109.2421875
2025-08-04 06:27:22,435 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9919137466307277
2025-08-04 06:27:22,435 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:27:22,436 - evaluation_results_class.py:141 - Variance of Return = 538605.3125
2025-08-04 06:27:22,436 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:27:22,436 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:27:22,436 - evaluation_results_class.py:147 - Average Episode Length = 173.82614555256066
2025-08-04 06:27:22,436 - evaluation_results_class.py:149 - Counted Episodes = 1484
2025-08-04 06:27:22,807 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:27:22,811 - self_interpretable_extractor.py:286 - True
2025-08-04 06:27:22,829 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:27:40,999 - evaluation_results_class.py:131 - Average Return = -409.79364013671875
2025-08-04 06:27:41,000 - evaluation_results_class.py:133 - Average Virtual Goal Value = -331.11639404296875
2025-08-04 06:27:41,000 - evaluation_results_class.py:135 - Average Discounted Reward = -131.2557830810547
2025-08-04 06:27:41,000 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9834656084656085
2025-08-04 06:27:41,000 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:27:41,000 - evaluation_results_class.py:141 - Variance of Return = 793332.8125
2025-08-04 06:27:41,000 - evaluation_results_class.py:143 - Current Best Return = -409.79364013671875
2025-08-04 06:27:41,000 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9834656084656085
2025-08-04 06:27:41,000 - evaluation_results_class.py:147 - Average Episode Length = 195.29232804232805
2025-08-04 06:27:41,000 - evaluation_results_class.py:149 - Counted Episodes = 1512
2025-08-04 06:27:41,000 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 06:27:41,000 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 7451 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 651, 139, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
Buffer 1
2025-08-04 06:28:53,177 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 14809 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 651, 139, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
Buffer 2
2025-08-04 06:30:05,554 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 22133 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 651, 139, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
All trajectories collected
2025-08-04 06:31:17,816 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 06:31:17,817 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 22133 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_5.dot.
Learned FSC of size 2
2025-08-04 06:31:27,472 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 06:31:43,523 - evaluation_results_class.py:131 - Average Return = -616.4118041992188
2025-08-04 06:31:43,523 - evaluation_results_class.py:133 - Average Virtual Goal Value = -537.0706787109375
2025-08-04 06:31:43,523 - evaluation_results_class.py:135 - Average Discounted Reward = -203.97727966308594
2025-08-04 06:31:43,523 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9917638984214139
2025-08-04 06:31:43,523 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:31:43,523 - evaluation_results_class.py:141 - Variance of Return = 956254.8125
2025-08-04 06:31:43,523 - evaluation_results_class.py:143 - Current Best Return = -616.4118041992188
2025-08-04 06:31:43,523 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9917638984214139
2025-08-04 06:31:43,523 - evaluation_results_class.py:147 - Average Episode Length = 200.38091969800962
2025-08-04 06:31:43,523 - evaluation_results_class.py:149 - Counted Episodes = 1457
FSC Result: {'best_episode_return': -537.0707, 'best_return': -616.4118, 'goal_value': 0.0, 'returns_episodic': [-537.0707], 'returns': [-616.4118], 'reach_probs': [0.9917638984214139], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.9917638984214139, 'losses': [], 'best_updated': True, 'each_episode_variance': [956254.8], 'each_episode_virtual_variance': [960503.2], 'combined_variance': [3833463.8], 'num_episodes': [1457], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [200.38091969800962], 'counted_episodes': [1457], 'discounted_rewards': [-203.97728], 'new_pomdp_iteration_numbers': []}
[[6, 7, 8, 9], [1], [2], [3], [4], [5], [1], [1], [2], [2], [3], [3], [4], [4], [5], [5], [6, 7, 8, 9], [6, 7, 8, 9], [1], [2], [3], [4], [5], [6, 7, 8, 9], [0]]
2025-08-04 06:31:43,619 - statistic.py:67 - synthesis initiated, design space: 11664
2025-08-04 06:31:43,619 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 06:31:43,657 - synthesizer_ar.py:122 - value 643.3054 achieved after 3322.54 seconds
2025-08-04 06:31:43,671 - synthesizer_ar.py:122 - value 643.3163 achieved after 3322.55 seconds
2025-08-04 06:31:43,703 - synthesizer_ar.py:122 - value 654.2942 achieved after 3322.58 seconds
2025-08-04 06:31:43,717 - synthesizer_ar.py:122 - value 654.3051 achieved after 3322.6 seconds
2025-08-04 06:31:43,775 - synthesizer_ar.py:122 - value 662.2745 achieved after 3322.66 seconds
2025-08-04 06:31:43,790 - synthesizer_ar.py:122 - value 662.2853 achieved after 3322.67 seconds
2025-08-04 06:31:43,815 - synthesizer_ar.py:122 - value 662.6997 achieved after 3322.7 seconds
2025-08-04 06:31:43,829 - synthesizer_ar.py:122 - value 662.7105 achieved after 3322.71 seconds
2025-08-04 06:31:43,866 - synthesizer_ar.py:122 - value 733.2462 achieved after 3322.75 seconds
2025-08-04 06:31:43,880 - synthesizer_ar.py:122 - value 733.257 achieved after 3322.76 seconds
2025-08-04 06:31:43,895 - synthesizer_ar.py:122 - value 744.235 achieved after 3322.77 seconds
2025-08-04 06:31:43,908 - synthesizer_ar.py:122 - value 744.2458 achieved after 3322.79 seconds
2025-08-04 06:31:43,956 - synthesizer_ar.py:122 - value 752.2152 achieved after 3322.84 seconds
2025-08-04 06:31:43,969 - synthesizer_ar.py:122 - value 752.2261 achieved after 3322.85 seconds
2025-08-04 06:31:43,980 - synthesizer_ar.py:122 - value 752.6404 achieved after 3322.86 seconds
2025-08-04 06:31:43,994 - synthesizer_ar.py:122 - value 752.6512 achieved after 3322.87 seconds
2025-08-04 06:31:44,078 - synthesizer_ar.py:122 - value 784.8384 achieved after 3322.96 seconds
2025-08-04 06:31:44,095 - synthesizer_ar.py:122 - value 784.8492 achieved after 3322.98 seconds
2025-08-04 06:31:44,116 - synthesizer_ar.py:122 - value 792.8186 achieved after 3323.0 seconds
2025-08-04 06:31:44,130 - synthesizer_ar.py:122 - value 792.8294 achieved after 3323.01 seconds
2025-08-04 06:31:44,141 - synthesizer_ar.py:122 - value 793.2438 achieved after 3323.02 seconds
2025-08-04 06:31:44,156 - synthesizer_ar.py:122 - value 793.2546 achieved after 3323.04 seconds
2025-08-04 06:31:44,214 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 06:31:44,214 - synthesizer.py:193 - o1x=1, o1y=2, o2x=1, o2y=4, o3x=3, o3y=2, o4x=3, o4y=6, o5x=8, o5y=5
2025-08-04 06:31:44,215 - synthesizer.py:198 - double-checking specification satisfiability:  : 793.2546178429669
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 8) & (y = 8))] 

method: AR, synthesis time: 0.59 s
number of holes: 10, family size: 11664, quotient: 1232 states / 1448 actions
explored: 100 %
MDP stats: avg MDP size: 1008, iterations: 274

optimum: 793.254618
--------------------
2025-08-04 06:31:44,215 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=4, o3x=3, o3y=2, o4x=3, o4y=6, o5x=8, o5y=5
2025-08-04 06:31:44,224 - robust_rl_trainer.py:432 - Iteration 7 of pure RL loop
2025-08-04 06:31:44,262 - storm_vec_env.py:70 - Computing row map
2025-08-04 06:31:44,269 - storm_vec_env.py:97 - Computing transitions
2025-08-04 06:31:44,284 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 06:31:44,284 - storm_vec_env.py:114 - Computing sinks
2025-08-04 06:31:44,284 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 06:31:44,289 - storm_vec_env.py:143 - Computing labels
2025-08-04 06:31:44,289 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 06:31:44,290 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 06:31:44,290 - storm_vec_env.py:175 - Computing observations
2025-08-04 06:31:44,470 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 06:31:44,471 - father_agent.py:540 - Before training evaluation.
2025-08-04 06:31:44,637 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:31:44,640 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:31:55,442 - evaluation_results_class.py:131 - Average Return = -368.1172790527344
2025-08-04 06:31:55,442 - evaluation_results_class.py:133 - Average Virtual Goal Value = -288.3694763183594
2025-08-04 06:31:55,442 - evaluation_results_class.py:135 - Average Discounted Reward = -180.93247985839844
2025-08-04 06:31:55,442 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9968474148802018
2025-08-04 06:31:55,443 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:31:55,443 - evaluation_results_class.py:141 - Variance of Return = 411842.09375
2025-08-04 06:31:55,443 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:31:55,443 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:31:55,443 - evaluation_results_class.py:147 - Average Episode Length = 167.69987389659522
2025-08-04 06:31:55,443 - evaluation_results_class.py:149 - Counted Episodes = 1586
2025-08-04 06:31:55,705 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:31:55,718 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:31:55,834 - father_agent.py:436 - Training agent on-policy
2025-08-04 06:32:04,257 - father_agent.py:386 - Step: 0, Training loss: 6.306576251983643
2025-08-04 06:32:06,288 - father_agent.py:386 - Step: 5, Training loss: 5.247657775878906
2025-08-04 06:32:08,229 - father_agent.py:386 - Step: 10, Training loss: 5.304285049438477
2025-08-04 06:32:10,183 - father_agent.py:386 - Step: 15, Training loss: 4.007203102111816
2025-08-04 06:32:12,109 - father_agent.py:386 - Step: 20, Training loss: 3.377537965774536
2025-08-04 06:32:14,064 - father_agent.py:386 - Step: 25, Training loss: 4.044365406036377
2025-08-04 06:32:15,995 - father_agent.py:386 - Step: 30, Training loss: 4.6132917404174805
2025-08-04 06:32:17,955 - father_agent.py:386 - Step: 35, Training loss: 5.5369791984558105
2025-08-04 06:32:19,907 - father_agent.py:386 - Step: 40, Training loss: 4.426538944244385
2025-08-04 06:32:21,837 - father_agent.py:386 - Step: 45, Training loss: 4.8888044357299805
2025-08-04 06:32:23,816 - father_agent.py:386 - Step: 50, Training loss: 3.9638566970825195
2025-08-04 06:32:25,792 - father_agent.py:386 - Step: 55, Training loss: 4.862960338592529
2025-08-04 06:32:27,724 - father_agent.py:386 - Step: 60, Training loss: 3.6655778884887695
2025-08-04 06:32:29,646 - father_agent.py:386 - Step: 65, Training loss: 4.331271171569824
2025-08-04 06:32:31,622 - father_agent.py:386 - Step: 70, Training loss: 4.103673934936523
2025-08-04 06:32:33,575 - father_agent.py:386 - Step: 75, Training loss: 3.40673565864563
2025-08-04 06:32:35,517 - father_agent.py:386 - Step: 80, Training loss: 2.5710062980651855
2025-08-04 06:32:37,472 - father_agent.py:386 - Step: 85, Training loss: 4.418276309967041
2025-08-04 06:32:39,434 - father_agent.py:386 - Step: 90, Training loss: 3.2360589504241943
2025-08-04 06:32:41,378 - father_agent.py:386 - Step: 95, Training loss: 3.3730404376983643
2025-08-04 06:32:43,312 - father_agent.py:386 - Step: 100, Training loss: 2.898056745529175
2025-08-04 06:32:43,576 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:32:43,579 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:32:54,343 - evaluation_results_class.py:131 - Average Return = -356.70489501953125
2025-08-04 06:32:54,344 - evaluation_results_class.py:133 - Average Virtual Goal Value = -276.70489501953125
2025-08-04 06:32:54,344 - evaluation_results_class.py:135 - Average Discounted Reward = -151.0916748046875
2025-08-04 06:32:54,344 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 06:32:54,344 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:32:54,344 - evaluation_results_class.py:141 - Variance of Return = 174139.375
2025-08-04 06:32:54,344 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:32:54,344 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:32:54,344 - evaluation_results_class.py:147 - Average Episode Length = 128.8632626568585
2025-08-04 06:32:54,344 - evaluation_results_class.py:149 - Counted Episodes = 2311
2025-08-04 06:32:54,613 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:32:54,626 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:33:01,986 - father_agent.py:386 - Step: 105, Training loss: 2.3539822101593018
2025-08-04 06:33:03,935 - father_agent.py:386 - Step: 110, Training loss: 2.7999823093414307
2025-08-04 06:33:05,888 - father_agent.py:386 - Step: 115, Training loss: 5.366123199462891
2025-08-04 06:33:07,832 - father_agent.py:386 - Step: 120, Training loss: 4.115611553192139
2025-08-04 06:33:09,755 - father_agent.py:386 - Step: 125, Training loss: 2.9175260066986084
2025-08-04 06:33:11,685 - father_agent.py:386 - Step: 130, Training loss: 2.463230848312378
2025-08-04 06:33:13,630 - father_agent.py:386 - Step: 135, Training loss: 3.135735034942627
2025-08-04 06:33:15,566 - father_agent.py:386 - Step: 140, Training loss: 2.4800782203674316
2025-08-04 06:33:17,504 - father_agent.py:386 - Step: 145, Training loss: 3.4799249172210693
2025-08-04 06:33:19,451 - father_agent.py:386 - Step: 150, Training loss: 2.9929862022399902
2025-08-04 06:33:21,399 - father_agent.py:386 - Step: 155, Training loss: 2.9831297397613525
2025-08-04 06:33:23,366 - father_agent.py:386 - Step: 160, Training loss: 2.646099805831909
2025-08-04 06:33:25,358 - father_agent.py:386 - Step: 165, Training loss: 2.9890921115875244
2025-08-04 06:33:27,322 - father_agent.py:386 - Step: 170, Training loss: 3.3377060890197754
2025-08-04 06:33:29,297 - father_agent.py:386 - Step: 175, Training loss: 2.7951533794403076
2025-08-04 06:33:31,255 - father_agent.py:386 - Step: 180, Training loss: 2.36064076423645
2025-08-04 06:33:33,214 - father_agent.py:386 - Step: 185, Training loss: 2.3749358654022217
2025-08-04 06:33:35,185 - father_agent.py:386 - Step: 190, Training loss: 3.2385783195495605
2025-08-04 06:33:37,137 - father_agent.py:386 - Step: 195, Training loss: 3.257692337036133
2025-08-04 06:33:39,065 - father_agent.py:386 - Step: 200, Training loss: 2.800631046295166
2025-08-04 06:33:39,336 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:33:39,339 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:33:51,790 - evaluation_results_class.py:131 - Average Return = -696.9189453125
2025-08-04 06:33:51,791 - evaluation_results_class.py:133 - Average Virtual Goal Value = -616.9189453125
2025-08-04 06:33:51,791 - evaluation_results_class.py:135 - Average Discounted Reward = -250.66876220703125
2025-08-04 06:33:51,791 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 06:33:51,791 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:33:51,791 - evaluation_results_class.py:141 - Variance of Return = 391703.6875
2025-08-04 06:33:51,791 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:33:51,791 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:33:51,791 - evaluation_results_class.py:147 - Average Episode Length = 171.4410011918951
2025-08-04 06:33:51,791 - evaluation_results_class.py:149 - Counted Episodes = 1678
2025-08-04 06:33:52,046 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:33:52,056 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:33:56,197 - father_agent.py:386 - Step: 205, Training loss: 3.0209035873413086
2025-08-04 06:33:58,120 - father_agent.py:386 - Step: 210, Training loss: 3.2599070072174072
2025-08-04 06:34:00,032 - father_agent.py:386 - Step: 215, Training loss: 2.9842214584350586
2025-08-04 06:34:01,953 - father_agent.py:386 - Step: 220, Training loss: 3.5509119033813477
2025-08-04 06:34:03,844 - father_agent.py:386 - Step: 225, Training loss: 3.0329089164733887
2025-08-04 06:34:05,757 - father_agent.py:386 - Step: 230, Training loss: 2.7986209392547607
2025-08-04 06:34:07,672 - father_agent.py:386 - Step: 235, Training loss: 2.539236545562744
2025-08-04 06:34:09,597 - father_agent.py:386 - Step: 240, Training loss: 2.6268556118011475
2025-08-04 06:34:11,533 - father_agent.py:386 - Step: 245, Training loss: 3.6354923248291016
2025-08-04 06:34:13,448 - father_agent.py:386 - Step: 250, Training loss: 3.0585954189300537
2025-08-04 06:34:15,353 - father_agent.py:386 - Step: 255, Training loss: 4.043916702270508
2025-08-04 06:34:17,287 - father_agent.py:386 - Step: 260, Training loss: 3.2954587936401367
2025-08-04 06:34:19,230 - father_agent.py:386 - Step: 265, Training loss: 3.1200830936431885
2025-08-04 06:34:21,204 - father_agent.py:386 - Step: 270, Training loss: 3.458191394805908
2025-08-04 06:34:23,131 - father_agent.py:386 - Step: 275, Training loss: 2.4658071994781494
2025-08-04 06:34:25,045 - father_agent.py:386 - Step: 280, Training loss: 2.517910957336426
2025-08-04 06:34:26,954 - father_agent.py:386 - Step: 285, Training loss: 2.2603280544281006
2025-08-04 06:34:28,889 - father_agent.py:386 - Step: 290, Training loss: 2.944383144378662
2025-08-04 06:34:30,821 - father_agent.py:386 - Step: 295, Training loss: 2.442885160446167
2025-08-04 06:34:32,751 - father_agent.py:386 - Step: 300, Training loss: 2.4197466373443604
2025-08-04 06:34:33,009 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:34:33,012 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:34:43,859 - evaluation_results_class.py:131 - Average Return = -538.32080078125
2025-08-04 06:34:43,860 - evaluation_results_class.py:133 - Average Virtual Goal Value = -458.3208312988281
2025-08-04 06:34:43,860 - evaluation_results_class.py:135 - Average Discounted Reward = -210.60096740722656
2025-08-04 06:34:43,860 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 06:34:43,860 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:34:43,860 - evaluation_results_class.py:141 - Variance of Return = 235295.03125
2025-08-04 06:34:43,860 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:34:43,860 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:34:43,860 - evaluation_results_class.py:147 - Average Episode Length = 145.396879570941
2025-08-04 06:34:43,860 - evaluation_results_class.py:149 - Counted Episodes = 2051
2025-08-04 06:34:44,133 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:34:44,145 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:34:48,266 - father_agent.py:386 - Step: 305, Training loss: 2.399122714996338
2025-08-04 06:34:50,200 - father_agent.py:386 - Step: 310, Training loss: 2.9359054565429688
2025-08-04 06:34:52,140 - father_agent.py:386 - Step: 315, Training loss: 2.9338955879211426
2025-08-04 06:34:54,124 - father_agent.py:386 - Step: 320, Training loss: 2.8860442638397217
2025-08-04 06:34:56,082 - father_agent.py:386 - Step: 325, Training loss: 2.9393484592437744
2025-08-04 06:34:58,022 - father_agent.py:386 - Step: 330, Training loss: 2.3816936016082764
2025-08-04 06:35:00,022 - father_agent.py:386 - Step: 335, Training loss: 2.9392457008361816
2025-08-04 06:35:02,121 - father_agent.py:386 - Step: 340, Training loss: 2.3981449604034424
2025-08-04 06:35:04,219 - father_agent.py:386 - Step: 345, Training loss: 2.295818567276001
2025-08-04 06:35:06,329 - father_agent.py:386 - Step: 350, Training loss: 2.696608304977417
2025-08-04 06:35:08,440 - father_agent.py:386 - Step: 355, Training loss: 2.6601853370666504
2025-08-04 06:35:10,568 - father_agent.py:386 - Step: 360, Training loss: 3.2745189666748047
2025-08-04 06:35:12,591 - father_agent.py:386 - Step: 365, Training loss: 3.0317211151123047
2025-08-04 06:35:14,604 - father_agent.py:386 - Step: 370, Training loss: 2.240746021270752
2025-08-04 06:35:16,598 - father_agent.py:386 - Step: 375, Training loss: 2.7738397121429443
2025-08-04 06:35:18,586 - father_agent.py:386 - Step: 380, Training loss: 2.339210271835327
2025-08-04 06:35:20,596 - father_agent.py:386 - Step: 385, Training loss: 2.6740217208862305
2025-08-04 06:35:22,633 - father_agent.py:386 - Step: 390, Training loss: 3.339839458465576
2025-08-04 06:35:24,717 - father_agent.py:386 - Step: 395, Training loss: 2.8771820068359375
2025-08-04 06:35:26,797 - father_agent.py:386 - Step: 400, Training loss: 3.0200107097625732
2025-08-04 06:35:27,174 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:35:27,177 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:35:38,080 - evaluation_results_class.py:131 - Average Return = -928.3860473632812
2025-08-04 06:35:38,081 - evaluation_results_class.py:133 - Average Virtual Goal Value = -848.3860473632812
2025-08-04 06:35:38,081 - evaluation_results_class.py:135 - Average Discounted Reward = -279.72235107421875
2025-08-04 06:35:38,081 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 06:35:38,081 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:35:38,081 - evaluation_results_class.py:141 - Variance of Return = 743800.9375
2025-08-04 06:35:38,081 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:35:38,082 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:35:38,082 - evaluation_results_class.py:147 - Average Episode Length = 214.7196554424432
2025-08-04 06:35:38,082 - evaluation_results_class.py:149 - Counted Episodes = 1277
2025-08-04 06:35:38,356 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:35:38,368 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:35:42,980 - father_agent.py:386 - Step: 405, Training loss: 1.7761036157608032
2025-08-04 06:35:44,996 - father_agent.py:386 - Step: 410, Training loss: 3.0184144973754883
2025-08-04 06:35:47,009 - father_agent.py:386 - Step: 415, Training loss: 5.8965744972229
2025-08-04 06:35:49,032 - father_agent.py:386 - Step: 420, Training loss: 3.139925479888916
2025-08-04 06:35:51,062 - father_agent.py:386 - Step: 425, Training loss: 3.5868163108825684
2025-08-04 06:35:53,114 - father_agent.py:386 - Step: 430, Training loss: 3.0300076007843018
2025-08-04 06:35:55,120 - father_agent.py:386 - Step: 435, Training loss: 2.4940805435180664
2025-08-04 06:35:57,137 - father_agent.py:386 - Step: 440, Training loss: 2.2228281497955322
2025-08-04 06:35:59,144 - father_agent.py:386 - Step: 445, Training loss: 2.3068931102752686
2025-08-04 06:36:01,194 - father_agent.py:386 - Step: 450, Training loss: 2.3855607509613037
2025-08-04 06:36:03,240 - father_agent.py:386 - Step: 455, Training loss: 3.727864980697632
2025-08-04 06:36:05,290 - father_agent.py:386 - Step: 460, Training loss: 3.197092056274414
2025-08-04 06:36:07,282 - father_agent.py:386 - Step: 465, Training loss: 3.0024795532226562
2025-08-04 06:36:09,299 - father_agent.py:386 - Step: 470, Training loss: 2.799459934234619
2025-08-04 06:36:11,282 - father_agent.py:386 - Step: 475, Training loss: 2.5725021362304688
2025-08-04 06:36:13,236 - father_agent.py:386 - Step: 480, Training loss: 2.8211426734924316
2025-08-04 06:36:15,299 - father_agent.py:386 - Step: 485, Training loss: 2.421739339828491
2025-08-04 06:36:17,394 - father_agent.py:386 - Step: 490, Training loss: 2.955615758895874
2025-08-04 06:36:19,520 - father_agent.py:386 - Step: 495, Training loss: 2.8332011699676514
2025-08-04 06:36:21,683 - father_agent.py:386 - Step: 500, Training loss: 2.7973532676696777
2025-08-04 06:36:22,025 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:36:22,028 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:36:33,407 - evaluation_results_class.py:131 - Average Return = -637.735107421875
2025-08-04 06:36:33,407 - evaluation_results_class.py:133 - Average Virtual Goal Value = -557.735107421875
2025-08-04 06:36:33,407 - evaluation_results_class.py:135 - Average Discounted Reward = -216.01837158203125
2025-08-04 06:36:33,407 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 06:36:33,407 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:36:33,407 - evaluation_results_class.py:141 - Variance of Return = 474516.15625
2025-08-04 06:36:33,407 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:36:33,407 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:36:33,407 - evaluation_results_class.py:147 - Average Episode Length = 182.21891191709844
2025-08-04 06:36:33,407 - evaluation_results_class.py:149 - Counted Episodes = 1544
2025-08-04 06:36:33,673 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:36:33,685 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:36:33,801 - father_agent.py:547 - Training finished.
2025-08-04 06:36:33,957 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:36:33,959 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:36:33,962 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 06:36:44,920 - evaluation_results_class.py:131 - Average Return = -660.6353149414062
2025-08-04 06:36:44,920 - evaluation_results_class.py:133 - Average Virtual Goal Value = -580.6353149414062
2025-08-04 06:36:44,920 - evaluation_results_class.py:135 - Average Discounted Reward = -223.9340057373047
2025-08-04 06:36:44,920 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 06:36:44,920 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:36:44,920 - evaluation_results_class.py:141 - Variance of Return = 488365.4375
2025-08-04 06:36:44,921 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:36:44,921 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:36:44,921 - evaluation_results_class.py:147 - Average Episode Length = 182.489941596366
2025-08-04 06:36:44,921 - evaluation_results_class.py:149 - Counted Episodes = 1541
2025-08-04 06:36:45,195 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:36:45,197 - self_interpretable_extractor.py:286 - True
2025-08-04 06:36:45,210 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:37:02,384 - evaluation_results_class.py:131 - Average Return = -642.896728515625
2025-08-04 06:37:02,384 - evaluation_results_class.py:133 - Average Virtual Goal Value = -562.896728515625
2025-08-04 06:37:02,384 - evaluation_results_class.py:135 - Average Discounted Reward = -218.75242614746094
2025-08-04 06:37:02,384 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 06:37:02,384 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:37:02,384 - evaluation_results_class.py:141 - Variance of Return = 451659.84375
2025-08-04 06:37:02,384 - evaluation_results_class.py:143 - Current Best Return = -642.896728515625
2025-08-04 06:37:02,385 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:37:02,385 - evaluation_results_class.py:147 - Average Episode Length = 182.42792281498296
2025-08-04 06:37:02,385 - evaluation_results_class.py:149 - Counted Episodes = 1762
2025-08-04 06:37:02,385 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 06:37:02,385 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 8236 trajectories
Learned trajectory lengths  {259, 133, 265, 139, 271, 145, 277, 151, 157, 289, 163, 169, 175, 49, 181, 55, 187, 61, 193, 67, 199, 73, 205, 79, 211, 85, 217, 91, 223, 97, 229, 103, 235, 109, 241, 115, 247, 121, 253, 127}
Buffer 1
2025-08-04 06:38:14,301 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 16462 trajectories
Learned trajectory lengths  {259, 133, 265, 139, 271, 145, 277, 151, 283, 157, 289, 163, 295, 169, 301, 175, 49, 181, 55, 187, 61, 319, 193, 67, 199, 73, 205, 79, 211, 85, 217, 91, 223, 97, 229, 103, 235, 109, 241, 115, 247, 121, 253, 127}
Buffer 2
2025-08-04 06:39:26,344 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 24692 trajectories
Learned trajectory lengths  {259, 133, 265, 139, 271, 145, 277, 151, 283, 157, 289, 163, 295, 169, 301, 175, 49, 307, 181, 55, 187, 61, 319, 193, 67, 199, 73, 331, 205, 79, 211, 85, 217, 91, 223, 97, 229, 103, 235, 109, 241, 115, 247, 121, 253, 127}
All trajectories collected
2025-08-04 06:40:38,950 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 06:40:38,950 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 24692 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_6.dot.
Learned FSC of size 2
2025-08-04 06:40:41,785 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 06:40:57,762 - evaluation_results_class.py:131 - Average Return = -614.1401977539062
2025-08-04 06:40:57,762 - evaluation_results_class.py:133 - Average Virtual Goal Value = -534.1401977539062
2025-08-04 06:40:57,762 - evaluation_results_class.py:135 - Average Discounted Reward = -226.98135375976562
2025-08-04 06:40:57,762 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 06:40:57,762 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:40:57,762 - evaluation_results_class.py:141 - Variance of Return = 383636.25
2025-08-04 06:40:57,762 - evaluation_results_class.py:143 - Current Best Return = -614.1401977539062
2025-08-04 06:40:57,762 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:40:57,763 - evaluation_results_class.py:147 - Average Episode Length = 177.37543453070683
2025-08-04 06:40:57,763 - evaluation_results_class.py:149 - Counted Episodes = 1726
FSC Result: {'best_episode_return': -534.1402, 'best_return': -614.1402, 'goal_value': 0.0, 'returns_episodic': [-534.1402], 'returns': [-614.1402], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [383636.25], 'each_episode_virtual_variance': [383636.25], 'combined_variance': [1534545.0], 'num_episodes': [1726], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [177.37543453070683], 'counted_episodes': [1726], 'discounted_rewards': [-226.98135], 'new_pomdp_iteration_numbers': []}
[[6, 7, 8, 9], [1], [2], [3], [4], [5], [1], [1], [2], [2], [3], [3], [4], [4], [5], [5], [6, 7, 8, 9], [6, 7, 8, 9], [1], [2], [3], [4], [5], [6, 7, 8, 9], [0]]
2025-08-04 06:40:57,863 - statistic.py:67 - synthesis initiated, design space: 11664
2025-08-04 06:40:57,863 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 06:40:57,898 - synthesizer_ar.py:122 - value 657.6374 achieved after 3876.78 seconds
2025-08-04 06:40:57,904 - synthesizer_ar.py:122 - value 658.8999 achieved after 3876.78 seconds
2025-08-04 06:40:57,911 - synthesizer_ar.py:122 - value 664.1769 achieved after 3876.79 seconds
2025-08-04 06:40:57,926 - synthesizer_ar.py:122 - value 664.2037 achieved after 3876.81 seconds
2025-08-04 06:40:57,943 - synthesizer_ar.py:122 - value 666.7627 achieved after 3876.82 seconds
2025-08-04 06:40:57,945 - synthesizer_ar.py:122 - value 668.0251 achieved after 3876.83 seconds
2025-08-04 06:40:57,946 - synthesizer_ar.py:122 - value 673.3021 achieved after 3876.83 seconds
2025-08-04 06:40:57,957 - synthesizer_ar.py:122 - value 674.1805 achieved after 3876.84 seconds
2025-08-04 06:40:57,973 - synthesizer_ar.py:122 - value 730.5728 achieved after 3876.85 seconds
2025-08-04 06:40:57,979 - synthesizer_ar.py:122 - value 731.8352 achieved after 3876.86 seconds
2025-08-04 06:40:57,985 - synthesizer_ar.py:122 - value 737.1122 achieved after 3876.87 seconds
2025-08-04 06:40:58,001 - synthesizer_ar.py:122 - value 737.139 achieved after 3876.88 seconds
2025-08-04 06:40:58,018 - synthesizer_ar.py:122 - value 739.698 achieved after 3876.9 seconds
2025-08-04 06:40:58,019 - synthesizer_ar.py:122 - value 740.9604 achieved after 3876.9 seconds
2025-08-04 06:40:58,021 - synthesizer_ar.py:122 - value 746.2374 achieved after 3876.9 seconds
2025-08-04 06:40:58,032 - synthesizer_ar.py:122 - value 747.1158 achieved after 3876.91 seconds
2025-08-04 06:40:58,104 - synthesizer_ar.py:122 - value 748.7524 achieved after 3876.98 seconds
2025-08-04 06:40:58,106 - synthesizer_ar.py:122 - value 750.0148 achieved after 3876.99 seconds
2025-08-04 06:40:58,107 - synthesizer_ar.py:122 - value 755.2918 achieved after 3876.99 seconds
2025-08-04 06:40:58,118 - synthesizer_ar.py:122 - value 756.1702 achieved after 3877.0 seconds
2025-08-04 06:40:58,171 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 06:40:58,171 - synthesizer.py:193 - o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
2025-08-04 06:40:58,172 - synthesizer.py:198 - double-checking specification satisfiability:  : 756.1701781831465
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 8) & (y = 8))] 

method: AR, synthesis time: 0.31 s
number of holes: 10, family size: 11664, quotient: 1232 states / 1448 actions
explored: 100 %
MDP stats: avg MDP size: 1005, iterations: 167

optimum: 756.170178
--------------------
2025-08-04 06:40:58,172 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
2025-08-04 06:40:58,181 - robust_rl_trainer.py:432 - Iteration 8 of pure RL loop
2025-08-04 06:40:58,218 - storm_vec_env.py:70 - Computing row map
2025-08-04 06:40:58,225 - storm_vec_env.py:97 - Computing transitions
2025-08-04 06:40:58,240 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 06:40:58,240 - storm_vec_env.py:114 - Computing sinks
2025-08-04 06:40:58,240 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 06:40:58,245 - storm_vec_env.py:143 - Computing labels
2025-08-04 06:40:58,246 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 06:40:58,246 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 06:40:58,246 - storm_vec_env.py:175 - Computing observations
2025-08-04 06:40:58,418 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 06:40:58,419 - father_agent.py:540 - Before training evaluation.
2025-08-04 06:40:58,581 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:40:58,584 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:41:09,314 - evaluation_results_class.py:131 - Average Return = -739.1237182617188
2025-08-04 06:41:09,315 - evaluation_results_class.py:133 - Average Virtual Goal Value = -659.1237182617188
2025-08-04 06:41:09,315 - evaluation_results_class.py:135 - Average Discounted Reward = -290.0628967285156
2025-08-04 06:41:09,315 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 06:41:09,315 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:41:09,315 - evaluation_results_class.py:141 - Variance of Return = 485758.59375
2025-08-04 06:41:09,315 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:41:09,315 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:41:09,315 - evaluation_results_class.py:147 - Average Episode Length = 181.61269430051814
2025-08-04 06:41:09,315 - evaluation_results_class.py:149 - Counted Episodes = 1544
2025-08-04 06:41:09,574 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:41:09,586 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:41:09,705 - father_agent.py:436 - Training agent on-policy
2025-08-04 06:41:18,133 - father_agent.py:386 - Step: 0, Training loss: 2.6499714851379395
2025-08-04 06:41:20,082 - father_agent.py:386 - Step: 5, Training loss: 5.0809760093688965
2025-08-04 06:41:22,014 - father_agent.py:386 - Step: 10, Training loss: 4.372861862182617
2025-08-04 06:41:23,969 - father_agent.py:386 - Step: 15, Training loss: 4.455509662628174
2025-08-04 06:41:25,917 - father_agent.py:386 - Step: 20, Training loss: 3.628080368041992
2025-08-04 06:41:27,860 - father_agent.py:386 - Step: 25, Training loss: 4.1170830726623535
2025-08-04 06:41:29,805 - father_agent.py:386 - Step: 30, Training loss: 4.214115142822266
2025-08-04 06:41:31,742 - father_agent.py:386 - Step: 35, Training loss: 3.8861823081970215
2025-08-04 06:41:33,696 - father_agent.py:386 - Step: 40, Training loss: 4.554192066192627
2025-08-04 06:41:35,632 - father_agent.py:386 - Step: 45, Training loss: 4.162882328033447
2025-08-04 06:41:37,561 - father_agent.py:386 - Step: 50, Training loss: 3.6361823081970215
2025-08-04 06:41:39,501 - father_agent.py:386 - Step: 55, Training loss: 3.6061487197875977
2025-08-04 06:41:41,429 - father_agent.py:386 - Step: 60, Training loss: 3.4349067211151123
2025-08-04 06:41:43,390 - father_agent.py:386 - Step: 65, Training loss: 3.6819450855255127
2025-08-04 06:41:45,335 - father_agent.py:386 - Step: 70, Training loss: 4.269874095916748
2025-08-04 06:41:47,282 - father_agent.py:386 - Step: 75, Training loss: 3.3487963676452637
2025-08-04 06:41:49,234 - father_agent.py:386 - Step: 80, Training loss: 3.148735761642456
2025-08-04 06:41:51,171 - father_agent.py:386 - Step: 85, Training loss: 3.3536806106567383
2025-08-04 06:41:53,117 - father_agent.py:386 - Step: 90, Training loss: 3.6306586265563965
2025-08-04 06:41:55,074 - father_agent.py:386 - Step: 95, Training loss: 3.459899425506592
2025-08-04 06:41:56,999 - father_agent.py:386 - Step: 100, Training loss: 3.962453842163086
2025-08-04 06:41:57,264 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:41:57,267 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:42:08,722 - evaluation_results_class.py:131 - Average Return = -1312.57080078125
2025-08-04 06:42:08,723 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1245.8846435546875
2025-08-04 06:42:08,723 - evaluation_results_class.py:135 - Average Discounted Reward = -336.3995361328125
2025-08-04 06:42:08,723 - evaluation_results_class.py:137 - Goal Reach Probability = 0.8335766423357664
2025-08-04 06:42:08,723 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:42:08,723 - evaluation_results_class.py:141 - Variance of Return = 3736301.75
2025-08-04 06:42:08,723 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:42:08,723 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:42:08,723 - evaluation_results_class.py:147 - Average Episode Length = 357.3094890510949
2025-08-04 06:42:08,723 - evaluation_results_class.py:149 - Counted Episodes = 685
2025-08-04 06:42:08,984 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:42:08,996 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:42:16,430 - father_agent.py:386 - Step: 105, Training loss: 3.8875207901000977
2025-08-04 06:42:18,425 - father_agent.py:386 - Step: 110, Training loss: 2.142296314239502
2025-08-04 06:42:20,493 - father_agent.py:386 - Step: 115, Training loss: 5.215323448181152
2025-08-04 06:42:22,475 - father_agent.py:386 - Step: 120, Training loss: 4.336369514465332
2025-08-04 06:42:24,441 - father_agent.py:386 - Step: 125, Training loss: 3.5687928199768066
2025-08-04 06:42:26,432 - father_agent.py:386 - Step: 130, Training loss: 3.1085152626037598
2025-08-04 06:42:28,444 - father_agent.py:386 - Step: 135, Training loss: 2.2228004932403564
2025-08-04 06:42:30,473 - father_agent.py:386 - Step: 140, Training loss: 3.109621047973633
2025-08-04 06:42:32,616 - father_agent.py:386 - Step: 145, Training loss: 2.7032017707824707
2025-08-04 06:42:34,720 - father_agent.py:386 - Step: 150, Training loss: 1.7573686838150024
2025-08-04 06:42:36,759 - father_agent.py:386 - Step: 155, Training loss: 1.616333246231079
2025-08-04 06:42:38,810 - father_agent.py:386 - Step: 160, Training loss: 2.61625599861145
2025-08-04 06:42:40,948 - father_agent.py:386 - Step: 165, Training loss: 2.6076390743255615
2025-08-04 06:42:43,115 - father_agent.py:386 - Step: 170, Training loss: 2.6039440631866455
2025-08-04 06:42:45,234 - father_agent.py:386 - Step: 175, Training loss: 3.972346544265747
2025-08-04 06:42:47,388 - father_agent.py:386 - Step: 180, Training loss: 2.695416212081909
2025-08-04 06:42:49,511 - father_agent.py:386 - Step: 185, Training loss: 2.9705464839935303
2025-08-04 06:42:51,647 - father_agent.py:386 - Step: 190, Training loss: 2.580756902694702
2025-08-04 06:42:53,789 - father_agent.py:386 - Step: 195, Training loss: 2.697824239730835
2025-08-04 06:42:55,976 - father_agent.py:386 - Step: 200, Training loss: 2.8245882987976074
2025-08-04 06:42:56,345 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:42:56,349 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:43:07,900 - evaluation_results_class.py:131 - Average Return = -123.15214538574219
2025-08-04 06:43:07,900 - evaluation_results_class.py:133 - Average Virtual Goal Value = -43.54478454589844
2025-08-04 06:43:07,901 - evaluation_results_class.py:135 - Average Discounted Reward = -53.911258697509766
2025-08-04 06:43:07,901 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9950920245398773
2025-08-04 06:43:07,901 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:43:07,901 - evaluation_results_class.py:141 - Variance of Return = 68752.1171875
2025-08-04 06:43:07,901 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:43:07,901 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:43:07,901 - evaluation_results_class.py:147 - Average Episode Length = 156.06257668711658
2025-08-04 06:43:07,901 - evaluation_results_class.py:149 - Counted Episodes = 1630
2025-08-04 06:43:08,168 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:43:08,180 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:43:12,724 - father_agent.py:386 - Step: 205, Training loss: 1.4079102277755737
2025-08-04 06:43:14,827 - father_agent.py:386 - Step: 210, Training loss: 2.247417449951172
2025-08-04 06:43:16,970 - father_agent.py:386 - Step: 215, Training loss: 3.27091908454895
2025-08-04 06:43:19,050 - father_agent.py:386 - Step: 220, Training loss: 3.535022735595703
2025-08-04 06:43:21,164 - father_agent.py:386 - Step: 225, Training loss: 3.004117250442505
2025-08-04 06:43:23,305 - father_agent.py:386 - Step: 230, Training loss: 2.607685089111328
2025-08-04 06:43:25,462 - father_agent.py:386 - Step: 235, Training loss: 2.295959234237671
2025-08-04 06:43:27,593 - father_agent.py:386 - Step: 240, Training loss: 2.6591594219207764
2025-08-04 06:43:29,706 - father_agent.py:386 - Step: 245, Training loss: 2.301877498626709
2025-08-04 06:43:31,818 - father_agent.py:386 - Step: 250, Training loss: 3.175391674041748
2025-08-04 06:43:33,793 - father_agent.py:386 - Step: 255, Training loss: 3.143864631652832
2025-08-04 06:43:35,768 - father_agent.py:386 - Step: 260, Training loss: 3.000398635864258
2025-08-04 06:43:37,750 - father_agent.py:386 - Step: 265, Training loss: 3.1108951568603516
2025-08-04 06:43:39,756 - father_agent.py:386 - Step: 270, Training loss: 2.5374252796173096
2025-08-04 06:43:41,838 - father_agent.py:386 - Step: 275, Training loss: 2.9050285816192627
2025-08-04 06:43:43,830 - father_agent.py:386 - Step: 280, Training loss: 2.263007879257202
2025-08-04 06:43:45,815 - father_agent.py:386 - Step: 285, Training loss: 2.862431764602661
2025-08-04 06:43:47,809 - father_agent.py:386 - Step: 290, Training loss: 3.3505659103393555
2025-08-04 06:43:49,811 - father_agent.py:386 - Step: 295, Training loss: 3.4547977447509766
2025-08-04 06:43:51,876 - father_agent.py:386 - Step: 300, Training loss: 3.495133876800537
2025-08-04 06:43:52,172 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:43:52,176 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:44:03,714 - evaluation_results_class.py:131 - Average Return = -174.69529724121094
2025-08-04 06:44:03,714 - evaluation_results_class.py:133 - Average Virtual Goal Value = -94.76838684082031
2025-08-04 06:44:03,714 - evaluation_results_class.py:135 - Average Discounted Reward = -76.9610595703125
2025-08-04 06:44:03,714 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9990863407948835
2025-08-04 06:44:03,714 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:44:03,714 - evaluation_results_class.py:141 - Variance of Return = 92222.234375
2025-08-04 06:44:03,714 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:44:03,714 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:44:03,714 - evaluation_results_class.py:147 - Average Episode Length = 129.09776153494747
2025-08-04 06:44:03,714 - evaluation_results_class.py:149 - Counted Episodes = 2189
2025-08-04 06:44:03,994 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:44:04,006 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:44:08,199 - father_agent.py:386 - Step: 305, Training loss: 3.0464062690734863
2025-08-04 06:44:10,201 - father_agent.py:386 - Step: 310, Training loss: 3.469757556915283
2025-08-04 06:44:12,192 - father_agent.py:386 - Step: 315, Training loss: 3.8373641967773438
2025-08-04 06:44:14,200 - father_agent.py:386 - Step: 320, Training loss: 2.7125961780548096
2025-08-04 06:44:16,205 - father_agent.py:386 - Step: 325, Training loss: 2.7685840129852295
2025-08-04 06:44:18,209 - father_agent.py:386 - Step: 330, Training loss: 2.1922080516815186
2025-08-04 06:44:20,229 - father_agent.py:386 - Step: 335, Training loss: 2.562770366668701
2025-08-04 06:44:22,214 - father_agent.py:386 - Step: 340, Training loss: 3.232098340988159
2025-08-04 06:44:24,210 - father_agent.py:386 - Step: 345, Training loss: 3.0082786083221436
2025-08-04 06:44:26,166 - father_agent.py:386 - Step: 350, Training loss: 3.5401828289031982
2025-08-04 06:44:28,143 - father_agent.py:386 - Step: 355, Training loss: 2.925603151321411
2025-08-04 06:44:30,163 - father_agent.py:386 - Step: 360, Training loss: 3.536914587020874
2025-08-04 06:44:32,146 - father_agent.py:386 - Step: 365, Training loss: 2.8099873065948486
2025-08-04 06:44:34,104 - father_agent.py:386 - Step: 370, Training loss: 2.915147542953491
2025-08-04 06:44:36,073 - father_agent.py:386 - Step: 375, Training loss: 2.4067957401275635
2025-08-04 06:44:38,061 - father_agent.py:386 - Step: 380, Training loss: 3.4612741470336914
2025-08-04 06:44:40,024 - father_agent.py:386 - Step: 385, Training loss: 2.8734281063079834
2025-08-04 06:44:41,958 - father_agent.py:386 - Step: 390, Training loss: 2.637073516845703
2025-08-04 06:44:43,873 - father_agent.py:386 - Step: 395, Training loss: 2.687495231628418
2025-08-04 06:44:45,799 - father_agent.py:386 - Step: 400, Training loss: 2.5851263999938965
2025-08-04 06:44:46,085 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:44:46,088 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:44:56,693 - evaluation_results_class.py:131 - Average Return = -169.47735595703125
2025-08-04 06:44:56,693 - evaluation_results_class.py:133 - Average Virtual Goal Value = -89.51589965820312
2025-08-04 06:44:56,693 - evaluation_results_class.py:135 - Average Discounted Reward = -72.37262725830078
2025-08-04 06:44:56,693 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9995183044315993
2025-08-04 06:44:56,693 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:44:56,693 - evaluation_results_class.py:141 - Variance of Return = 146845.078125
2025-08-04 06:44:56,693 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:44:56,693 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:44:56,693 - evaluation_results_class.py:147 - Average Episode Length = 135.60693641618496
2025-08-04 06:44:56,693 - evaluation_results_class.py:149 - Counted Episodes = 2076
2025-08-04 06:44:56,971 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:44:56,984 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:45:01,129 - father_agent.py:386 - Step: 405, Training loss: 1.6561026573181152
2025-08-04 06:45:03,071 - father_agent.py:386 - Step: 410, Training loss: 2.579211711883545
2025-08-04 06:45:04,990 - father_agent.py:386 - Step: 415, Training loss: 2.398768424987793
2025-08-04 06:45:06,909 - father_agent.py:386 - Step: 420, Training loss: 3.7649612426757812
2025-08-04 06:45:08,822 - father_agent.py:386 - Step: 425, Training loss: 2.99684476852417
2025-08-04 06:45:10,764 - father_agent.py:386 - Step: 430, Training loss: 3.465611457824707
2025-08-04 06:45:12,691 - father_agent.py:386 - Step: 435, Training loss: 1.8534172773361206
2025-08-04 06:45:14,609 - father_agent.py:386 - Step: 440, Training loss: 2.8928656578063965
2025-08-04 06:45:16,547 - father_agent.py:386 - Step: 445, Training loss: 2.881655693054199
2025-08-04 06:45:18,467 - father_agent.py:386 - Step: 450, Training loss: 2.7552273273468018
2025-08-04 06:45:20,416 - father_agent.py:386 - Step: 455, Training loss: 2.786261796951294
2025-08-04 06:45:22,356 - father_agent.py:386 - Step: 460, Training loss: 2.946444034576416
2025-08-04 06:45:24,277 - father_agent.py:386 - Step: 465, Training loss: 2.8071682453155518
2025-08-04 06:45:26,207 - father_agent.py:386 - Step: 470, Training loss: 2.8243913650512695
2025-08-04 06:45:28,144 - father_agent.py:386 - Step: 475, Training loss: 2.772853374481201
2025-08-04 06:45:30,158 - father_agent.py:386 - Step: 480, Training loss: 2.4454615116119385
2025-08-04 06:45:32,193 - father_agent.py:386 - Step: 485, Training loss: 2.4930639266967773
2025-08-04 06:45:34,198 - father_agent.py:386 - Step: 490, Training loss: 3.669114828109741
2025-08-04 06:45:36,139 - father_agent.py:386 - Step: 495, Training loss: 1.9524846076965332
2025-08-04 06:45:38,049 - father_agent.py:386 - Step: 500, Training loss: 2.994488000869751
2025-08-04 06:45:38,324 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:45:38,326 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:45:49,253 - evaluation_results_class.py:131 - Average Return = -371.2537841796875
2025-08-04 06:45:49,254 - evaluation_results_class.py:133 - Average Virtual Goal Value = -291.6219482421875
2025-08-04 06:45:49,254 - evaluation_results_class.py:135 - Average Discounted Reward = -137.4110107421875
2025-08-04 06:45:49,254 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9953977646285339
2025-08-04 06:45:49,254 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:45:49,254 - evaluation_results_class.py:141 - Variance of Return = 559809.875
2025-08-04 06:45:49,254 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:45:49,254 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:45:49,254 - evaluation_results_class.py:147 - Average Episode Length = 174.85798816568047
2025-08-04 06:45:49,254 - evaluation_results_class.py:149 - Counted Episodes = 1521
2025-08-04 06:45:49,532 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:45:49,544 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:45:49,660 - father_agent.py:547 - Training finished.
2025-08-04 06:45:49,819 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:45:49,821 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:45:49,824 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 06:46:00,571 - evaluation_results_class.py:131 - Average Return = -373.306884765625
2025-08-04 06:46:00,571 - evaluation_results_class.py:133 - Average Virtual Goal Value = -293.6163635253906
2025-08-04 06:46:00,571 - evaluation_results_class.py:135 - Average Discounted Reward = -138.32675170898438
2025-08-04 06:46:00,571 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9961315280464217
2025-08-04 06:46:00,571 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:46:00,571 - evaluation_results_class.py:141 - Variance of Return = 555714.75
2025-08-04 06:46:00,571 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:46:00,571 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:46:00,571 - evaluation_results_class.py:147 - Average Episode Length = 172.6234687298517
2025-08-04 06:46:00,572 - evaluation_results_class.py:149 - Counted Episodes = 1551
2025-08-04 06:46:00,844 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:46:00,846 - self_interpretable_extractor.py:286 - True
2025-08-04 06:46:00,859 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:46:18,132 - evaluation_results_class.py:131 - Average Return = -453.9092102050781
2025-08-04 06:46:18,133 - evaluation_results_class.py:133 - Average Virtual Goal Value = -374.763671875
2025-08-04 06:46:18,133 - evaluation_results_class.py:135 - Average Discounted Reward = -151.6955108642578
2025-08-04 06:46:18,133 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9893190921228304
2025-08-04 06:46:18,133 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:46:18,133 - evaluation_results_class.py:141 - Variance of Return = 873490.6875
2025-08-04 06:46:18,133 - evaluation_results_class.py:143 - Current Best Return = -453.9092102050781
2025-08-04 06:46:18,133 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9893190921228304
2025-08-04 06:46:18,133 - evaluation_results_class.py:147 - Average Episode Length = 198.5460614152203
2025-08-04 06:46:18,133 - evaluation_results_class.py:149 - Counted Episodes = 1498
2025-08-04 06:46:18,133 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 06:46:18,133 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 7373 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 139, 651, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
Buffer 1
2025-08-04 06:47:30,061 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 14793 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 139, 651, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
Buffer 2
2025-08-04 06:48:42,301 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 22137 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 139, 651, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
All trajectories collected
2025-08-04 06:49:54,968 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 06:49:54,968 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 22137 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_7.dot.
Learned FSC of size 2
2025-08-04 06:50:05,395 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 06:50:21,750 - evaluation_results_class.py:131 - Average Return = -600.7933959960938
2025-08-04 06:50:21,750 - evaluation_results_class.py:133 - Average Virtual Goal Value = -521.1572265625
2025-08-04 06:50:21,750 - evaluation_results_class.py:135 - Average Discounted Reward = -211.15049743652344
2025-08-04 06:50:21,750 - evaluation_results_class.py:137 - Goal Reach Probability = 0.99545159194282
2025-08-04 06:50:21,750 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:50:21,750 - evaluation_results_class.py:141 - Variance of Return = 740900.9375
2025-08-04 06:50:21,750 - evaluation_results_class.py:143 - Current Best Return = -600.7933959960938
2025-08-04 06:50:21,751 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.99545159194282
2025-08-04 06:50:21,751 - evaluation_results_class.py:147 - Average Episode Length = 193.75503573749188
2025-08-04 06:50:21,751 - evaluation_results_class.py:149 - Counted Episodes = 1539
FSC Result: {'best_episode_return': -521.1572, 'best_return': -600.7934, 'goal_value': 0.0, 'returns_episodic': [-521.1572], 'returns': [-600.7934], 'reach_probs': [0.99545159194282], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.99545159194282, 'losses': [], 'best_updated': True, 'each_episode_variance': [740900.94], 'each_episode_virtual_variance': [742453.75], 'combined_variance': [2966680.2], 'num_episodes': [1539], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [193.75503573749188], 'counted_episodes': [1539], 'discounted_rewards': [-211.1505], 'new_pomdp_iteration_numbers': []}
[[6, 7, 8, 9], [1], [2], [3], [4], [5], [1], [1], [2], [2], [3], [3], [4], [4], [5], [5], [6, 7, 8, 9], [6, 7, 8, 9], [1], [2], [3], [4], [5], [6, 7, 8, 9], [0]]
2025-08-04 06:50:21,846 - statistic.py:67 - synthesis initiated, design space: 11664
2025-08-04 06:50:21,846 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 06:50:21,883 - synthesizer_ar.py:122 - value 637.7507 achieved after 4440.76 seconds
2025-08-04 06:50:21,896 - synthesizer_ar.py:122 - value 637.8018 achieved after 4440.78 seconds
2025-08-04 06:50:21,922 - synthesizer_ar.py:122 - value 640.6591 achieved after 4440.8 seconds
2025-08-04 06:50:21,937 - synthesizer_ar.py:122 - value 640.7102 achieved after 4440.82 seconds
2025-08-04 06:50:21,966 - synthesizer_ar.py:122 - value 652.3421 achieved after 4440.85 seconds
2025-08-04 06:50:21,979 - synthesizer_ar.py:122 - value 652.3932 achieved after 4440.86 seconds
2025-08-04 06:50:22,007 - synthesizer_ar.py:122 - value 655.2505 achieved after 4440.89 seconds
2025-08-04 06:50:22,020 - synthesizer_ar.py:122 - value 655.3016 achieved after 4440.9 seconds
2025-08-04 06:50:22,081 - synthesizer_ar.py:122 - value 656.8792 achieved after 4440.96 seconds
2025-08-04 06:50:22,095 - synthesizer_ar.py:122 - value 656.9304 achieved after 4440.98 seconds
2025-08-04 06:50:22,121 - synthesizer_ar.py:122 - value 662.613 achieved after 4441.0 seconds
2025-08-04 06:50:22,134 - synthesizer_ar.py:122 - value 662.6641 achieved after 4441.01 seconds
2025-08-04 06:50:22,172 - synthesizer_ar.py:122 - value 726.9666 achieved after 4441.05 seconds
2025-08-04 06:50:22,185 - synthesizer_ar.py:122 - value 727.0177 achieved after 4441.07 seconds
2025-08-04 06:50:22,196 - synthesizer_ar.py:122 - value 729.875 achieved after 4441.08 seconds
2025-08-04 06:50:22,210 - synthesizer_ar.py:122 - value 729.9261 achieved after 4441.09 seconds
2025-08-04 06:50:22,225 - synthesizer_ar.py:122 - value 741.558 achieved after 4441.1 seconds
2025-08-04 06:50:22,238 - synthesizer_ar.py:122 - value 741.6091 achieved after 4441.12 seconds
2025-08-04 06:50:22,249 - synthesizer_ar.py:122 - value 744.4664 achieved after 4441.13 seconds
2025-08-04 06:50:22,263 - synthesizer_ar.py:122 - value 744.5175 achieved after 4441.14 seconds
2025-08-04 06:50:22,303 - synthesizer_ar.py:122 - value 746.0951 achieved after 4441.18 seconds
2025-08-04 06:50:22,317 - synthesizer_ar.py:122 - value 746.1463 achieved after 4441.2 seconds
2025-08-04 06:50:22,328 - synthesizer_ar.py:122 - value 751.8289 achieved after 4441.21 seconds
2025-08-04 06:50:22,341 - synthesizer_ar.py:122 - value 751.88 achieved after 4441.22 seconds
2025-08-04 06:50:22,423 - synthesizer_ar.py:122 - value 778.6471 achieved after 4441.3 seconds
2025-08-04 06:50:22,436 - synthesizer_ar.py:122 - value 778.6983 achieved after 4441.32 seconds
2025-08-04 06:50:22,453 - synthesizer_ar.py:122 - value 781.5555 achieved after 4441.33 seconds
2025-08-04 06:50:22,466 - synthesizer_ar.py:122 - value 781.6067 achieved after 4441.35 seconds
2025-08-04 06:50:22,486 - synthesizer_ar.py:122 - value 783.1843 achieved after 4441.37 seconds
2025-08-04 06:50:22,499 - synthesizer_ar.py:122 - value 783.2355 achieved after 4441.38 seconds
2025-08-04 06:50:22,510 - synthesizer_ar.py:122 - value 788.9181 achieved after 4441.39 seconds
2025-08-04 06:50:22,526 - synthesizer_ar.py:122 - value 788.9692 achieved after 4441.41 seconds
2025-08-04 06:50:22,575 - synthesizer_ar.py:122 - value 788.9798 achieved after 4441.46 seconds
2025-08-04 06:50:22,589 - synthesizer_ar.py:122 - value 789.031 achieved after 4441.47 seconds
2025-08-04 06:50:22,607 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 06:50:22,607 - synthesizer.py:193 - o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=3, o4y=6, o5x=8, o5y=5
2025-08-04 06:50:22,608 - synthesizer.py:198 - double-checking specification satisfiability:  : 789.0309738108817
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 8) & (y = 8))] 

method: AR, synthesis time: 0.76 s
number of holes: 10, family size: 11664, quotient: 1232 states / 1448 actions
explored: 100 %
MDP stats: avg MDP size: 999, iterations: 344

optimum: 789.030974
--------------------
2025-08-04 06:50:22,608 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=3, o4y=6, o5x=8, o5y=5
2025-08-04 06:50:22,615 - robust_rl_trainer.py:432 - Iteration 9 of pure RL loop
2025-08-04 06:50:22,651 - storm_vec_env.py:70 - Computing row map
2025-08-04 06:50:22,658 - storm_vec_env.py:97 - Computing transitions
2025-08-04 06:50:22,673 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 06:50:22,673 - storm_vec_env.py:114 - Computing sinks
2025-08-04 06:50:22,673 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 06:50:22,678 - storm_vec_env.py:143 - Computing labels
2025-08-04 06:50:22,678 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 06:50:22,679 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 06:50:22,679 - storm_vec_env.py:175 - Computing observations
2025-08-04 06:50:22,852 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 06:50:22,853 - father_agent.py:540 - Before training evaluation.
2025-08-04 06:50:23,029 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:50:23,032 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:50:33,741 - evaluation_results_class.py:131 - Average Return = -455.83184814453125
2025-08-04 06:50:33,741 - evaluation_results_class.py:133 - Average Virtual Goal Value = -376.3240051269531
2025-08-04 06:50:33,742 - evaluation_results_class.py:135 - Average Discounted Reward = -208.5066680908203
2025-08-04 06:50:33,742 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9938482570061518
2025-08-04 06:50:33,742 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:50:33,742 - evaluation_results_class.py:141 - Variance of Return = 677092.25
2025-08-04 06:50:33,742 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:50:33,742 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:50:33,742 - evaluation_results_class.py:147 - Average Episode Length = 178.32057416267943
2025-08-04 06:50:33,742 - evaluation_results_class.py:149 - Counted Episodes = 1463
2025-08-04 06:50:34,012 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:50:34,025 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:50:34,145 - father_agent.py:436 - Training agent on-policy
2025-08-04 06:50:42,716 - father_agent.py:386 - Step: 0, Training loss: 5.296750068664551
2025-08-04 06:50:44,676 - father_agent.py:386 - Step: 5, Training loss: 4.704479694366455
2025-08-04 06:50:46,625 - father_agent.py:386 - Step: 10, Training loss: 2.710440158843994
2025-08-04 06:50:48,561 - father_agent.py:386 - Step: 15, Training loss: 3.23624324798584
2025-08-04 06:50:50,495 - father_agent.py:386 - Step: 20, Training loss: 3.274709939956665
2025-08-04 06:50:52,412 - father_agent.py:386 - Step: 25, Training loss: 3.6064131259918213
2025-08-04 06:50:54,342 - father_agent.py:386 - Step: 30, Training loss: 3.306636095046997
2025-08-04 06:50:56,281 - father_agent.py:386 - Step: 35, Training loss: 3.8008084297180176
2025-08-04 06:50:58,254 - father_agent.py:386 - Step: 40, Training loss: 3.650980234146118
2025-08-04 06:51:00,182 - father_agent.py:386 - Step: 45, Training loss: 4.341536998748779
2025-08-04 06:51:02,110 - father_agent.py:386 - Step: 50, Training loss: 5.7018537521362305
2025-08-04 06:51:04,040 - father_agent.py:386 - Step: 55, Training loss: 4.180350303649902
2025-08-04 06:51:05,983 - father_agent.py:386 - Step: 60, Training loss: 5.247307777404785
2025-08-04 06:51:07,899 - father_agent.py:386 - Step: 65, Training loss: 4.552698612213135
2025-08-04 06:51:09,821 - father_agent.py:386 - Step: 70, Training loss: 3.556204080581665
2025-08-04 06:51:11,733 - father_agent.py:386 - Step: 75, Training loss: 4.004881381988525
2025-08-04 06:51:13,654 - father_agent.py:386 - Step: 80, Training loss: 2.9468607902526855
2025-08-04 06:51:15,577 - father_agent.py:386 - Step: 85, Training loss: 4.755509853363037
2025-08-04 06:51:17,528 - father_agent.py:386 - Step: 90, Training loss: 3.04976224899292
2025-08-04 06:51:19,495 - father_agent.py:386 - Step: 95, Training loss: 3.353724718093872
2025-08-04 06:51:21,455 - father_agent.py:386 - Step: 100, Training loss: 3.1216766834259033
2025-08-04 06:51:21,738 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:51:21,741 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:51:32,499 - evaluation_results_class.py:131 - Average Return = -476.6862487792969
2025-08-04 06:51:32,499 - evaluation_results_class.py:133 - Average Virtual Goal Value = -396.6862487792969
2025-08-04 06:51:32,499 - evaluation_results_class.py:135 - Average Discounted Reward = -187.20327758789062
2025-08-04 06:51:32,499 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 06:51:32,499 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:51:32,499 - evaluation_results_class.py:141 - Variance of Return = 382910.8125
2025-08-04 06:51:32,499 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:51:32,499 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:51:32,499 - evaluation_results_class.py:147 - Average Episode Length = 157.47362637362636
2025-08-04 06:51:32,499 - evaluation_results_class.py:149 - Counted Episodes = 1820
2025-08-04 06:51:32,780 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:51:32,793 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:51:40,313 - father_agent.py:386 - Step: 105, Training loss: 2.2109696865081787
2025-08-04 06:51:42,262 - father_agent.py:386 - Step: 110, Training loss: 2.3809316158294678
2025-08-04 06:51:44,215 - father_agent.py:386 - Step: 115, Training loss: 3.3684372901916504
2025-08-04 06:51:46,157 - father_agent.py:386 - Step: 120, Training loss: 3.0924887657165527
2025-08-04 06:51:48,079 - father_agent.py:386 - Step: 125, Training loss: 2.957622528076172
2025-08-04 06:51:50,042 - father_agent.py:386 - Step: 130, Training loss: 2.7294344902038574
2025-08-04 06:51:51,973 - father_agent.py:386 - Step: 135, Training loss: 2.3979427814483643
2025-08-04 06:51:53,913 - father_agent.py:386 - Step: 140, Training loss: 2.4721314907073975
2025-08-04 06:51:55,858 - father_agent.py:386 - Step: 145, Training loss: 2.3880271911621094
2025-08-04 06:51:57,820 - father_agent.py:386 - Step: 150, Training loss: 2.6478872299194336
2025-08-04 06:51:59,739 - father_agent.py:386 - Step: 155, Training loss: 2.9887475967407227
2025-08-04 06:52:01,654 - father_agent.py:386 - Step: 160, Training loss: 3.092938184738159
2025-08-04 06:52:03,598 - father_agent.py:386 - Step: 165, Training loss: 3.1115458011627197
2025-08-04 06:52:05,546 - father_agent.py:386 - Step: 170, Training loss: 2.489952564239502
2025-08-04 06:52:07,502 - father_agent.py:386 - Step: 175, Training loss: 2.3098621368408203
2025-08-04 06:52:09,481 - father_agent.py:386 - Step: 180, Training loss: 2.065952777862549
2025-08-04 06:52:11,456 - father_agent.py:386 - Step: 185, Training loss: 2.7420601844787598
2025-08-04 06:52:13,456 - father_agent.py:386 - Step: 190, Training loss: 2.661367654800415
2025-08-04 06:52:15,441 - father_agent.py:386 - Step: 195, Training loss: 2.731764554977417
2025-08-04 06:52:17,391 - father_agent.py:386 - Step: 200, Training loss: 3.163701057434082
2025-08-04 06:52:17,676 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:52:17,679 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:52:28,594 - evaluation_results_class.py:131 - Average Return = -635.5347290039062
2025-08-04 06:52:28,595 - evaluation_results_class.py:133 - Average Virtual Goal Value = -555.5347290039062
2025-08-04 06:52:28,595 - evaluation_results_class.py:135 - Average Discounted Reward = -243.6350860595703
2025-08-04 06:52:28,595 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 06:52:28,595 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:52:28,595 - evaluation_results_class.py:141 - Variance of Return = 325844.09375
2025-08-04 06:52:28,595 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:52:28,595 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:52:28,595 - evaluation_results_class.py:147 - Average Episode Length = 153.11925708699903
2025-08-04 06:52:28,595 - evaluation_results_class.py:149 - Counted Episodes = 2046
2025-08-04 06:52:28,883 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:52:28,896 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:52:33,303 - father_agent.py:386 - Step: 205, Training loss: 2.053725481033325
2025-08-04 06:52:35,368 - father_agent.py:386 - Step: 210, Training loss: 2.8253557682037354
2025-08-04 06:52:37,456 - father_agent.py:386 - Step: 215, Training loss: 3.4419617652893066
2025-08-04 06:52:39,568 - father_agent.py:386 - Step: 220, Training loss: 3.014920234680176
2025-08-04 06:52:41,664 - father_agent.py:386 - Step: 225, Training loss: 3.3662638664245605
2025-08-04 06:52:43,730 - father_agent.py:386 - Step: 230, Training loss: 2.693960189819336
2025-08-04 06:52:45,819 - father_agent.py:386 - Step: 235, Training loss: 2.6983439922332764
2025-08-04 06:52:47,865 - father_agent.py:386 - Step: 240, Training loss: 2.341911554336548
2025-08-04 06:52:49,913 - father_agent.py:386 - Step: 245, Training loss: 1.9832217693328857
2025-08-04 06:52:51,955 - father_agent.py:386 - Step: 250, Training loss: 2.4031307697296143
2025-08-04 06:52:54,008 - father_agent.py:386 - Step: 255, Training loss: 2.4226953983306885
2025-08-04 06:52:56,046 - father_agent.py:386 - Step: 260, Training loss: 3.25996732711792
2025-08-04 06:52:58,117 - father_agent.py:386 - Step: 265, Training loss: 2.5554585456848145
2025-08-04 06:53:00,234 - father_agent.py:386 - Step: 270, Training loss: 3.2609546184539795
2025-08-04 06:53:02,300 - father_agent.py:386 - Step: 275, Training loss: 2.179121255874634
2025-08-04 06:53:04,420 - father_agent.py:386 - Step: 280, Training loss: 2.7131824493408203
2025-08-04 06:53:06,567 - father_agent.py:386 - Step: 285, Training loss: 2.2770564556121826
2025-08-04 06:53:08,660 - father_agent.py:386 - Step: 290, Training loss: 2.4733786582946777
2025-08-04 06:53:10,747 - father_agent.py:386 - Step: 295, Training loss: 2.402221918106079
2025-08-04 06:53:12,831 - father_agent.py:386 - Step: 300, Training loss: 2.369391441345215
2025-08-04 06:53:13,116 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:53:13,119 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:53:24,286 - evaluation_results_class.py:131 - Average Return = -1983.37890625
2025-08-04 06:53:24,286 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1922.0767822265625
2025-08-04 06:53:24,286 - evaluation_results_class.py:135 - Average Discounted Reward = -352.2502136230469
2025-08-04 06:53:24,287 - evaluation_results_class.py:137 - Goal Reach Probability = 0.7662771285475793
2025-08-04 06:53:24,287 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:53:24,287 - evaluation_results_class.py:141 - Variance of Return = 4356138.5
2025-08-04 06:53:24,287 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:53:24,287 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:53:24,287 - evaluation_results_class.py:147 - Average Episode Length = 432.7829716193656
2025-08-04 06:53:24,287 - evaluation_results_class.py:149 - Counted Episodes = 599
2025-08-04 06:53:24,572 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:53:24,585 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:53:28,827 - father_agent.py:386 - Step: 305, Training loss: 1.676194667816162
2025-08-04 06:53:30,919 - father_agent.py:386 - Step: 310, Training loss: 2.512361764907837
2025-08-04 06:53:32,975 - father_agent.py:386 - Step: 315, Training loss: 2.582444667816162
2025-08-04 06:53:35,077 - father_agent.py:386 - Step: 320, Training loss: 2.8179430961608887
2025-08-04 06:53:37,120 - father_agent.py:386 - Step: 325, Training loss: 3.7992520332336426
2025-08-04 06:53:39,165 - father_agent.py:386 - Step: 330, Training loss: 2.645080804824829
2025-08-04 06:53:41,207 - father_agent.py:386 - Step: 335, Training loss: 2.790818929672241
2025-08-04 06:53:43,309 - father_agent.py:386 - Step: 340, Training loss: 2.696591377258301
2025-08-04 06:53:45,399 - father_agent.py:386 - Step: 345, Training loss: 2.7087056636810303
2025-08-04 06:53:47,493 - father_agent.py:386 - Step: 350, Training loss: 2.168999195098877
2025-08-04 06:53:49,498 - father_agent.py:386 - Step: 355, Training loss: 3.108738422393799
2025-08-04 06:53:51,493 - father_agent.py:386 - Step: 360, Training loss: 2.703523874282837
2025-08-04 06:53:53,537 - father_agent.py:386 - Step: 365, Training loss: 2.8918983936309814
2025-08-04 06:53:55,577 - father_agent.py:386 - Step: 370, Training loss: 2.1318836212158203
2025-08-04 06:53:57,597 - father_agent.py:386 - Step: 375, Training loss: 2.125915765762329
2025-08-04 06:53:59,548 - father_agent.py:386 - Step: 380, Training loss: 2.438403367996216
2025-08-04 06:54:01,505 - father_agent.py:386 - Step: 385, Training loss: 2.6347246170043945
2025-08-04 06:54:03,449 - father_agent.py:386 - Step: 390, Training loss: 2.6936981678009033
2025-08-04 06:54:05,469 - father_agent.py:386 - Step: 395, Training loss: 2.105302572250366
2025-08-04 06:54:07,498 - father_agent.py:386 - Step: 400, Training loss: 3.1996593475341797
2025-08-04 06:54:07,810 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:54:07,814 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:54:18,388 - evaluation_results_class.py:131 - Average Return = -1399.263427734375
2025-08-04 06:54:18,388 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1326.7161865234375
2025-08-04 06:54:18,388 - evaluation_results_class.py:135 - Average Discounted Reward = -320.42816162109375
2025-08-04 06:54:18,388 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9068413391557496
2025-08-04 06:54:18,388 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:54:18,388 - evaluation_results_class.py:141 - Variance of Return = 2044847.75
2025-08-04 06:54:18,388 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:54:18,388 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:54:18,388 - evaluation_results_class.py:147 - Average Episode Length = 350.72052401746726
2025-08-04 06:54:18,388 - evaluation_results_class.py:149 - Counted Episodes = 687
2025-08-04 06:54:18,676 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:54:18,689 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:54:22,949 - father_agent.py:386 - Step: 405, Training loss: 1.7734880447387695
2025-08-04 06:54:24,899 - father_agent.py:386 - Step: 410, Training loss: 2.7137160301208496
2025-08-04 06:54:26,854 - father_agent.py:386 - Step: 415, Training loss: 2.5297293663024902
2025-08-04 06:54:28,792 - father_agent.py:386 - Step: 420, Training loss: 2.8095414638519287
2025-08-04 06:54:30,775 - father_agent.py:386 - Step: 425, Training loss: 3.3929758071899414
2025-08-04 06:54:32,699 - father_agent.py:386 - Step: 430, Training loss: 3.1768810749053955
2025-08-04 06:54:34,617 - father_agent.py:386 - Step: 435, Training loss: 2.695871591567993
2025-08-04 06:54:36,540 - father_agent.py:386 - Step: 440, Training loss: 2.457157850265503
2025-08-04 06:54:38,462 - father_agent.py:386 - Step: 445, Training loss: 2.3801462650299072
2025-08-04 06:54:40,375 - father_agent.py:386 - Step: 450, Training loss: 2.749817371368408
2025-08-04 06:54:42,346 - father_agent.py:386 - Step: 455, Training loss: 2.8766777515411377
2025-08-04 06:54:44,281 - father_agent.py:386 - Step: 460, Training loss: 3.5531508922576904
2025-08-04 06:54:46,226 - father_agent.py:386 - Step: 465, Training loss: 2.9261701107025146
2025-08-04 06:54:48,172 - father_agent.py:386 - Step: 470, Training loss: 3.0960354804992676
2025-08-04 06:54:50,106 - father_agent.py:386 - Step: 475, Training loss: 2.5525026321411133
2025-08-04 06:54:52,036 - father_agent.py:386 - Step: 480, Training loss: 2.4051859378814697
2025-08-04 06:54:53,978 - father_agent.py:386 - Step: 485, Training loss: 2.62202787399292
2025-08-04 06:54:55,932 - father_agent.py:386 - Step: 490, Training loss: 2.2966597080230713
2025-08-04 06:54:57,863 - father_agent.py:386 - Step: 495, Training loss: 2.2431211471557617
2025-08-04 06:54:59,820 - father_agent.py:386 - Step: 500, Training loss: 2.453308582305908
2025-08-04 06:55:00,114 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:55:00,117 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:55:10,866 - evaluation_results_class.py:131 - Average Return = -1455.485107421875
2025-08-04 06:55:10,866 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1382.8717041015625
2025-08-04 06:55:10,866 - evaluation_results_class.py:135 - Average Discounted Reward = -317.70672607421875
2025-08-04 06:55:10,866 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9076682316118936
2025-08-04 06:55:10,866 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:55:10,866 - evaluation_results_class.py:141 - Variance of Return = 2372748.5
2025-08-04 06:55:10,866 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:55:10,866 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:55:10,867 - evaluation_results_class.py:147 - Average Episode Length = 373.3035993740219
2025-08-04 06:55:10,867 - evaluation_results_class.py:149 - Counted Episodes = 639
2025-08-04 06:55:11,163 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:55:11,176 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:55:11,296 - father_agent.py:547 - Training finished.
2025-08-04 06:55:11,454 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:55:11,457 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:55:11,459 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 06:55:23,932 - evaluation_results_class.py:131 - Average Return = -1522.381591796875
2025-08-04 06:55:23,932 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1452.517333984375
2025-08-04 06:55:23,932 - evaluation_results_class.py:135 - Average Discounted Reward = -306.7095031738281
2025-08-04 06:55:23,933 - evaluation_results_class.py:137 - Goal Reach Probability = 0.8733031674208145
2025-08-04 06:55:23,933 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:55:23,933 - evaluation_results_class.py:141 - Variance of Return = 2666512.75
2025-08-04 06:55:23,933 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 06:55:23,933 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 06:55:23,933 - evaluation_results_class.py:147 - Average Episode Length = 368.5897435897436
2025-08-04 06:55:23,933 - evaluation_results_class.py:149 - Counted Episodes = 663
2025-08-04 06:55:24,199 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:55:24,201 - self_interpretable_extractor.py:286 - True
2025-08-04 06:55:24,213 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:55:41,339 - evaluation_results_class.py:131 - Average Return = -1518.5189208984375
2025-08-04 06:55:41,342 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1449.1781005859375
2025-08-04 06:55:41,342 - evaluation_results_class.py:135 - Average Discounted Reward = -320.3142395019531
2025-08-04 06:55:41,342 - evaluation_results_class.py:137 - Goal Reach Probability = 0.8667601683029453
2025-08-04 06:55:41,342 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:55:41,342 - evaluation_results_class.py:141 - Variance of Return = 2461712.25
2025-08-04 06:55:41,342 - evaluation_results_class.py:143 - Current Best Return = -1518.5189208984375
2025-08-04 06:55:41,342 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.8667601683029453
2025-08-04 06:55:41,342 - evaluation_results_class.py:147 - Average Episode Length = 384.56241234221596
2025-08-04 06:55:41,342 - evaluation_results_class.py:149 - Counted Episodes = 713
2025-08-04 06:55:41,342 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 06:55:41,342 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 3684 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 651, 139, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
Buffer 1
2025-08-04 06:56:53,369 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 7355 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 651, 139, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
Buffer 2
2025-08-04 06:58:06,418 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 11038 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 613, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 651, 139, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
All trajectories collected
2025-08-04 06:59:18,404 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 06:59:18,404 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 11038 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_8.dot.
Learned FSC of size 2
2025-08-04 06:59:35,689 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 06:59:52,029 - evaluation_results_class.py:131 - Average Return = -1546.63916015625
2025-08-04 06:59:52,029 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1476.3734130859375
2025-08-04 06:59:52,029 - evaluation_results_class.py:135 - Average Discounted Reward = -320.1080017089844
2025-08-04 06:59:52,029 - evaluation_results_class.py:137 - Goal Reach Probability = 0.8783216783216783
2025-08-04 06:59:52,029 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 06:59:52,029 - evaluation_results_class.py:141 - Variance of Return = 2374682.5
2025-08-04 06:59:52,029 - evaluation_results_class.py:143 - Current Best Return = -1546.63916015625
2025-08-04 06:59:52,029 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.8783216783216783
2025-08-04 06:59:52,029 - evaluation_results_class.py:147 - Average Episode Length = 387.16783216783216
2025-08-04 06:59:52,029 - evaluation_results_class.py:149 - Counted Episodes = 715
FSC Result: {'best_episode_return': -1476.3734, 'best_return': -1546.6392, 'goal_value': 0.0, 'returns_episodic': [-1476.3734], 'returns': [-1546.6392], 'reach_probs': [0.8783216783216783], 'trap_reach_probs': [0.0], 'best_reach_prob': 0.8783216783216783, 'losses': [], 'best_updated': True, 'each_episode_variance': [2374682.5], 'each_episode_virtual_variance': [2402829.5], 'combined_variance': [9554340.0], 'num_episodes': [715], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [387.16783216783216], 'counted_episodes': [715], 'discounted_rewards': [-320.108], 'new_pomdp_iteration_numbers': []}
[[6, 7, 8, 9], [1], [2], [3], [4], [5], [1], [1], [2], [2], [3], [3], [4], [4], [5], [5], [6, 7, 8, 9], [6, 7, 8, 9], [1], [2], [3], [4], [5], [6, 7, 8, 9], [0]]
2025-08-04 06:59:52,109 - statistic.py:67 - synthesis initiated, design space: 11664
2025-08-04 06:59:52,110 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 06:59:52,148 - synthesizer_ar.py:122 - value 1742.6167 achieved after 5011.03 seconds
2025-08-04 06:59:52,156 - synthesizer_ar.py:122 - value 1742.8859 achieved after 5011.04 seconds
2025-08-04 06:59:52,163 - synthesizer_ar.py:122 - value 1744.8037 achieved after 5011.04 seconds
2025-08-04 06:59:52,181 - synthesizer_ar.py:122 - value 1744.8124 achieved after 5011.06 seconds
2025-08-04 06:59:52,199 - synthesizer_ar.py:122 - value 1751.6685 achieved after 5011.08 seconds
2025-08-04 06:59:52,201 - synthesizer_ar.py:122 - value 1751.9376 achieved after 5011.08 seconds
2025-08-04 06:59:52,203 - synthesizer_ar.py:122 - value 1753.8554 achieved after 5011.08 seconds
2025-08-04 06:59:52,215 - synthesizer_ar.py:122 - value 1754.799 achieved after 5011.1 seconds
2025-08-04 06:59:52,233 - synthesizer_ar.py:122 - value 1815.8626 achieved after 5011.11 seconds
2025-08-04 06:59:52,240 - synthesizer_ar.py:122 - value 1816.1318 achieved after 5011.12 seconds
2025-08-04 06:59:52,247 - synthesizer_ar.py:122 - value 1818.0496 achieved after 5011.13 seconds
2025-08-04 06:59:52,265 - synthesizer_ar.py:122 - value 1818.0583 achieved after 5011.15 seconds
2025-08-04 06:59:52,283 - synthesizer_ar.py:122 - value 1824.9144 achieved after 5011.16 seconds
2025-08-04 06:59:52,285 - synthesizer_ar.py:122 - value 1825.1835 achieved after 5011.17 seconds
2025-08-04 06:59:52,287 - synthesizer_ar.py:122 - value 1827.1013 achieved after 5011.17 seconds
2025-08-04 06:59:52,300 - synthesizer_ar.py:122 - value 1828.0448 achieved after 5011.18 seconds
2025-08-04 06:59:52,368 - synthesizer_ar.py:122 - value 1833.9406 achieved after 5011.25 seconds
2025-08-04 06:59:52,370 - synthesizer_ar.py:122 - value 1834.2097 achieved after 5011.25 seconds
2025-08-04 06:59:52,372 - synthesizer_ar.py:122 - value 1836.1275 achieved after 5011.25 seconds
2025-08-04 06:59:52,385 - synthesizer_ar.py:122 - value 1837.071 achieved after 5011.27 seconds
2025-08-04 06:59:52,433 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 06:59:52,433 - synthesizer.py:193 - o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
2025-08-04 06:59:52,434 - synthesizer.py:198 - double-checking specification satisfiability:  : 1837.0710447767483
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 8) & (y = 8))] 

method: AR, synthesis time: 0.32 s
number of holes: 10, family size: 11664, quotient: 1232 states / 1448 actions
explored: 100 %
MDP stats: avg MDP size: 1005, iterations: 151

optimum: 1837.071045
--------------------
2025-08-04 06:59:52,435 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=2, o1y=1, o2x=2, o2y=4, o3x=3, o3y=1, o4x=5, o4y=6, o5x=8, o5y=5
2025-08-04 06:59:52,443 - robust_rl_trainer.py:432 - Iteration 10 of pure RL loop
2025-08-04 06:59:52,480 - storm_vec_env.py:70 - Computing row map
2025-08-04 06:59:52,487 - storm_vec_env.py:97 - Computing transitions
2025-08-04 06:59:52,502 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 06:59:52,502 - storm_vec_env.py:114 - Computing sinks
2025-08-04 06:59:52,502 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 06:59:52,507 - storm_vec_env.py:143 - Computing labels
2025-08-04 06:59:52,507 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 06:59:52,507 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 06:59:52,507 - storm_vec_env.py:175 - Computing observations
2025-08-04 06:59:52,684 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 06:59:52,685 - father_agent.py:540 - Before training evaluation.
2025-08-04 06:59:52,861 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 06:59:52,864 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:00:03,764 - evaluation_results_class.py:131 - Average Return = -1590.2047119140625
2025-08-04 07:00:03,764 - evaluation_results_class.py:133 - Average Virtual Goal Value = -1519.9837646484375
2025-08-04 07:00:03,764 - evaluation_results_class.py:135 - Average Discounted Reward = -379.6455383300781
2025-08-04 07:00:03,764 - evaluation_results_class.py:137 - Goal Reach Probability = 0.8777614138438881
2025-08-04 07:00:03,764 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 07:00:03,764 - evaluation_results_class.py:141 - Variance of Return = 2699256.0
2025-08-04 07:00:03,764 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 07:00:03,764 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 07:00:03,764 - evaluation_results_class.py:147 - Average Episode Length = 373.1266568483063
2025-08-04 07:00:03,764 - evaluation_results_class.py:149 - Counted Episodes = 679
2025-08-04 07:00:04,045 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:00:04,059 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:00:04,178 - father_agent.py:436 - Training agent on-policy
2025-08-04 07:00:12,727 - father_agent.py:386 - Step: 0, Training loss: 3.103874921798706
2025-08-04 07:00:14,729 - father_agent.py:386 - Step: 5, Training loss: 4.754324436187744
2025-08-04 07:00:16,720 - father_agent.py:386 - Step: 10, Training loss: 4.364187240600586
2025-08-04 07:00:18,722 - father_agent.py:386 - Step: 15, Training loss: 4.085613250732422
2025-08-04 07:00:20,750 - father_agent.py:386 - Step: 20, Training loss: 3.7412631511688232
2025-08-04 07:00:22,748 - father_agent.py:386 - Step: 25, Training loss: 4.028572082519531
2025-08-04 07:00:24,757 - father_agent.py:386 - Step: 30, Training loss: 4.224595546722412
2025-08-04 07:00:26,821 - father_agent.py:386 - Step: 35, Training loss: 4.0806565284729
2025-08-04 07:00:28,818 - father_agent.py:386 - Step: 40, Training loss: 3.8014423847198486
2025-08-04 07:00:30,770 - father_agent.py:386 - Step: 45, Training loss: 3.162777900695801
2025-08-04 07:00:32,689 - father_agent.py:386 - Step: 50, Training loss: 2.91119384765625
2025-08-04 07:00:34,597 - father_agent.py:386 - Step: 55, Training loss: 2.2585601806640625
2025-08-04 07:00:36,519 - father_agent.py:386 - Step: 60, Training loss: 1.3198044300079346
2025-08-04 07:00:38,452 - father_agent.py:386 - Step: 65, Training loss: 0.8923126459121704
2025-08-04 07:00:40,385 - father_agent.py:386 - Step: 70, Training loss: 0.7427567839622498
2025-08-04 07:00:42,324 - father_agent.py:386 - Step: 75, Training loss: 0.10271242260932922
2025-08-04 07:00:44,249 - father_agent.py:386 - Step: 80, Training loss: 0.061182890087366104
2025-08-04 07:00:46,177 - father_agent.py:386 - Step: 85, Training loss: 1.2887672185897827
2025-08-04 07:00:48,105 - father_agent.py:386 - Step: 90, Training loss: 4.169142723083496
2025-08-04 07:00:50,041 - father_agent.py:386 - Step: 95, Training loss: 7.071200847625732
2025-08-04 07:00:51,968 - father_agent.py:386 - Step: 100, Training loss: 5.389435291290283
2025-08-04 07:00:52,255 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:00:52,258 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:01:02,928 - evaluation_results_class.py:131 - Average Return = -520.2759399414062
2025-08-04 07:01:02,930 - evaluation_results_class.py:133 - Average Virtual Goal Value = -465.557861328125
2025-08-04 07:01:02,930 - evaluation_results_class.py:135 - Average Discounted Reward = -119.50330352783203
2025-08-04 07:01:02,930 - evaluation_results_class.py:137 - Goal Reach Probability = 0.6839762611275965
2025-08-04 07:01:02,931 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 07:01:02,931 - evaluation_results_class.py:141 - Variance of Return = 2617232.25
2025-08-04 07:01:02,931 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 07:01:02,931 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 07:01:02,931 - evaluation_results_class.py:147 - Average Episode Length = 361.3353115727003
2025-08-04 07:01:02,931 - evaluation_results_class.py:149 - Counted Episodes = 674
2025-08-04 07:01:03,219 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:01:03,232 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:01:10,794 - father_agent.py:386 - Step: 105, Training loss: 3.4980356693267822
2025-08-04 07:01:12,712 - father_agent.py:386 - Step: 110, Training loss: 2.933694839477539
2025-08-04 07:01:14,639 - father_agent.py:386 - Step: 115, Training loss: 2.544525384902954
2025-08-04 07:01:16,560 - father_agent.py:386 - Step: 120, Training loss: 2.5784807205200195
2025-08-04 07:01:18,500 - father_agent.py:386 - Step: 125, Training loss: 3.6789968013763428
2025-08-04 07:01:20,442 - father_agent.py:386 - Step: 130, Training loss: 2.274683713912964
2025-08-04 07:01:22,369 - father_agent.py:386 - Step: 135, Training loss: 1.5101333856582642
2025-08-04 07:01:24,286 - father_agent.py:386 - Step: 140, Training loss: 1.5116440057754517
2025-08-04 07:01:26,226 - father_agent.py:386 - Step: 145, Training loss: 3.335744619369507
2025-08-04 07:01:28,159 - father_agent.py:386 - Step: 150, Training loss: 2.421180248260498
2025-08-04 07:01:30,090 - father_agent.py:386 - Step: 155, Training loss: 3.1813488006591797
2025-08-04 07:01:31,992 - father_agent.py:386 - Step: 160, Training loss: 3.3205442428588867
2025-08-04 07:01:33,926 - father_agent.py:386 - Step: 165, Training loss: 1.973427176475525
2025-08-04 07:01:35,849 - father_agent.py:386 - Step: 170, Training loss: 2.363349437713623
2025-08-04 07:01:37,762 - father_agent.py:386 - Step: 175, Training loss: 3.2426419258117676
2025-08-04 07:01:39,675 - father_agent.py:386 - Step: 180, Training loss: 3.023942470550537
2025-08-04 07:01:41,614 - father_agent.py:386 - Step: 185, Training loss: 3.1204721927642822
2025-08-04 07:01:43,546 - father_agent.py:386 - Step: 190, Training loss: 2.7092628479003906
2025-08-04 07:01:45,466 - father_agent.py:386 - Step: 195, Training loss: 2.9661149978637695
2025-08-04 07:01:47,377 - father_agent.py:386 - Step: 200, Training loss: 2.509674072265625
2025-08-04 07:01:47,669 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:01:47,671 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:01:58,260 - evaluation_results_class.py:131 - Average Return = -87.8469009399414
2025-08-04 07:01:58,260 - evaluation_results_class.py:133 - Average Virtual Goal Value = -8.493800163269043
2025-08-04 07:01:58,260 - evaluation_results_class.py:135 - Average Discounted Reward = -36.11245346069336
2025-08-04 07:01:58,260 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9919137466307277
2025-08-04 07:01:58,260 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 07:01:58,260 - evaluation_results_class.py:141 - Variance of Return = 27914.654296875
2025-08-04 07:01:58,260 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 07:01:58,260 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 07:01:58,260 - evaluation_results_class.py:147 - Average Episode Length = 138.533153638814
2025-08-04 07:01:58,260 - evaluation_results_class.py:149 - Counted Episodes = 1855
2025-08-04 07:01:58,546 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:01:58,559 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:02:02,791 - father_agent.py:386 - Step: 205, Training loss: 0.8634767532348633
2025-08-04 07:02:04,753 - father_agent.py:386 - Step: 210, Training loss: 2.5360844135284424
2025-08-04 07:02:06,654 - father_agent.py:386 - Step: 215, Training loss: 2.5316104888916016
2025-08-04 07:02:08,559 - father_agent.py:386 - Step: 220, Training loss: 3.1630496978759766
2025-08-04 07:02:10,499 - father_agent.py:386 - Step: 225, Training loss: 3.854671001434326
2025-08-04 07:02:12,540 - father_agent.py:386 - Step: 230, Training loss: 3.499530792236328
2025-08-04 07:02:14,475 - father_agent.py:386 - Step: 235, Training loss: 2.618825912475586
2025-08-04 07:02:16,425 - father_agent.py:386 - Step: 240, Training loss: 2.4043948650360107
2025-08-04 07:02:18,388 - father_agent.py:386 - Step: 245, Training loss: 2.3117246627807617
2025-08-04 07:02:20,321 - father_agent.py:386 - Step: 250, Training loss: 2.765873908996582
2025-08-04 07:02:22,259 - father_agent.py:386 - Step: 255, Training loss: 2.2340447902679443
2025-08-04 07:02:24,197 - father_agent.py:386 - Step: 260, Training loss: 2.4930267333984375
2025-08-04 07:02:26,137 - father_agent.py:386 - Step: 265, Training loss: 2.488159656524658
2025-08-04 07:02:28,056 - father_agent.py:386 - Step: 270, Training loss: 2.3693344593048096
2025-08-04 07:02:30,002 - father_agent.py:386 - Step: 275, Training loss: 2.2359378337860107
2025-08-04 07:02:31,962 - father_agent.py:386 - Step: 280, Training loss: 2.144805908203125
2025-08-04 07:02:33,904 - father_agent.py:386 - Step: 285, Training loss: 1.9610942602157593
2025-08-04 07:02:35,840 - father_agent.py:386 - Step: 290, Training loss: 2.056533098220825
2025-08-04 07:02:37,774 - father_agent.py:386 - Step: 295, Training loss: 2.778982162475586
2025-08-04 07:02:39,717 - father_agent.py:386 - Step: 300, Training loss: 3.027907371520996
2025-08-04 07:02:40,006 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:02:40,009 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:02:50,694 - evaluation_results_class.py:131 - Average Return = -142.94308471679688
2025-08-04 07:02:50,694 - evaluation_results_class.py:133 - Average Virtual Goal Value = -65.0446548461914
2025-08-04 07:02:50,694 - evaluation_results_class.py:135 - Average Discounted Reward = -73.44417572021484
2025-08-04 07:02:50,694 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9737302977232924
2025-08-04 07:02:50,694 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 07:02:50,694 - evaluation_results_class.py:141 - Variance of Return = 22850.0078125
2025-08-04 07:02:50,694 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 07:02:50,694 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 07:02:50,694 - evaluation_results_class.py:147 - Average Episode Length = 213.38353765323993
2025-08-04 07:02:50,694 - evaluation_results_class.py:149 - Counted Episodes = 1142
2025-08-04 07:02:50,991 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:02:51,005 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:02:55,192 - father_agent.py:386 - Step: 305, Training loss: 1.3080759048461914
2025-08-04 07:02:57,117 - father_agent.py:386 - Step: 310, Training loss: 2.29099440574646
2025-08-04 07:02:59,057 - father_agent.py:386 - Step: 315, Training loss: 2.3333160877227783
2025-08-04 07:03:00,989 - father_agent.py:386 - Step: 320, Training loss: 3.2946910858154297
2025-08-04 07:03:02,922 - father_agent.py:386 - Step: 325, Training loss: 2.689316511154175
2025-08-04 07:03:04,884 - father_agent.py:386 - Step: 330, Training loss: 2.4307198524475098
2025-08-04 07:03:06,843 - father_agent.py:386 - Step: 335, Training loss: 2.9846854209899902
2025-08-04 07:03:08,781 - father_agent.py:386 - Step: 340, Training loss: 3.2187583446502686
2025-08-04 07:03:10,745 - father_agent.py:386 - Step: 345, Training loss: 2.8011324405670166
2025-08-04 07:03:12,689 - father_agent.py:386 - Step: 350, Training loss: 3.0165982246398926
2025-08-04 07:03:14,637 - father_agent.py:386 - Step: 355, Training loss: 2.5040786266326904
2025-08-04 07:03:16,554 - father_agent.py:386 - Step: 360, Training loss: 3.2390849590301514
2025-08-04 07:03:18,474 - father_agent.py:386 - Step: 365, Training loss: 3.090437173843384
2025-08-04 07:03:20,415 - father_agent.py:386 - Step: 370, Training loss: 2.352808952331543
2025-08-04 07:03:22,351 - father_agent.py:386 - Step: 375, Training loss: 3.0927157402038574
2025-08-04 07:03:24,277 - father_agent.py:386 - Step: 380, Training loss: 2.110685348510742
2025-08-04 07:03:26,192 - father_agent.py:386 - Step: 385, Training loss: 2.775390863418579
2025-08-04 07:03:28,125 - father_agent.py:386 - Step: 390, Training loss: 2.816016912460327
2025-08-04 07:03:30,044 - father_agent.py:386 - Step: 395, Training loss: 3.1077094078063965
2025-08-04 07:03:31,986 - father_agent.py:386 - Step: 400, Training loss: 1.8677700757980347
2025-08-04 07:03:32,278 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:03:32,281 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:03:42,891 - evaluation_results_class.py:131 - Average Return = -135.79708862304688
2025-08-04 07:03:42,891 - evaluation_results_class.py:133 - Average Virtual Goal Value = -55.797088623046875
2025-08-04 07:03:42,891 - evaluation_results_class.py:135 - Average Discounted Reward = -64.2376708984375
2025-08-04 07:03:42,891 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 07:03:42,891 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 07:03:42,892 - evaluation_results_class.py:141 - Variance of Return = 37907.41796875
2025-08-04 07:03:42,892 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 07:03:42,892 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 07:03:42,892 - evaluation_results_class.py:147 - Average Episode Length = 138.75728155339806
2025-08-04 07:03:42,892 - evaluation_results_class.py:149 - Counted Episodes = 2060
2025-08-04 07:03:43,186 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:03:43,199 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:03:47,410 - father_agent.py:386 - Step: 405, Training loss: 1.5533994436264038
2025-08-04 07:03:49,356 - father_agent.py:386 - Step: 410, Training loss: 2.437347173690796
2025-08-04 07:03:51,324 - father_agent.py:386 - Step: 415, Training loss: 2.790024518966675
2025-08-04 07:03:53,316 - father_agent.py:386 - Step: 420, Training loss: 2.983447790145874
2025-08-04 07:03:55,294 - father_agent.py:386 - Step: 425, Training loss: 3.886817216873169
2025-08-04 07:03:57,267 - father_agent.py:386 - Step: 430, Training loss: 2.8926570415496826
2025-08-04 07:03:59,199 - father_agent.py:386 - Step: 435, Training loss: 2.5880510807037354
2025-08-04 07:04:01,148 - father_agent.py:386 - Step: 440, Training loss: 1.5721986293792725
2025-08-04 07:04:03,089 - father_agent.py:386 - Step: 445, Training loss: 2.001075267791748
2025-08-04 07:04:05,020 - father_agent.py:386 - Step: 450, Training loss: 2.9770925045013428
2025-08-04 07:04:06,968 - father_agent.py:386 - Step: 455, Training loss: 2.872246742248535
2025-08-04 07:04:08,930 - father_agent.py:386 - Step: 460, Training loss: 3.417011022567749
2025-08-04 07:04:10,867 - father_agent.py:386 - Step: 465, Training loss: 2.9542229175567627
2025-08-04 07:04:12,798 - father_agent.py:386 - Step: 470, Training loss: 2.0163748264312744
2025-08-04 07:04:14,738 - father_agent.py:386 - Step: 475, Training loss: 2.3175952434539795
2025-08-04 07:04:16,669 - father_agent.py:386 - Step: 480, Training loss: 3.2231316566467285
2025-08-04 07:04:18,594 - father_agent.py:386 - Step: 485, Training loss: 2.1598310470581055
2025-08-04 07:04:20,521 - father_agent.py:386 - Step: 490, Training loss: 3.2377519607543945
2025-08-04 07:04:22,468 - father_agent.py:386 - Step: 495, Training loss: 2.725789785385132
2025-08-04 07:04:24,395 - father_agent.py:386 - Step: 500, Training loss: 2.7520980834960938
2025-08-04 07:04:24,682 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:04:24,685 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:04:35,484 - evaluation_results_class.py:131 - Average Return = -192.35667419433594
2025-08-04 07:04:35,484 - evaluation_results_class.py:133 - Average Virtual Goal Value = -112.3566665649414
2025-08-04 07:04:35,484 - evaluation_results_class.py:135 - Average Discounted Reward = -83.0365982055664
2025-08-04 07:04:35,484 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 07:04:35,484 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 07:04:35,484 - evaluation_results_class.py:141 - Variance of Return = 172984.140625
2025-08-04 07:04:35,484 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 07:04:35,484 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 07:04:35,484 - evaluation_results_class.py:147 - Average Episode Length = 127.29153743907843
2025-08-04 07:04:35,484 - evaluation_results_class.py:149 - Counted Episodes = 2257
2025-08-04 07:04:35,778 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:04:35,790 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:04:35,910 - father_agent.py:547 - Training finished.
2025-08-04 07:04:36,072 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:04:36,075 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:04:36,077 - father_agent.py:654 - Evaluating agent with greedy masked policy.
2025-08-04 07:04:46,826 - evaluation_results_class.py:131 - Average Return = -183.63143920898438
2025-08-04 07:04:46,826 - evaluation_results_class.py:133 - Average Virtual Goal Value = -103.66757202148438
2025-08-04 07:04:46,826 - evaluation_results_class.py:135 - Average Discounted Reward = -81.09632110595703
2025-08-04 07:04:46,826 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9995483288166215
2025-08-04 07:04:46,826 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 07:04:46,826 - evaluation_results_class.py:141 - Variance of Return = 127122.140625
2025-08-04 07:04:46,826 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 07:04:46,826 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 07:04:46,826 - evaluation_results_class.py:147 - Average Episode Length = 129.34959349593495
2025-08-04 07:04:46,826 - evaluation_results_class.py:149 - Counted Episodes = 2214
2025-08-04 07:04:47,123 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:04:47,126 - self_interpretable_extractor.py:286 - True
2025-08-04 07:04:47,139 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:05:04,380 - evaluation_results_class.py:131 - Average Return = -210.47650146484375
2025-08-04 07:05:04,380 - evaluation_results_class.py:133 - Average Virtual Goal Value = -130.511962890625
2025-08-04 07:05:04,380 - evaluation_results_class.py:135 - Average Discounted Reward = -88.57559204101562
2025-08-04 07:05:04,380 - evaluation_results_class.py:137 - Goal Reach Probability = 0.9995567375886525
2025-08-04 07:05:04,380 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 07:05:04,380 - evaluation_results_class.py:141 - Variance of Return = 187535.171875
2025-08-04 07:05:04,380 - evaluation_results_class.py:143 - Current Best Return = -210.47650146484375
2025-08-04 07:05:04,380 - evaluation_results_class.py:145 - Current Best Reach Probability = 0.9995567375886525
2025-08-04 07:05:04,380 - evaluation_results_class.py:147 - Average Episode Length = 137.5336879432624
2025-08-04 07:05:04,380 - evaluation_results_class.py:149 - Counted Episodes = 2256
2025-08-04 07:05:04,381 - self_interpretable_extractor.py:295 - Sampling data with original policy
Buffer 0
2025-08-04 07:05:04,381 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 10629 trajectories
Learned trajectory lengths  {523, 529, 535, 541, 547, 553, 559, 49, 55, 571, 61, 577, 67, 73, 79, 595, 85, 601, 91, 97, 103, 619, 109, 625, 115, 631, 121, 637, 127, 133, 139, 651, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
Buffer 1
2025-08-04 07:06:15,999 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 21133 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 73, 589, 79, 595, 85, 601, 91, 607, 97, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 139, 651, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
Buffer 2
2025-08-04 07:07:27,842 - environment_wrapper_vec.py:518 - Resetting the environment.
Learned 31843 trajectories
Learned trajectory lengths  {517, 523, 529, 535, 541, 547, 553, 559, 49, 565, 55, 571, 61, 577, 67, 583, 73, 589, 79, 595, 85, 601, 91, 607, 97, 103, 619, 109, 625, 115, 631, 121, 637, 127, 643, 133, 649, 139, 651, 145, 151, 157, 163, 169, 175, 181, 187, 193, 199, 205, 211, 217, 223, 229, 235, 241, 247, 253, 259, 265, 271, 277, 283, 289, 295, 301, 307, 313, 319, 325, 331, 337, 343, 349, 355, 361, 367, 373, 379, 385, 391, 397, 403, 409, 415, 421, 427, 433, 439, 445, 451, 457, 463, 469, 475, 481, 487, 493, 499, 505, 511}
All trajectories collected
2025-08-04 07:08:43,002 - self_interpretable_extractor.py:346 - Data sampled
2025-08-04 07:08:43,002 - self_interpretable_extractor.py:349 - Learning FSC from original policy
Learning from 31843 trajectories
DOF: 0
DOF: 0
DOF: 0
DOF: 3
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
DOF: 0
Learned Chi2 SMM of size 2
Model saved to fsc_9.dot.
Learned FSC of size 2
2025-08-04 07:08:48,280 - environment_wrapper_vec.py:518 - Resetting the environment.
Action: Tensor("Squeeze:0", shape=(256,), dtype=int32), Update: Tensor("Reshape:0", shape=(256, 1), dtype=int32), Memory: Tensor("policy_state:0", shape=(256, 1), dtype=int32)
2025-08-04 07:09:04,711 - evaluation_results_class.py:131 - Average Return = -409.1049499511719
2025-08-04 07:09:04,711 - evaluation_results_class.py:133 - Average Virtual Goal Value = -329.1049499511719
2025-08-04 07:09:04,711 - evaluation_results_class.py:135 - Average Discounted Reward = -160.3438720703125
2025-08-04 07:09:04,711 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 07:09:04,711 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 07:09:04,711 - evaluation_results_class.py:141 - Variance of Return = 445298.75
2025-08-04 07:09:04,711 - evaluation_results_class.py:143 - Current Best Return = -409.1049499511719
2025-08-04 07:09:04,711 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 07:09:04,711 - evaluation_results_class.py:147 - Average Episode Length = 148.92225461613216
2025-08-04 07:09:04,711 - evaluation_results_class.py:149 - Counted Episodes = 2058
FSC Result: {'best_episode_return': -329.10495, 'best_return': -409.10495, 'goal_value': 0.0, 'returns_episodic': [-329.10495], 'returns': [-409.10495], 'reach_probs': [1.0], 'trap_reach_probs': [0.0], 'best_reach_prob': 1.0, 'losses': [], 'best_updated': True, 'each_episode_variance': [445298.75], 'each_episode_virtual_variance': [445298.75], 'combined_variance': [1781195.0], 'num_episodes': [2058], 'paynt_bounds': [], 'last_from_interpretation': False, 'extracted_fsc_episode_return': -1.0, 'extracted_fsc_return': -1.0, 'extracted_fsc_reach_prob': -1.0, 'extracted_fsc_variance': -1.0, 'extracted_fsc_num_episodes': -1, 'extracted_fsc_virtual_variance': -1.0, 'extracted_fsc_combined_variance': -1.0, 'artificial_reward_means': [], 'artificial_reward_stds': [], 'average_episode_length': [148.92225461613216], 'counted_episodes': [2058], 'discounted_rewards': [-160.34387], 'new_pomdp_iteration_numbers': []}
[[6, 7, 8, 9], [1], [2], [3], [4], [5], [1], [1], [2], [2], [3], [3], [4], [4], [5], [5], [6, 7, 8, 9], [6, 7, 8, 9], [1], [2], [3], [4], [5], [6, 7, 8, 9], [0]]
2025-08-04 07:09:04,851 - statistic.py:67 - synthesis initiated, design space: 11664
2025-08-04 07:09:04,851 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-08-04 07:09:04,888 - synthesizer_ar.py:122 - value 407.8674 achieved after 5563.77 seconds
2025-08-04 07:09:04,901 - synthesizer_ar.py:122 - value 407.874 achieved after 5563.78 seconds
2025-08-04 07:09:04,932 - synthesizer_ar.py:122 - value 415.2156 achieved after 5563.81 seconds
2025-08-04 07:09:04,945 - synthesizer_ar.py:122 - value 415.2223 achieved after 5563.83 seconds
2025-08-04 07:09:04,977 - synthesizer_ar.py:122 - value 497.7854 achieved after 5563.86 seconds
2025-08-04 07:09:04,991 - synthesizer_ar.py:122 - value 497.7921 achieved after 5563.87 seconds
2025-08-04 07:09:05,008 - synthesizer_ar.py:122 - value 505.1337 achieved after 5563.89 seconds
2025-08-04 07:09:05,021 - synthesizer_ar.py:122 - value 505.1403 achieved after 5563.9 seconds
2025-08-04 07:09:05,080 - synthesizer_ar.py:122 - value 508.0112 achieved after 5563.96 seconds
2025-08-04 07:09:05,094 - synthesizer_ar.py:122 - value 508.0179 achieved after 5563.97 seconds
2025-08-04 07:09:05,109 - synthesizer_ar.py:122 - value 515.3595 achieved after 5563.99 seconds
2025-08-04 07:09:05,122 - synthesizer_ar.py:122 - value 515.3661 achieved after 5564.0 seconds
2025-08-04 07:09:05,221 - synthesizer_ar.py:122 - value 548.1717 achieved after 5564.1 seconds
2025-08-04 07:09:05,238 - synthesizer_ar.py:122 - value 548.1784 achieved after 5564.12 seconds
2025-08-04 07:09:05,275 - synthesizer_ar.py:122 - value 558.3975 achieved after 5564.16 seconds
2025-08-04 07:09:05,288 - synthesizer_ar.py:122 - value 558.4042 achieved after 5564.17 seconds
2025-08-04 07:09:05,356 - synthesizer_ar.py:122 - value 558.4092 achieved after 5564.24 seconds
2025-08-04 07:09:05,392 - synthesizer.py:192 - printing synthesized assignment below:
2025-08-04 07:09:05,392 - synthesizer.py:193 - o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=3, o4y=8, o5x=8, o5y=5
2025-08-04 07:09:05,393 - synthesizer.py:198 - double-checking specification satisfiability:  : 558.4091997119147
--------------------
Synthesis summary:
optimality objective: R{"rewardmodel_penalty"}max=? [F ((x = 8) & (y = 8))] 

method: AR, synthesis time: 0.54 s
number of holes: 10, family size: 11664, quotient: 1232 states / 1448 actions
explored: 100 %
MDP stats: avg MDP size: 1021, iterations: 258

optimum: 558.4092
--------------------
2025-08-04 07:09:05,394 - robust_rl_trainer.py:454 - Extracted FSC for hole assignment: o1x=1, o1y=2, o2x=1, o2y=3, o3x=3, o3y=2, o4x=3, o4y=8, o5x=8, o5y=5
2025-08-04 07:09:05,405 - robust_rl_trainer.py:432 - Iteration 11 of pure RL loop
2025-08-04 07:09:05,444 - storm_vec_env.py:70 - Computing row map
2025-08-04 07:09:05,451 - storm_vec_env.py:97 - Computing transitions
2025-08-04 07:09:05,467 - storm_vec_env.py:100 - Computing allowed actions
2025-08-04 07:09:05,467 - storm_vec_env.py:114 - Computing sinks
2025-08-04 07:09:05,467 - storm_vec_env.py:119 - Computing raw rewards
2025-08-04 07:09:05,472 - storm_vec_env.py:143 - Computing labels
2025-08-04 07:09:05,472 - storm_vec_env.py:152 - Computing scalarized rewards
2025-08-04 07:09:05,472 - storm_vec_env.py:161 - Computing metalabels
2025-08-04 07:09:05,472 - storm_vec_env.py:175 - Computing observations
2025-08-04 07:09:05,652 - father_agent.py:538 - Training agent with replay buffer option: 1
2025-08-04 07:09:05,652 - father_agent.py:540 - Before training evaluation.
2025-08-04 07:09:05,838 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:09:05,841 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:09:16,584 - evaluation_results_class.py:131 - Average Return = -234.4906768798828
2025-08-04 07:09:16,584 - evaluation_results_class.py:133 - Average Virtual Goal Value = -154.4906768798828
2025-08-04 07:09:16,584 - evaluation_results_class.py:135 - Average Discounted Reward = -137.6419677734375
2025-08-04 07:09:16,584 - evaluation_results_class.py:137 - Goal Reach Probability = 1.0
2025-08-04 07:09:16,584 - evaluation_results_class.py:139 - Trap Reach Probability = 0.0
2025-08-04 07:09:16,584 - evaluation_results_class.py:141 - Variance of Return = 100613.140625
2025-08-04 07:09:16,584 - evaluation_results_class.py:143 - Current Best Return = -37.25813674926758
2025-08-04 07:09:16,585 - evaluation_results_class.py:145 - Current Best Reach Probability = 1.0
2025-08-04 07:09:16,585 - evaluation_results_class.py:147 - Average Episode Length = 130.49704411095954
2025-08-04 07:09:16,585 - evaluation_results_class.py:149 - Counted Episodes = 2199
2025-08-04 07:09:16,870 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:09:16,883 - environment_wrapper_vec.py:518 - Resetting the environment.
2025-08-04 07:09:17,000 - father_agent.py:436 - Training agent on-policy
2025-08-04 07:09:25,629 - father_agent.py:386 - Step: 0, Training loss: 4.628857612609863
2025-08-04 07:09:27,590 - father_agent.py:386 - Step: 5, Training loss: 4.373992443084717
2025-08-04 07:09:29,569 - father_agent.py:386 - Step: 10, Training loss: 2.5734314918518066
2025-08-04 07:09:31,543 - father_agent.py:386 - Step: 15, Training loss: 2.9826693534851074
2025-08-04 07:09:33,497 - father_agent.py:386 - Step: 20, Training loss: 2.4946017265319824
2025-08-04 07:09:35,435 - father_agent.py:386 - Step: 25, Training loss: 2.0261313915252686
2025-08-04 07:09:37,372 - father_agent.py:386 - Step: 30, Training loss: 2.4965412616729736
2025-08-04 07:09:39,304 - father_agent.py:386 - Step: 35, Training loss: 2.944085121154785
2025-08-04 07:09:41,250 - father_agent.py:386 - Step: 40, Training loss: 2.3592748641967773
